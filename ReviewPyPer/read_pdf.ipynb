{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.10.2-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m981.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pypdfium2>=4.18.0\n",
      "  Downloading pypdfium2-4.19.0-py3-none-macosx_10_13_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /Users/cu135/fsl/lib/python3.10/site-packages (from pdfplumber) (9.2.0)\n",
      "Collecting pdfminer.six==20221105\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /Users/cu135/fsl/lib/python3.10/site-packages (from pdfminer.six==20221105->pdfplumber) (2.1.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/cu135/fsl/lib/python3.10/site-packages (from pdfminer.six==20221105->pdfplumber) (38.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/cu135/fsl/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/cu135/fsl/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
      "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "\u001b[33m  WARNING: The script pypdfium2 is installed in '/Users/cu135/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pdfplumber is installed in '/Users/cu135/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pdfminer.six-20221105 pdfplumber-0.10.2 pypdfium2-4.19.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fitz\n",
      "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pyxnat\n",
      "  Downloading pyxnat-1.6-py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/cu135/fsl/lib/python3.10/site-packages (from fitz) (1.21.6)\n",
      "Requirement already satisfied: pandas in /Users/cu135/fsl/lib/python3.10/site-packages (from fitz) (1.5.2)\n",
      "Collecting nipype\n",
      "  Downloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: configobj in /Users/cu135/fsl/lib/python3.10/site-packages (from fitz) (5.0.6)\n",
      "Requirement already satisfied: scipy in /Users/cu135/fsl/lib/python3.10/site-packages (from fitz) (1.9.3)\n",
      "Collecting configparser\n",
      "  Downloading configparser-6.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting httplib2\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nibabel in /Users/cu135/fsl/lib/python3.10/site-packages (from fitz) (4.0.2)\n",
      "Requirement already satisfied: six in /Users/cu135/fsl/lib/python3.10/site-packages (from configobj->fitz) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/cu135/fsl/lib/python3.10/site-packages (from httplib2->fitz) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /Users/cu135/fsl/lib/python3.10/site-packages (from nibabel->fitz) (65.5.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/cu135/fsl/lib/python3.10/site-packages (from nibabel->fitz) (22.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/cu135/fsl/lib/python3.10/site-packages (from nipype->fitz) (2.8.8)\n",
      "Collecting etelemetry>=0.2.0\n",
      "  Downloading etelemetry-0.3.0-py3-none-any.whl (6.3 kB)\n",
      "Requirement already satisfied: click>=6.6.0 in /Users/cu135/fsl/lib/python3.10/site-packages (from nipype->fitz) (8.1.3)\n",
      "Collecting looseversion\n",
      "  Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /Users/cu135/fsl/lib/python3.10/site-packages (from nipype->fitz) (2.8.2)\n",
      "Collecting filelock>=3.0.0\n",
      "  Downloading filelock-3.12.3-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /Users/cu135/fsl/lib/python3.10/site-packages (from nipype->fitz) (1.4.2)\n",
      "Collecting simplejson>=3.8.0\n",
      "  Downloading simplejson-3.19.1-cp310-cp310-macosx_10_9_x86_64.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting traits!=5.0,<6.4,>=4.6\n",
      "  Downloading traits-6.3.2-cp310-cp310-macosx_10_9_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: prov>=1.5.2 in /Users/cu135/fsl/lib/python3.10/site-packages (from nipype->fitz) (2.0.0)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /Users/cu135/fsl/lib/python3.10/site-packages (from nipype->fitz) (6.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/cu135/fsl/lib/python3.10/site-packages (from pandas->fitz) (2022.6)\n",
      "Requirement already satisfied: future>=0.16 in /Users/cu135/fsl/lib/python3.10/site-packages (from pyxnat->fitz) (0.18.2)\n",
      "Requirement already satisfied: lxml>=4.3 in /Users/cu135/fsl/lib/python3.10/site-packages (from pyxnat->fitz) (4.9.2)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/cu135/fsl/lib/python3.10/site-packages (from pyxnat->fitz) (2.28.1)\n",
      "Collecting pathlib>=1.0\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Collecting ci-info>=0.2\n",
      "  Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting typing-extensions>=4.7.1\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: isodate in /Users/cu135/fsl/lib/python3.10/site-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/cu135/fsl/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cu135/fsl/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/cu135/fsl/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cu135/fsl/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (2022.9.24)\n",
      "Installing collected packages: pathlib, looseversion, typing-extensions, traits, simplejson, httplib2, configparser, ci-info, pyxnat, filelock, etelemetry, nipype, fitz\n",
      "\u001b[33m  WARNING: The script nipypecli is installed in '/Users/cu135/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed ci-info-0.3.0 configparser-6.0.0 etelemetry-0.3.0 filelock-3.12.3 fitz-0.0.1.dev2 httplib2-0.22.0 looseversion-1.3.0 nipype-1.8.6 pathlib-1.0.1 pyxnat-1.6 simplejson-3.19.1 traits-6.3.2 typing-extensions-4.7.1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'frontend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpdfplumber\u001b[39;00m \n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install fitz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfitz\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      8\u001b[0m pdf_dir \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mcalvin.howard\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOneDrive\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOneDrive_Documents\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mResearch\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m_memory_and_ad_dbs\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLiterature\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdistinct_ad_atrophy_resultds_in_distinct_impairment.pdf\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fitz/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfrontend\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtools\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mop\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'frontend'"
     ]
    }
   ],
   "source": [
    "# from PDFDataExtractor.pdfdataextractor import Reader\n",
    "!pip install pdfplumber\n",
    "import pdfplumber \n",
    "!pip install fitz\n",
    "import fitz\n",
    "\n",
    "import re\n",
    "pdf_dir = r'C:\\Users\\calvin.howard\\OneDrive\\OneDrive_Documents\\Research\\_memory_and_ad_dbs\\Literature\\distinct_ad_atrophy_resultds_in_distinct_impairment.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acceptable Functions\n",
    "def extract_without_page_headers_and_footers(pdf_file, margin_top=100, margin_bottom=100):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            words = page.extract_words()\n",
    "            for word in words:\n",
    "                if margin_top < word['top'] < (page.height - margin_bottom):\n",
    "                    text += word['text'] + ' '\n",
    "            text += \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_without_page_margins(pdf_file, margin_left=50, margin_top=50, margin_right=550, margin_bottom=750):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_width = page.width\n",
    "            page_height = page.height\n",
    "            if margin_right is None:\n",
    "                margin_right = page_width\n",
    "            if margin_bottom is None:\n",
    "                margin_bottom = page_height\n",
    "            words = page.crop((margin_left, margin_top, margin_right, margin_bottom)).extract_words()\n",
    "            for word in words:\n",
    "                text += word['text'] + ' '\n",
    "            text += \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_reference_sections(text):\n",
    "    # Regular expression pattern to match common reference section headers\n",
    "    pattern = r'\\b(?:references|citations|bibliography)\\b'\n",
    "    \n",
    "    # Find the reference section\n",
    "    reference_section_start = re.search(pattern, text)\n",
    "    \n",
    "    # If a reference section is found, remove it and the text after it\n",
    "    if reference_section_start:\n",
    "        print('found it')\n",
    "        text = text[:reference_section_start.start()]\n",
    "    return text\n",
    "\n",
    "def add_newlines_around_all_capitalized_words(text):\n",
    "    pattern = r'\\b([A-Z]{5,}(?:\\s+[A-Z]{5,})*)\\b'\n",
    "    replacement = r'\\n\\1\\n'\n",
    "    formatted_text = re.sub(pattern, replacement, text)\n",
    "    return formatted_text\n",
    "\n",
    "def detect_columns(page):\n",
    "    # Extract words from the page\n",
    "    words = page.extract_words()\n",
    "\n",
    "    # Group words by their X coordinate\n",
    "    word_groups = {}\n",
    "    for word in words:\n",
    "        x = round(word[\"x0\"])\n",
    "        if x not in word_groups:\n",
    "            word_groups[x] = []\n",
    "        word_groups[x].append(word)\n",
    "\n",
    "    # Sort the groups by X coordinate\n",
    "    sorted_word_groups = sorted(word_groups.items(), key=lambda x: x[0])\n",
    "\n",
    "    # Compute the spacing between groups\n",
    "    spacings = []\n",
    "    for i in range(len(sorted_word_groups) - 1):\n",
    "        spacing = sorted_word_groups[i + 1][0] - sorted_word_groups[i][0]\n",
    "        spacings.append(spacing)\n",
    "\n",
    "    # Compute the median spacing\n",
    "    median_spacing = sorted(spacings)[len(spacings) // 2]\n",
    "\n",
    "    # Group columns based on the spacing\n",
    "    columns = []\n",
    "    current_column = []\n",
    "    for i in range(len(sorted_word_groups)):\n",
    "        current_column.append(sorted_word_groups[i][1])\n",
    "        if i == len(sorted_word_groups) - 1 or sorted_word_groups[i + 1][0] - sorted_word_groups[i][0] > median_spacing:\n",
    "            columns.append(current_column)\n",
    "            current_column = []\n",
    "\n",
    "    return columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pdfplumber\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Add space after punctuation\n",
    "    text = re.sub(r'([,.:;])', r'\\1 ', text)\n",
    "\n",
    "    # Add space before and after parentheses\n",
    "    text = re.sub(r'([(])', r' \\1', text)\n",
    "    text = re.sub(r'([)])', r'\\1 ', text)\n",
    "\n",
    "    # Add space before and after hyphens, but not within words containing hyphens\n",
    "    text = re.sub(r'(?<![a-zA-Z0-9-])-(?![a-zA-Z0-9-])', r' - ', text)\n",
    "\n",
    "    # Remove unnecessary hyphenation at line breaks\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_without_page_margins(pdf_file, n_columns, margin_left=50, margin_top=100, margin_right=550, margin_bottom=725, space_threshold=0.00000001):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_width = page.width\n",
    "            page_height = page.height\n",
    "            if margin_right is None:\n",
    "                margin_right = page_width\n",
    "            if margin_bottom is None:\n",
    "                margin_bottom = page_height\n",
    "            cropped_page = page.crop((margin_left, margin_top, margin_right, margin_bottom))\n",
    "\n",
    "            column_width = (margin_right - margin_left) / n_columns\n",
    "            for idx in range(n_columns):\n",
    "                col_left = margin_left + idx * column_width\n",
    "                col_right = col_left + column_width\n",
    "                cropped_col = cropped_page.crop((col_left, margin_top, col_right, margin_bottom))\n",
    "                col_words = cropped_col.extract_words()\n",
    "                lines = defaultdict(list)\n",
    "                for word in col_words:\n",
    "                    line_key = round(word['top'], 2)\n",
    "                    lines[line_key].append(word)\n",
    "\n",
    "                sorted_lines = sorted(lines.items(), key=lambda x: x[0], reverse=True)\n",
    "                text_lines = []\n",
    "                for line in sorted_lines:\n",
    "                    sorted_words = sorted(line[1], key=lambda x: x['x0'])\n",
    "                    line_text = sorted_words[0]['text']\n",
    "                    for i in range(1, len(sorted_words)):\n",
    "                        prev_word = sorted_words[i - 1]\n",
    "                        curr_word = sorted_words[i]\n",
    "                        if curr_word['x0'] - prev_word['x1'] > space_threshold:\n",
    "                            line_text += ' '\n",
    "                        line_text += curr_word['text']\n",
    "                    text_lines.append(line_text)\n",
    "\n",
    "                text += ' '.join(text_lines) + '\\n'\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pdf_file = pdf_dir\n",
    "extracted_text = extract_without_page_margins(pdf_file, n_columns=2)\n",
    "formatted_text = add_newlines_around_all_capitalized_words(extracted_text)\n",
    "formatted_text = formatted_text.lower()\n",
    "formatted_text = remove_reference_sections(formatted_text)\n",
    "formatted_text = preprocess_text(formatted_text)\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_without_page_margins() missing 1 required positional argument: 'n_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23488\\905509571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Works nicely, fails to detect columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpdf_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdf_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mextracted_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_without_page_margins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mformatted_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_newlines_around_all_capitalized_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextracted_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mformatted_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatted_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: extract_without_page_margins() missing 1 required positional argument: 'n_columns'"
     ]
    }
   ],
   "source": [
    "#Works nicely, fails to detect columns\n",
    "pdf_file = pdf_dir\n",
    "extracted_text = extract_without_page_margins(pdf_file)\n",
    "formatted_text = add_newlines_around_all_capitalized_words(extracted_text)\n",
    "formatted_text = formatted_text.lower()\n",
    "formatted_text = remove_reference_sections(formatted_text)\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 121\u001b[0m\n\u001b[1;32m    115\u001b[0m             current_column \u001b[39m=\u001b[39m []\n\u001b[1;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m columns\n\u001b[0;32m--> 121\u001b[0m pdf_file \u001b[39m=\u001b[39m pdf_dir\n\u001b[1;32m    122\u001b[0m extracted_text \u001b[39m=\u001b[39m extract_without_page_margins(pdf_file, n_columns\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    123\u001b[0m formatted_text \u001b[39m=\u001b[39m full_preprocess(extracted_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing updates\n",
    "#----------------------------------------------------------------PREPROCESSING \n",
    "def preprocess_text(text):\n",
    "    # Add space after punctuation\n",
    "    text = re.sub(r'([,.:;])', r'\\1 ', text)\n",
    "\n",
    "    # Add space before and after parentheses\n",
    "    text = re.sub(r'([(])', r' \\1', text)\n",
    "    text = re.sub(r'([)])', r'\\1 ', text)\n",
    "\n",
    "    # Add space before and after hyphens, but not within words containing hyphens\n",
    "    text = re.sub(r'(?<![a-zA-Z0-9-])-(?![a-zA-Z0-9-])', r' - ', text)\n",
    "\n",
    "    # Remove unnecessary hyphenation at line breaks\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "def remove_email_addresses(text):\n",
    "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "    return email_pattern.sub('', text)\n",
    "\n",
    "def full_preprocess(text):\n",
    "    text = add_newlines_around_all_capitalized_words(text)\n",
    "    text = text.lower()\n",
    "    # text = remove_reference_sections(text)\n",
    "    text = preprocess_text(text)\n",
    "    # text = remove_urls(text)\n",
    "    # text = remove_email_addresses(text)\n",
    "    return text\n",
    "\n",
    "##---------------------------------------------------------------- EXTRACTION\n",
    "def extract_without_page_margins_v2(pdf_file, margin_left=0, margin_top=0, margin_right=None, margin_bottom=None, space_threshold=0.00000001):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_width = page.width\n",
    "            page_height = page.height\n",
    "            if margin_right is None:\n",
    "                margin_right = page_width\n",
    "            if margin_bottom is None:\n",
    "                margin_bottom = page_height\n",
    "            cropped_page = page.crop((margin_left, margin_top, margin_right, margin_bottom))\n",
    "\n",
    "            columns = detect_columns(cropped_page)\n",
    "            for column_words in columns:\n",
    "                col_words = [word for group in column_words for word in group]\n",
    "                lines = {}\n",
    "                for word in col_words:\n",
    "                    line_key = round(word['top'], 2)\n",
    "                    if line_key not in lines:\n",
    "                        lines[line_key] = []\n",
    "                    lines[line_key].append(word)\n",
    "\n",
    "                sorted_lines = sorted(lines.items(), key=lambda x: x[0], reverse=True)\n",
    "                text_lines = []\n",
    "                for line in sorted_lines:\n",
    "                    sorted_words = sorted(line[1], key=lambda x: x['x0'])\n",
    "                    line_text = sorted_words[0]['text']\n",
    "                    for i in range(1, len(sorted_words)):\n",
    "                        prev_word = sorted_words[i - 1]\n",
    "                        curr_word = sorted_words[i]\n",
    "                        if curr_word['x0'] - prev_word['x1'] > space_threshold:\n",
    "                            line_text += ' '\n",
    "                        line_text += curr_word['text']\n",
    "                    text_lines.append(line_text)\n",
    "\n",
    "                text += '\\n'.join(text_lines) + '\\n'\n",
    "    return text\n",
    "\n",
    "# The rest of your code remains the same\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_columns(page):\n",
    "    # Extract words from the page\n",
    "    words = page.extract_words()\n",
    "\n",
    "    # Group words by their X coordinate\n",
    "    word_groups = {}\n",
    "    for word in words:\n",
    "        x = round(word[\"x0\"])\n",
    "        if x not in word_groups:\n",
    "            word_groups[x] = []\n",
    "        word_groups[x].append(word)\n",
    "\n",
    "    # Sort the groups by X coordinate\n",
    "    sorted_word_groups = sorted(word_groups.items(), key=lambda x: x[0])\n",
    "\n",
    "    # Compute the spacing between groups\n",
    "    spacings = []\n",
    "    for i in range(len(sorted_word_groups) - 1):\n",
    "        spacing = sorted_word_groups[i + 1][0] - sorted_word_groups[i][0]\n",
    "        spacings.append(spacing)\n",
    "\n",
    "    # Compute the median spacing\n",
    "    median_spacing = sorted(spacings)[len(spacings) // 2]\n",
    "\n",
    "    # Group columns based on the spacing\n",
    "    columns = []\n",
    "    current_column = []\n",
    "    for i in range(len(sorted_word_groups)):\n",
    "        current_column.append(sorted_word_groups[i][1])\n",
    "        if i == len(sorted_word_groups) - 1 or sorted_word_groups[i + 1][0] - sorted_word_groups[i][0] > median_spacing:\n",
    "            columns.append(current_column)\n",
    "            current_column = []\n",
    "\n",
    "    return columns\n",
    "\n",
    "\n",
    "\n",
    "pdf_file = pdf_dir\n",
    "extracted_text = extract_without_page_margins(pdf_file, n_columns=2)\n",
    "formatted_text = full_preprocess(extracted_text)\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(formatted_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt with textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textract in /Users/cu135/.local/lib/python3.10/site-packages (1.6.5)\n",
      "Requirement already satisfied: extract-msg<=0.29.* in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (0.28.7)\n",
      "Requirement already satisfied: SpeechRecognition~=3.8.1 in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (3.8.1)\n",
      "Requirement already satisfied: xlrd~=1.2.0 in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (1.2.0)\n",
      "Requirement already satisfied: python-pptx~=0.6.18 in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (0.6.22)\n",
      "Requirement already satisfied: chardet==3.* in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (3.0.4)\n",
      "Requirement already satisfied: six~=1.12.0 in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (1.12.0)\n",
      "Requirement already satisfied: argcomplete~=1.10.0 in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (1.10.3)\n",
      "Requirement already satisfied: pdfminer.six==20191110 in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (20191110)\n",
      "Requirement already satisfied: docx2txt~=0.8 in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (0.8)\n",
      "Requirement already satisfied: beautifulsoup4~=4.8.0 in /Users/cu135/.local/lib/python3.10/site-packages (from textract) (4.8.2)\n",
      "Requirement already satisfied: pycryptodome in /Users/cu135/.local/lib/python3.10/site-packages (from pdfminer.six==20191110->textract) (3.18.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/cu135/fsl/lib/python3.10/site-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Users/cu135/fsl/lib/python3.10/site-packages (from beautifulsoup4~=4.8.0->textract) (2.3.2.post1)\n",
      "Requirement already satisfied: imapclient==2.1.0 in /Users/cu135/.local/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (2.1.0)\n",
      "Requirement already satisfied: tzlocal>=2.1 in /Users/cu135/.local/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (5.0.1)\n",
      "Requirement already satisfied: compressed-rtf>=1.0.6 in /Users/cu135/.local/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (1.0.6)\n",
      "Requirement already satisfied: olefile>=0.46 in /Users/cu135/.local/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (0.46)\n",
      "Requirement already satisfied: ebcdic>=1.1.1 in /Users/cu135/.local/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (1.1.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/cu135/fsl/lib/python3.10/site-packages (from python-pptx~=0.6.18->textract) (4.9.2)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/cu135/.local/lib/python3.10/site-packages (from python-pptx~=0.6.18->textract) (3.1.2)\n",
      "Requirement already satisfied: Pillow<=9.5.0,>=3.3.2 in /Users/cu135/fsl/lib/python3.10/site-packages (from python-pptx~=0.6.18->textract) (9.2.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textract\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = textract.process(file_path)\n",
    "    return text.decode(\"utf-8\")\n",
    "\n",
    "def save_text_to_file(text, output_file_path):\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(text)\n",
    "\n",
    "def extract_text_from_pdf_dir(pdf_dir, output_dir):\n",
    "    for file_name in os.listdir(pdf_dir):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            input_file_path = os.path.join(pdf_dir, file_name)\n",
    "            output_file_path = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}.txt\")\n",
    "            text = extract_text_from_pdf(input_file_path)\n",
    "            save_text_to_file(text, output_file_path)\n",
    "\n",
    "pdf_dir = r'/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/Software/Research/nimlab/gpt_document_reader/writes/test'\n",
    "output_dir = r\"/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/Software/Research/nimlab/gpt_document_reader/writes\"\n",
    "extract_text_from_pdf_dir(pdf_dir, output_dir)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out tesseract. Also seems extremely powerful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.7.7_venv_ACE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "722d89dc901e449868ad4bf6f33e42f6d1e0a5c74d296773dce51109f1315592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
