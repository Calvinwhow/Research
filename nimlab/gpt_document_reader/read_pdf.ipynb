{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PDFDataExtractor.pdfdataextractor import Reader\n",
    "import pdfplumber \n",
    "import fitz\n",
    "\n",
    "import re\n",
    "pdf_dir = r'C:\\Users\\calvin.howard\\OneDrive\\OneDrive_Documents\\Research\\_memory_and_ad_dbs\\Literature\\distinct_ad_atrophy_resultds_in_distinct_impairment.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acceptable Functions\n",
    "def extract_without_page_headers_and_footers(pdf_file, margin_top=100, margin_bottom=100):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            words = page.extract_words()\n",
    "            for word in words:\n",
    "                if margin_top < word['top'] < (page.height - margin_bottom):\n",
    "                    text += word['text'] + ' '\n",
    "            text += \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_without_page_margins(pdf_file, margin_left=50, margin_top=50, margin_right=550, margin_bottom=750):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_width = page.width\n",
    "            page_height = page.height\n",
    "            if margin_right is None:\n",
    "                margin_right = page_width\n",
    "            if margin_bottom is None:\n",
    "                margin_bottom = page_height\n",
    "            words = page.crop((margin_left, margin_top, margin_right, margin_bottom)).extract_words()\n",
    "            for word in words:\n",
    "                text += word['text'] + ' '\n",
    "            text += \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_reference_sections(text):\n",
    "    # Regular expression pattern to match common reference section headers\n",
    "    pattern = r'\\b(?:references|citations|bibliography)\\b'\n",
    "    \n",
    "    # Find the reference section\n",
    "    reference_section_start = re.search(pattern, text)\n",
    "    \n",
    "    # If a reference section is found, remove it and the text after it\n",
    "    if reference_section_start:\n",
    "        print('found it')\n",
    "        text = text[:reference_section_start.start()]\n",
    "    return text\n",
    "\n",
    "def add_newlines_around_all_capitalized_words(text):\n",
    "    pattern = r'\\b([A-Z]{5,}(?:\\s+[A-Z]{5,})*)\\b'\n",
    "    replacement = r'\\n\\1\\n'\n",
    "    formatted_text = re.sub(pattern, replacement, text)\n",
    "    return formatted_text\n",
    "\n",
    "def detect_columns(page):\n",
    "    # Extract words from the page\n",
    "    words = page.extract_words()\n",
    "\n",
    "    # Group words by their X coordinate\n",
    "    word_groups = {}\n",
    "    for word in words:\n",
    "        x = round(word[\"x0\"])\n",
    "        if x not in word_groups:\n",
    "            word_groups[x] = []\n",
    "        word_groups[x].append(word)\n",
    "\n",
    "    # Sort the groups by X coordinate\n",
    "    sorted_word_groups = sorted(word_groups.items(), key=lambda x: x[0])\n",
    "\n",
    "    # Compute the spacing between groups\n",
    "    spacings = []\n",
    "    for i in range(len(sorted_word_groups) - 1):\n",
    "        spacing = sorted_word_groups[i + 1][0] - sorted_word_groups[i][0]\n",
    "        spacings.append(spacing)\n",
    "\n",
    "    # Compute the median spacing\n",
    "    median_spacing = sorted(spacings)[len(spacings) // 2]\n",
    "\n",
    "    # Group columns based on the spacing\n",
    "    columns = []\n",
    "    current_column = []\n",
    "    for i in range(len(sorted_word_groups)):\n",
    "        current_column.append(sorted_word_groups[i][1])\n",
    "        if i == len(sorted_word_groups) - 1 or sorted_word_groups[i + 1][0] - sorted_word_groups[i][0] > median_spacing:\n",
    "            columns.append(current_column)\n",
    "            current_column = []\n",
    "\n",
    "    return columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pdfplumber\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Add space after punctuation\n",
    "    text = re.sub(r'([,.:;])', r'\\1 ', text)\n",
    "\n",
    "    # Add space before and after parentheses\n",
    "    text = re.sub(r'([(])', r' \\1', text)\n",
    "    text = re.sub(r'([)])', r'\\1 ', text)\n",
    "\n",
    "    # Add space before and after hyphens, but not within words containing hyphens\n",
    "    text = re.sub(r'(?<![a-zA-Z0-9-])-(?![a-zA-Z0-9-])', r' - ', text)\n",
    "\n",
    "    # Remove unnecessary hyphenation at line breaks\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_without_page_margins(pdf_file, n_columns, margin_left=50, margin_top=100, margin_right=550, margin_bottom=725, space_threshold=0.00000001):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_width = page.width\n",
    "            page_height = page.height\n",
    "            if margin_right is None:\n",
    "                margin_right = page_width\n",
    "            if margin_bottom is None:\n",
    "                margin_bottom = page_height\n",
    "            cropped_page = page.crop((margin_left, margin_top, margin_right, margin_bottom))\n",
    "\n",
    "            column_width = (margin_right - margin_left) / n_columns\n",
    "            for idx in range(n_columns):\n",
    "                col_left = margin_left + idx * column_width\n",
    "                col_right = col_left + column_width\n",
    "                cropped_col = cropped_page.crop((col_left, margin_top, col_right, margin_bottom))\n",
    "                col_words = cropped_col.extract_words()\n",
    "                lines = defaultdict(list)\n",
    "                for word in col_words:\n",
    "                    line_key = round(word['top'], 2)\n",
    "                    lines[line_key].append(word)\n",
    "\n",
    "                sorted_lines = sorted(lines.items(), key=lambda x: x[0], reverse=True)\n",
    "                text_lines = []\n",
    "                for line in sorted_lines:\n",
    "                    sorted_words = sorted(line[1], key=lambda x: x['x0'])\n",
    "                    line_text = sorted_words[0]['text']\n",
    "                    for i in range(1, len(sorted_words)):\n",
    "                        prev_word = sorted_words[i - 1]\n",
    "                        curr_word = sorted_words[i]\n",
    "                        if curr_word['x0'] - prev_word['x1'] > space_threshold:\n",
    "                            line_text += ' '\n",
    "                        line_text += curr_word['text']\n",
    "                    text_lines.append(line_text)\n",
    "\n",
    "                text += ' '.join(text_lines) + '\\n'\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pdf_file = pdf_dir\n",
    "extracted_text = extract_without_page_margins(pdf_file, n_columns=2)\n",
    "formatted_text = add_newlines_around_all_capitalized_words(extracted_text)\n",
    "formatted_text = formatted_text.lower()\n",
    "formatted_text = remove_reference_sections(formatted_text)\n",
    "formatted_text = preprocess_text(formatted_text)\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_without_page_margins() missing 1 required positional argument: 'n_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23488\\905509571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Works nicely, fails to detect columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpdf_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdf_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mextracted_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_without_page_margins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mformatted_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_newlines_around_all_capitalized_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextracted_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mformatted_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatted_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: extract_without_page_margins() missing 1 required positional argument: 'n_columns'"
     ]
    }
   ],
   "source": [
    "#Works nicely, fails to detect columns\n",
    "pdf_file = pdf_dir\n",
    "extracted_text = extract_without_page_margins(pdf_file)\n",
    "formatted_text = add_newlines_around_all_capitalized_words(extracted_text)\n",
    "formatted_text = formatted_text.lower()\n",
    "formatted_text = remove_reference_sections(formatted_text)\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing updates\n",
    "#----------------------------------------------------------------PREPROCESSING \n",
    "def preprocess_text(text):\n",
    "    # Add space after punctuation\n",
    "    text = re.sub(r'([,.:;])', r'\\1 ', text)\n",
    "\n",
    "    # Add space before and after parentheses\n",
    "    text = re.sub(r'([(])', r' \\1', text)\n",
    "    text = re.sub(r'([)])', r'\\1 ', text)\n",
    "\n",
    "    # Add space before and after hyphens, but not within words containing hyphens\n",
    "    text = re.sub(r'(?<![a-zA-Z0-9-])-(?![a-zA-Z0-9-])', r' - ', text)\n",
    "\n",
    "    # Remove unnecessary hyphenation at line breaks\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "def remove_email_addresses(text):\n",
    "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "    return email_pattern.sub('', text)\n",
    "\n",
    "def full_preprocess(text):\n",
    "    text = add_newlines_around_all_capitalized_words(text)\n",
    "    text = text.lower()\n",
    "    # text = remove_reference_sections(text)\n",
    "    text = preprocess_text(text)\n",
    "    # text = remove_urls(text)\n",
    "    # text = remove_email_addresses(text)\n",
    "    return text\n",
    "\n",
    "##---------------------------------------------------------------- EXTRACTION\n",
    "def extract_without_page_margins_v2(pdf_file, margin_left=0, margin_top=0, margin_right=None, margin_bottom=None, space_threshold=0.00000001):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_width = page.width\n",
    "            page_height = page.height\n",
    "            if margin_right is None:\n",
    "                margin_right = page_width\n",
    "            if margin_bottom is None:\n",
    "                margin_bottom = page_height\n",
    "            cropped_page = page.crop((margin_left, margin_top, margin_right, margin_bottom))\n",
    "\n",
    "            columns = detect_columns(cropped_page)\n",
    "            for column_words in columns:\n",
    "                col_words = [word for group in column_words for word in group]\n",
    "                lines = {}\n",
    "                for word in col_words:\n",
    "                    line_key = round(word['top'], 2)\n",
    "                    if line_key not in lines:\n",
    "                        lines[line_key] = []\n",
    "                    lines[line_key].append(word)\n",
    "\n",
    "                sorted_lines = sorted(lines.items(), key=lambda x: x[0], reverse=True)\n",
    "                text_lines = []\n",
    "                for line in sorted_lines:\n",
    "                    sorted_words = sorted(line[1], key=lambda x: x['x0'])\n",
    "                    line_text = sorted_words[0]['text']\n",
    "                    for i in range(1, len(sorted_words)):\n",
    "                        prev_word = sorted_words[i - 1]\n",
    "                        curr_word = sorted_words[i]\n",
    "                        if curr_word['x0'] - prev_word['x1'] > space_threshold:\n",
    "                            line_text += ' '\n",
    "                        line_text += curr_word['text']\n",
    "                    text_lines.append(line_text)\n",
    "\n",
    "                text += '\\n'.join(text_lines) + '\\n'\n",
    "    return text\n",
    "\n",
    "# The rest of your code remains the same\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_columns(page):\n",
    "    # Extract words from the page\n",
    "    words = page.extract_words()\n",
    "\n",
    "    # Group words by their X coordinate\n",
    "    word_groups = {}\n",
    "    for word in words:\n",
    "        x = round(word[\"x0\"])\n",
    "        if x not in word_groups:\n",
    "            word_groups[x] = []\n",
    "        word_groups[x].append(word)\n",
    "\n",
    "    # Sort the groups by X coordinate\n",
    "    sorted_word_groups = sorted(word_groups.items(), key=lambda x: x[0])\n",
    "\n",
    "    # Compute the spacing between groups\n",
    "    spacings = []\n",
    "    for i in range(len(sorted_word_groups) - 1):\n",
    "        spacing = sorted_word_groups[i + 1][0] - sorted_word_groups[i][0]\n",
    "        spacings.append(spacing)\n",
    "\n",
    "    # Compute the median spacing\n",
    "    median_spacing = sorted(spacings)[len(spacings) // 2]\n",
    "\n",
    "    # Group columns based on the spacing\n",
    "    columns = []\n",
    "    current_column = []\n",
    "    for i in range(len(sorted_word_groups)):\n",
    "        current_column.append(sorted_word_groups[i][1])\n",
    "        if i == len(sorted_word_groups) - 1 or sorted_word_groups[i + 1][0] - sorted_word_groups[i][0] > median_spacing:\n",
    "            columns.append(current_column)\n",
    "            current_column = []\n",
    "\n",
    "    return columns\n",
    "\n",
    "\n",
    "\n",
    "pdf_file = pdf_dir\n",
    "extracted_text = extract_without_page_margins(pdf_file, n_columns=2)\n",
    "formatted_text = full_preprocess(extracted_text)\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(formatted_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt with textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textract\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = textract.process(file_path)\n",
    "    return text.decode(\"utf-8\")\n",
    "\n",
    "def save_text_to_file(text, output_file_path):\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(text)\n",
    "\n",
    "def extract_text_from_pdf_dir(pdf_dir, output_dir):\n",
    "    for file_name in os.listdir(pdf_dir):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            input_file_path = os.path.join(pdf_dir, file_name)\n",
    "            output_file_path = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}.txt\")\n",
    "            text = extract_text_from_pdf(input_file_path)\n",
    "            save_text_to_file(text, output_file_path)\n",
    "\n",
    "pdf_dir = r'C:\\Users\\calvin.howard\\OneDrive\\OneDrive_Documents\\Research\\_memory_and_ad_dbs\\Literature'\n",
    "output_dir = r\"C:\\Users\\calvin.howard\\OneDrive\\OneDrive_Documents\\Work\\PostDoc\\Nimlab\\software_env\\python_modules\\nimlab\\gpt_document_reader\\writes\"\n",
    "extract_text_from_pdf_dir(pdf_dir, output_dir)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out tesseract. Also seems extremely powerful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.7.7_venv_ACE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "722d89dc901e449868ad4bf6f33e42f6d1e0a5c74d296773dce51109f1315592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
