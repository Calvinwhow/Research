{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: calvin-utils 0.1\n",
      "Uninstalling calvin-utils-0.1:\n",
      "  Would remove:\n",
      "    /usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/lib/python3.10/site-packages/calvin-utils.egg-link\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall calvin-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/Software/Research/nimlab/notebooks/neuroimaging_notebooks/server_submission_notebooks', '/usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/lib/python310.zip', '/usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/lib/python3.10', '/usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/lib/python3.10/lib-dynload', '', '/Users/cu135/.local/lib/python3.10/site-packages', '/usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/lib/python3.10/site-packages', '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/Software/Research/nimlab/calvin_utils']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/Software/Research/nimlab/calvin_utils\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: calvin-utils\n",
      "  Attempting uninstall: calvin-utils\n",
      "    Found existing installation: calvin-utils 0.1\n",
      "    Uninstalling calvin-utils-0.1:\n",
      "      Successfully uninstalled calvin-utils-0.1\n",
      "  Running setup.py develop for calvin-utils\n",
      "Successfully installed calvin-utils-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "%pip install -e /Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/Software/Research/nimlab/calvin_utils\n",
    "import sys\n",
    "import os\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'calvin_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcalvin_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfile_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe_utilities\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocess_colnames_for_regression\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'calvin_utils'"
     ]
    }
   ],
   "source": [
    "from calvin_utils.file_utils.dataframe_utilities import preprocess_colnames_for_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = 'response_topology/voxelwise_mediated_moderation/age_mediated_by_grey_matter'\n",
    "clin_path = r'/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/study_metadata/derivative_metadata/quantitative_atrophy/grey_matter_damage_score_and_outcomes/grey_matter_damage_score_and_outcomes.csv'\n",
    "out_dir = os.path.join(os.path.dirname(clin_path), f'{analysis}')\n",
    "save = True\n",
    "if os.path.exists(out_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Data to Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------user input above----------------------------------------------------------------\n",
    "data_df = pd.read_csv(clin_path)\n",
    "    \n",
    "# #Remove subjects\n",
    "# outlier_index=[11, 47, 48, 49]\n",
    "# data_df = data_df.drop(index=outlier_index)\n",
    "# data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outocme Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_column = 'Patient # CDR, ADAS'\n",
    "outcome_column = '% Change from baseline (ADAS-Cog11)'\n",
    "#----------------------------------------------------------------DO NOT MODIFY--------------------------------------------------------\n",
    "outcomes_df = pd.DataFrame()\n",
    "outcomes_df['outcome'] = data_df.loc[:, [outcome_column]]\n",
    "\n",
    "#Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "outcomes_df.iloc[:,:] = scaler.fit_transform(outcomes_df.iloc[:,:])\n",
    "\n",
    "\n",
    "outcomes_df['subject_id'] = data_df.loc[:, [subject_column]]\n",
    "# outcomes_df['subject_id'] = [id.split('_')[0] for id in data_df[subject_column].to_list()] #<------------------------problem code. must generalize to extract subject id better\n",
    "outcomes_df.set_index('subject_id', inplace=True)\n",
    "\n",
    "# data_df = data_df.set_index('Patient # CDR, ADAS')\n",
    "# data_df['subject_id'] = data_df.index\n",
    "# data_df['outcome'] = data_df\n",
    "\n",
    "# Convert the 'subject_id' column to strings for each DataFrame\n",
    "outcomes_df.index = outcomes_df.index.astype(str)\n",
    "display(outcomes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clinical Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m#----------------------------------------------------------------DO NOT MODIFY--------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      4\u001b[0m clinical_df_1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m----> 5\u001b[0m clinical_df_1[\u001b[39m'\u001b[39m\u001b[39mclinical_data\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data_df\u001b[39m.\u001b[39mloc[:, [clinical_information_column]]\n\u001b[1;32m      7\u001b[0m clinical_df_1[\u001b[39m'\u001b[39m\u001b[39msubject_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data_df\u001b[39m.\u001b[39mloc[:, [subject_column]]\n\u001b[1;32m      8\u001b[0m \u001b[39m# clinical_df_1['subject_id'] = [id.split('_')[0] for id in data_df[subject_column].to_list()] \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "subject_column = 'Patient # CDR, ADAS'\n",
    "clinical_information_column = 'Age'\n",
    "#----------------------------------------------------------------DO NOT MODIFY--------------------------------------------------------\n",
    "clinical_df_1 = pd.DataFrame()\n",
    "clinical_df_1['clinical_data'] = data_df.loc[:, [clinical_information_column]]\n",
    "\n",
    "clinical_df_1['subject_id'] = data_df.loc[:, [subject_column]]\n",
    "# clinical_df_1['subject_id'] = [id.split('_')[0] for id in data_df[subject_column].to_list()] \n",
    "clinical_df_1.set_index('subject_id', inplace=True)\n",
    "\n",
    "# Convert the 'subject_id' column to strings for each DataFrame\n",
    "clinical_df_1.index = clinical_df_1.index.astype(str)\n",
    "display(clinical_df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxelwise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "from calvin_utils.nifti_utils.generate_nifti import nifti_from_matrix\n",
    "from nimlab import datasets as nimds\n",
    "import numpy as np\n",
    "from nilearn import image, plotting, maskers\n",
    "\n",
    "#get conectivity values of interest\n",
    "matrix_path = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/connectivity_data/vta_published_t_connectivity'\n",
    "matrix = import_matrices_from_folder(matrix_path, file_pattern='/*_T*.nii*')\n",
    "\n",
    "#Standardize matrix via z score\n",
    "prepped_matrix = pd.DataFrame()\n",
    "for column in matrix.columns:\n",
    "    print(column)\n",
    "    new_name = int(column.split('_')[0]) #.split('T')[1]) #<----------------------------------THIS IS A BUG-CREATOR. MUST BE TAILORED TO PATIENT DATA\n",
    "\n",
    "#Set patients to those in the clinical data dataframe\n",
    "prepped_matrix = prepped_matrix.transpose()\n",
    "prepped_matrix['subject_id'] = [str(col).split('_')[0] for col in prepped_matrix.index]\n",
    "prepped_matrix.set_index('subject_id', inplace=True)\n",
    "neuroimaging_df_1 = prepped_matrix\n",
    "neuroimaging_df_1.index = neuroimaging_df_1.index.astype(str)\n",
    "#Display results\n",
    "display(neuroimaging_df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxelwise Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Another Dataframe if desired\n",
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "from calvin_utils.nifti_utils.generate_nifti import nifti_from_matrix\n",
    "from nimlab import datasets as nimds\n",
    "import numpy as np\n",
    "from nilearn import image, plotting, maskers\n",
    "\n",
    "#get conectivity values of interest\n",
    "matrix_path = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/cat12/cat12_ultrafine-reg/CAT12.8.2_2170'\n",
    "neuroimaging_df_2 = import_matrices_from_folder(matrix_path, file_pattern='/*/*/*/*mwp1*resamp*.nii*')\n",
    "performed_z_score = False \n",
    "#----------------------------------------------------------------DO NOT MODIFY!----------------------------------------------------------------\n",
    "\n",
    "#Set patients to those in the clinical data dataframe\n",
    "# prepped_matrix = prepped_matrix.loc[:, data_df.index]\n",
    "\n",
    "neuroimaging_df_2 = neuroimaging_df_2.transpose()\n",
    "neuroimaging_df_2['subject_id'] = [col.split('_')[0] for col in neuroimaging_df_2.index]\n",
    "neuroimaging_df_2.set_index('subject_id', inplace=True)\n",
    "\n",
    "neuroimaging_df_2.index = neuroimaging_df_2.index.astype(str)\n",
    "\n",
    "\n",
    "#Display results\n",
    "display(neuroimaging_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask Dataframes\n",
    "mask_path = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/response_topology/voxelwise_glm/stim_by_age/palm_results/palm_statistic_topology/generated_nifti.nii' #None #\n",
    "masking_df = None # # \n",
    "#----------------------------------------------------------------DO NOT MODIFY\n",
    "from calvin_utils.nifti_utils.matrix_utilities import mask_matrix\n",
    "# def mask_matrix(df_1, mask_path=None, mask_threshold=0.2, mask_by='rows', dataframe_to_mask_by=None):\n",
    "\n",
    "neuroimaging_df_1 = mask_matrix(neuroimaging_df_1, mask_path=mask_path, mask_threshold=0, mask_by='columns', dataframe_to_mask_by=masking_df)\n",
    "neuroimaging_df_2 = mask_matrix(neuroimaging_df_2, mask_path=mask_path, mask_threshold=0, mask_by='columns', dataframe_to_mask_by=masking_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional - Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroimaging_df_2 = neuroimaging_df_2.reindex(neuroimaging_df_1.index).dropna()\n",
    "neuroimaging_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframes for Access by Script\n",
    "    \"\"\"\n",
    "    Saves DataFrames to CSV files and returns the paths.\n",
    "\n",
    "    Parameters:\n",
    "    - outcome_dfs (list): A list of outcome DataFrames.\n",
    "    - covariate_dfs (list): A list of covariate DataFrames.\n",
    "    - voxelwise_dfs (list): A list of voxelwise DataFrames.\n",
    "    - path_to_dataframes (str): The directory where the DataFrames will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing lists of paths for each DataFrame type.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.dataframe_utilities import save_dataframes_to_csv\n",
    "\n",
    "df_paths_dict = save_dataframes_to_csv(outcome_dfs = [outcomes_df], \n",
    "                                       covariate_dfs = [clinical_df_1],\n",
    "                                       voxelwise_dfs = [neuroimaging_df_1, neuroimaging_df_2], \n",
    "                                       path_to_dataframes = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/response_topology/mediated_moderation/server_prep')\n",
    "print(df_paths_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer Files From Above**\n",
    "\n",
    "    ScpTransfer class provides a convenient way to transfer files to a remote server\n",
    "    using SCP (Secure Copy).\n",
    "    Will request your password for transfer if no ssh_key provided.\n",
    "\n",
    "    Example:\n",
    "        remote_base_path = \"/path/on/remote/server\"\n",
    "        remote_hostname = \"example.com\"\n",
    "        remote_username = \"your_username\"\n",
    "        ssh_key = \"path/to/ssh_key/\"\n",
    "\n",
    "        scp_transfer = ScpTransfer(remote_hostname, remote_username, remote_password)\n",
    "        scp_transfer.transfer_files_in_dict(dict_files_to_transfer, remote_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install paramiko\n",
    "from calvin_utils.server_utils.file_transfer_helper import ScpTransfer\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "scp_transfer = ScpTransfer(hostname=\"erisone.partners.org\",\n",
    "                           username=\"cu135\", \n",
    "                           ssh_key=None\n",
    "                        )\n",
    "scp_transfer.transfer_files_in_dict(dict_files=df_paths_dict, \n",
    "                                    base_remote_path='/PHShome/cu135/permutation_tests/f_test/age_by_stim_ad_dbs_redone/inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit Jobs**\n",
    "\n",
    "    Initialize server\n",
    "    server = LSFServer(\"erisone.partners.org\", \"cu135\")\n",
    "\n",
    "    Initialize job\n",
    "    job = LSFJob(\n",
    "    job_name=\"f_test_bm\",\n",
    "    user_email=\"choward12@bwh.harvard.edu\",\n",
    "    cpus=1,\n",
    "    output_dir=\"/PHShome/cu135/terminal_outputs\",\n",
    "    error_dir=\"/PHShome/cu135/error_outputs\",\n",
    "    queue=\"big-multi\",\n",
    "    script_path=\"/PHShome/cu135/python_scripts/launch_f_test_palm.py\",\n",
    "    options=\"-o /PHShome/cu135/permutation_tests/f_test/age_by_stim_pd_dbs/results\",\n",
    "    Gb_requested=4,\n",
    "    wait_time=0\n",
    "    )\n",
    "\n",
    "    Initialize job submitter\n",
    "    job_submitter = JobSubmitter(server, job)\n",
    "\n",
    "    Submit jobs\n",
    "    job_submitter.submit_jobs(10)\n",
    "\n",
    "This example will log into the server 'erisone.partners.org' as user 'cu135'.\n",
    " \n",
    "Will submit 10 jobs named 'f_test_bm' to the queue 'big-multi'. \n",
    "\n",
    "Each job will run the Python script at '/PHShome/cu135/python_scripts/launch_f_test_palm.py' with the options specified.\n",
    "\n",
    "Putputting to '/PHShome/cu135/terminal_outputs' and '/PHShome/cu135/error_outputs' for terminal and error logs respectively. \n",
    "\n",
    "The job requests 1 CPU and 4GB of memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.server_utils.job_submission_helper import LSFServer, LSFJob\n",
    "how_many_permutations_would_you_like_to_do = 10\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# Create an instance of the LSF job.\n",
    "# Assuming a resource requirement of 500GB memory and 6 cores for each job.\n",
    "# And a job script with some options.\n",
    "\n",
    "lsf_job = LSFJob(job_name=\"f-test_big\",\n",
    "                 user_email=\"choward12@bwh.harvard.edu\",\n",
    "                 output_dir=\"/PHShome/cu135/terminal_outputs\",\n",
    "                 error_dir=\"/PHShome/cu135/terminal_outputs\",\n",
    "                 queue=\"big\",\n",
    "                 n_jobs=20,\n",
    "                 options=\"-o /PHShome/cu135/permutation_tests/f_test/age_by_stim_pd_dbs/results\",\n",
    "                 work_dir=\"/PHShome/cu135/permutation_tests\",\n",
    "                 cpus=6,\n",
    "                 Gb_requested=498,\n",
    "                 wait_time=None\n",
    "                 script_path=\"/PHShome/cu135/python_scripts/launch_f_test_palm.py\",\n",
    "                 options='q'\n",
    "                 )\n",
    "\n",
    "# Create an instance of the LSF server.\n",
    "lsf_server = LSFServer(server_name=\"erisone.parters.org\", \n",
    "                       username=\"cu135\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.server_utils.job_submission_helper import JobSubmitter\n",
    "\n",
    "# Submit to server\n",
    "job_submitter = JobSubmitter(server=lsf_server, job=lsf_job)\n",
    "job_submitter.submit_jobs(n_jobs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recompose Data\n",
    "This code uses a file-staging approach to large-scale computation. The resultant files have been saved to your output directory. You must now recompose them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_csvs(directory, output_filename):\n",
    "    \"\"\"\n",
    "    Combine all CSV files in a directory into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The path to the directory containing the CSV files.\n",
    "    - output_filename (str): The path to the output CSV file.\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Get a list of all CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "\n",
    "    # Loop through the CSV files and append each one to the combined DataFrame\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(directory, csv_file))\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "    # Save the combined DataFrame as a new CSV file\n",
    "    combined_df.to_csv(output_filename, index=False)\n",
    "    return output_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposed_csv_path = combine_csvs(\"/path/to/your/directory\", \"combined.csv\")\n",
    "recomposed_csv_df = pd.read_csv(recomposed_csv_path)\n",
    "recomposed_csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import confidence_intervals\n",
    "# run confidence intervals on the csv\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "def calculate_confidence_intervals(ab_paths, mediators):\n",
    "    \"\"\"\n",
    "    Calculates the confidence intervals and p-value based on the bootstrapped samples.\n",
    "\n",
    "    Parameters:\n",
    "    - ab_paths: list of lists containing the bootstrapped ab paths for each mediator.\n",
    "    - total_indirect_effects: list of bootstrapped summed ab paths.\n",
    "    - mediators: list of mediator names.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the mean indirect effect, confidence intervals, and p-values for each mediator and the total indirect effect.\n",
    "    \"\"\"\n",
    "    ab_path_values = np.array(ab_paths)\n",
    "\n",
    "    # Check if there's only one mediator\n",
    "    if isinstance(mediators, str):\n",
    "        mediators = [mediators]\n",
    "\n",
    "    # Calculate mean indirect effect and confidence intervals for each mediator\n",
    "    mean_ab_paths = np.mean(ab_path_values, axis=0)\n",
    "    lower_bounds = np.percentile(ab_path_values, 2.5, axis=0)\n",
    "    upper_bounds = np.percentile(ab_path_values, 97.5, axis=0)\n",
    "\n",
    "    # Calculate p-values for each mediator\n",
    "    ab_path_p_values = [np.mean(np.sign(mean_ab_paths) * ab_path_values <= 0)]\n",
    "\n",
    "    # Create DataFrame to store the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'Point Estimate': mean_ab_paths,\n",
    "        '2.5th Percentile': lower_bounds,\n",
    "        '97.5th Percentile': upper_bounds,\n",
    "        'P-value': ab_path_p_values\n",
    "    }, index=mediators)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = list(tqdm(pool.imap(calculate_confidence_intervals, [row for _, row in recomposed_csv_df.iterrows()]), total=recomposed_csv_df.shape[0]))\n",
    "pool.close() \n",
    "\n",
    "# Create the results DataFrame\n",
    "results_df = pd.concat(results, axis=1).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Nifti Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/response_topology/mediated_moderation/age_mediated_by_grey_matter/P-value_mediated_moderation_analysis_results.csv')\n",
    "\n",
    "mask_threshold=0\n",
    "#----------------------------------------------------------------DO NOT TOUCH\n",
    "#Prepare Indices to Unmask\n",
    "from calvin_utils.nifti_utils.matrix_utilities import unmask_matrix\n",
    "from nimlab import datasets as nimds\n",
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti\n",
    "from nilearn import image\n",
    "\n",
    "#Perform Unmasking\n",
    "if mask_path is not None:\n",
    "    mni_mask = image.load_img(mask_path).get_fdata().flatten()\n",
    "    brain_indices = np.where(mni_mask > mask_threshold)[0]\n",
    "elif masking_df is not None:\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\").get_fdata().flatten()\n",
    "    mask = masking_df.transpose().reset_index(drop=True).copy()\n",
    "    mask['mask_index'] = mask.sum(axis=1)\n",
    "    brain_indices = np.where(mask['mask_index'] != 0)[0]\n",
    "else:\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\").get_fdata().flatten()\n",
    "    brain_indices = np.where(mni_mask > 0)[0]\n",
    "\n",
    "# Create a boolean mask for brain_indices\n",
    "bool_mask = np.zeros_like(mni_mask, dtype=bool)\n",
    "bool_mask[brain_indices] = True\n",
    "\n",
    "for statistic in results_df.columns:\n",
    "    try:\n",
    "        print(statistic)\n",
    "        \n",
    "        # Initialize the output mask with NaN values\n",
    "        output_mask = np.full_like(mni_mask, np.nan)\n",
    "        \n",
    "        # Reinstate the values at brain_indices\n",
    "        output_mask[bool_mask] = results_df[statistic]\n",
    "        \n",
    "        # View and save\n",
    "        view_and_save_nifti(output_mask, out_dir=out_dir, output_name=statistic)\n",
    "    except:\n",
    "        print(f\"Couldn't convert {statistic} to nifti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
