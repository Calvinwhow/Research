{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import platform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have set pathnames in the Mac style\n",
      "I will save to: /Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_PD_DBS_STN_WURZBURG/cat12/z_score\n"
     ]
    }
   ],
   "source": [
    "## Paths Input Here\n",
    "analysis = \"z_score\"\n",
    "if platform.uname().system == 'Darwin': #------------------------------Mac OS X---------------------------------------------------------------\n",
    "    path_1 = r'/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_PD_DBS_STN_WURZBURG/cat12/ROI_memory_roi_Vgm.csv'\n",
    "    path_2 = r'/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/ADNI/NIFTIS/true_control/cat_12_results/roi_volumes/ROI_memory_roi_Vgm.csv'\n",
    "    # clin_path = 'path to clinical values'\n",
    "    out_dir = os.path.join(os.path.dirname(path_1), f'{analysis}')\n",
    "    #out_dir = r'path to out dir here'\n",
    "    #roi_names = '<path to roi name location>'\n",
    "    print('I have set pathnames in the Mac style')\n",
    "    print('I will save to:', out_dir)\n",
    "else: #----------------------------------------------------------------Windows----------------------------------------------------------------\n",
    "    print('I have set pathnames in the Windows style')\n",
    "\n",
    "if os.path.isdir(out_dir) != True:\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Niftis from a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "def csv_of_nifti_filepaths_to_dataframe(csv_path: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Reads a CSV file containing paths to nifti files, imports the nifti files, flattens them,\n",
    "    removes NaNs, and creates a dataframe in the specified format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        Path to the CSV file containing paths to nifti files.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A dataframe where columns represent flattened nifti files and rows represent voxels.\n",
    "        All values are zero, except for lesions which are binarized at 1.\n",
    "    \n",
    "    '''\n",
    "    # Read the CSV file\n",
    "    file_paths = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Initialize an empty list to store flattened nifti data\n",
    "    nifti_data = []\n",
    "    \n",
    "    # Iterate through the file paths and import nifti files\n",
    "    for index, row in file_paths.iterrows():\n",
    "        nifti_file_path = row[0]\n",
    "        \n",
    "        # Load the nifti file\n",
    "        nifti_image = nib.load(nifti_file_path)\n",
    "        \n",
    "        # Get the data as a numpy array\n",
    "        nifti_array = nifti_image.get_fdata()\n",
    "        \n",
    "        # Flatten the numpy array\n",
    "        flattened_array = nifti_array.flatten()\n",
    "        \n",
    "        # Replace NaNs with zeros\n",
    "        flattened_array[np.isnan(flattened_array)] = 0\n",
    "        \n",
    "        # Binarize the flattened array\n",
    "        flattened_array = np.where(flattened_array > 0, 1, 0)\n",
    "        \n",
    "        # Append the flattened array to the list\n",
    "        nifti_data.append(flattened_array)\n",
    "    \n",
    "    # Create a dataframe from the list of flattened arrays\n",
    "    df = pd.DataFrame(np.column_stack(nifti_data))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Niftis from a Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will search:  /Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/cat12/cat12_ultrafine-reg/roi_volumes/ROI_memory_roi_Vgm.csv/*/z_score_atrophy/grey_matter/*.nii\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from calvin_utils.import_matrices import import_matrices_from_folder\n",
    "#set file path to'' if you have specified the full path to the nifti file itself\n",
    "# /sub-101/z_score_atrophy/CSF/sub-101_generated_nifti.nii\n",
    "# /Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/cat12/cat12_ultrafine-reg/CAT12.8.2_2170/sub-150/z_score_atrophy/white_matter\n",
    "# /Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_PD_DBS_STN_WURZBURG/z_scored_segments\n",
    "df_1 = import_matrices_from_folder(path_1, file_pattern='/*/z_score_atrophy/grey_matter/*.nii')\n",
    "# /Users/cu135/Dropbox (Partners HealthCare)/memory/functional_networks/ferguson_2019_networks/control_lesions/auditory_hallucination_lesions/sub-08uNodau1/roi/sub-08uNodau1_lesionMask.nii.gz\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will search:  /Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/ADNI/NIFTIS/true_control/cat_12_results/roi_volumes/ROI_memory_roi_Vgm.csv/*resampled*.nii\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = import_matrices_from_folder(path_2, file_pattern='/*resampled*.nii')\n",
    "df_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from nilearn import image as nli\n",
    "from nilearn.image import resample_to_img\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "from tqdm import tqdm\n",
    "from nimlab import datasets as nimds\n",
    "\n",
    "\n",
    "def downsample_image(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Function to downsample a 3D image to a new voxel size using a target affine.\n",
    "    \n",
    "    Args:\n",
    "    input_path (str): Filepath to the input image.\n",
    "    output_path (str): Filepath to save the output image.\n",
    "    target_voxel_size (list): Target voxels to resample to.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    img = nib.load(input_path)\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "    \n",
    "    # Downsample the image using the target affine\n",
    "    resampled_img = resample_to_img(img, mni_mask)\n",
    "\n",
    "    # Save the downsampled image\n",
    "    nib.save(resampled_img, output_path)\n",
    "    \n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_data = mni_mask.get_fdata().flatten()\n",
    "\n",
    "def downsample_to_mni152_images_in_folder(input_folder_pattern):\n",
    "    \"\"\"\n",
    "    Function to downsample all 3D images in a folder to a new voxel size.\n",
    "    \n",
    "    Args:\n",
    "    input_folder_pattern (str): Glob pattern to find the input images.\n",
    "    target_voxel_size (list): Target voxels to resample to.\n",
    "    \"\"\"\n",
    "    # Find all input image filepaths\n",
    "    input_filepaths = glob.glob(input_folder_pattern)\n",
    "    print('Will search:, ', input_folder_pattern)\n",
    "\n",
    "    # Loop over each input image\n",
    "    for input_path in tqdm(input_filepaths):\n",
    "        # Define the output path\n",
    "        base, ext = os.path.splitext(input_path)\n",
    "        if ext == '.gz':\n",
    "            base, ext2 = os.path.splitext(base)\n",
    "            ext = ext2 + ext\n",
    "        output_path = base + '_resampled' + ext\n",
    "\n",
    "        # Downsample the image\n",
    "        downsample_image(input_path, output_path)\n",
    "    print('Drownsampled images saved to: ' + output_path)\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# downsample_images_in_folder('/path/to/your/images/*/*/anat/*mwp1*.nii', '/path/to/target/resolution/image.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_to_check = path_2\n",
    "file_pattern = '*.nii'\n",
    "#----------------------------------------------------------------DO NOT TOUCH\n",
    "downsample_to_mni152_images_in_folder(os.path.join(directory_to_check, file_pattern))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimlab import datasets as nimds\n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_data = mni_mask.get_fdata().flatten()\n",
    "brain_indices = np.where(mask_data > 0)[0]\n",
    "df_1 = df_1.iloc[brain_indices, :]\n",
    "df_2 = df_2.iloc[brain_indices, :]\n",
    "\n",
    "print('Dataframes have been masked such that their shapes are: ', df_1.shape, df_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.matrix_utilities import threshold_matrix \n",
    "threshold_1 = 2\n",
    "threshold_2 = -2\n",
    "#This will make everything NOT meeting the condition 0 \n",
    "df_1 = df_1.where(df_1 < threshold_2, 0)\n",
    "# df_2.where(df_2 < threshold_2, 0)\n",
    "df_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Coordinates to Index\n",
    "from calvin_utils.matrix_utilities import convert_coordinate_to_index\n",
    "from nimlab import datasets as nimds\n",
    "#Mask within the brain\n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_affine = mni_mask.affine\n",
    "\n",
    "coordinate_tuple = (-2,30,56)\n",
    "index = convert_coordinate_to_index(coordinate_tuple, mask_affine)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Index to Coordinates\n",
    "from calvin_utils.matrix_utilities import convert_index_to_coordinate\n",
    "from nimlab import datasets as nimds\n",
    "#Mask within the brain\n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_affine = mni_mask.affine\n",
    "\n",
    "index_tuple = (46, 78, 64)\n",
    "index = convert_index_to_coordinate(index_tuple, mask_affine)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Index (Voxel Coordinates) To Flat Array Index (1 dimensional array after running ___.flatten())\n",
    "from calvin_utils.matrix_utilities import index_in_flattened_nifti\n",
    "from nimlab import datasets as nimds\n",
    "#Mask within the brain\n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_shape = mni_mask.shape\n",
    "\n",
    "index_tuple = (46, 78, 64)\n",
    "index = index_in_flattened_nifti(index_tuple, mask_shape)\n",
    "index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run FSL Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.run_fsl_cluster import run_fsl_cluster\n",
    "run_fsl_cluster(path_1, outdir=out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dice Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def dice_coefficient(df1: pd.DataFrame, df2: pd.DataFrame) -> float:\n",
    "    '''\n",
    "    Calculates the Dice Coefficient between two dataframes containing binary lesion masks.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df1 : pd.DataFrame\n",
    "        The first dataframe, where columns represent flattened nifti files and rows represent voxels.\n",
    "        All values are zero, except for lesions which are binarized at 1.\n",
    "        \n",
    "    df2 : pd.DataFrame\n",
    "        The second dataframe, where columns represent flattened nifti files and rows represent voxels.\n",
    "        All values are zero, except for lesions which are binarized at 1.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The Dice Coefficient, a value between 0 and 1, where 1 represents a perfect overlap.\n",
    "        \n",
    "    '''\n",
    "    # Check if in numpy array, and convert the dataframes to numpy arrays if required\n",
    "    if isinstance(df1, np.ndarray):\n",
    "        array1 = df1\n",
    "    else:\n",
    "        array1 = df1.to_numpy()\n",
    "    if isinstance(df2, np.ndarray):\n",
    "        array2 = df2\n",
    "    else:\n",
    "        array2 = df2.to_numpy()\n",
    "    \n",
    "    # Calculate the intersection of non-zero elements\n",
    "    intersection = np.sum(np.logical_and(array1, array2))\n",
    "    \n",
    "    # Calculate the number of non-zero elements in each array\n",
    "    num_elements_array1 = np.sum(np.count_nonzero(array1))\n",
    "    num_elements_array2 = np.sum(np.count_nonzero(array2))\n",
    "    \n",
    "    # Calculate the Dice Coefficient\n",
    "    dice_coefficient = (2 * intersection) / (num_elements_array1 + num_elements_array2)\n",
    "    \n",
    "    return dice_coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.matrix_utilities import threshold_matrix\n",
    "from calvin_utils.fisher_z_transform import fisher_z_transform\n",
    "from nimlab import datasets as nimds\n",
    "\n",
    "#If you want to enter a threshold (quantile) to threhsold the matrices at, enter True\n",
    "threshold=True\n",
    "#if you have an R map as a matrix and want to fisher transform, enter True\n",
    "fish_transform=False\n",
    "# if you have a whole host of matrices in df_1 or df_2, enter summate=True, otherwise summate=False (this compares 2 matrices)\n",
    "summate=False\n",
    "\n",
    "if threshold: \n",
    "    #Threshold by quantile if desirde\n",
    "    quantile_target =  0.95\n",
    "    \n",
    "    threshold_1 = np.quantile(df_1, quantile_target)\n",
    "    threshold_2 = np.quantile(df_2, quantile_target)\n",
    "    \n",
    "    thresholded_df_1 = threshold_matrix(df_1, threshold = threshold_1, probability=False, direction='keep_greater')\n",
    "    thresholded_df_2 = threshold_matrix(df_2, threshold = threshold_2, probability=False, direction='keep_greater')\n",
    "    \n",
    "    thresholded_df_1[thresholded_df_1 > 0] = 1\n",
    "    thresholded_df_2[thresholded_df_2 > 0] = 1\n",
    "    \n",
    "#Fisher transform \n",
    "if fish_transform: \n",
    "    df_1 = fisher_z_transform(df_1)\n",
    "\n",
    "if summate:\n",
    "    thresholded_df_1['for_dice'] = thresholded_df_1.sum(axis=1)\n",
    "    thresholded_df_2['for_dice'] = thresholded_df_2.sum(axis=1)\n",
    "else:\n",
    "    thresholded_df_1['for_dice'] = thresholded_df_1\n",
    "    thresholded_df_2['for_dice'] = thresholded_df_2\n",
    "\n",
    "#Dice Coefficient Calculation\n",
    "#This can only compare TWO COLUMNS. \n",
    "#Make sure you specify what column you want. \n",
    "observed_dice_coefficient = dice_coefficient(thresholded_df_1['for_dice'], thresholded_df_2['for_dice'])\n",
    "print('Dice coefficient:', observed_dice_coefficient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute the Dice Coefficient\n",
    "from calvin_utils.palm import brain_permutation\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Assuming df_1 and df_2 are your original dataframes\n",
    "n_permutations = 1000\n",
    "dice_coefficients = []\n",
    "voxel_index = 0\n",
    "for i in tqdm(range(n_permutations)):\n",
    "    # Permute dataframes\n",
    "    permuted_df_1 = brain_permutation(thresholded_df_1.copy().to_numpy().reshape(1,-1), looped_permutation=True)\n",
    "    permuted_df_2 = brain_permutation(thresholded_df_2.copy().to_numpy().reshape(1,-1), looped_permutation=True)\n",
    "\n",
    "    # Threshold and calculate the Dice coefficient for the permuted dataframes\n",
    "    permuted_dice_coefficient = dice_coefficient(permuted_df_1, permuted_df_2)\n",
    "\n",
    "    # Store the Dice coefficient\n",
    "    dice_coefficients.append(permuted_dice_coefficient)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "dice_coefficients = np.array(dice_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same, but with multiprocessing\n",
    "import concurrent.futures\n",
    "from calvin_utils.matrix_utilities import dice_coefficient\n",
    "\n",
    "n_permutations = 1000\n",
    "dice_coefficients = []\n",
    "voxel_index = 0\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    #Begin submitting the masked data to the permutor\n",
    "    results = []\n",
    "    for i in tqdm(range(n_permutations), desc=\"Jobs Launched\"):\n",
    "        permuted_df_1 = brain_permutation(thresholded_df_1.copy().to_numpy().reshape(1,-1), looped_permutation=True)\n",
    "        permuted_df_2 = brain_permutation(thresholded_df_2.copy().to_numpy().reshape(1,-1), looped_permutation=True)\n",
    "        \n",
    "        result = executor.submit(dice_coefficient, permuted_df_1, permuted_df_2)\n",
    "        results.append(result)\n",
    "        \n",
    "    progress_bar = tqdm(total=n_permutations, desc=\"Jobs Finalized\")\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        \n",
    "        #Input the permuted data into the array\n",
    "        permuted_dice_coefficient = result.result()\n",
    "        dice_coefficients.append(permuted_dice_coefficient)\n",
    "        \n",
    "        #Update visualization\n",
    "        progress_bar.update()\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('empiric p: ', np.count_nonzero(dice_coefficients>observed_dice_coefficient))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of Lesion Incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "summed_voxels = df_1.sum(axis=1)\n",
    "summed_voxels2 = df_2.sum(axis=1)\n",
    "\n",
    "summed_voxels_df = pd.DataFrame({'Voxel_Index': summed_voxels.index, 'Summed_Voxel_Value': summed_voxels.values})\n",
    "summed_voxels_df2 = pd.DataFrame({'Voxel_Index': summed_voxels.index, 'Summed_Voxel_Value': summed_voxels2.values})\n",
    "\n",
    "\n",
    "summed_voxels_df['Normalized_Summed_Voxel_Value'] = normalize(summed_voxels_df['Summed_Voxel_Value'])\n",
    "summed_voxels_df2['Normalized_Summed_Voxel_Value'] = normalize(summed_voxels_df2['Summed_Voxel_Value'])\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the first DataFrame with normalized values\n",
    "plt.plot(summed_voxels_df['Voxel_Index'], summed_voxels_df['Normalized_Summed_Voxel_Value'], label='Dataset 1')\n",
    "\n",
    "# Plot the second DataFrame with normalized values\n",
    "plt.plot(summed_voxels_df2['Voxel_Index'], summed_voxels_df2['Normalized_Summed_Voxel_Value'], label='Dataset 2')\n",
    "\n",
    "plt.xlabel('Voxel Index')\n",
    "plt.ylabel('Normalized Summed Voxel Value')\n",
    "plt.title('Normalized Summed Voxel Values vs. Voxel Index')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Heatmap from a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_1, index_col=False)\n",
    "display(df)\n",
    "#Create heatmap of correlation matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15, 13))\n",
    "sns.heatmap(df, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "#Save the Elbow Plot Above\n",
    "save_dirsvg = os.path.join(out_dir, 'heatmap.svg')\n",
    "save_dirpng = os.path.join(out_dir, 'heatmap.png')\n",
    "fig.savefig(save_dirsvg)\n",
    "fig.savefig(save_dirpng)\n",
    "print(f'Fig saved to ', save_dirpng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ROIs from a CSV of Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.generate_nifti import read_coordinates_csv\n",
    "\n",
    "coordinates_df = read_coordinates_csv(filename=path_1, radius=3)\n",
    "coordinates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_df.to_csv(out_dir+'/coordinates_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BIDS Directory from Subjects/Coordinates CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.generate_nifti import read_subject_coordinates_csv\n",
    "\n",
    "file_path_df = read_subject_coordinates_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/VOSS_TMS/Memory_Change_TMS_SimonKwon_to_CalvinHoward.csv', radius=12, method='concentric')\n",
    "file_path_df.to_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/VOSS_TMS/Memory_Change_TMS_SimonKwon_to_CalvinHoward' + '_filepaths.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Several Niftis Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.generate_nifti import add_matrices_together\n",
    "from calvin_utils.generate_nifti import view_and_save_nifti\n",
    "\n",
    "summed_matrix = add_matrices_together(folder=path_1)\n",
    "summed_matrix_img = view_and_save_nifti(summed_matrix, out_dir=path_1)\n",
    "summed_matrix_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Matrix by Another Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.generate_nifti import threshold_matrix_by_another\n",
    "thresholded_matrix = threshold_matrix_by_another(matrix_file_1=path_1, matrix_file_2=path_1, method='under_threshold', threshold=0.05)\n",
    "thresholded_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold a Matrix By a Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.matrix_utilities import threshold_matrix\n",
    "from calvin_utils.fisher_z_transform import fisher_z_transform\n",
    "from calvin_utils.generate_nifti import view_and_save_nifti\n",
    "\n",
    "fisher_transform = False\n",
    "#Fisher transform \n",
    "if fisher_transform: \n",
    "    df_1 = fisher_z_transform(df_1)\n",
    "    \n",
    "#Threshold by quantile if desirde\n",
    "quantile_target =  0.95\n",
    "\n",
    "threshold_1 = np.quantile(df_1, quantile_target)\n",
    "\n",
    "thresholded_df_1 = threshold_matrix(df_1, threshold = threshold_1, probability=False, direction='keep_greater')\n",
    "# thresholded_df_1[thresholded_df_1 > 0] = 1\n",
    "threhsodled_matrix_img = view_and_save_nifti(thresholded_df_1, out_dir=out_dir)\n",
    "threhsodled_matrix_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Matrix from a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimlab import datasets as nimds\n",
    "\n",
    "data_df = pd.read_csv(path_2)\n",
    "if len(data_df) == 225222:\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "    mask_data = mni_mask.get_fdata().flatten()\n",
    "    brain_indices = np.where(mask_data > 0)[0]\n",
    "    mask_data[brain_indices] = data_df.iloc[:,-1]\n",
    "    data_df = pd.DataFrame(mask_data)\n",
    "display(data_df)\n",
    "print(np.min(data_df))\n",
    "print(np.max(data_df))\n",
    "\n",
    "# data_df = (1/data_df)/10000\n",
    "# data_df = data_df/np.max(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.generate_nifti import view_and_save_nifti\n",
    "matrix_img = view_and_save_nifti(data_df, out_dir)\n",
    "matrix_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Z-Scores for VBM Atrophy Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from nilearn import datasets\n",
    "from nilearn import image\n",
    "def threshold_probabilities(patient_df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    patient_df = patient_df.where(patient_df > threshold, 0)\n",
    "    return patient_df\n",
    "\n",
    "def calculate_z_scores(control_df: pd.DataFrame, patient_df: pd.DataFrame, matter_type=None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Function to calculate voxel-wise mean, standard deviation for control group and z-scores for patient group.\n",
    "\n",
    "    Args:\n",
    "    control_df (pd.DataFrame): DataFrame where each column represents a control subject, \n",
    "                               and each row represents flattened image data for a voxel.\n",
    "    patient_df (pd.DataFrame): DataFrame where each column represents a patient, \n",
    "                               and each row represents flattened image data for a voxel.\n",
    "\n",
    "    Returns:\n",
    "    control_mean (pd.DataFrame): DataFrame of voxel-wise means calculated across the control group.\n",
    "    control_std (pd.DataFrame): DataFrame of voxel-wise standard deviations calculated across the control group.\n",
    "    patient_z_scores (pd.DataFrame): DataFrame of voxel-wise z-scores calculated for each patient using control mean and std.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mask the dataframes to only consider tissues over acceptable probability thresholds\n",
    "    # Using p>0.2, as typical masking to MNI152 segments uses P > 0.2 for a given segment\n",
    "    \n",
    "    # Now you can use the function to apply a threshold to patient_df and control_df\n",
    "    threshold = 0.2\n",
    "    patient_df = threshold_probabilities(patient_df, threshold)\n",
    "    control_df = threshold_probabilities(control_df, threshold)\n",
    "\n",
    "    # Calculate mean and standard deviation for each voxel in control group\n",
    "    control_mean = control_df.mean(axis=1)\n",
    "    control_std = control_df.std(axis=1)\n",
    "\n",
    "    # Initialize DataFrame to store patient z-scores\n",
    "    patient_z_scores = pd.DataFrame()\n",
    "\n",
    "    # Calculate z-scores for each patient using control mean and std\n",
    "    for patient in patient_df.columns:\n",
    "        patient_z_scores[patient] = (patient_df[patient] - control_mean) / control_std\n",
    "\n",
    "    # Set values back into brain_mask\n",
    "    # if matter_type == None:\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "    mask_data = mni_mask.get_fdata().flatten()\n",
    "    apply_mask = lambda patient_z_scores: np.where(mask_data > 0, patient_z_scores, 0)\n",
    "    patient_z_scores = patient_z_scores.apply(apply_mask, axis=0)\n",
    "    print('Not sure what matter class to mask to, returning mask within MNI152 space')\n",
    "    # elif matter_type == 'grey_matter':\n",
    "    #     mask_data = image.load_img('/Users/cu135/Dropbox (Partners HealthCare)/resources/mni_spaces/6th_gen/mni_152_gm_mask_resampled.nii').get_fdata().flatten()\n",
    "    #     patient_z_scores[mask_data < 0.2] = 0\n",
    "    #     print('Masked to MNI152 Grey Matter')\n",
    "    # elif matter_type == 'white_matter':\n",
    "    #     mask_data = image.load_img('/Users/cu135/Dropbox (Partners HealthCare)/resources/mni_spaces/6th_gen/mni_152_wm_mask_resampled.nii').get_fdata().flatten()\n",
    "    #     patient_z_scores[mask_data < 0.2] = 0\n",
    "    #     print('Masked to MNI152 White matter')\n",
    "    # elif matter_type == 'CSF':\n",
    "    #     mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "    #     mask_data = mni_mask.get_fdata().flatten()\n",
    "    #     apply_mask = lambda patient_z_scores: np.where(mask_data > 0.2, patient_z_scores, 0)\n",
    "    #     patient_z_scores = patient_z_scores.apply(apply_mask, axis=0)\n",
    "    #     print('Masking within the MNI brain mask')\n",
    "    # else:\n",
    "    #     raise ValueError('Please select a valid matter_type: None, grey_matter, white_matter are currently supported')\n",
    "\n",
    "    \n",
    "    return control_mean, control_std, patient_z_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matter_type = 'csf'\n",
    "\n",
    "#----------------------------------------------------------------DO NOT TOUCH ----------------------------------------------------------------\n",
    "\n",
    "control_mean, control_std, patient_z_score_df = calculate_z_scores(control_df=df_2, patient_df=df_1, matter_type=matter_type)\n",
    "\n",
    "# Plot pairplot and display descriptive statistics\n",
    "# print(patient_z_score_df.describe())\n",
    "display(patient_z_score_df)\n",
    "# patient_z_score_df.to_csv(os.path.join(out_dir, 'z_scores.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the atrophy files\n",
    "character_after_subject_id = '-'\n",
    "#--------------------------------DO NOT TOUCH--------------------------------------------------------\n",
    "from calvin_utils.generate_nifti import nifti_from_matrix\n",
    "root_dir = out_dir\n",
    "for patient in patient_z_score_df.columns:\n",
    "    subject = patient.split(character_after_subject_id)[0]\n",
    "    out_dir = os.path.join(root_dir, ('sub-'+subject+f'/z_score_atrophy/{matter_type}'))\n",
    "    nifti_from_matrix(patient_z_score_df[patient], output_file=out_dir, output_name=f'sub-{subject}')\n",
    "    \n",
    "    # /Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/rawdata/sub-150/ses-01/anat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Z-Scores for Atrophy ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_roi_z_scores(path_atrophy, path_control):\n",
    "    \"\"\"\n",
    "    Function to compute z-scores for all brain regions for atrophy patients\n",
    "    in comparison to control group.\n",
    "\n",
    "    Parameters:\n",
    "    - path_atrophy: str, path to csv file for atrophy patients\n",
    "    - path_control: str, path to csv file for control group\n",
    "\n",
    "    Returns:\n",
    "    - df_atrophy_z_scored: DataFrame, atrophy patients data with z-scores in place of original values\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    df_atrophy = pd.read_csv(path_atrophy)\n",
    "    df_control = pd.read_csv(path_control)\n",
    "\n",
    "    # Create a copy of df_atrophy to hold the z-scored data\n",
    "    df_atrophy_z_scored = df_atrophy.copy()\n",
    "\n",
    "    # Loop over all columns (brain regions) in the dataframe\n",
    "    for column in tqdm(df_atrophy.columns):\n",
    "        \n",
    "        # Skip if the column is 'names'\n",
    "        if column == 'names':\n",
    "            continue\n",
    "        else:\n",
    "            # Compute the mean and standard deviation of the control group for the current brain region\n",
    "            control_mean = df_control[column].mean()\n",
    "            control_std = df_control[column].std()\n",
    "\n",
    "            # Calculate the z-scores for the atrophy patients relative to the control group\n",
    "            df_atrophy_z_scored[column] = df_atrophy[column].apply(lambda x: (x - control_mean) / control_std)\n",
    "\n",
    "    # Set the index to 'names'\n",
    "    df_atrophy_z_scored.set_index('names', inplace=True)\n",
    "\n",
    "    return df_atrophy_z_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3663.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROI001</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>names</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.272702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.390599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>4.522047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>4.858367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>2.805814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>4.513402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.633305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.482619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.587716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>4.699325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>1.296646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>4.573199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>5.217654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>5.769222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.106181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.671542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.248557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>2.855096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>4.211557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.110959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.826025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.566480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>2.718825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>2.813373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.299074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.103596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>4.442191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.588524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>4.106968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>-9.374411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.789381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>4.624409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glanat</th>\n",
       "      <td>3.344444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROI001\n",
       "names           \n",
       "glanat  3.272702\n",
       "glanat  3.390599\n",
       "glanat  4.522047\n",
       "glanat  4.858367\n",
       "glanat  2.805814\n",
       "glanat  4.513402\n",
       "glanat  3.633305\n",
       "glanat  3.482619\n",
       "glanat  3.587716\n",
       "glanat  4.699325\n",
       "glanat  1.296646\n",
       "glanat  4.573199\n",
       "glanat  5.217654\n",
       "glanat  5.769222\n",
       "glanat  3.106181\n",
       "glanat  3.671542\n",
       "glanat  3.248557\n",
       "glanat  2.855096\n",
       "glanat  4.211557\n",
       "glanat  3.110959\n",
       "glanat  3.826025\n",
       "glanat  3.566480\n",
       "glanat  2.718825\n",
       "glanat  2.813373\n",
       "glanat  3.299074\n",
       "glanat  3.103596\n",
       "glanat  4.442191\n",
       "glanat  3.588524\n",
       "glanat  4.106968\n",
       "glanat -9.374411\n",
       "glanat  3.789381\n",
       "glanat  4.624409\n",
       "glanat  3.344444"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to:  /Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_PD_DBS_STN_WURZBURG/cat12/z_score\n"
     ]
    }
   ],
   "source": [
    "path_1 = r'/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_PD_DBS_STN_WURZBURG/cat12/ROI_mni_Cerebellum_Vgm.csv'\n",
    "path_2 = r'/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/ADNI/NIFTIS/true_control/cat_12_results/roi_volumes/ROI_mni_Cerebellum_Vgm.csv'\n",
    "roi_name = 'Cerebellum'\n",
    "roi_tissue = 'Vgm'\n",
    "#----------------------------------------------------------------DO NOT CHANGE\n",
    "df_atrophy_z_scored = compute_roi_z_scores(path_1, path_2)\n",
    "display(df_atrophy_z_scored)\n",
    "df_atrophy_z_scored.to_csv(os.path.join(out_dir + f'/{roi_name}_{roi_tissue}_z_scores.csv'))\n",
    "print('saved to: ', out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract XML File Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def parse_xml(file_path):\n",
    "    # Parse XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Find required tags and extract data\n",
    "    for memory_roi in root.findall('memory_roi'):\n",
    "        Vgm = memory_roi.find('data/Vgm').text\n",
    "        Vwm = memory_roi.find('data/Vwm').text\n",
    "        Vcsf = memory_roi.find('data/Vcsf').text\n",
    "        \n",
    "        return Vgm, Vwm, Vcsf\n",
    "\n",
    "def extract_data_from_xmls(directory):\n",
    "    # Create an empty dataframe\n",
    "    data = pd.DataFrame(columns=['Patient_ID', 'GM', 'WM', 'CSF'])\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                patient_id = os.path.splitext(file)[0]  # Use filename as patient ID\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                Vgm, Vwm, Vcsf = parse_xml(file_path)\n",
    "                data = data.append({'Patient_ID': patient_id, 'GM': Vgm, 'WM': Vwm, 'CSF': Vcsf}, ignore_index=True)\n",
    "                \n",
    "    return data\n",
    "\n",
    "# Directory containing XML files\n",
    "directory = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/cat12/cat12_ultrafine-reg/CAT12.8.2_2170'\n",
    "\n",
    "# Extract data and save to CSV\n",
    "data = extract_data_from_xmls(directory)\n",
    "# data.to_csv('patient_data.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "# Fetch the MNI152 1mm white matter mask\n",
    "white_matter_mask = datasets.load_mni152_wm_mask(resolution=2)\n",
    "\n",
    "# Example usage\n",
    "mask_data = white_matter_mask.get_fdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_matter_mask = datasets.load_mni152_gm_template(resolution=2)\n",
    "gray_matter_mask.to_filename('/Users/cu135/Dropbox (Partners HealthCare)/resources/mni_spaces/6th_gen/mni_152_gm_mask.nii')\n",
    "\n",
    "w_matter_mask = datasets.load_mni152_wm_template(resolution=2)\n",
    "w_matter_mask.to_filename('/Users/cu135/Dropbox (Partners HealthCare)/resources/mni_spaces/6th_gen/mni_152_wm_mask.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "mask_data = image.load_img('/Users/cu135/Dropbox (Partners HealthCare)/resources/mni_spaces/6th_gen/mni_152_gm_mask_resampled.nii')\n",
    "plotting.view_img(mask_data, cut_coords=(0,0,0), black_bg=False, opacity=.75, cmap='ocean_hot')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gray_matter_mask = datasets.load_mni152_gm_template(resolution=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Analysis of Overlap R Map Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_voxels = 200000  # number of voxels per map\n",
    "n_iterations = 10000  # number of iterations\n",
    "threshold = 0.3  # threshold for overlap\n",
    "np.random.seed(0)  # set seed for reproducibility\n",
    "\n",
    "# store results\n",
    "results = {}\n",
    "\n",
    "for n_maps in range(2, 14):  # for each number of maps from 2 to 5\n",
    "    overlaps = 0  # counter for number of overlaps\n",
    "    pbar = tqdm(total=n_iterations, desc=f'Processing {n_maps} maps')\n",
    "    for _ in range(n_iterations):\n",
    "        # generate n_maps random maps\n",
    "        maps = [np.random.normal(0, 0.2, n_voxels) for _ in range(n_maps)]\n",
    "        # check if there's an overlap\n",
    "        if np.any(np.all([np.abs(map) > threshold for map in maps], axis=0)):\n",
    "            overlaps += 1\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "    false_positive_rate = overlaps / n_iterations\n",
    "    results[n_maps] = false_positive_rate\n",
    "\n",
    "# print results\n",
    "for n_maps, rate in results.items():\n",
    "    print(f'False positive rate for {n_maps} maps: {rate}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Damage Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damaging_thing = 'grey_matter'\n",
    "things_damaged = 'hippocampus'\n",
    "descriptor = 'all_patient'\n",
    "#----------------------------------------------------------------DO NOT MODIFY!----------------------------------------------------------------\n",
    "#Initialize dataframe\n",
    "damage_df = pd.DataFrame(index=df_1.columns, columns=df_2.columns)\n",
    "\n",
    "for matrix in damage_df.columns:\n",
    "    for subject in damage_df.index:\n",
    "        # Mask the subject dataframe to the matrix at hand\n",
    "        intersection = df_1[subject].where(df_2[matrix] > 0, 0)\n",
    "        # Weight the overlapping components by multiplication\n",
    "        weighted_overlap = intersection * df_2[matrix]\n",
    "        # Assess overall damage score\n",
    "        damage = weighted_overlap.sum()\n",
    "        damage_df.loc[subject, matrix] = damage\n",
    "\n",
    "if not os.path.exists(out_dir + '/damage_scores'):\n",
    "    os.makedirs(out_dir + '/damage_scores')\n",
    "\n",
    "damage_df.to_csv(out_dir + f'/damage_scores/{descriptor}_{things_damaged}_damage_scores_{damaging_thing}.csv')\n",
    "print('saved to: ', out_dir + f'/damage_scores/{descriptor}_{things_damaged}_damage_scores_{damaging_thing}.csv')\n",
    "\n",
    "display(damage_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Niftis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "filename = 'mwp1glanat.nii'\n",
    "base_dir = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_PD_DBS_STN_WURZBURG/neuroimaging'\n",
    "folder_to_name_by = -4\n",
    "save = True\n",
    "\n",
    "#----------------------------------------------------------------DO NOT MODIFY!----------------------------------------------------------------\n",
    "\n",
    "# Use glob to find all mwp1glanat.nii files with incorrect names\n",
    "file_paths = glob.glob(os.path.join(base_dir, '**', 'mwp1glanat_resampled.nii'), recursive=True)\n",
    "print(file_paths)\n",
    "for file_path in file_paths:\n",
    "    # Print the found file\n",
    "    print(f'Found file: {file_path}')\n",
    "    \n",
    "    # Extract the sub-id\n",
    "    sub_id = file_path.split(os.sep)[folder_to_name_by]\n",
    "    print(f'Extracted sub-id: {sub_id}')\n",
    "\n",
    "    # Construct the new file path\n",
    "    new_file_path = os.path.join(os.path.dirname(file_path), f'{sub_id}-{filename}')\n",
    "    print(f'Intended absolute filename: {new_file_path}')\n",
    "    if save:\n",
    "        shutil.copy(file_path, new_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62880161f19d28ddb8a8f59c63374d84ace356c39e36cc839cb3fb3bb03fb010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
