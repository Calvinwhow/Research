{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Connectome from Niftis\n",
    "====\n",
    "\n",
    "This notebook generates a connectome set for use with connectome_quick from 4d Nifti files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import multiprocessing\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from nilearn import image, maskers, surface\n",
    "from tqdm import tqdm \n",
    "from natsort import natsorted\n",
    "from nimlab import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Don't be a pest when multiprocessing\n",
    "os.nice(19)\n",
    "\n",
    "# Function that transforms a set of nifti files to an npy file and a norms file for use with connectome_quick\n",
    "# args is a tuple in the form (lh_files[], rh_files[], subject_name, lh_mask_file, rh_mask_file, output_dir, gsr=False)\n",
    "def transform(args):\n",
    "    lh_files = args[0]\n",
    "    rh_files = args[1]\n",
    "    subject_name = args[2]\n",
    "    lh_mask_file = args[3]\n",
    "    rh_mask_file = args[4]\n",
    "    output_dir = args[5]\n",
    "    global_signal_regression = args[6]\n",
    "    \n",
    "    lh_subject_img = np.concatenate([image.get_data(f) for f in lh_files[:2]], axis=3)\n",
    "    rh_subject_img = np.concatenate([image.get_data(f) for f in rh_files[:2]], axis=3)\n",
    "    \n",
    "    lh_mask = nib.load(lh_mask_file).agg_data().astype(bool)\n",
    "    rh_mask = nib.load(rh_mask_file).agg_data().astype(bool)\n",
    "    \n",
    "    if lh_subject_img.shape[-1] == rh_subject_img.shape[-1]:\n",
    "        timepoints = lh_subject_img.shape[-1]\n",
    "    else:\n",
    "        raise ValueError(\"Differing number of time points\")\n",
    "\n",
    "    lh_subject_img = lh_subject_img.reshape((10242, timepoints), order=\"F\")\n",
    "    rh_subject_img = rh_subject_img.reshape((10242, timepoints), order=\"F\")\n",
    "    masked = np.transpose(np.concatenate((lh_subject_img[lh_mask,:], rh_subject_img[rh_mask,:])))\n",
    "\n",
    "    # \"global\" signal regression - remove average global signal from each voxel timecourse\n",
    "    if global_signal_regression:\n",
    "        global_signal = np.mean(masked, axis=0)\n",
    "        masked = masked - global_signal\n",
    "        \n",
    "    norms = np.linalg.norm(masked, axis = 0)\n",
    "    np.save(os.path.join(output_dir,subject_name),masked.astype('float16'))\n",
    "    np.save(os.path.join(output_dir,subject_name+'_norms'),norms.astype('float16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather the files you want to convert from nifti to numpy arrays and specify where you want the output to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the files to be transformed, you may need wildcards, two examples provided:\n",
    "# files = natsorted(glob.glob(\"/lab-share/Neuro-Cohen-e2/Public/projects/GSP/CBIG_fMRI_preprocess_Legacy_GSP_500M/sub*/vol/sub*gz\"))\n",
    "# files = natsorted(glob.glob(\"/data/nimlab/Yeo_1000_nii/vol/*\"))\n",
    "\n",
    "input_dir = '/data/nimlab/connectomes/fMRI/GSP1000_MF'\n",
    "output_dir = '/data/nimlab/connectome_npy/GSP1000_MF_surf_fs5_GSR'\n",
    "\n",
    "# All NIMLAB analyses have used the FSL 2mm_brain_mask_dil file since shifting to python code (older analyses used the 222.nii.gz mask as in Lead-DBS)\n",
    "lh_mask_img = datasets.get_img_path(\"fs5_mask_lh\")\n",
    "rh_mask_img = datasets.get_img_path(\"fs5_mask_rh\")\n",
    "# lh_mask_img = os.path.abspath(\"OLD_fs5_mask_lh.gii\")\n",
    "# rh_mask_img = os.path.abspath(\"OLD_fs5_mask_rh.gii\")\n",
    "\n",
    "lh_files = natsorted(glob.glob(os.path.join(input_dir,\"sub-*/surf/lh.*fs5.nii.gz\")))\n",
    "rh_files = natsorted(glob.glob(os.path.join(input_dir,\"sub-*/surf/rh.*fs5.nii.gz\")))\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "if len(lh_files) != len(rh_files):\n",
    "    raise ValueError(\"Some subjects are missing hemispheres\")\n",
    "    \n",
    "print(lh_files[:5])\n",
    "print(rh_files[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate a list of unique subject names (which we will then use to combine runs within subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique subject names from dataset. This cell requires the user to change it based on the naming scheme of the files\n",
    "# For instance, for files named: sub-0003_bld002_rest_skip4_stc_mc_bp_0.0001_0.08_resid_FS1mm_MNI1mm_MNI2mm_sm7_finalmask.nii.gz\n",
    "# sub = f.split('/')[-1].split('_bld')[0]   will split the files using '_bld' and take what's before it to be the subject name\n",
    "\n",
    "subjects = []\n",
    "for lh, rh in zip(lh_files, rh_files):\n",
    "    sub = lh.split('/')[-1].split('_bld')[0].split(\"lh.\")[-1]\n",
    "    subjects.append(sub)\n",
    "    sub = rh.split('/')[-1].split('_bld')[0].split(\"rh.\")[-1]\n",
    "    subjects.append(sub)\n",
    "unique_subjects = list(set(subjects))\n",
    "print(len(unique_subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Makes a list of runs for each subject and set-up the arguments for conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transform function expects lists of runs that belong to the same subject so that they can be concatenated. \n",
    "# Since every dataset may have different naming conventions, you will have to test this yourself. 's' is the subject name.\n",
    "\n",
    "global_signal_regression = False\n",
    "\n",
    "subject_args = []\n",
    "for s in unique_subjects:\n",
    "    lh_runs = natsorted(glob.glob(os.path.join(input_dir,s+'*','surf','lh.'+s+'*fs5.nii.gz')))\n",
    "    rh_runs = natsorted(glob.glob(os.path.join(input_dir,s+'*','surf','rh.'+s+'*fs5.nii.gz')))\n",
    "    subject_args.append((lh_runs, rh_runs, s, lh_mask_img, rh_mask_img, output_dir, global_signal_regression))\n",
    "\n",
    "# show the results for the first five subjects\n",
    "print(subject_args[:5])\n",
    "                     \n",
    "# make sure we got everyone\n",
    "number_of_subjects = len(subject_args)\n",
    "print(number_of_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the conversion in serial or parallel as able for your computer (you want processes <= the number of cores you have access to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it!\n",
    "\n",
    "## One at a time (but without depending on multiprocessing.Pool)\n",
    "\n",
    "for i in tqdm(subject_args):\n",
    "    transform(i)\n",
    "\n",
    "    \n",
    "## Run all at once (as long as your python environment is set up correctly)\n",
    "\n",
    "# pool = multiprocessing.Pool(processes=80)\n",
    "# list(tqdm(pool.imap(transform, subject_args), total=number_of_subjects))\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Py3.9 Nimlab",
   "language": "python",
   "name": "nimlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
