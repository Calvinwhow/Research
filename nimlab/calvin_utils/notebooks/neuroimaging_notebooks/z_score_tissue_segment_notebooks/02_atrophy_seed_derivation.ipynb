{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nifti Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Directory**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a brief markup (in Markdown format) that explains the purpose and usage of the `segments_dict`:\n",
    "\n",
    "---\n",
    "\n",
    "## Neuroimaging File Extraction Dictionary\n",
    "\n",
    "The `segments_dict` is a predefined dictionary structured to facilitate the extraction of specific types of neuroimaging files. Each key in the dictionary represents a distinct neuroimaging segment, and its associated value is another dictionary containing the following fields:\n",
    "\n",
    "- **path**: This should be filled with the absolute path to the base directory containing the neuroimaging files for the corresponding segment. \n",
    "- **glob_name_pattern**: This is the string pattern that will be used to \"glob\" or search for the specific files within the provided path. It helps in identifying and extracting the desired files based on their naming conventions.\n",
    "\n",
    "Here's a breakdown of the segments and their respective fields:\n",
    "\n",
    "### 1. Cerebrospinal Fluid (CSF)\n",
    "- **path**: Absolute path to the base directory containing CSF files.\n",
    "- **glob_name_pattern**: File pattern to search for CSF files.\n",
    "\n",
    "### 2. Grey Matter\n",
    "- **path**: Absolute path to the base directory containing grey matter files.\n",
    "- **glob_name_pattern**: File pattern to search for grey matter files.\n",
    "\n",
    "### 3. White Matter\n",
    "- **path**: Absolute path to the base directory containing white matter files.\n",
    "- **glob_name_pattern**: File pattern to search for white matter files.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions**: Please fill out the `path` and `glob_name_pattern` fields for each segment in the `segments_dict`. This will ensure that the extraction process can locate and identify the appropriate neuroimaging files for further analysis.\n",
    "- < *_name_pattern > variables do not need a leading slash (\"/\"). This is already accounted for. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = r'/Users/cu135/Dropbox (Partners HealthCare)/studies/atrophy_seeds_2023/shared_analysis/niftis_for_elmira'\n",
    "grey_matter_glob_name_pattern = 'mri/*smwp1*resampled*'\n",
    "white_matter_glob_name_pattern = 'mri/*smwp2*resampled*'\n",
    "csf_glob_name_pattern = 'mri/*smwp3*resampled*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder #<----- CALVIN IMPORT\n",
    "\n",
    "def import_dataframes_from_folders(base_directory, grey_matter_glob_name_pattern, white_matter_glob_name_pattern, csf_glob_name_pattern):\n",
    "    \"\"\"\n",
    "    Imports dataframes from specified directories and glob name patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_directory (str): The base directory where the data resides.\n",
    "    - grey_matter_glob_name_pattern (str): Glob pattern for grey matter data.\n",
    "    - white_matter_glob_name_pattern (str): Glob pattern for white matter data.\n",
    "    - csf_glob_name_pattern (str): Glob pattern for cerebrospinal fluid data.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing dataframes for grey matter, white matter, and cerebrospinal fluid.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    segments_dict = {\n",
    "        'grey_matter': {'path': base_directory, 'glob_name_pattern': grey_matter_glob_name_pattern},\n",
    "        'white_matter': {'path': base_directory, 'glob_name_pattern': white_matter_glob_name_pattern},\n",
    "        'cerebrospinal_fluid': {'path': base_directory, 'glob_name_pattern': csf_glob_name_pattern}\n",
    "    }\n",
    "\n",
    "    dataframes_dict = {}\n",
    "\n",
    "    for k, v in segments_dict.items():\n",
    "        dataframes_dict[k] = import_matrices_from_folder(connectivity_path=v['path'], file_pattern=v['glob_name_pattern'])\n",
    "        print(f'Imported data {k} data with {dataframes_dict[k].shape[0]} voxels and {dataframes_dict[k].shape[1]} patients')\n",
    "        print(f'These are the filenames per subject {dataframes_dict[k].columns}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict = import_dataframes_from_folders(base_directory, grey_matter_glob_name_pattern, white_matter_glob_name_pattern, csf_glob_name_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Subject ID From File Names**\n",
    "- Using the example filenames that have been printed above, please define a general string:\n",
    "1) Preceding the subject ID. If nothing preceding subject identifier, enter \"\".\n",
    "- Do NOT include mwp[1/2/3] in this. \n",
    "2) Proceeding the subject ID. If nothing proceeding subject identifier, enter \"\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preceding_id = 's'\n",
    "proceeding_id = '_re'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_specific_mwp_integer_pattern(text):\n",
    "    # Define the pattern to search for: 'mwp' followed by [1], [2], or [3]\n",
    "    pattern = r'mwp[123]'\n",
    "    # Replace the first occurrence of the pattern with an empty string\n",
    "    return re.sub(pattern, '', text, count=1)\n",
    "\n",
    "\n",
    "def extract_and_rename_subject_id(dataframe, split_command_dict):\n",
    "    \"\"\"\n",
    "    Renames the columns of a dataframe based on specified split commands.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The dataframe whose columns need to be renamed.\n",
    "    - split_command_dict (dict): A dictionary where the key is the split string \n",
    "                                 and the value is the order to take after splitting \n",
    "                                 (0 for before the split, 1 for after the split, etc.).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Dataframe with renamed columns.\n",
    "\n",
    "    Example:\n",
    "    >>> data = {'subject_001': [1, 2, 3], 'patient_002': [4, 5, 6], 'control_003': [7, 8, 9]}\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> split_commands = {'_': 1}\n",
    "    >>> new_df = extract_and_rename_subject_id(df, split_commands)\n",
    "    >>> print(new_df.columns)\n",
    "    Index(['001', '002', '003'], dtype='object')\n",
    "    \"\"\"\n",
    "\n",
    "    raw_names = dataframe.columns\n",
    "    name_mapping = {}\n",
    "\n",
    "    # For each column name in the dataframe\n",
    "    for name in raw_names:\n",
    "        new_name = name  # Default to the original name in case it doesn't match any split command\n",
    "\n",
    "        # Check each split command to see if it applies to this column name\n",
    "        for k, v in split_command_dict.items():\n",
    "            if k in new_name:\n",
    "                new_name = remove_specific_mwp_integer_pattern(new_name)\n",
    "                if k !='':\n",
    "                    new_name = new_name.split(k)[v]\n",
    "        # Add the original and new name to the mapping\n",
    "        name_mapping[name] = new_name\n",
    "\n",
    "    # Rename columns in the dataframe based on the mapping\n",
    "    return dataframe.rename(columns=name_mapping)\n",
    "\n",
    "def rename_dataframe_subjects(dataframes_dict, preceding_id, proceeding_id):\n",
    "    \"\"\"\n",
    "    Renames the subjects in the provided dataframes based on the split commands.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary containing dataframes with subjects to be renamed.\n",
    "    - preceding_id (str): The delimiter for taking the part after the split.\n",
    "    - proceeding_id (str): The delimiter for taking the part before the split.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing dataframes with subjects renamed.\n",
    "    \"\"\"\n",
    "    \n",
    "    split_command_dict = {preceding_id: 1, proceeding_id: 0}\n",
    "    \n",
    "    for k, v in dataframes_dict.items():\n",
    "        dataframes_dict[k] = extract_and_rename_subject_id(dataframe=dataframes_dict[k], split_command_dict=split_command_dict)\n",
    "        print('Dataframe: ', k)\n",
    "        display(dataframes_dict[k])\n",
    "        print('------------- \\n')\n",
    "\n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_dfs = rename_dataframe_subjects(dataframes_dict, preceding_id, proceeding_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Control Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory_control = '/Volumes/OneTouch/resources/adni/neuroimaging/true_control/anat/mri'\n",
    "control_grey_matter_glob_name_pattern = '*smwp1*resampled*'\n",
    "control_white_matter_glob_name_pattern = '*smwp2*resampled*'\n",
    "control_csf_glob_name_pattern = '*smwp3*resampled*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "def import_control_dataframes(base_directory, control_grey_matter_glob_name_pattern, control_white_matter_glob_name_pattern, control_csf_glob_name_pattern):\n",
    "    \"\"\"\n",
    "    Imports control dataframes from specified directories and glob name patterns.\n",
    "\n",
    "    Parameters:\n",
    "    - base_directory (str): The base directory where the data resides.\n",
    "    - control_grey_matter_glob_name_pattern (str): Glob pattern for grey matter data.\n",
    "    - control_white_matter_glob_name_pattern (str): Glob pattern for white matter data.\n",
    "    - control_csf_glob_name_pattern (str): Glob pattern for cerebrospinal fluid data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing control dataframes for grey matter, white matter, and cerebrospinal fluid.\n",
    "    \"\"\"\n",
    "    \n",
    "    segments_dict = {\n",
    "        'grey_matter': {'path': base_directory, 'glob_name_pattern': control_grey_matter_glob_name_pattern},\n",
    "        'white_matter': {'path': base_directory, 'glob_name_pattern': control_white_matter_glob_name_pattern},\n",
    "        'cerebrospinal_fluid': {'path': base_directory, 'glob_name_pattern': control_csf_glob_name_pattern}\n",
    "    }\n",
    "\n",
    "    control_dataframes_dict = {}\n",
    "    for k, v in segments_dict.items():\n",
    "        control_dataframes_dict[k] = import_matrices_from_folder(connectivity_path=v['path'], file_pattern=v['glob_name_pattern']);\n",
    "        print(f'Imported data {k} data with {control_dataframes_dict[k].shape[0]} voxels and {control_dataframes_dict[k].shape[1]} patients')\n",
    "        print(f'Example subject filename: {control_dataframes_dict[k].columns[-1]}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "    return control_dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data white_matter data with 902629 voxels and 136 patients\n",
      "Example subject filename: smwp2941_S_4376_resampled.nii\n",
      "--------------------------------\n",
      "I will search:  /Volumes/OneTouch/resources/adni/neuroimaging/true_control/anat/mri/*smwp3*resampled*\n",
      "Imported data cerebrospinal_fluid data with 902629 voxels and 136 patients\n",
      "Example subject filename: smwp3941_S_4376_resampled.nii\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "control_dataframes_dict = import_control_dataframes(base_directory_control, control_grey_matter_glob_name_pattern, control_white_matter_glob_name_pattern, control_csf_glob_name_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Z-Scored Atrophy Maps for Each Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "def threshold_probabilities(patient_df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    patient_df = patient_df.where(patient_df > threshold, 0)\n",
    "    return patient_df\n",
    "\n",
    "def calculate_z_scores(control_df: pd.DataFrame, patient_df: pd.DataFrame, matter_type=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to calculate voxel-wise mean, standard deviation for control group and z-scores for patient group.\n",
    "\n",
    "    Args:\n",
    "    control_df (pd.DataFrame): DataFrame where each column represents a control subject, \n",
    "                               and each row represents flattened image data for a voxel.\n",
    "    patient_df (pd.DataFrame): DataFrame where each column represents a patient, \n",
    "                               and each row represents flattened image data for a voxel.\n",
    "\n",
    "    Returns:\n",
    "    patient_z_scores (pd.DataFrame): DataFrame of voxel-wise z-scores calculated for each patient using control mean and std.\n",
    "    \"\"\"\n",
    "\n",
    "    # # Mask the dataframes to only consider tissues over acceptable probability thresholds\n",
    "    # # Using p>0.2, as typical masking to MNI152 segments uses P > 0.2 for a given segment\n",
    "    \n",
    "    # # Now you can use the function to apply a threshold to patient_df and control_df\n",
    "    threshold = 0.2\n",
    "    patient_df = threshold_probabilities(patient_df, threshold)\n",
    "    control_df = threshold_probabilities(control_df, threshold)\n",
    "\n",
    "    # Calculate mean and standard deviation for each voxel in control group\n",
    "    control_mean = control_df.mean(axis=1)\n",
    "    control_std = control_df.std(axis=1)\n",
    "\n",
    "    # Initialize DataFrame to store patient z-scores\n",
    "    patient_z_scores = pd.DataFrame()\n",
    "\n",
    "    # Calculate z-scores for each patient using control mean and std\n",
    "    for patient in patient_df.columns:\n",
    "        patient_z_scores[patient] = (patient_df[patient] - control_mean) / control_std\n",
    "\n",
    "    # # Set values back into brain_mask\n",
    "    # # if matter_type == None:\n",
    "    # mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "    # mask_data = mni_mask.get_fdata().flatten()\n",
    "    # apply_mask = lambda patient_z_scores: np.where(mask_data > 0, patient_z_scores, 0)\n",
    "    # patient_z_scores = patient_z_scores.apply(apply_mask, axis=0)\n",
    "    # print('Not sure what matter class to mask to, returning mask within MNI152 space')\n",
    "    return patient_z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_atrophy_dataframes(dataframes_dict, control_dataframes_dict):\n",
    "    \"\"\"\n",
    "    Processes the provided dataframes to calculate z-scores and determine significant atrophy.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): Dictionary containing patient dataframes.\n",
    "    - control_dataframes_dict (dict): Dictionary containing control dataframes.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two dictionaries - atrophy_dataframes_dict and significant_atrophy_dataframes_dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    atrophy_dataframes_dict = {}\n",
    "    significant_atrophy_dataframes_dict = {}\n",
    "\n",
    "    for k in dataframes_dict.keys():\n",
    "        atrophy_dataframes_dict[k] = calculate_z_scores(control_df=control_dataframes_dict[k], patient_df=dataframes_dict[k])\n",
    "        if k == 'cerebrospinal_fluid':\n",
    "            significant_atrophy_dataframes_dict[k] = atrophy_dataframes_dict[k].where(atrophy_dataframes_dict[k] > 2, 0)\n",
    "        else:\n",
    "            significant_atrophy_dataframes_dict[k] = atrophy_dataframes_dict[k].where(atrophy_dataframes_dict[k] < -2, 0)\n",
    "        print('Dataframe: ', k)\n",
    "        display(dataframes_dict[k])\n",
    "        print('------------- \\n')\n",
    "\n",
    "    return atrophy_dataframes_dict, significant_atrophy_dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unthresholded_atrophy_dataframes_dict, significant_atrophy_dataframes_dict = process_atrophy_dataframes(dataframes_dict, control_dataframes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derive Significant Atrophy Map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def finalize_atrophy_dataframes(dataframes_dict):\n",
    "    \"\"\"\n",
    "    Summates the absolute values of DataFrames within a dictionary \n",
    "    and adds the summation as a new key-value pair with the key 'composite'.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary containing DataFrames.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: The input dictionary updated with the 'composite' key representing the summation of absolute values.\n",
    "    \n",
    "    Example:\n",
    "    >>> dfs = {\n",
    "    ...     'a': pd.DataFrame({'col1': [-1, 2], 'col2': [3, -4]}),\n",
    "    ...     'b': pd.DataFrame({'col1': [5, -6], 'col2': [-7, 8]})\n",
    "    ... }\n",
    "    >>> summed_dfs = summate_absolute_dataframes(dfs)\n",
    "    >>> print(summed_dfs['composite'])\n",
    "       col1  col2\n",
    "    0     6    10\n",
    "    1     8    12\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an empty DataFrame to store the summation of absolute values\n",
    "    composite_df = pd.DataFrame()\n",
    "    for k in dataframes_dict.keys():\n",
    "        abs_df = dataframes_dict[k].abs() # Take the absolute value of the DataFrame\n",
    "\n",
    "        if composite_df.empty:  # If the composite_df is still empty, initialize it with the first absolute DataFrame\n",
    "            composite_df = dataframes_dict[k].abs().copy()\n",
    "        else:\n",
    "            composite_df += abs_df  # Otherwise, add the absolute values to the composite DataFrame\n",
    "    \n",
    "    # Add the composite DataFrame to the dictionary with key 'composite'\n",
    "    dataframes_dict['composite'] = composite_df\n",
    "    \n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_atrophy_dataframes_dict = finalize_atrophy_dataframes(significant_atrophy_dataframes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the Atrophy Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Raw Z-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti #<-----CAlVIN IMPORT\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_nifti_to_bids(dataframes_dict, bids_base_dir, analysis='tissue_segment_z_scores', ses=None, dry_run=True):\n",
    "    \"\"\"\n",
    "    Saves NIFTI images to a BIDS directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): Dictionary containing dataframes with NIFTI data.\n",
    "    - bids_base_dir (str): The base directory where the BIDS structure starts.\n",
    "    - ses (str, optional): Session identifier. If None, defaults to '01'.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes a predefined BIDS directory structure and saves the NIFTI \n",
    "    images accordingly. The function currently has the view_and_save_nifti call commented out \n",
    "    for safety. Uncomment this call if you wish to actually save the NIFTI images.\n",
    "    \n",
    "    Example:\n",
    "    >>> dfs = { ... }  # some dictionary with dataframes\n",
    "    >>> save_nifti_to_bids(dfs, '/path/to/base/dir')\n",
    "    \"\"\"\n",
    "    \n",
    "    for k in tqdm(dataframes_dict.keys()):\n",
    "        for col in dataframes_dict[k].columns:\n",
    "            \n",
    "            # Define BIDS Directory Architecture\n",
    "            sub_no = col\n",
    "            if ses is None:\n",
    "                ses_no = '01'\n",
    "            else:\n",
    "                ses_no = ses\n",
    "            \n",
    "            # Define and Initialize the Save Directory\n",
    "            out_dir = os.path.join(bids_base_dir, f'sub-{sub_no}', f'ses-{ses_no}', analysis)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            \n",
    "            # Save Image to BIDS Directory\n",
    "            if dry_run:\n",
    "                print(out_dir+f'/sub-{sub_no}_{k}')\n",
    "            else:\n",
    "                view_and_save_nifti(matrix=dataframes_dict[k][col],\n",
    "                                    out_dir=out_dir,\n",
    "                                    output_name=(f'sub-{sub_no}_{k}'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Z-Scored Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unthresholded Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_nifti_to_bids(unthresholded_atrophy_dataframes_dict, bids_base_dir=base_directory, analysis='unthresholded_tissue_segment_z_scores', dry_run=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thresholded Maps - The 'Real' Atrophy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_nifti_to_bids(thresholded_atrophy_dataframes_dict, bids_base_dir=base_directory, analysis='thresholded_tissue_segment_z_scores', dry_run=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Done. Enjoy your atrophy seeds.\n",
    "\n",
    "--Calvin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti #<-----CAlVIN IMPORT\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_control_nifti_to_bids(dataframes_dict, out_dir=None, dry_run=True):\n",
    "    \"\"\"\n",
    "    Saves NIFTI images to a BIDS directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): Dictionary containing dataframes with NIFTI data.\n",
    "    - bids_base_dir (str): The base directory where the BIDS structure starts.\n",
    "    - ses (str, optional): Session identifier. If None, defaults to '01'.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes a predefined BIDS directory structure and saves the NIFTI \n",
    "    images accordingly. The function currently has the view_and_save_nifti call commented out \n",
    "    for safety. Uncomment this call if you wish to actually save the NIFTI images.\n",
    "    \n",
    "    Example:\n",
    "    >>> dfs = { ... }  # some dictionary with dataframes\n",
    "    >>> save_nifti_to_bids(dfs, '/path/to/base/dir')\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    for segment, dataframe in tqdm(dataframes_dict.items()):\n",
    "            dataframe['Mean'] = dataframe.mean(axis=1)\n",
    "            dataframe['Std'] = dataframe.std(axis=1)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            \n",
    "            view_and_save_nifti(matrix=dataframe['Mean'],\n",
    "                                out_dir=out_dir,\n",
    "                                output_name=(f'{segment}_mean'))\n",
    "            view_and_save_nifti(matrix=dataframe['Std'],\n",
    "                                out_dir=out_dir,\n",
    "                                output_name=(f'{segment}_stdev'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to: \n",
      " /Volumes/OneTouch/resources/adni/neuroimaging/true_control/control_distribution\n",
      "Image saved to: \n",
      " /Volumes/OneTouch/resources/adni/neuroimaging/true_control/control_distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:06<00:13,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to: \n",
      " /Volumes/OneTouch/resources/adni/neuroimaging/true_control/control_distribution\n",
      "Image saved to: \n",
      " /Volumes/OneTouch/resources/adni/neuroimaging/true_control/control_distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to: \n",
      " /Volumes/OneTouch/resources/adni/neuroimaging/true_control/control_distribution\n",
      "Image saved to: \n",
      " /Volumes/OneTouch/resources/adni/neuroimaging/true_control/control_distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:18<00:00,  6.15s/it]\n"
     ]
    }
   ],
   "source": [
    "save_control_nifti_to_bids(control_dataframes_dict, out_dir='/Volumes/OneTouch/resources/adni/neuroimaging/true_control/control_distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
