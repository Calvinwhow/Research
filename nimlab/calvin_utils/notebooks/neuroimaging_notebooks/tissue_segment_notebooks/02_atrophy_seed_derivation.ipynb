{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import platform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "subject_id = '<subject_id>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep Input and Output Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paths Input Here\n",
    "base_dir = r'/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/rawdata'\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "analysis = fr\"<subject_id>/ses-01/single_subject_atrophy\"\n",
    "out_dir = os.path.join(base_dir, analysis)\n",
    "print('I will save each file to: \\n', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nifti Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Directory**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a brief markup (in Markdown format) that explains the purpose and usage of the `segments_dict`:\n",
    "\n",
    "---\n",
    "\n",
    "## Neuroimaging File Extraction Dictionary\n",
    "\n",
    "The `segments_dict` is a predefined dictionary structured to facilitate the extraction of specific types of neuroimaging files. Each key in the dictionary represents a distinct neuroimaging segment, and its associated value is another dictionary containing the following fields:\n",
    "\n",
    "- **path**: This should be filled with the absolute path to the base directory containing the neuroimaging files for the corresponding segment. \n",
    "- **glob_name_pattern**: This is the string pattern that will be used to \"glob\" or search for the specific files within the provided path. It helps in identifying and extracting the desired files based on their naming conventions.\n",
    "\n",
    "Here's a breakdown of the segments and their respective fields:\n",
    "\n",
    "### 1. Cerebrospinal Fluid (CSF)\n",
    "- **path**: Absolute path to the base directory containing CSF files.\n",
    "- **glob_name_pattern**: File pattern to search for CSF files.\n",
    "\n",
    "### 2. Grey Matter\n",
    "- **path**: Absolute path to the base directory containing grey matter files.\n",
    "- **glob_name_pattern**: File pattern to search for grey matter files.\n",
    "\n",
    "### 3. White Matter\n",
    "- **path**: Absolute path to the base directory containing white matter files.\n",
    "- **glob_name_pattern**: File pattern to search for white matter files.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions**: Please fill out the `path` and `glob_name_pattern` fields for each segment in the `segments_dict`. This will ensure that the extraction process can locate and identify the appropriate neuroimaging files for further analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = r'/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/cat12/cat12_ultrafine-reg/CAT12.8.2_2170'\n",
    "grey_matter_glob_name_pattern = '*/*/*/*mwp1*resampled*'\n",
    "white_matter_glob_name_pattern = '*/*/*/*mwp2*resampled*'\n",
    "csf_glob_name_pattern = '*/*/*/*mwp3*resampled*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "\n",
    "def import_dataframes_from_folders(base_directory, grey_matter_glob_name_pattern, white_matter_glob_name_pattern, csf_glob_name_pattern):\n",
    "    \"\"\"\n",
    "    Imports dataframes from specified directories and glob name patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_directory (str): The base directory where the data resides.\n",
    "    - grey_matter_glob_name_pattern (str): Glob pattern for grey matter data.\n",
    "    - white_matter_glob_name_pattern (str): Glob pattern for white matter data.\n",
    "    - csf_glob_name_pattern (str): Glob pattern for cerebrospinal fluid data.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing dataframes for grey matter, white matter, and cerebrospinal fluid.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    segments_dict = {\n",
    "        'grey_matter': {'path': base_directory, 'glob_name_pattern': grey_matter_glob_name_pattern},\n",
    "        'white_matter': {'path': base_directory, 'glob_name_pattern': white_matter_glob_name_pattern},\n",
    "        'cerebrospinal_fluid': {'path': base_directory, 'glob_name_pattern': csf_glob_name_pattern}\n",
    "    }\n",
    "\n",
    "    dataframes_dict = {}\n",
    "\n",
    "    for k, v in segments_dict.items():\n",
    "        dataframes_dict[k] = import_matrices_from_folder(connectivity_path=v['path'], file_pattern=v['glob_name_pattern'])\n",
    "        print(f'Imported data {k} data with {dataframes_dict[k].shape[0]} voxels and {dataframes_dict[k].shape[1]} patients')\n",
    "        print(f'These are the filenames per subject {dataframes_dict[k].columns}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict = import_dataframes_from_folders(base_directory, grey_matter_glob_name_pattern, white_matter_glob_name_pattern, csf_glob_name_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "# from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "\n",
    "# segments_dict = {\n",
    "#     'grey_matter': {'path': base_directory, 'glob_name_pattern': grey_matter_glob_name_pattern},\n",
    "#     'white_matter': {'path': base_directory, 'glob_name_pattern': white_matter_glob_name_pattern},\n",
    "#     'cerebrospinal_fluid': {'path': base_directory, 'glob_name_pattern': csf_glob_name_pattern}\n",
    "# }\n",
    "\n",
    "# dataframes_dict = {}\n",
    "# for k, v in segments_dict.items():\n",
    "#     dataframes_dict[k] = import_matrices_from_folder(connectivity_path=v['path'], file_pattern=v['glob_name_pattern']);\n",
    "#     print(f'Imported data {k} data with {dataframes_dict[k].shape[0]} voxels and {dataframes_dict[k].shape[1]} patients')\n",
    "#     print(f'These are the filenames per subject {dataframes_dict[k].columns}')\n",
    "#     print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Subject ID From File Names**\n",
    "Using the example filenames that have been printed above, please define a general string:\n",
    "1) Preceding the subject ID.\n",
    "2) Proceeding the subject ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preceding_id = 'sub-'\n",
    "proceeding_id = '_ses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dataframe_subjects(dataframes_dict, preceding_id, proceeding_id):\n",
    "    \"\"\"\n",
    "    Renames the subjects in the provided dataframes based on the split commands.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary containing dataframes with subjects to be renamed.\n",
    "    - preceding_id (str): The delimiter for taking the part after the split.\n",
    "    - proceeding_id (str): The delimiter for taking the part before the split.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing dataframes with subjects renamed.\n",
    "    \"\"\"\n",
    "    \n",
    "    from calvin_utils.file_utils.dataframe_utilities import extract_and_rename_subject_id\n",
    "\n",
    "    split_command_dict = {preceding_id: 1, proceeding_id: 0}\n",
    "    \n",
    "    for k, v in dataframes_dict.items():\n",
    "        dataframes_dict[k] = extract_and_rename_subject_id(dataframe=dataframes_dict[k], split_command_dict=split_command_dict)\n",
    "        print('Dataframe: ', k)\n",
    "        display(dataframes_dict[k])\n",
    "        print('------------- \\n')\n",
    "\n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_dfs = rename_dataframe_subjects(dataframes_dict, preceding_id, proceeding_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Control Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/ADNI/NIFTIS/true_control/cat_12_results/cat_12mri'\n",
    "control_grey_matter_glob_name_pattern = '*mwp1*resampled*'\n",
    "control_white_matter_glob_name_pattern = '*mwp2*resampled*'\n",
    "control_csf_glob_name_pattern = '*mwp3*resampled*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_control_dataframes(base_directory, control_grey_matter_glob_name_pattern, control_white_matter_glob_name_pattern, control_csf_glob_name_pattern):\n",
    "    \"\"\"\n",
    "    Imports control dataframes from specified directories and glob name patterns.\n",
    "\n",
    "    Parameters:\n",
    "    - base_directory (str): The base directory where the data resides.\n",
    "    - control_grey_matter_glob_name_pattern (str): Glob pattern for grey matter data.\n",
    "    - control_white_matter_glob_name_pattern (str): Glob pattern for white matter data.\n",
    "    - control_csf_glob_name_pattern (str): Glob pattern for cerebrospinal fluid data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing control dataframes for grey matter, white matter, and cerebrospinal fluid.\n",
    "    \"\"\"\n",
    "    \n",
    "    from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "\n",
    "    segments_dict = {\n",
    "        'grey_matter': {'path': base_directory, 'glob_name_pattern': control_grey_matter_glob_name_pattern},\n",
    "        'white_matter': {'path': base_directory, 'glob_name_pattern': control_white_matter_glob_name_pattern},\n",
    "        'cerebrospinal_fluid': {'path': base_directory, 'glob_name_pattern': control_csf_glob_name_pattern}\n",
    "    }\n",
    "\n",
    "    control_dataframes_dict = {}\n",
    "    for k, v in segments_dict.items():\n",
    "        control_dataframes_dict[k] = import_matrices_from_folder(connectivity_path=v['path'], file_pattern=v['glob_name_pattern']);\n",
    "        print(f'Imported data {k} data with {control_dataframes_dict[k].shape[0]} voxels and {control_dataframes_dict[k].shape[1]} patients')\n",
    "        print(f'Example subject filename: {control_dataframes_dict[k].columns[-1]}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "    return control_dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dataframes_dict = import_control_dataframes(base_directory, control_grey_matter_glob_name_pattern, control_white_matter_glob_name_pattern, control_csf_glob_name_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Z-Scored Atrophy Maps for Each Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from nilearn import datasets\n",
    "from nilearn import image\n",
    "def threshold_probabilities(patient_df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    patient_df = patient_df.where(patient_df > threshold, 0)\n",
    "    return patient_df\n",
    "\n",
    "def calculate_z_scores(control_df: pd.DataFrame, patient_df: pd.DataFrame, matter_type=None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Function to calculate voxel-wise mean, standard deviation for control group and z-scores for patient group.\n",
    "\n",
    "    Args:\n",
    "    control_df (pd.DataFrame): DataFrame where each column represents a control subject, \n",
    "                               and each row represents flattened image data for a voxel.\n",
    "    patient_df (pd.DataFrame): DataFrame where each column represents a patient, \n",
    "                               and each row represents flattened image data for a voxel.\n",
    "\n",
    "    Returns:\n",
    "    patient_z_scores (pd.DataFrame): DataFrame of voxel-wise z-scores calculated for each patient using control mean and std.\n",
    "    \"\"\"\n",
    "\n",
    "    # # Mask the dataframes to only consider tissues over acceptable probability thresholds\n",
    "    # # Using p>0.2, as typical masking to MNI152 segments uses P > 0.2 for a given segment\n",
    "    \n",
    "    # # Now you can use the function to apply a threshold to patient_df and control_df\n",
    "    threshold = 0.2\n",
    "    patient_df = threshold_probabilities(patient_df, threshold)\n",
    "    control_df = threshold_probabilities(control_df, threshold)\n",
    "\n",
    "    # Calculate mean and standard deviation for each voxel in control group\n",
    "    control_mean = control_df.mean(axis=1)\n",
    "    control_std = control_df.std(axis=1)\n",
    "\n",
    "    # Initialize DataFrame to store patient z-scores\n",
    "    patient_z_scores = pd.DataFrame()\n",
    "\n",
    "    # Calculate z-scores for each patient using control mean and std\n",
    "    for patient in patient_df.columns:\n",
    "        patient_z_scores[patient] = (patient_df[patient] - control_mean) / control_std\n",
    "\n",
    "    # # Set values back into brain_mask\n",
    "    # # if matter_type == None:\n",
    "    # mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "    # mask_data = mni_mask.get_fdata().flatten()\n",
    "    # apply_mask = lambda patient_z_scores: np.where(mask_data > 0, patient_z_scores, 0)\n",
    "    # patient_z_scores = patient_z_scores.apply(apply_mask, axis=0)\n",
    "    # print('Not sure what matter class to mask to, returning mask within MNI152 space')\n",
    "    return patient_z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_atrophy_dataframes(dataframes_dict, control_dataframes_dict):\n",
    "    \"\"\"\n",
    "    Processes the provided dataframes to calculate z-scores and determine significant atrophy.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): Dictionary containing patient dataframes.\n",
    "    - control_dataframes_dict (dict): Dictionary containing control dataframes.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two dictionaries - atrophy_dataframes_dict and significant_atrophy_dataframes_dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    atrophy_dataframes_dict = {}\n",
    "    significant_atrophy_dataframes_dict = {}\n",
    "\n",
    "    for k in dataframes_dict.keys():\n",
    "        atrophy_dataframes_dict[k] = calculate_z_scores(control_df=control_dataframes_dict[k], patient_df=dataframes_dict[k])\n",
    "        if k == 'cerebrospinal_fluid':\n",
    "            significant_atrophy_dataframes_dict[k] = atrophy_dataframes_dict[k].where(atrophy_dataframes_dict[k] > 2, 0)\n",
    "        else:\n",
    "            significant_atrophy_dataframes_dict[k] = atrophy_dataframes_dict[k].where(atrophy_dataframes_dict[k] < -2, 0)\n",
    "        print('Dataframe: ', k)\n",
    "        display(dataframes_dict[k])\n",
    "        print('------------- \\n')\n",
    "\n",
    "    return atrophy_dataframes_dict, significant_atrophy_dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unthresholded_atrophy_dataframes_dict, significant_atrophy_dataframes_dict = process_atrophy_dataframes(dataframes_dict, control_dataframes_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derive Significant Atrophy Map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def finalize_atrophy_dataframes(dataframes_dict):\n",
    "    \"\"\"\n",
    "    Summates the absolute values of DataFrames within a dictionary \n",
    "    and adds the summation as a new key-value pair with the key 'composite'.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): A dictionary containing DataFrames.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: The input dictionary updated with the 'composite' key representing the summation of absolute values.\n",
    "    \n",
    "    Example:\n",
    "    >>> dfs = {\n",
    "    ...     'a': pd.DataFrame({'col1': [-1, 2], 'col2': [3, -4]}),\n",
    "    ...     'b': pd.DataFrame({'col1': [5, -6], 'col2': [-7, 8]})\n",
    "    ... }\n",
    "    >>> summed_dfs = summate_absolute_dataframes(dfs)\n",
    "    >>> print(summed_dfs['composite'])\n",
    "       col1  col2\n",
    "    0     6    10\n",
    "    1     8    12\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an empty DataFrame to store the summation of absolute values\n",
    "    composite_df = pd.DataFrame()\n",
    "    for k in dataframes_dict.keys():\n",
    "        abs_df = dataframes_dict[k].abs() # Take the absolute value of the DataFrame\n",
    "\n",
    "        if composite_df.empty:  # If the composite_df is still empty, initialize it with the first absolute DataFrame\n",
    "            composite_df = dataframes_dict[k].abs().copy()\n",
    "        else:\n",
    "            composite_df += abs_df  # Otherwise, add the absolute values to the composite DataFrame\n",
    "    \n",
    "    # Add the composite DataFrame to the dictionary with key 'composite'\n",
    "    dataframes_dict['composite'] = composite_df\n",
    "    \n",
    "    return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_atrophy_dataframes_dict = finalize_atrophy_dataframes(significant_atrophy_dataframes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the Atrophy Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Raw Z-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_nifti_to_bids(dataframes_dict, bids_base_dir, analysis='tissue_segment_z_scores', ses=None, dry_run=True):\n",
    "    \"\"\"\n",
    "    Saves NIFTI images to a BIDS directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframes_dict (dict): Dictionary containing dataframes with NIFTI data.\n",
    "    - bids_base_dir (str): The base directory where the BIDS structure starts.\n",
    "    - ses (str, optional): Session identifier. If None, defaults to '01'.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes a predefined BIDS directory structure and saves the NIFTI \n",
    "    images accordingly. The function currently has the view_and_save_nifti call commented out \n",
    "    for safety. Uncomment this call if you wish to actually save the NIFTI images.\n",
    "    \n",
    "    Example:\n",
    "    >>> dfs = { ... }  # some dictionary with dataframes\n",
    "    >>> save_nifti_to_bids(dfs, '/path/to/base/dir')\n",
    "    \"\"\"\n",
    "    \n",
    "    for k in tqdm(dataframes_dict.keys()):\n",
    "        for col in dataframes_dict[k].columns:\n",
    "            \n",
    "            # Define BIDS Directory Architecture\n",
    "            sub_no = col\n",
    "            if ses is None:\n",
    "                ses_no = '01'\n",
    "            else:\n",
    "                ses_no = ses\n",
    "            \n",
    "            # Define and Initialize the Save Directory\n",
    "            out_dir = os.path.join(bids_base_dir, f'sub-{sub_no}', f'ses-{ses_no}', analysis)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            \n",
    "            # Save Image to BIDS Directory\n",
    "            if dry_run:\n",
    "                print(out_dir+f'/sub-{sub_no}_{k}')\n",
    "            else:\n",
    "                view_and_save_nifti(matrix=dataframes_dict[k][col],\n",
    "                                    out_dir=out_dir,\n",
    "                                    output_name=(f'sub-{sub_no}_{k}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_base_dir = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/rawdata'\n",
    "save_nifti_to_bids(unthresholded_atrophy_dataframes_dict, bids_base_dir=bids_base_dir, analysis='tissue_segment_z_scores', dry_run=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save The Thresholded Data -- These are the Real Atrophy Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_nifti_to_bids(thresholded_atrophy_dataframes_dict, bids_base_dir=bids_base_dir, analysis='thresholded_tissue_segment_z_scores', dry_run=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Done. Enjoy your atrophy seeds.\n",
    "\n",
    "--Calvin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
