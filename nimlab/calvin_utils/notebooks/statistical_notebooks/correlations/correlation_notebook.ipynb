{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Correlation-Based Analyses\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "#### Last updated: July 6, 2023\n",
    "\n",
    "Use this to assess if a correlation between a dependent variable and an independent variable is statistically significant using permutation analysis. \n",
    "\n",
    "Further, follow this up with a contrast analysis which sees which categorical variables have significantly different correlations from each other. \n",
    "\n",
    "Notes:\n",
    "- To best use this notebook, you should be familar with mixed effects models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list_proper_subjects.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/supplements/age_does_not_crr_with_conn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>...</th>\n",
       "      <th>Estimated_Outcome</th>\n",
       "      <th>Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>211</td>\n",
       "      <td>58.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.415745</td>\n",
       "      <td>-0.189000</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>152</td>\n",
       "      <td>69.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.701419</td>\n",
       "      <td>-0.455000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>DECLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>208</td>\n",
       "      <td>79.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.929958</td>\n",
       "      <td>-0.669000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>DECLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>223</td>\n",
       "      <td>71.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.829972</td>\n",
       "      <td>-0.575000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>DECLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>214</td>\n",
       "      <td>76.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.244199</td>\n",
       "      <td>-0.958000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>DECLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0        101  62.0                                 -0.392857   \n",
       "1        102  77.0                                 -0.666667   \n",
       "2        103  76.0                                 -1.447368   \n",
       "3        104  65.0                                 -2.372549   \n",
       "4        105  50.0                                 -0.192982   \n",
       "..       ...   ...                                       ...   \n",
       "194      211  58.7                                       NaN   \n",
       "195      152  69.4                                       NaN   \n",
       "196      208  79.2                                       NaN   \n",
       "197      223  71.1                                       NaN   \n",
       "198      214  76.6                                       NaN   \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                             0.314066        \n",
       "1                                             0.013999        \n",
       "2                                            -0.841572        \n",
       "3                                            -1.855477        \n",
       "4                                             0.533109        \n",
       "..                                                 ...        \n",
       "194                                                NaN        \n",
       "195                                                NaN        \n",
       "196                                                NaN        \n",
       "197                                                NaN        \n",
       "198                                                NaN        \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                  0.314066                     -21.428571   \n",
       "1                                  0.013999                     -36.363636   \n",
       "2                                 -0.841572                     -78.947368   \n",
       "3                                 -1.855477                    -129.411765   \n",
       "4                                  0.533109                     -10.526316   \n",
       "..                                      ...                            ...   \n",
       "194                                     NaN                            NaN   \n",
       "195                                     NaN                            NaN   \n",
       "196                                     NaN                            NaN   \n",
       "197                                     NaN                            NaN   \n",
       "198                                     NaN                            NaN   \n",
       "\n",
       "     Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                                -1.282630                          -1.282630   \n",
       "1                                -1.760917                          -1.760917   \n",
       "2                                -0.595369                          -0.595369   \n",
       "3                                -0.945206                          -0.945206   \n",
       "4                                -1.151973                          -1.151973   \n",
       "..                                     ...                                ...   \n",
       "194                              -0.415745                          -0.189000   \n",
       "195                              -0.701419                          -0.455000   \n",
       "196                              -0.929958                          -0.669000   \n",
       "197                              -0.829972                          -0.575000   \n",
       "198                              -1.244199                          -0.958000   \n",
       "\n",
       "     Subiculum_Connectivity_T  Amnesia_Lesion_T_Map  ...  Estimated_Outcome  \\\n",
       "0                   56.864683              0.447264  ...                NaN   \n",
       "1                   52.970984              0.436157  ...                NaN   \n",
       "2                   62.459631              0.497749  ...                NaN   \n",
       "3                   59.611631              0.432617  ...                NaN   \n",
       "4                   57.928350              0.193389  ...                NaN   \n",
       "..                        ...                   ...  ...                ...   \n",
       "194                 19.900000                   NaN  ...               GOOD   \n",
       "195                 17.900000                   NaN  ...            DECLINE   \n",
       "196                 16.300000                   NaN  ...            DECLINE   \n",
       "197                 17.000000                   NaN  ...            DECLINE   \n",
       "198                 14.100000                   NaN  ...            DECLINE   \n",
       "\n",
       "     Cognitive_Baseline  Z_Scored_Cognitive_Baseline  \\\n",
       "0                    28                     1.518764   \n",
       "1                    22                     0.465551   \n",
       "2                    19                    -0.061056   \n",
       "3                    17                    -0.412127   \n",
       "4                    19                    -0.061056   \n",
       "..                  ...                          ...   \n",
       "194                 NaN                          NaN   \n",
       "195                 NaN                          NaN   \n",
       "196                 NaN                          NaN   \n",
       "197                 NaN                          NaN   \n",
       "198                 NaN                          NaN   \n",
       "\n",
       "     Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                        -1.518764   \n",
       "1                                        -0.465551   \n",
       "2                                         0.061056   \n",
       "3                                         0.412127   \n",
       "4                                         0.061056   \n",
       "..                                             ...   \n",
       "194                                            NaN   \n",
       "195                                            NaN   \n",
       "196                                            NaN   \n",
       "197                                            NaN   \n",
       "198                                            NaN   \n",
       "\n",
       "     Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                           0.72                                 0.28   \n",
       "1                           0.48                                 0.52   \n",
       "2                           0.36                                 0.64   \n",
       "3                           0.28                                 0.72   \n",
       "4                           0.36                                 0.64   \n",
       "..                           ...                                  ...   \n",
       "194                          NaN                                  NaN   \n",
       "195                          NaN                                  NaN   \n",
       "196                          NaN                                  NaN   \n",
       "197                          NaN                                  NaN   \n",
       "198                          NaN                                  NaN   \n",
       "\n",
       "     ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0         12.222658      14.493929            -1.714513            -1.227368  \n",
       "1         14.020048      15.257338            -1.155843            -1.022243  \n",
       "2         15.118727      17.376384            -0.814348            -0.452865  \n",
       "3         13.112424      15.287916            -1.437954            -1.014027  \n",
       "4         15.086568      12.951426            -0.824344            -1.641831  \n",
       "..              ...            ...                  ...                  ...  \n",
       "194             NaN            NaN                  NaN                  NaN  \n",
       "195             NaN            NaN                  NaN                  NaN  \n",
       "196             NaN            NaN                  NaN                  NaN  \n",
       "197             NaN            NaN                  NaN                  NaN  \n",
       "198             NaN            NaN                  NaN                  NaN  \n",
       "\n",
       "[199 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet='master_list_proper_subjects')\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Subiculum_T_By_Origin_Group_',\n",
       "       'Z_Scored_Subiculum_Connectivity_T', 'Subiculum_Connectivity_T',\n",
       "       'Amnesia_Lesion_T_Map', 'Memory_Network_T', 'Z_Scored_Memory_Network_R',\n",
       "       'Memory_Network_R', 'Subiculum_Grey_Matter', 'Subiculum_White_Matter',\n",
       "       'Subiculum_CSF', 'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Z_Score_Sign',\n",
       "       'Subiculum_Group_By_Inflection_Point', 'Cognitive_Outcome',\n",
       "       'Estimated_Outcome', 'Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline__Lower_is_Better_',\n",
       "       'Min_Max_Normalized_Baseline', 'MinMaxNormBaseline_Higher_is_Better',\n",
       "       'ROI_to_Alz_Max', 'ROI_to_PD_Max', 'Standardzied_AD_Max',\n",
       "       'Standardized_PD_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Z_Scored_Percent_Cognitive_Improvement', 'Subiculum_Connectivity_T', 'Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>...</th>\n",
       "      <th>Estimated_Outcome</th>\n",
       "      <th>Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>86</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598787</td>\n",
       "      <td>-0.099428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.087220</td>\n",
       "      <td>-0.621000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598787</td>\n",
       "      <td>-0.099428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598397</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>88</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.854050</td>\n",
       "      <td>2.637141</td>\n",
       "      <td>15.384615</td>\n",
       "      <td>0.269872</td>\n",
       "      <td>-0.207000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>89</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598787</td>\n",
       "      <td>-0.099428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.158639</td>\n",
       "      <td>-0.694000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>91</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.818759</td>\n",
       "      <td>0.535847</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0.469844</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0        101  62.0                                 -0.392857   \n",
       "1        102  77.0                                 -0.666667   \n",
       "2        103  76.0                                 -1.447368   \n",
       "3        104  65.0                                 -2.372549   \n",
       "4        105  50.0                                 -0.192982   \n",
       "..       ...   ...                                       ...   \n",
       "160       86  57.0                                       NaN   \n",
       "161       87  65.0                                       NaN   \n",
       "162       88  65.0                                       NaN   \n",
       "163       89  67.0                                       NaN   \n",
       "164       91  45.0                                       NaN   \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                             0.314066        \n",
       "1                                             0.013999        \n",
       "2                                            -0.841572        \n",
       "3                                            -1.855477        \n",
       "4                                             0.533109        \n",
       "..                                                 ...        \n",
       "160                                           0.598787        \n",
       "161                                           0.598787        \n",
       "162                                           5.854050        \n",
       "163                                           0.598787        \n",
       "164                                           1.818759        \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                  0.314066                     -21.428571   \n",
       "1                                  0.013999                     -36.363636   \n",
       "2                                 -0.841572                     -78.947368   \n",
       "3                                 -1.855477                    -129.411765   \n",
       "4                                  0.533109                     -10.526316   \n",
       "..                                      ...                            ...   \n",
       "160                               -0.099428                       0.000000   \n",
       "161                               -0.099428                       0.000000   \n",
       "162                                2.637141                      15.384615   \n",
       "163                               -0.099428                       0.000000   \n",
       "164                                0.535847                       3.571429   \n",
       "\n",
       "     Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                                -1.282630                          -1.282630   \n",
       "1                                -1.760917                          -1.760917   \n",
       "2                                -0.595369                          -0.595369   \n",
       "3                                -0.945206                          -0.945206   \n",
       "4                                -1.151973                          -1.151973   \n",
       "..                                     ...                                ...   \n",
       "160                              -0.087220                          -0.621000   \n",
       "161                               0.598397                           0.173000   \n",
       "162                               0.269872                          -0.207000   \n",
       "163                              -0.158639                          -0.694000   \n",
       "164                               0.469844                           0.019400   \n",
       "\n",
       "     Subiculum_Connectivity_T  Amnesia_Lesion_T_Map  ...  Estimated_Outcome  \\\n",
       "0                   56.864683              0.447264  ...                NaN   \n",
       "1                   52.970984              0.436157  ...                NaN   \n",
       "2                   62.459631              0.497749  ...                NaN   \n",
       "3                   59.611631              0.432617  ...                NaN   \n",
       "4                   57.928350              0.193389  ...                NaN   \n",
       "..                        ...                   ...  ...                ...   \n",
       "160                 22.200000                   NaN  ...                NaN   \n",
       "161                 27.000000                   NaN  ...                NaN   \n",
       "162                 24.700000                   NaN  ...                NaN   \n",
       "163                 21.700000                   NaN  ...                NaN   \n",
       "164                 26.100000                   NaN  ...                NaN   \n",
       "\n",
       "     Cognitive_Baseline  Z_Scored_Cognitive_Baseline  \\\n",
       "0                    28                     1.518764   \n",
       "1                    22                     0.465551   \n",
       "2                    19                    -0.061056   \n",
       "3                    17                    -0.412127   \n",
       "4                    19                    -0.061056   \n",
       "..                  ...                          ...   \n",
       "160                 NaN                          NaN   \n",
       "161                 NaN                          NaN   \n",
       "162                 NaN                          NaN   \n",
       "163                 NaN                          NaN   \n",
       "164                 NaN                          NaN   \n",
       "\n",
       "     Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                        -1.518764   \n",
       "1                                        -0.465551   \n",
       "2                                         0.061056   \n",
       "3                                         0.412127   \n",
       "4                                         0.061056   \n",
       "..                                             ...   \n",
       "160                                            NaN   \n",
       "161                                            NaN   \n",
       "162                                            NaN   \n",
       "163                                            NaN   \n",
       "164                                            NaN   \n",
       "\n",
       "     Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                           0.72                                 0.28   \n",
       "1                           0.48                                 0.52   \n",
       "2                           0.36                                 0.64   \n",
       "3                           0.28                                 0.72   \n",
       "4                           0.36                                 0.64   \n",
       "..                           ...                                  ...   \n",
       "160                          NaN                                  NaN   \n",
       "161                          NaN                                  NaN   \n",
       "162                          NaN                                  NaN   \n",
       "163                          NaN                                  NaN   \n",
       "164                          NaN                                  NaN   \n",
       "\n",
       "     ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0         12.222658      14.493929            -1.714513            -1.227368  \n",
       "1         14.020048      15.257338            -1.155843            -1.022243  \n",
       "2         15.118727      17.376384            -0.814348            -0.452865  \n",
       "3         13.112424      15.287916            -1.437954            -1.014027  \n",
       "4         15.086568      12.951426            -0.824344            -1.641831  \n",
       "..              ...            ...                  ...                  ...  \n",
       "160             NaN            NaN                  NaN                  NaN  \n",
       "161             NaN            NaN                  NaN                  NaN  \n",
       "162             NaN            NaN                  NaN                  NaN  \n",
       "163             NaN            NaN                  NaN                  NaN  \n",
       "164             NaN            NaN                  NaN                  NaN  \n",
       "\n",
       "[148 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'City'  # The column you'd like to evaluate\n",
    "condition = 'not'  # Thecondition to check ('equal', 'above', 'below', 'not')\n",
    "value = 'Toronto' # The value to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>...</th>\n",
       "      <th>Estimated_Outcome</th>\n",
       "      <th>Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.705128</td>\n",
       "      <td>-0.028151</td>\n",
       "      <td>-0.028151</td>\n",
       "      <td>-38.461538</td>\n",
       "      <td>-0.489205</td>\n",
       "      <td>-0.489205</td>\n",
       "      <td>63.323903</td>\n",
       "      <td>0.513828</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>15.816634</td>\n",
       "      <td>17.617107</td>\n",
       "      <td>-0.597423</td>\n",
       "      <td>-0.388183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.282051</td>\n",
       "      <td>0.435498</td>\n",
       "      <td>0.435498</td>\n",
       "      <td>-15.384615</td>\n",
       "      <td>-1.718309</td>\n",
       "      <td>-1.718309</td>\n",
       "      <td>53.317851</td>\n",
       "      <td>0.249080</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>15.524025</td>\n",
       "      <td>13.452311</td>\n",
       "      <td>-0.688373</td>\n",
       "      <td>-1.507246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.534722</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>-29.166667</td>\n",
       "      <td>-1.145694</td>\n",
       "      <td>-1.145694</td>\n",
       "      <td>57.979468</td>\n",
       "      <td>0.247062</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>0.816622</td>\n",
       "      <td>-0.816622</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>16.546984</td>\n",
       "      <td>13.932696</td>\n",
       "      <td>-0.370413</td>\n",
       "      <td>-1.378169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.557971</td>\n",
       "      <td>0.133118</td>\n",
       "      <td>0.133118</td>\n",
       "      <td>-30.434783</td>\n",
       "      <td>-0.043697</td>\n",
       "      <td>-0.043697</td>\n",
       "      <td>66.950749</td>\n",
       "      <td>0.514661</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>0.641086</td>\n",
       "      <td>-0.641086</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>19.669539</td>\n",
       "      <td>21.341523</td>\n",
       "      <td>0.600149</td>\n",
       "      <td>0.612551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.551282</td>\n",
       "      <td>-0.955451</td>\n",
       "      <td>-0.955451</td>\n",
       "      <td>-84.615385</td>\n",
       "      <td>0.240855</td>\n",
       "      <td>0.240855</td>\n",
       "      <td>69.267271</td>\n",
       "      <td>0.530628</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>18.295718</td>\n",
       "      <td>19.263977</td>\n",
       "      <td>0.173133</td>\n",
       "      <td>0.054323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>111</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>1.581744</td>\n",
       "      <td>1.581744</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>2.004069</td>\n",
       "      <td>2.004069</td>\n",
       "      <td>83.621480</td>\n",
       "      <td>0.514182</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.289805</td>\n",
       "      <td>1.289805</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.92</td>\n",
       "      <td>17.022328</td>\n",
       "      <td>16.151031</td>\n",
       "      <td>-0.222665</td>\n",
       "      <td>-0.782112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>113</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>-0.460891</td>\n",
       "      <td>-0.460891</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.301223</td>\n",
       "      <td>0.301223</td>\n",
       "      <td>69.758721</td>\n",
       "      <td>0.497508</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.763198</td>\n",
       "      <td>0.763198</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>16.225059</td>\n",
       "      <td>17.818822</td>\n",
       "      <td>-0.470475</td>\n",
       "      <td>-0.333983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-0.295699</td>\n",
       "      <td>0.420542</td>\n",
       "      <td>0.420542</td>\n",
       "      <td>-16.129032</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>67.432125</td>\n",
       "      <td>0.461019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>2.045371</td>\n",
       "      <td>-2.045371</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.16</td>\n",
       "      <td>15.295109</td>\n",
       "      <td>18.046377</td>\n",
       "      <td>-0.759525</td>\n",
       "      <td>-0.272840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>115</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.885057</td>\n",
       "      <td>-0.225336</td>\n",
       "      <td>-0.225336</td>\n",
       "      <td>-48.275862</td>\n",
       "      <td>0.472801</td>\n",
       "      <td>0.472801</td>\n",
       "      <td>71.155530</td>\n",
       "      <td>0.227943</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>1.694300</td>\n",
       "      <td>-1.694300</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.24</td>\n",
       "      <td>18.623736</td>\n",
       "      <td>16.113043</td>\n",
       "      <td>0.275089</td>\n",
       "      <td>-0.792319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>116</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-0.675439</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>-36.842105</td>\n",
       "      <td>-0.657659</td>\n",
       "      <td>-0.657659</td>\n",
       "      <td>61.952527</td>\n",
       "      <td>0.485376</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>16.462336</td>\n",
       "      <td>18.503053</td>\n",
       "      <td>-0.396724</td>\n",
       "      <td>-0.150134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>118</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-0.057292</td>\n",
       "      <td>0.681812</td>\n",
       "      <td>0.681812</td>\n",
       "      <td>-3.125000</td>\n",
       "      <td>0.643782</td>\n",
       "      <td>0.643782</td>\n",
       "      <td>72.547473</td>\n",
       "      <td>0.247475</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>2.220907</td>\n",
       "      <td>-2.220907</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.842273</td>\n",
       "      <td>19.964170</td>\n",
       "      <td>-0.589454</td>\n",
       "      <td>0.242462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>119</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-1.489583</td>\n",
       "      <td>-0.887835</td>\n",
       "      <td>-0.887835</td>\n",
       "      <td>-81.250000</td>\n",
       "      <td>-0.637257</td>\n",
       "      <td>-0.637257</td>\n",
       "      <td>62.118622</td>\n",
       "      <td>0.503218</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>17.332886</td>\n",
       "      <td>19.198042</td>\n",
       "      <td>-0.126137</td>\n",
       "      <td>0.036607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>120</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-0.509259</td>\n",
       "      <td>0.186501</td>\n",
       "      <td>0.186501</td>\n",
       "      <td>-27.777778</td>\n",
       "      <td>-0.781964</td>\n",
       "      <td>-0.781964</td>\n",
       "      <td>60.940570</td>\n",
       "      <td>0.284504</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.236591</td>\n",
       "      <td>0.236591</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>10.126563</td>\n",
       "      <td>9.630690</td>\n",
       "      <td>-2.366028</td>\n",
       "      <td>-2.534098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>121</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.079710</td>\n",
       "      <td>0.657244</td>\n",
       "      <td>0.657244</td>\n",
       "      <td>-4.347826</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>71.223601</td>\n",
       "      <td>0.483995</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>0.641086</td>\n",
       "      <td>-0.641086</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>22.476761</td>\n",
       "      <td>23.985143</td>\n",
       "      <td>1.472698</td>\n",
       "      <td>1.322880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>122</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-2.566667</td>\n",
       "      <td>-2.068210</td>\n",
       "      <td>-2.068210</td>\n",
       "      <td>-140.000000</td>\n",
       "      <td>0.190387</td>\n",
       "      <td>0.190387</td>\n",
       "      <td>68.856418</td>\n",
       "      <td>0.512125</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.763198</td>\n",
       "      <td>0.763198</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>21.466327</td>\n",
       "      <td>23.582758</td>\n",
       "      <td>1.158632</td>\n",
       "      <td>1.214761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>123</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.840498</td>\n",
       "      <td>1.840498</td>\n",
       "      <td>54.545455</td>\n",
       "      <td>-0.341607</td>\n",
       "      <td>-0.341607</td>\n",
       "      <td>64.525483</td>\n",
       "      <td>0.507649</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>19.453283</td>\n",
       "      <td>21.676897</td>\n",
       "      <td>0.532931</td>\n",
       "      <td>0.702664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>124</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-0.343750</td>\n",
       "      <td>0.367883</td>\n",
       "      <td>0.367883</td>\n",
       "      <td>-18.750000</td>\n",
       "      <td>-0.864253</td>\n",
       "      <td>-0.864253</td>\n",
       "      <td>60.270662</td>\n",
       "      <td>0.460080</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>13.457540</td>\n",
       "      <td>15.819523</td>\n",
       "      <td>-1.330684</td>\n",
       "      <td>-0.871186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>125</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-0.687500</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-37.500000</td>\n",
       "      <td>0.536747</td>\n",
       "      <td>0.536747</td>\n",
       "      <td>71.676110</td>\n",
       "      <td>0.479488</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>22.443084</td>\n",
       "      <td>24.738357</td>\n",
       "      <td>1.462230</td>\n",
       "      <td>1.525265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>126</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-1.833333</td>\n",
       "      <td>-1.264551</td>\n",
       "      <td>-1.264551</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.081582</td>\n",
       "      <td>1.081582</td>\n",
       "      <td>76.111573</td>\n",
       "      <td>0.477498</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>20.186102</td>\n",
       "      <td>22.065363</td>\n",
       "      <td>0.760708</td>\n",
       "      <td>0.807044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>127</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-1.401961</td>\n",
       "      <td>-0.791810</td>\n",
       "      <td>-0.791810</td>\n",
       "      <td>-76.470588</td>\n",
       "      <td>0.009987</td>\n",
       "      <td>0.009987</td>\n",
       "      <td>67.387791</td>\n",
       "      <td>0.501404</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>16.996513</td>\n",
       "      <td>18.394062</td>\n",
       "      <td>-0.230689</td>\n",
       "      <td>-0.179419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>128</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.381944</td>\n",
       "      <td>0.326026</td>\n",
       "      <td>0.326026</td>\n",
       "      <td>-20.833333</td>\n",
       "      <td>1.010239</td>\n",
       "      <td>1.010239</td>\n",
       "      <td>75.530777</td>\n",
       "      <td>0.511383</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>0.816622</td>\n",
       "      <td>-0.816622</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>17.204945</td>\n",
       "      <td>18.264558</td>\n",
       "      <td>-0.165904</td>\n",
       "      <td>-0.214216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>129</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-0.654762</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>-35.714286</td>\n",
       "      <td>1.271528</td>\n",
       "      <td>1.271528</td>\n",
       "      <td>77.657909</td>\n",
       "      <td>0.496203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>21.078380</td>\n",
       "      <td>22.489816</td>\n",
       "      <td>1.038049</td>\n",
       "      <td>0.921092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>130</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.130952</td>\n",
       "      <td>0.601088</td>\n",
       "      <td>0.601088</td>\n",
       "      <td>-7.142857</td>\n",
       "      <td>-0.161232</td>\n",
       "      <td>-0.161232</td>\n",
       "      <td>65.993904</td>\n",
       "      <td>0.510956</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.938734</td>\n",
       "      <td>0.938734</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.84</td>\n",
       "      <td>18.617648</td>\n",
       "      <td>20.415928</td>\n",
       "      <td>0.273197</td>\n",
       "      <td>0.363848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>131</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-0.733333</td>\n",
       "      <td>-0.059061</td>\n",
       "      <td>-0.059061</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>0.576257</td>\n",
       "      <td>0.576257</td>\n",
       "      <td>71.997755</td>\n",
       "      <td>0.458336</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>0.114480</td>\n",
       "      <td>-0.114480</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>18.410547</td>\n",
       "      <td>19.822564</td>\n",
       "      <td>0.208825</td>\n",
       "      <td>0.204413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>133</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.938667</td>\n",
       "      <td>0.938667</td>\n",
       "      <td>74.948112</td>\n",
       "      <td>0.503698</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>18.420784</td>\n",
       "      <td>19.612924</td>\n",
       "      <td>0.212007</td>\n",
       "      <td>0.148084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>134</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>1.335524</td>\n",
       "      <td>1.335524</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>0.609143</td>\n",
       "      <td>0.609143</td>\n",
       "      <td>72.265483</td>\n",
       "      <td>0.402215</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>23.613479</td>\n",
       "      <td>26.048153</td>\n",
       "      <td>1.826016</td>\n",
       "      <td>1.877202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>135</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-0.838095</td>\n",
       "      <td>-0.173870</td>\n",
       "      <td>-0.173870</td>\n",
       "      <td>-45.714286</td>\n",
       "      <td>-0.484585</td>\n",
       "      <td>-0.484585</td>\n",
       "      <td>63.361510</td>\n",
       "      <td>0.481574</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>2.747514</td>\n",
       "      <td>-2.747514</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.101986</td>\n",
       "      <td>17.647381</td>\n",
       "      <td>-0.508729</td>\n",
       "      <td>-0.380049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>137</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.416667</td>\n",
       "      <td>-0.807926</td>\n",
       "      <td>-0.807926</td>\n",
       "      <td>-77.272727</td>\n",
       "      <td>-0.210108</td>\n",
       "      <td>-0.210108</td>\n",
       "      <td>65.596015</td>\n",
       "      <td>0.486037</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>16.837360</td>\n",
       "      <td>18.171505</td>\n",
       "      <td>-0.280158</td>\n",
       "      <td>-0.239219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>138</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.174603</td>\n",
       "      <td>0.553251</td>\n",
       "      <td>0.553251</td>\n",
       "      <td>-9.523810</td>\n",
       "      <td>1.634297</td>\n",
       "      <td>1.634297</td>\n",
       "      <td>80.611188</td>\n",
       "      <td>0.471613</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>20.531868</td>\n",
       "      <td>22.371502</td>\n",
       "      <td>0.868180</td>\n",
       "      <td>0.889302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>139</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-1.941176</td>\n",
       "      <td>-1.382736</td>\n",
       "      <td>-1.382736</td>\n",
       "      <td>-105.882353</td>\n",
       "      <td>-0.768437</td>\n",
       "      <td>-0.768437</td>\n",
       "      <td>61.050691</td>\n",
       "      <td>0.520899</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>17.957341</td>\n",
       "      <td>19.952870</td>\n",
       "      <td>0.067958</td>\n",
       "      <td>0.239426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>140</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.109898</td>\n",
       "      <td>1.109898</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>0.660878</td>\n",
       "      <td>0.660878</td>\n",
       "      <td>72.686647</td>\n",
       "      <td>0.415110</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>24.418011</td>\n",
       "      <td>27.143129</td>\n",
       "      <td>2.076083</td>\n",
       "      <td>2.171417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>141</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744598</td>\n",
       "      <td>0.744598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.169749</td>\n",
       "      <td>1.169749</td>\n",
       "      <td>76.829337</td>\n",
       "      <td>0.503535</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>17.856530</td>\n",
       "      <td>19.670979</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.163683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>142</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.328286</td>\n",
       "      <td>0.328286</td>\n",
       "      <td>69.979040</td>\n",
       "      <td>0.521941</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.236591</td>\n",
       "      <td>0.236591</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>14.769545</td>\n",
       "      <td>16.131533</td>\n",
       "      <td>-0.922882</td>\n",
       "      <td>-0.787351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>143</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>-0.118729</td>\n",
       "      <td>-0.118729</td>\n",
       "      <td>66.339918</td>\n",
       "      <td>0.569043</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>20.179952</td>\n",
       "      <td>19.975031</td>\n",
       "      <td>0.758797</td>\n",
       "      <td>0.245380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>144</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-1.128205</td>\n",
       "      <td>-0.491801</td>\n",
       "      <td>-0.491801</td>\n",
       "      <td>-61.538462</td>\n",
       "      <td>-1.032653</td>\n",
       "      <td>-1.032653</td>\n",
       "      <td>58.899731</td>\n",
       "      <td>0.470849</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>11.801552</td>\n",
       "      <td>14.042861</td>\n",
       "      <td>-1.845403</td>\n",
       "      <td>-1.348567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>145</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>2.183535</td>\n",
       "      <td>2.183535</td>\n",
       "      <td>85.082502</td>\n",
       "      <td>0.420971</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.236591</td>\n",
       "      <td>0.236591</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>21.327670</td>\n",
       "      <td>22.294652</td>\n",
       "      <td>1.115534</td>\n",
       "      <td>0.868652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>146</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.351301</td>\n",
       "      <td>-0.351301</td>\n",
       "      <td>-54.545455</td>\n",
       "      <td>-2.576474</td>\n",
       "      <td>-2.576474</td>\n",
       "      <td>46.331586</td>\n",
       "      <td>0.469571</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.465341</td>\n",
       "      <td>1.465341</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>16.512422</td>\n",
       "      <td>18.197520</td>\n",
       "      <td>-0.381156</td>\n",
       "      <td>-0.232229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>147</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-0.087302</td>\n",
       "      <td>0.648924</td>\n",
       "      <td>0.648924</td>\n",
       "      <td>-4.761905</td>\n",
       "      <td>0.262965</td>\n",
       "      <td>0.262965</td>\n",
       "      <td>69.447270</td>\n",
       "      <td>0.530166</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>19.159295</td>\n",
       "      <td>19.553377</td>\n",
       "      <td>0.441553</td>\n",
       "      <td>0.132084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>148</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-3.807692</td>\n",
       "      <td>-3.428250</td>\n",
       "      <td>-3.428250</td>\n",
       "      <td>-207.692308</td>\n",
       "      <td>1.035924</td>\n",
       "      <td>1.035924</td>\n",
       "      <td>75.739873</td>\n",
       "      <td>0.489891</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>19.340269</td>\n",
       "      <td>21.114119</td>\n",
       "      <td>0.497804</td>\n",
       "      <td>0.551449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>149</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>-1.063636</td>\n",
       "      <td>-1.063636</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>-0.650900</td>\n",
       "      <td>-0.650900</td>\n",
       "      <td>62.007555</td>\n",
       "      <td>0.480527</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.640876</td>\n",
       "      <td>1.640876</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>19.062652</td>\n",
       "      <td>20.569076</td>\n",
       "      <td>0.411514</td>\n",
       "      <td>0.404998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>150</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.018573</td>\n",
       "      <td>1.018573</td>\n",
       "      <td>13.636364</td>\n",
       "      <td>0.759359</td>\n",
       "      <td>0.759359</td>\n",
       "      <td>73.488381</td>\n",
       "      <td>0.461720</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>23.954833</td>\n",
       "      <td>26.691505</td>\n",
       "      <td>1.932117</td>\n",
       "      <td>2.050067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0       101  62.0                                 -0.392857   \n",
       "1       102  77.0                                 -0.666667   \n",
       "2       103  76.0                                 -1.447368   \n",
       "3       104  65.0                                 -2.372549   \n",
       "4       105  50.0                                 -0.192982   \n",
       "5       106  66.0                                 -0.705128   \n",
       "6       107  64.0                                 -0.282051   \n",
       "7       108  60.0                                 -0.534722   \n",
       "8       109  72.0                                 -0.557971   \n",
       "9       110  72.0                                 -1.551282   \n",
       "10      111  62.0                                  0.763889   \n",
       "11      113  69.0                                 -1.100000   \n",
       "12      114  67.0                                 -0.295699   \n",
       "13      115  60.0                                 -0.885057   \n",
       "14      116  67.0                                 -0.675439   \n",
       "15      118  52.0                                 -0.057292   \n",
       "16      119  75.0                                 -1.489583   \n",
       "17      120  68.0                                 -0.509259   \n",
       "18      121  72.0                                 -0.079710   \n",
       "19      122  58.0                                 -2.566667   \n",
       "20      123  47.0                                  1.000000   \n",
       "21      124  61.0                                 -0.343750   \n",
       "22      125  73.0                                 -0.687500   \n",
       "23      126  69.0                                 -1.833333   \n",
       "24      127  74.0                                 -1.401961   \n",
       "25      128  72.0                                 -0.381944   \n",
       "26      129  69.0                                 -0.654762   \n",
       "27      130  66.0                                 -0.130952   \n",
       "28      131  68.0                                 -0.733333   \n",
       "29      133  74.0                                  0.114583   \n",
       "30      134  66.0                                  0.539216   \n",
       "31      135  57.0                                 -0.838095   \n",
       "32      137  57.0                                 -1.416667   \n",
       "33      138  72.0                                 -0.174603   \n",
       "34      139  58.0                                 -1.941176   \n",
       "35      140  73.0                                  0.333333   \n",
       "36      141  72.0                                  0.000000   \n",
       "37      142  77.0                                  0.101852   \n",
       "38      143  71.0                                  0.114583   \n",
       "39      144  79.0                                 -1.128205   \n",
       "40      145  74.0                                  0.101852   \n",
       "41      146  76.0                                 -1.000000   \n",
       "42      147  59.0                                 -0.087302   \n",
       "43      148  51.0                                 -3.807692   \n",
       "44      149  77.0                                 -1.650000   \n",
       "45      150  71.0                                  0.250000   \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                            0.314066        \n",
       "1                                            0.013999        \n",
       "2                                           -0.841572        \n",
       "3                                           -1.855477        \n",
       "4                                            0.533109        \n",
       "5                                           -0.028151        \n",
       "6                                            0.435498        \n",
       "7                                            0.158596        \n",
       "8                                            0.133118        \n",
       "9                                           -0.955451        \n",
       "10                                           1.581744        \n",
       "11                                          -0.460891        \n",
       "12                                           0.420542        \n",
       "13                                          -0.225336        \n",
       "14                                           0.004385        \n",
       "15                                           0.681812        \n",
       "16                                          -0.887835        \n",
       "17                                           0.186501        \n",
       "18                                           0.657244        \n",
       "19                                          -2.068210        \n",
       "20                                           1.840498        \n",
       "21                                           0.367883        \n",
       "22                                          -0.008833        \n",
       "23                                          -1.264551        \n",
       "24                                          -0.791810        \n",
       "25                                           0.326026        \n",
       "26                                           0.027045        \n",
       "27                                           0.601088        \n",
       "28                                          -0.059061        \n",
       "29                                           0.870170        \n",
       "30                                           1.335524        \n",
       "31                                          -0.173870        \n",
       "32                                          -0.807926        \n",
       "33                                           0.553251        \n",
       "34                                          -1.382736        \n",
       "35                                           1.109898        \n",
       "36                                           0.744598        \n",
       "37                                           0.856218        \n",
       "38                                           0.870170        \n",
       "39                                          -0.491801        \n",
       "40                                           0.856218        \n",
       "41                                          -0.351301        \n",
       "42                                           0.648924        \n",
       "43                                          -3.428250        \n",
       "44                                          -1.063636        \n",
       "45                                           1.018573        \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                 0.314066                     -21.428571   \n",
       "1                                 0.013999                     -36.363636   \n",
       "2                                -0.841572                     -78.947368   \n",
       "3                                -1.855477                    -129.411765   \n",
       "4                                 0.533109                     -10.526316   \n",
       "5                                -0.028151                     -38.461538   \n",
       "6                                 0.435498                     -15.384615   \n",
       "7                                 0.158596                     -29.166667   \n",
       "8                                 0.133118                     -30.434783   \n",
       "9                                -0.955451                     -84.615385   \n",
       "10                                1.581744                      41.666667   \n",
       "11                               -0.460891                     -60.000000   \n",
       "12                                0.420542                     -16.129032   \n",
       "13                               -0.225336                     -48.275862   \n",
       "14                                0.004385                     -36.842105   \n",
       "15                                0.681812                      -3.125000   \n",
       "16                               -0.887835                     -81.250000   \n",
       "17                                0.186501                     -27.777778   \n",
       "18                                0.657244                      -4.347826   \n",
       "19                               -2.068210                    -140.000000   \n",
       "20                                1.840498                      54.545455   \n",
       "21                                0.367883                     -18.750000   \n",
       "22                               -0.008833                     -37.500000   \n",
       "23                               -1.264551                    -100.000000   \n",
       "24                               -0.791810                     -76.470588   \n",
       "25                                0.326026                     -20.833333   \n",
       "26                                0.027045                     -35.714286   \n",
       "27                                0.601088                      -7.142857   \n",
       "28                               -0.059061                     -40.000000   \n",
       "29                                0.870170                       6.250000   \n",
       "30                                1.335524                      29.411765   \n",
       "31                               -0.173870                     -45.714286   \n",
       "32                               -0.807926                     -77.272727   \n",
       "33                                0.553251                      -9.523810   \n",
       "34                               -1.382736                    -105.882353   \n",
       "35                                1.109898                      18.181818   \n",
       "36                                0.744598                       0.000000   \n",
       "37                                0.856218                       5.555556   \n",
       "38                                0.870170                       6.250000   \n",
       "39                               -0.491801                     -61.538462   \n",
       "40                                0.856218                       5.555556   \n",
       "41                               -0.351301                     -54.545455   \n",
       "42                                0.648924                      -4.761905   \n",
       "43                               -3.428250                    -207.692308   \n",
       "44                               -1.063636                     -90.000000   \n",
       "45                                1.018573                      13.636364   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                               -1.282630                          -1.282630   \n",
       "1                               -1.760917                          -1.760917   \n",
       "2                               -0.595369                          -0.595369   \n",
       "3                               -0.945206                          -0.945206   \n",
       "4                               -1.151973                          -1.151973   \n",
       "5                               -0.489205                          -0.489205   \n",
       "6                               -1.718309                          -1.718309   \n",
       "7                               -1.145694                          -1.145694   \n",
       "8                               -0.043697                          -0.043697   \n",
       "9                                0.240855                           0.240855   \n",
       "10                               2.004069                           2.004069   \n",
       "11                               0.301223                           0.301223   \n",
       "12                               0.015433                           0.015433   \n",
       "13                               0.472801                           0.472801   \n",
       "14                              -0.657659                          -0.657659   \n",
       "15                               0.643782                           0.643782   \n",
       "16                              -0.637257                          -0.637257   \n",
       "17                              -0.781964                          -0.781964   \n",
       "18                               0.481163                           0.481163   \n",
       "19                               0.190387                           0.190387   \n",
       "20                              -0.341607                          -0.341607   \n",
       "21                              -0.864253                          -0.864253   \n",
       "22                               0.536747                           0.536747   \n",
       "23                               1.081582                           1.081582   \n",
       "24                               0.009987                           0.009987   \n",
       "25                               1.010239                           1.010239   \n",
       "26                               1.271528                           1.271528   \n",
       "27                              -0.161232                          -0.161232   \n",
       "28                               0.576257                           0.576257   \n",
       "29                               0.938667                           0.938667   \n",
       "30                               0.609143                           0.609143   \n",
       "31                              -0.484585                          -0.484585   \n",
       "32                              -0.210108                          -0.210108   \n",
       "33                               1.634297                           1.634297   \n",
       "34                              -0.768437                          -0.768437   \n",
       "35                               0.660878                           0.660878   \n",
       "36                               1.169749                           1.169749   \n",
       "37                               0.328286                           0.328286   \n",
       "38                              -0.118729                          -0.118729   \n",
       "39                              -1.032653                          -1.032653   \n",
       "40                               2.183535                           2.183535   \n",
       "41                              -2.576474                          -2.576474   \n",
       "42                               0.262965                           0.262965   \n",
       "43                               1.035924                           1.035924   \n",
       "44                              -0.650900                          -0.650900   \n",
       "45                               0.759359                           0.759359   \n",
       "\n",
       "    Subiculum_Connectivity_T  Amnesia_Lesion_T_Map  ...  Estimated_Outcome  \\\n",
       "0                  56.864683              0.447264  ...                NaN   \n",
       "1                  52.970984              0.436157  ...                NaN   \n",
       "2                  62.459631              0.497749  ...                NaN   \n",
       "3                  59.611631              0.432617  ...                NaN   \n",
       "4                  57.928350              0.193389  ...                NaN   \n",
       "5                  63.323903              0.513828  ...                NaN   \n",
       "6                  53.317851              0.249080  ...                NaN   \n",
       "7                  57.979468              0.247062  ...                NaN   \n",
       "8                  66.950749              0.514661  ...                NaN   \n",
       "9                  69.267271              0.530628  ...                NaN   \n",
       "10                 83.621480              0.514182  ...                NaN   \n",
       "11                 69.758721              0.497508  ...                NaN   \n",
       "12                 67.432125              0.461019  ...                NaN   \n",
       "13                 71.155530              0.227943  ...                NaN   \n",
       "14                 61.952527              0.485376  ...                NaN   \n",
       "15                 72.547473              0.247475  ...                NaN   \n",
       "16                 62.118622              0.503218  ...                NaN   \n",
       "17                 60.940570              0.284504  ...                NaN   \n",
       "18                 71.223601              0.483995  ...                NaN   \n",
       "19                 68.856418              0.512125  ...                NaN   \n",
       "20                 64.525483              0.507649  ...                NaN   \n",
       "21                 60.270662              0.460080  ...                NaN   \n",
       "22                 71.676110              0.479488  ...                NaN   \n",
       "23                 76.111573              0.477498  ...                NaN   \n",
       "24                 67.387791              0.501404  ...                NaN   \n",
       "25                 75.530777              0.511383  ...                NaN   \n",
       "26                 77.657909              0.496203  ...                NaN   \n",
       "27                 65.993904              0.510956  ...                NaN   \n",
       "28                 71.997755              0.458336  ...                NaN   \n",
       "29                 74.948112              0.503698  ...                NaN   \n",
       "30                 72.265483              0.402215  ...                NaN   \n",
       "31                 63.361510              0.481574  ...                NaN   \n",
       "32                 65.596015              0.486037  ...                NaN   \n",
       "33                 80.611188              0.471613  ...                NaN   \n",
       "34                 61.050691              0.520899  ...                NaN   \n",
       "35                 72.686647              0.415110  ...                NaN   \n",
       "36                 76.829337              0.503535  ...                NaN   \n",
       "37                 69.979040              0.521941  ...                NaN   \n",
       "38                 66.339918              0.569043  ...                NaN   \n",
       "39                 58.899731              0.470849  ...                NaN   \n",
       "40                 85.082502              0.420971  ...                NaN   \n",
       "41                 46.331586              0.469571  ...                NaN   \n",
       "42                 69.447270              0.530166  ...                NaN   \n",
       "43                 75.739873              0.489891  ...                NaN   \n",
       "44                 62.007555              0.480527  ...                NaN   \n",
       "45                 73.488381              0.461720  ...                NaN   \n",
       "\n",
       "    Cognitive_Baseline  Z_Scored_Cognitive_Baseline  \\\n",
       "0                   28                     1.518764   \n",
       "1                   22                     0.465551   \n",
       "2                   19                    -0.061056   \n",
       "3                   17                    -0.412127   \n",
       "4                   19                    -0.061056   \n",
       "5                   13                    -1.114269   \n",
       "6                   13                    -1.114269   \n",
       "7                   24                     0.816622   \n",
       "8                   23                     0.641086   \n",
       "9                   13                    -1.114269   \n",
       "10                  12                    -1.289805   \n",
       "11                  15                    -0.763198   \n",
       "12                  31                     2.045371   \n",
       "13                  29                     1.694300   \n",
       "14                  19                    -0.061056   \n",
       "15                  32                     2.220907   \n",
       "16                  16                    -0.587663   \n",
       "17                  18                    -0.236591   \n",
       "18                  23                     0.641086   \n",
       "19                  15                    -0.763198   \n",
       "20                  22                     0.465551   \n",
       "21                  16                    -0.587663   \n",
       "22                  16                    -0.587663   \n",
       "23                  21                     0.290015   \n",
       "24                  17                    -0.412127   \n",
       "25                  24                     0.816622   \n",
       "26                  28                     1.518764   \n",
       "27                  14                    -0.938734   \n",
       "28                  20                     0.114480   \n",
       "29                  16                    -0.587663   \n",
       "30                  17                    -0.412127   \n",
       "31                  35                     2.747514   \n",
       "32                  22                     0.465551   \n",
       "33                  21                     0.290015   \n",
       "34                  17                    -0.412127   \n",
       "35                  22                     0.465551   \n",
       "36                  19                    -0.061056   \n",
       "37                  18                    -0.236591   \n",
       "38                  16                    -0.587663   \n",
       "39                  13                    -1.114269   \n",
       "40                  18                    -0.236591   \n",
       "41                  11                    -1.465341   \n",
       "42                  21                     0.290015   \n",
       "43                  13                    -1.114269   \n",
       "44                  10                    -1.640876   \n",
       "45                  22                     0.465551   \n",
       "\n",
       "    Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                       -1.518764   \n",
       "1                                       -0.465551   \n",
       "2                                        0.061056   \n",
       "3                                        0.412127   \n",
       "4                                        0.061056   \n",
       "5                                        1.114269   \n",
       "6                                        1.114269   \n",
       "7                                       -0.816622   \n",
       "8                                       -0.641086   \n",
       "9                                        1.114269   \n",
       "10                                       1.289805   \n",
       "11                                       0.763198   \n",
       "12                                      -2.045371   \n",
       "13                                      -1.694300   \n",
       "14                                       0.061056   \n",
       "15                                      -2.220907   \n",
       "16                                       0.587663   \n",
       "17                                       0.236591   \n",
       "18                                      -0.641086   \n",
       "19                                       0.763198   \n",
       "20                                      -0.465551   \n",
       "21                                       0.587663   \n",
       "22                                       0.587663   \n",
       "23                                      -0.290015   \n",
       "24                                       0.412127   \n",
       "25                                      -0.816622   \n",
       "26                                      -1.518764   \n",
       "27                                       0.938734   \n",
       "28                                      -0.114480   \n",
       "29                                       0.587663   \n",
       "30                                       0.412127   \n",
       "31                                      -2.747514   \n",
       "32                                      -0.465551   \n",
       "33                                      -0.290015   \n",
       "34                                       0.412127   \n",
       "35                                      -0.465551   \n",
       "36                                       0.061056   \n",
       "37                                       0.236591   \n",
       "38                                       0.587663   \n",
       "39                                       1.114269   \n",
       "40                                       0.236591   \n",
       "41                                       1.465341   \n",
       "42                                      -0.290015   \n",
       "43                                       1.114269   \n",
       "44                                       1.640876   \n",
       "45                                      -0.465551   \n",
       "\n",
       "    Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                          0.72                                 0.28   \n",
       "1                          0.48                                 0.52   \n",
       "2                          0.36                                 0.64   \n",
       "3                          0.28                                 0.72   \n",
       "4                          0.36                                 0.64   \n",
       "5                          0.12                                 0.88   \n",
       "6                          0.12                                 0.88   \n",
       "7                          0.56                                 0.44   \n",
       "8                          0.52                                 0.48   \n",
       "9                          0.12                                 0.88   \n",
       "10                         0.08                                 0.92   \n",
       "11                         0.20                                 0.80   \n",
       "12                         0.84                                 0.16   \n",
       "13                         0.76                                 0.24   \n",
       "14                         0.36                                 0.64   \n",
       "15                         0.88                                 0.12   \n",
       "16                         0.24                                 0.76   \n",
       "17                         0.32                                 0.68   \n",
       "18                         0.52                                 0.48   \n",
       "19                         0.20                                 0.80   \n",
       "20                         0.48                                 0.52   \n",
       "21                         0.24                                 0.76   \n",
       "22                         0.24                                 0.76   \n",
       "23                         0.44                                 0.56   \n",
       "24                         0.28                                 0.72   \n",
       "25                         0.56                                 0.44   \n",
       "26                         0.72                                 0.28   \n",
       "27                         0.16                                 0.84   \n",
       "28                         0.40                                 0.60   \n",
       "29                         0.24                                 0.76   \n",
       "30                         0.28                                 0.72   \n",
       "31                         1.00                                 0.00   \n",
       "32                         0.48                                 0.52   \n",
       "33                         0.44                                 0.56   \n",
       "34                         0.28                                 0.72   \n",
       "35                         0.48                                 0.52   \n",
       "36                         0.36                                 0.64   \n",
       "37                         0.32                                 0.68   \n",
       "38                         0.24                                 0.76   \n",
       "39                         0.12                                 0.88   \n",
       "40                         0.32                                 0.68   \n",
       "41                         0.04                                 0.96   \n",
       "42                         0.44                                 0.56   \n",
       "43                         0.12                                 0.88   \n",
       "44                         0.00                                 1.00   \n",
       "45                         0.48                                 0.52   \n",
       "\n",
       "    ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0        12.222658      14.493929            -1.714513            -1.227368  \n",
       "1        14.020048      15.257338            -1.155843            -1.022243  \n",
       "2        15.118727      17.376384            -0.814348            -0.452865  \n",
       "3        13.112424      15.287916            -1.437954            -1.014027  \n",
       "4        15.086568      12.951426            -0.824344            -1.641831  \n",
       "5        15.816634      17.617107            -0.597423            -0.388183  \n",
       "6        15.524025      13.452311            -0.688373            -1.507246  \n",
       "7        16.546984      13.932696            -0.370413            -1.378169  \n",
       "8        19.669539      21.341523             0.600149             0.612551  \n",
       "9        18.295718      19.263977             0.173133             0.054323  \n",
       "10       17.022328      16.151031            -0.222665            -0.782112  \n",
       "11       16.225059      17.818822            -0.470475            -0.333983  \n",
       "12       15.295109      18.046377            -0.759525            -0.272840  \n",
       "13       18.623736      16.113043             0.275089            -0.792319  \n",
       "14       16.462336      18.503053            -0.396724            -0.150134  \n",
       "15       15.842273      19.964170            -0.589454             0.242462  \n",
       "16       17.332886      19.198042            -0.126137             0.036607  \n",
       "17       10.126563       9.630690            -2.366028            -2.534098  \n",
       "18       22.476761      23.985143             1.472698             1.322880  \n",
       "19       21.466327      23.582758             1.158632             1.214761  \n",
       "20       19.453283      21.676897             0.532931             0.702664  \n",
       "21       13.457540      15.819523            -1.330684            -0.871186  \n",
       "22       22.443084      24.738357             1.462230             1.525265  \n",
       "23       20.186102      22.065363             0.760708             0.807044  \n",
       "24       16.996513      18.394062            -0.230689            -0.179419  \n",
       "25       17.204945      18.264558            -0.165904            -0.214216  \n",
       "26       21.078380      22.489816             1.038049             0.921092  \n",
       "27       18.617648      20.415928             0.273197             0.363848  \n",
       "28       18.410547      19.822564             0.208825             0.204413  \n",
       "29       18.420784      19.612924             0.212007             0.148084  \n",
       "30       23.613479      26.048153             1.826016             1.877202  \n",
       "31       16.101986      17.647381            -0.508729            -0.380049  \n",
       "32       16.837360      18.171505            -0.280158            -0.239219  \n",
       "33       20.531868      22.371502             0.868180             0.889302  \n",
       "34       17.957341      19.952870             0.067958             0.239426  \n",
       "35       24.418011      27.143129             2.076083             2.171417  \n",
       "36       17.856530      19.670979             0.036624             0.163683  \n",
       "37       14.769545      16.131533            -0.922882            -0.787351  \n",
       "38       20.179952      19.975031             0.758797             0.245380  \n",
       "39       11.801552      14.042861            -1.845403            -1.348567  \n",
       "40       21.327670      22.294652             1.115534             0.868652  \n",
       "41       16.512422      18.197520            -0.381156            -0.232229  \n",
       "42       19.159295      19.553377             0.441553             0.132084  \n",
       "43       19.340269      21.114119             0.497804             0.551449  \n",
       "44       19.062652      20.569076             0.411514             0.404998  \n",
       "45       23.954833      26.691505             1.932117             2.050067  \n",
       "\n",
       "[46 rows x 44 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Age',  'Subiculum_Connectivity_T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to standardize column Disease: Could not convert ParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinson to numeric\n",
      "Unable to standardize column City: Could not convert WurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburgWurzburg to numeric\n",
      "Unable to standardize column Age_Group: Could not convert youngyoungyoungyoungyoungoldyoungyoungyoungyoungyoungyoungyoungyoungyoungyoungoldoldyoungyoungyoungyoungyoungyoungyoungyoung to numeric\n",
      "Unable to standardize column Age_And_Disease: Could not convert Wurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_oldWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_oldWurzburg_oldWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_youngWurzburg_young to numeric\n",
      "Unable to standardize column Age_Disease_and_Cohort: Could not convert Wurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_old_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_old_ParkinsonWurzburg_old_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_ParkinsonWurzburg_young_Parkinson to numeric\n",
      "Unable to standardize column Subiculum_Group_By_Z_Score_Sign: Could not convert highlowhighlowhighhighlowlowhighlowhighhighlowhighhighlowlowhighlowhighhighlowlowlowlowlow to numeric\n",
      "Unable to standardize column Subiculum_Group_By_Inflection_Point: Could not convert highlowhighlowhighhighlowlowhighlowhighhighlowlowhighlowlowhighlowhighhighlowlowlowlowlow to numeric\n",
      "Unable to standardize column Cognitive_Outcome: Could not convert declineimproveimprovedeclineimproveimprovedeclineimproveimproveimprovedeclineimproveimproveimprovedeclineimproveimproveimproveimproveimproveimproveimproveimproveimprovedeclinedecline to numeric\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>...</th>\n",
       "      <th>Estimated_Outcome</th>\n",
       "      <th>Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-1.614033</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.607729</td>\n",
       "      <td>-1.607729</td>\n",
       "      <td>-1.607729</td>\n",
       "      <td>-1.607729</td>\n",
       "      <td>1.050889</td>\n",
       "      <td>1.050889</td>\n",
       "      <td>30.376565</td>\n",
       "      <td>1.595922</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.18612</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>1.070622</td>\n",
       "      <td>0.440491</td>\n",
       "      <td>1.070622</td>\n",
       "      <td>0.440491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-1.521803</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.319953</td>\n",
       "      <td>1.319953</td>\n",
       "      <td>1.319953</td>\n",
       "      <td>1.319953</td>\n",
       "      <td>-0.985072</td>\n",
       "      <td>-0.985072</td>\n",
       "      <td>16.295870</td>\n",
       "      <td>-0.754608</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-0.832138</td>\n",
       "      <td>-1.837101</td>\n",
       "      <td>-0.832138</td>\n",
       "      <td>-1.837101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-1.429572</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>1.126581</td>\n",
       "      <td>1.126581</td>\n",
       "      <td>30.900051</td>\n",
       "      <td>-0.124002</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.250035</td>\n",
       "      <td>0.686676</td>\n",
       "      <td>1.250035</td>\n",
       "      <td>0.686676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-1.337342</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.287847</td>\n",
       "      <td>-0.287847</td>\n",
       "      <td>-0.287847</td>\n",
       "      <td>-0.287847</td>\n",
       "      <td>-0.274873</td>\n",
       "      <td>-0.274873</td>\n",
       "      <td>21.207602</td>\n",
       "      <td>-0.293543</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.703645</td>\n",
       "      <td>0.215384</td>\n",
       "      <td>-0.703645</td>\n",
       "      <td>0.215384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-1.152881</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0.067831</td>\n",
       "      <td>0.067831</td>\n",
       "      <td>23.577739</td>\n",
       "      <td>-0.462348</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2538</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.278272</td>\n",
       "      <td>0.097268</td>\n",
       "      <td>0.278272</td>\n",
       "      <td>0.097268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-1.060651</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.248059</td>\n",
       "      <td>0.248059</td>\n",
       "      <td>0.248059</td>\n",
       "      <td>0.248059</td>\n",
       "      <td>1.959062</td>\n",
       "      <td>1.959062</td>\n",
       "      <td>36.657479</td>\n",
       "      <td>1.205097</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.18612</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>1.457516</td>\n",
       "      <td>1.472620</td>\n",
       "      <td>1.457516</td>\n",
       "      <td>1.472620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.876190</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.287847</td>\n",
       "      <td>-0.287847</td>\n",
       "      <td>-0.287847</td>\n",
       "      <td>-0.287847</td>\n",
       "      <td>-0.455878</td>\n",
       "      <td>-0.455878</td>\n",
       "      <td>19.955774</td>\n",
       "      <td>-0.705828</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.959382</td>\n",
       "      <td>-0.366948</td>\n",
       "      <td>-0.959382</td>\n",
       "      <td>-0.366948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.691729</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.253582</td>\n",
       "      <td>0.253582</td>\n",
       "      <td>0.253582</td>\n",
       "      <td>0.253582</td>\n",
       "      <td>-1.149857</td>\n",
       "      <td>-1.149857</td>\n",
       "      <td>15.156220</td>\n",
       "      <td>-0.787678</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>-1.163398</td>\n",
       "      <td>-0.801365</td>\n",
       "      <td>-1.163398</td>\n",
       "      <td>-0.801365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.599498</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.754732</td>\n",
       "      <td>0.754732</td>\n",
       "      <td>28.328345</td>\n",
       "      <td>0.948260</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.385722</td>\n",
       "      <td>-2.385722</td>\n",
       "      <td>-2.385722</td>\n",
       "      <td>-2.385722</td>\n",
       "      <td>-2.385722</td>\n",
       "      <td>0.980426</td>\n",
       "      <td>0.554314</td>\n",
       "      <td>0.980426</td>\n",
       "      <td>0.554314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.415037</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.774576</td>\n",
       "      <td>0.774576</td>\n",
       "      <td>0.774576</td>\n",
       "      <td>0.774576</td>\n",
       "      <td>-0.504571</td>\n",
       "      <td>-0.504571</td>\n",
       "      <td>19.619016</td>\n",
       "      <td>-0.380867</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>-0.941562</td>\n",
       "      <td>-0.328713</td>\n",
       "      <td>-0.941562</td>\n",
       "      <td>-0.328713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.322807</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-2.342312</td>\n",
       "      <td>-2.342312</td>\n",
       "      <td>-2.342312</td>\n",
       "      <td>-2.342312</td>\n",
       "      <td>2.343476</td>\n",
       "      <td>2.343476</td>\n",
       "      <td>39.316089</td>\n",
       "      <td>2.565513</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.325293</td>\n",
       "      <td>2.300159</td>\n",
       "      <td>1.325293</td>\n",
       "      <td>2.300159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.138346</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.998571</td>\n",
       "      <td>0.998571</td>\n",
       "      <td>30.014730</td>\n",
       "      <td>0.030778</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.365938</td>\n",
       "      <td>0.681196</td>\n",
       "      <td>1.365938</td>\n",
       "      <td>0.681196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.046115</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.782099</td>\n",
       "      <td>0.782099</td>\n",
       "      <td>0.782099</td>\n",
       "      <td>0.782099</td>\n",
       "      <td>-1.059392</td>\n",
       "      <td>-1.059392</td>\n",
       "      <td>15.781874</td>\n",
       "      <td>0.384339</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.047671</td>\n",
       "      <td>-1.390992</td>\n",
       "      <td>-1.047671</td>\n",
       "      <td>-1.390992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.046115</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>23.128431</td>\n",
       "      <td>0.190930</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>0.452392</td>\n",
       "      <td>0.258833</td>\n",
       "      <td>0.452392</td>\n",
       "      <td>0.258833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.230576</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-3.172267</td>\n",
       "      <td>-3.172267</td>\n",
       "      <td>-3.172267</td>\n",
       "      <td>-3.172267</td>\n",
       "      <td>0.420446</td>\n",
       "      <td>0.420446</td>\n",
       "      <td>26.016428</td>\n",
       "      <td>1.981519</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2538</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.663237</td>\n",
       "      <td>0.849504</td>\n",
       "      <td>0.663237</td>\n",
       "      <td>0.849504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.322807</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>-0.456257</td>\n",
       "      <td>-0.456257</td>\n",
       "      <td>19.953155</td>\n",
       "      <td>-0.721337</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.013355</td>\n",
       "      <td>0.015025</td>\n",
       "      <td>0.013355</td>\n",
       "      <td>0.015025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.415037</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-0.017053</td>\n",
       "      <td>-0.017053</td>\n",
       "      <td>-0.017053</td>\n",
       "      <td>-0.017053</td>\n",
       "      <td>-1.097288</td>\n",
       "      <td>-1.097288</td>\n",
       "      <td>15.519790</td>\n",
       "      <td>-0.865102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.18612</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.931233</td>\n",
       "      <td>-1.746428</td>\n",
       "      <td>-0.931233</td>\n",
       "      <td>-1.746428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.691729</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.196578</td>\n",
       "      <td>0.196578</td>\n",
       "      <td>24.468157</td>\n",
       "      <td>0.607863</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>-0.626041</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.858178</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.858178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.783959</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.622962</td>\n",
       "      <td>-0.622962</td>\n",
       "      <td>18.800226</td>\n",
       "      <td>-0.674535</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>-0.231927</td>\n",
       "      <td>-0.585690</td>\n",
       "      <td>-0.231927</td>\n",
       "      <td>-0.585690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.876190</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.325800</td>\n",
       "      <td>1.325800</td>\n",
       "      <td>1.325800</td>\n",
       "      <td>1.325800</td>\n",
       "      <td>1.328612</td>\n",
       "      <td>1.328612</td>\n",
       "      <td>32.297293</td>\n",
       "      <td>-0.219190</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>1.499679</td>\n",
       "      <td>1.099034</td>\n",
       "      <td>1.499679</td>\n",
       "      <td>1.099034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.968420</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.854046</td>\n",
       "      <td>0.854046</td>\n",
       "      <td>29.015201</td>\n",
       "      <td>0.116164</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.18612</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>-0.186120</td>\n",
       "      <td>1.040773</td>\n",
       "      <td>1.110343</td>\n",
       "      <td>1.040773</td>\n",
       "      <td>1.110343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.060651</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.006007</td>\n",
       "      <td>-0.641116</td>\n",
       "      <td>-0.641116</td>\n",
       "      <td>18.674670</td>\n",
       "      <td>-1.063729</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>1.133641</td>\n",
       "      <td>-0.401090</td>\n",
       "      <td>-0.261177</td>\n",
       "      <td>-0.401090</td>\n",
       "      <td>-0.261177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.152881</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.054924</td>\n",
       "      <td>1.054924</td>\n",
       "      <td>1.054924</td>\n",
       "      <td>1.054924</td>\n",
       "      <td>-1.121400</td>\n",
       "      <td>-1.121400</td>\n",
       "      <td>15.353030</td>\n",
       "      <td>-0.773277</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>-1.505881</td>\n",
       "      <td>-0.932724</td>\n",
       "      <td>-0.860601</td>\n",
       "      <td>-0.932724</td>\n",
       "      <td>-0.860601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.337342</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.009638</td>\n",
       "      <td>-0.009638</td>\n",
       "      <td>-0.009638</td>\n",
       "      <td>-0.009638</td>\n",
       "      <td>-1.165184</td>\n",
       "      <td>-1.165184</td>\n",
       "      <td>15.050219</td>\n",
       "      <td>-1.702573</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>-1.682439</td>\n",
       "      <td>-1.274935</td>\n",
       "      <td>-1.682439</td>\n",
       "      <td>-1.274935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.614033</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-0.532447</td>\n",
       "      <td>-0.532447</td>\n",
       "      <td>-0.532447</td>\n",
       "      <td>-0.532447</td>\n",
       "      <td>-0.828023</td>\n",
       "      <td>-0.828023</td>\n",
       "      <td>17.382020</td>\n",
       "      <td>-0.906784</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>-1.129383</td>\n",
       "      <td>-0.169670</td>\n",
       "      <td>-1.129383</td>\n",
       "      <td>-0.169670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.706264</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.293610</td>\n",
       "      <td>-0.293610</td>\n",
       "      <td>-0.293610</td>\n",
       "      <td>-0.293610</td>\n",
       "      <td>-0.741815</td>\n",
       "      <td>-0.741815</td>\n",
       "      <td>17.978233</td>\n",
       "      <td>0.809016</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-1.065961</td>\n",
       "      <td>-0.901727</td>\n",
       "      <td>-1.015405</td>\n",
       "      <td>-0.901727</td>\n",
       "      <td>-1.015405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "46 -1.614033  57.0                                 -1.607729   \n",
       "47 -1.521803  50.0                                  1.319953   \n",
       "48 -1.429572  62.0                                 -0.006007   \n",
       "49 -1.337342  50.0                                 -0.287847   \n",
       "50 -1.152881  60.0                                  0.249926   \n",
       "51 -1.060651  73.0                                  0.248059   \n",
       "52 -0.876190  64.0                                 -0.287847   \n",
       "53 -0.691729  62.0                                  0.253582   \n",
       "54 -0.599498  54.0                                  0.513172   \n",
       "55 -0.415037  49.0                                  0.774576   \n",
       "56 -0.322807  46.0                                 -2.342312   \n",
       "57 -0.138346  52.0                                  0.513172   \n",
       "58 -0.046115  46.0                                  0.782099   \n",
       "59  0.046115  64.0                                 -0.006007   \n",
       "60  0.230576  58.0                                 -3.172267   \n",
       "61  0.322807  57.0                                  0.513172   \n",
       "62  0.415037  68.0                                 -0.017053   \n",
       "64  0.691729  79.0                                  0.513172   \n",
       "65  0.783959  61.0                                 -0.006007   \n",
       "66  0.876190  56.0                                  1.325800   \n",
       "67  0.968420  47.0                                  0.513172   \n",
       "68  1.060651  58.0                                 -0.006007   \n",
       "69  1.152881  64.0                                  1.054924   \n",
       "70  1.337342  60.0                                 -0.009638   \n",
       "72  1.614033  52.0                                 -0.532447   \n",
       "73  1.706264  64.0                                 -0.293610   \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "46                                          -1.607729        \n",
       "47                                           1.319953        \n",
       "48                                          -0.006007        \n",
       "49                                          -0.287847        \n",
       "50                                           0.249926        \n",
       "51                                           0.248059        \n",
       "52                                          -0.287847        \n",
       "53                                           0.253582        \n",
       "54                                           0.513172        \n",
       "55                                           0.774576        \n",
       "56                                          -2.342312        \n",
       "57                                           0.513172        \n",
       "58                                           0.782099        \n",
       "59                                          -0.006007        \n",
       "60                                          -3.172267        \n",
       "61                                           0.513172        \n",
       "62                                          -0.017053        \n",
       "64                                           0.513172        \n",
       "65                                          -0.006007        \n",
       "66                                           1.325800        \n",
       "67                                           0.513172        \n",
       "68                                          -0.006007        \n",
       "69                                           1.054924        \n",
       "70                                          -0.009638        \n",
       "72                                          -0.532447        \n",
       "73                                          -0.293610        \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "46                               -1.607729                      -1.607729   \n",
       "47                                1.319953                       1.319953   \n",
       "48                               -0.006007                      -0.006007   \n",
       "49                               -0.287847                      -0.287847   \n",
       "50                                0.249926                       0.249926   \n",
       "51                                0.248059                       0.248059   \n",
       "52                               -0.287847                      -0.287847   \n",
       "53                                0.253582                       0.253582   \n",
       "54                                0.513172                       0.513172   \n",
       "55                                0.774576                       0.774576   \n",
       "56                               -2.342312                      -2.342312   \n",
       "57                                0.513172                       0.513172   \n",
       "58                                0.782099                       0.782099   \n",
       "59                               -0.006007                      -0.006007   \n",
       "60                               -3.172267                      -3.172267   \n",
       "61                                0.513172                       0.513172   \n",
       "62                               -0.017053                      -0.017053   \n",
       "64                                0.513172                       0.513172   \n",
       "65                               -0.006007                      -0.006007   \n",
       "66                                1.325800                       1.325800   \n",
       "67                                0.513172                       0.513172   \n",
       "68                               -0.006007                      -0.006007   \n",
       "69                                1.054924                       1.054924   \n",
       "70                               -0.009638                      -0.009638   \n",
       "72                               -0.532447                      -0.532447   \n",
       "73                               -0.293610                      -0.293610   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "46                               1.050889                           1.050889   \n",
       "47                              -0.985072                          -0.985072   \n",
       "48                               1.126581                           1.126581   \n",
       "49                              -0.274873                          -0.274873   \n",
       "50                               0.067831                           0.067831   \n",
       "51                               1.959062                           1.959062   \n",
       "52                              -0.455878                          -0.455878   \n",
       "53                              -1.149857                          -1.149857   \n",
       "54                               0.754732                           0.754732   \n",
       "55                              -0.504571                          -0.504571   \n",
       "56                               2.343476                           2.343476   \n",
       "57                               0.998571                           0.998571   \n",
       "58                              -1.059392                          -1.059392   \n",
       "59                               0.002864                           0.002864   \n",
       "60                               0.420446                           0.420446   \n",
       "61                              -0.456257                          -0.456257   \n",
       "62                              -1.097288                          -1.097288   \n",
       "64                               0.196578                           0.196578   \n",
       "65                              -0.622962                          -0.622962   \n",
       "66                               1.328612                           1.328612   \n",
       "67                               0.854046                           0.854046   \n",
       "68                              -0.641116                          -0.641116   \n",
       "69                              -1.121400                          -1.121400   \n",
       "70                              -1.165184                          -1.165184   \n",
       "72                              -0.828023                          -0.828023   \n",
       "73                              -0.741815                          -0.741815   \n",
       "\n",
       "    Subiculum_Connectivity_T  Amnesia_Lesion_T_Map  ...  Estimated_Outcome  \\\n",
       "46                 30.376565              1.595922  ...                NaN   \n",
       "47                 16.295870             -0.754608  ...                NaN   \n",
       "48                 30.900051             -0.124002  ...                NaN   \n",
       "49                 21.207602             -0.293543  ...                NaN   \n",
       "50                 23.577739             -0.462348  ...                NaN   \n",
       "51                 36.657479              1.205097  ...                NaN   \n",
       "52                 19.955774             -0.705828  ...                NaN   \n",
       "53                 15.156220             -0.787678  ...                NaN   \n",
       "54                 28.328345              0.948260  ...                NaN   \n",
       "55                 19.619016             -0.380867  ...                NaN   \n",
       "56                 39.316089              2.565513  ...                NaN   \n",
       "57                 30.014730              0.030778  ...                NaN   \n",
       "58                 15.781874              0.384339  ...                NaN   \n",
       "59                 23.128431              0.190930  ...                NaN   \n",
       "60                 26.016428              1.981519  ...                NaN   \n",
       "61                 19.953155             -0.721337  ...                NaN   \n",
       "62                 15.519790             -0.865102  ...                NaN   \n",
       "64                 24.468157              0.607863  ...                NaN   \n",
       "65                 18.800226             -0.674535  ...                NaN   \n",
       "66                 32.297293             -0.219190  ...                NaN   \n",
       "67                 29.015201              0.116164  ...                NaN   \n",
       "68                 18.674670             -1.063729  ...                NaN   \n",
       "69                 15.353030             -0.773277  ...                NaN   \n",
       "70                 15.050219             -1.702573  ...                NaN   \n",
       "72                 17.382020             -0.906784  ...                NaN   \n",
       "73                 17.978233              0.809016  ...                NaN   \n",
       "\n",
       "    Cognitive_Baseline  Z_Scored_Cognitive_Baseline  \\\n",
       "46            -0.18612                    -0.186120   \n",
       "47           -1.065961                    -1.065961   \n",
       "48            1.133641                     1.133641   \n",
       "49           -0.626041                    -0.626041   \n",
       "50              0.2538                     0.253800   \n",
       "51            -0.18612                    -0.186120   \n",
       "52           -0.626041                    -0.626041   \n",
       "53            1.133641                     1.133641   \n",
       "54           -2.385722                    -2.385722   \n",
       "55            0.693721                     0.693721   \n",
       "56            1.133641                     1.133641   \n",
       "57            1.133641                     1.133641   \n",
       "58           -1.065961                    -1.065961   \n",
       "59            1.133641                     1.133641   \n",
       "60              0.2538                     0.253800   \n",
       "61            0.693721                     0.693721   \n",
       "62            -0.18612                    -0.186120   \n",
       "64           -0.626041                    -0.626041   \n",
       "65            1.133641                     1.133641   \n",
       "66           -1.505881                    -1.505881   \n",
       "67            -0.18612                    -0.186120   \n",
       "68            1.133641                     1.133641   \n",
       "69           -1.505881                    -1.505881   \n",
       "70            0.693721                     0.693721   \n",
       "72            0.693721                     0.693721   \n",
       "73           -1.065961                    -1.065961   \n",
       "\n",
       "    Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "46                                      -0.186120   \n",
       "47                                      -1.065961   \n",
       "48                                       1.133641   \n",
       "49                                      -0.626041   \n",
       "50                                       0.253800   \n",
       "51                                      -0.186120   \n",
       "52                                      -0.626041   \n",
       "53                                       1.133641   \n",
       "54                                      -2.385722   \n",
       "55                                       0.693721   \n",
       "56                                       1.133641   \n",
       "57                                       1.133641   \n",
       "58                                      -1.065961   \n",
       "59                                       1.133641   \n",
       "60                                       0.253800   \n",
       "61                                       0.693721   \n",
       "62                                      -0.186120   \n",
       "64                                      -0.626041   \n",
       "65                                       1.133641   \n",
       "66                                      -1.505881   \n",
       "67                                      -0.186120   \n",
       "68                                       1.133641   \n",
       "69                                      -1.505881   \n",
       "70                                       0.693721   \n",
       "72                                       0.693721   \n",
       "73                                      -1.065961   \n",
       "\n",
       "    Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "46                    -0.186120                            -0.186120   \n",
       "47                    -1.065961                            -1.065961   \n",
       "48                     1.133641                             1.133641   \n",
       "49                    -0.626041                            -0.626041   \n",
       "50                     0.253800                             0.253800   \n",
       "51                    -0.186120                            -0.186120   \n",
       "52                    -0.626041                            -0.626041   \n",
       "53                     1.133641                             1.133641   \n",
       "54                    -2.385722                            -2.385722   \n",
       "55                     0.693721                             0.693721   \n",
       "56                     1.133641                             1.133641   \n",
       "57                     1.133641                             1.133641   \n",
       "58                    -1.065961                            -1.065961   \n",
       "59                     1.133641                             1.133641   \n",
       "60                     0.253800                             0.253800   \n",
       "61                     0.693721                             0.693721   \n",
       "62                    -0.186120                            -0.186120   \n",
       "64                    -0.626041                            -0.626041   \n",
       "65                     1.133641                             1.133641   \n",
       "66                    -1.505881                            -1.505881   \n",
       "67                    -0.186120                            -0.186120   \n",
       "68                     1.133641                             1.133641   \n",
       "69                    -1.505881                            -1.505881   \n",
       "70                     0.693721                             0.693721   \n",
       "72                     0.693721                             0.693721   \n",
       "73                    -1.065961                            -1.065961   \n",
       "\n",
       "    ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "46        1.070622       0.440491             1.070622             0.440491  \n",
       "47       -0.832138      -1.837101            -0.832138            -1.837101  \n",
       "48        1.250035       0.686676             1.250035             0.686676  \n",
       "49       -0.703645       0.215384            -0.703645             0.215384  \n",
       "50        0.278272       0.097268             0.278272             0.097268  \n",
       "51        1.457516       1.472620             1.457516             1.472620  \n",
       "52       -0.959382      -0.366948            -0.959382            -0.366948  \n",
       "53       -1.163398      -0.801365            -1.163398            -0.801365  \n",
       "54        0.980426       0.554314             0.980426             0.554314  \n",
       "55       -0.941562      -0.328713            -0.941562            -0.328713  \n",
       "56        1.325293       2.300159             1.325293             2.300159  \n",
       "57        1.365938       0.681196             1.365938             0.681196  \n",
       "58       -1.047671      -1.390992            -1.047671            -1.390992  \n",
       "59        0.452392       0.258833             0.452392             0.258833  \n",
       "60        0.663237       0.849504             0.663237             0.849504  \n",
       "61        0.013355       0.015025             0.013355             0.015025  \n",
       "62       -0.931233      -1.746428            -0.931233            -1.746428  \n",
       "64        0.460784       0.858178             0.460784             0.858178  \n",
       "65       -0.231927      -0.585690            -0.231927            -0.585690  \n",
       "66        1.499679       1.099034             1.499679             1.099034  \n",
       "67        1.040773       1.110343             1.040773             1.110343  \n",
       "68       -0.401090      -0.261177            -0.401090            -0.261177  \n",
       "69       -0.932724      -0.860601            -0.932724            -0.860601  \n",
       "70       -1.682439      -1.274935            -1.682439            -1.274935  \n",
       "72       -1.129383      -0.169670            -1.129383            -0.169670  \n",
       "73       -0.901727      -1.015405            -0.901727            -1.015405  \n",
       "\n",
       "[26 rows x 44 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>...</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>Inclusion_Cohort</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21.258065</td>\n",
       "      <td>58.192308</td>\n",
       "      <td>-0.806348</td>\n",
       "      <td>-0.018912</td>\n",
       "      <td>-0.018912</td>\n",
       "      <td>-1.752931</td>\n",
       "      <td>0.038438</td>\n",
       "      <td>0.038438</td>\n",
       "      <td>23.079733</td>\n",
       "      <td>-0.385586</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.022728</td>\n",
       "      <td>-0.022728</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>16.054394</td>\n",
       "      <td>17.200621</td>\n",
       "      <td>0.027969</td>\n",
       "      <td>0.058555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.236469</td>\n",
       "      <td>8.173221</td>\n",
       "      <td>1.346635</td>\n",
       "      <td>1.010696</td>\n",
       "      <td>1.010696</td>\n",
       "      <td>2.927468</td>\n",
       "      <td>0.992208</td>\n",
       "      <td>0.992208</td>\n",
       "      <td>6.946440</td>\n",
       "      <td>0.164685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.008093</td>\n",
       "      <td>1.008093</td>\n",
       "      <td>0.307391</td>\n",
       "      <td>0.307391</td>\n",
       "      <td>5.823400</td>\n",
       "      <td>6.964462</td>\n",
       "      <td>1.003726</td>\n",
       "      <td>0.959144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>-4.535211</td>\n",
       "      <td>-2.817552</td>\n",
       "      <td>-2.817552</td>\n",
       "      <td>-9.859155</td>\n",
       "      <td>-1.171189</td>\n",
       "      <td>-1.171189</td>\n",
       "      <td>14.611144</td>\n",
       "      <td>-0.659502</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.164991</td>\n",
       "      <td>-2.164991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.944940</td>\n",
       "      <td>4.942970</td>\n",
       "      <td>-1.714504</td>\n",
       "      <td>-1.629565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.500000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>-0.989260</td>\n",
       "      <td>-0.156194</td>\n",
       "      <td>-0.156194</td>\n",
       "      <td>-2.150565</td>\n",
       "      <td>-0.732825</td>\n",
       "      <td>-0.732825</td>\n",
       "      <td>17.680126</td>\n",
       "      <td>-0.499729</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.730204</td>\n",
       "      <td>-0.730204</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>10.934497</td>\n",
       "      <td>11.801028</td>\n",
       "      <td>-0.854500</td>\n",
       "      <td>-0.685075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>-0.638889</td>\n",
       "      <td>0.106772</td>\n",
       "      <td>0.106772</td>\n",
       "      <td>-1.388889</td>\n",
       "      <td>-0.228971</td>\n",
       "      <td>-0.228971</td>\n",
       "      <td>21.207602</td>\n",
       "      <td>-0.426115</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.115295</td>\n",
       "      <td>-0.115295</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>14.809775</td>\n",
       "      <td>16.308768</td>\n",
       "      <td>-0.186554</td>\n",
       "      <td>-0.064270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.500000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586280</td>\n",
       "      <td>0.586280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.837188</td>\n",
       "      <td>0.837188</td>\n",
       "      <td>28.671773</td>\n",
       "      <td>-0.285509</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.909553</td>\n",
       "      <td>0.909553</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>21.688034</td>\n",
       "      <td>22.127247</td>\n",
       "      <td>0.998988</td>\n",
       "      <td>0.737049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.336814</td>\n",
       "      <td>1.336814</td>\n",
       "      <td>2.173913</td>\n",
       "      <td>2.357590</td>\n",
       "      <td>2.357590</td>\n",
       "      <td>39.316089</td>\n",
       "      <td>0.047449</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.114522</td>\n",
       "      <td>1.114522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.315090</td>\n",
       "      <td>33.143996</td>\n",
       "      <td>1.796511</td>\n",
       "      <td>2.254272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject        Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "count  31.000000  26.000000                                 31.000000   \n",
       "mean   21.258065  58.192308                                 -0.806348   \n",
       "std    12.236469   8.173221                                  1.346635   \n",
       "min     1.000000  46.000000                                 -4.535211   \n",
       "25%    11.500000  52.000000                                 -0.989260   \n",
       "50%    22.000000  58.000000                                 -0.638889   \n",
       "75%    30.500000  63.500000                                  0.000000   \n",
       "max    43.000000  79.000000                                  1.000000   \n",
       "\n",
       "       Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "count                                          31.000000        \n",
       "mean                                           -0.018912        \n",
       "std                                             1.010696        \n",
       "min                                            -2.817552        \n",
       "25%                                            -0.156194        \n",
       "50%                                             0.106772        \n",
       "75%                                             0.586280        \n",
       "max                                             1.336814        \n",
       "\n",
       "       Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "count                               31.000000                      31.000000   \n",
       "mean                                -0.018912                      -1.752931   \n",
       "std                                  1.010696                       2.927468   \n",
       "min                                 -2.817552                      -9.859155   \n",
       "25%                                 -0.156194                      -2.150565   \n",
       "50%                                  0.106772                      -1.388889   \n",
       "75%                                  0.586280                       0.000000   \n",
       "max                                  1.336814                       2.173913   \n",
       "\n",
       "       Z_Scored_Subiculum_T_By_Origin_Group_  \\\n",
       "count                              31.000000   \n",
       "mean                                0.038438   \n",
       "std                                 0.992208   \n",
       "min                                -1.171189   \n",
       "25%                                -0.732825   \n",
       "50%                                -0.228971   \n",
       "75%                                 0.837188   \n",
       "max                                 2.357590   \n",
       "\n",
       "       Z_Scored_Subiculum_Connectivity_T  Subiculum_Connectivity_T  \\\n",
       "count                          31.000000                 31.000000   \n",
       "mean                            0.038438                 23.079733   \n",
       "std                             0.992208                  6.946440   \n",
       "min                            -1.171189                 14.611144   \n",
       "25%                            -0.732825                 17.680126   \n",
       "50%                            -0.228971                 21.207602   \n",
       "75%                             0.837188                 28.671773   \n",
       "max                             2.357590                 39.316089   \n",
       "\n",
       "       Amnesia_Lesion_T_Map  ...  Cohort  Inclusion_Cohort  \\\n",
       "count             31.000000  ...    31.0              31.0   \n",
       "mean              -0.385586  ...     2.0               1.0   \n",
       "std                0.164685  ...     0.0               0.0   \n",
       "min               -0.659502  ...     2.0               1.0   \n",
       "25%               -0.499729  ...     2.0               1.0   \n",
       "50%               -0.426115  ...     2.0               1.0   \n",
       "75%               -0.285509  ...     2.0               1.0   \n",
       "max                0.047449  ...     2.0               1.0   \n",
       "\n",
       "       Z_Scored_Cognitive_Baseline  \\\n",
       "count                    31.000000   \n",
       "mean                     -0.022728   \n",
       "std                       1.008093   \n",
       "min                      -2.164991   \n",
       "25%                      -0.730204   \n",
       "50%                      -0.115295   \n",
       "75%                       0.909553   \n",
       "max                       1.114522   \n",
       "\n",
       "       Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "count                                      31.000000   \n",
       "mean                                       -0.022728   \n",
       "std                                         1.008093   \n",
       "min                                        -2.164991   \n",
       "25%                                        -0.730204   \n",
       "50%                                        -0.115295   \n",
       "75%                                         0.909553   \n",
       "max                                         1.114522   \n",
       "\n",
       "       Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "count                    31.000000                            31.000000   \n",
       "mean                      0.653226                             0.653226   \n",
       "std                       0.307391                             0.307391   \n",
       "min                       0.000000                             0.000000   \n",
       "25%                       0.437500                             0.437500   \n",
       "50%                       0.625000                             0.625000   \n",
       "75%                       0.937500                             0.937500   \n",
       "max                       1.000000                             1.000000   \n",
       "\n",
       "       ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "count       31.000000      31.000000            31.000000            31.000000  \n",
       "mean        16.054394      17.200621             0.027969             0.058555  \n",
       "std          5.823400       6.964462             1.003726             0.959144  \n",
       "min          5.944940       4.942970            -1.714504            -1.629565  \n",
       "25%         10.934497      11.801028            -0.854500            -0.685075  \n",
       "50%         14.809775      16.308768            -0.186554            -0.064270  \n",
       "75%         21.688034      22.127247             0.998988             0.737049  \n",
       "max         26.315090      33.143996             1.796511             2.254272  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miscellaneous Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Subiculum_T_By_Origin_Group_',\n",
       "       'Z_Scored_Subiculum_Connectivity_T', 'Subiculum_Connectivity_T',\n",
       "       'Amnesia_Lesion_T_Map', 'Memory_Network_T', 'Z_Scored_Memory_Network_R',\n",
       "       'Memory_Network_R', 'Subiculum_Grey_Matter', 'Subiculum_White_Matter',\n",
       "       'Subiculum_CSF', 'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Z_Score_Sign',\n",
       "       'Subiculum_Group_By_Inflection_Point', 'Cognitive_Outcome',\n",
       "       'Estimated_Outcome', 'Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline__Lower_is_Better_',\n",
       "       'Min_Max_Normalized_Baseline', 'MinMaxNormBaseline_Higher_is_Better',\n",
       "       'ROI_to_Alz_Max', 'ROI_to_PD_Max', 'Standardzied_AD_Max',\n",
       "       'Standardized_PD_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_values = ['MTLSurface', 'MTLGM', 'MTLWM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MTLSurface', 'MTLGM', 'MTLWM']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[listval for listval in list_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data_df is your DataFrame\n",
    "for column in data_df.columns:\n",
    "    for val in list_values:\n",
    "        if val == column:\n",
    "            data_df[column] = data_df[column] * -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Perform Basic Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "\n",
    "class ScatterplotGenerator:\n",
    "    def __init__(self, dataframe, data_dict, x_label='xlabel', y_label='ylabel', correlation='pearson', palette='tab10', out_dir=None, rows_per_fig=2, cols_per_fig=3, ylim=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.data_dict = data_dict\n",
    "        self.x_label = x_label\n",
    "        self.y_label = y_label\n",
    "        self.correlation = correlation\n",
    "        self.out_dir = out_dir if out_dir else 'scatterplot_figures'\n",
    "        self.palette = palette\n",
    "        self.rows_per_fig = rows_per_fig\n",
    "        self.cols_per_fig = cols_per_fig\n",
    "        self.figures = []\n",
    "        self.ylim=ylim\n",
    "\n",
    "    def set_palette(self):\n",
    "        sns.set_style('white')\n",
    "        if isinstance(self.palette, str) and self.palette.startswith('#'):\n",
    "            sns.set_palette(sns.color_palette([self.palette]))\n",
    "        else:\n",
    "            sns.set_palette(self.palette, 1, desat=1)\n",
    "\n",
    "    def plot_scatter_with_kde(self):\n",
    "        fig, axes = plt.subplots(self.rows_per_fig, self.cols_per_fig, figsize=(self.cols_per_fig * 5, self.rows_per_fig * 5))\n",
    "        axes = axes.flatten()\n",
    "        current_ax = 0\n",
    "\n",
    "        for dependent_var, independent_vars in self.data_dict.items():\n",
    "            self.dependent_var=dependent_var\n",
    "            for independent_var in independent_vars:\n",
    "                if current_ax >= len(axes):\n",
    "                    self.figures.append(fig)\n",
    "                    fig, axes = plt.subplots(self.rows_per_fig, self.cols_per_fig, figsize=(self.cols_per_fig * 5, self.rows_per_fig * 5))\n",
    "                    axes = axes.flatten()\n",
    "                    current_ax = 0\n",
    "\n",
    "                sns.regplot(x=independent_var, y=dependent_var, data=self.dataframe, ax=axes[current_ax])\n",
    "\n",
    "                if self.correlation == 'pearson':\n",
    "                    r, p = pearsonr(self.dataframe[independent_var], self.dataframe[dependent_var])\n",
    "                elif self.correlation == 'spearman':\n",
    "                    r, p = spearmanr(self.dataframe[independent_var], self.dataframe[dependent_var])\n",
    "                elif self.correlation == 'kendall':\n",
    "                    r, p = kendalltau(self.dataframe[independent_var], self.dataframe[dependent_var])\n",
    "                else:\n",
    "                    raise ValueError(f'Correlation {self.correlation} not specified, please select \"pearson\", \"kendall\" or \"spearman\"')\n",
    "                \n",
    "                if self.ylim is not None:\n",
    "                    axes[current_ax].set_ylim(self.ylim[0], self.ylim[1])\n",
    "                \n",
    "                axes[current_ax].set_title(independent_var)\n",
    "                axes[current_ax].annotate(f\"{self.correlation.capitalize()} r = {r:.2f}, p = {p:.3f}\", xy=(.05, .95), xycoords='axes fraction', horizontalalignment='left', verticalalignment='top')\n",
    "                axes[current_ax].set_xlabel(self.x_label)\n",
    "                axes[current_ax].set_ylabel(self.y_label)\n",
    "\n",
    "                current_ax += 1\n",
    "\n",
    "        self.figures.append(fig)  # Append the last figure\n",
    "\n",
    "    def save_plots(self):\n",
    "        if self.out_dir:\n",
    "            os.makedirs(self.out_dir, exist_ok=True)\n",
    "            for i, fig in enumerate(self.figures):\n",
    "                fig.savefig(os.path.join(self.out_dir, f'{self.dependent_var}{i}.png'))\n",
    "                fig.savefig(os.path.join(self.out_dir, f'{self.dependent_var}{i}.svg'))\n",
    "                print(f'Figure saved to: {os.path.join(self.out_dir, f\"{self.dependent_var}{i}\")}')\n",
    "                plt.show()\n",
    "\n",
    "    def run(self):\n",
    "        self.set_palette()\n",
    "        self.plot_scatter_with_kde()\n",
    "        self.save_plots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Variables to Correlate\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables to Correlate\n",
    "- dependent_variable = 'Z_Scored_Percent_Cognitive_Improvement'\n",
    "- independent_variable_list = ['Z_Scored_Cognitive_Baseline']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Subiculum_T_By_Origin_Group_',\n",
       "       'Z_Scored_Subiculum_Connectivity_T', 'Subiculum_Connectivity_T',\n",
       "       'Amnesia_Lesion_T_Map', 'Memory_Network_T', 'Z_Scored_Memory_Network_R',\n",
       "       'Memory_Network_R', 'Subiculum_Grey_Matter', 'Subiculum_White_Matter',\n",
       "       'Subiculum_CSF', 'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Z_Score_Sign',\n",
       "       'Subiculum_Group_By_Inflection_Point', 'Cognitive_Outcome',\n",
       "       'Estimated_Outcome', 'Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline__Lower_is_Better_',\n",
       "       'Min_Max_Normalized_Baseline', 'MinMaxNormBaseline_Higher_is_Better',\n",
       "       'ROI_to_Alz_Max', 'ROI_to_PD_Max', 'Standardzied_AD_Max',\n",
       "       'Standardized_PD_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable = 'Age'\n",
    "independent_variable_list = ['Subiculum_Connectivity_T', 'Memory_Network_T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Correlation Method\n",
    "- Options: 'spearman', 'pearson', 'kendall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = 'pearson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define plot Labels\n",
    "- These are the axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axis Labels\n",
    "x_label = 'Connectivity'\n",
    "y_label = 'Z-Scored Cognitive Improvement (MDRS)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved to: /Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/supplements/age_does_not_crr_with_conn/Age0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage Example\n",
    "generator = ScatterplotGenerator(dataframe=data_df, data_dict={dependent_variable: independent_variable_list}, \n",
    "                                 x_label=x_label, y_label=y_label, correlation=correlation, \n",
    "                                 out_dir=out_dir, palette='Blues',\n",
    "                                 rows_per_fig=1, cols_per_fig=2,\n",
    "                                 ylim=None)\n",
    "generator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run for a Bunch of Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_vars = ['Subiculum_Connectivity_T', 'Memory_Network_T']\n",
    "independent_variable_list = ['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dep_var \u001b[38;5;129;01min\u001b[39;00m dep_vars:\n\u001b[1;32m      2\u001b[0m     generator \u001b[38;5;241m=\u001b[39m ScatterplotGenerator(dataframe\u001b[38;5;241m=\u001b[39mdata_df, data_dict\u001b[38;5;241m=\u001b[39m{dep_var: independent_variable_list}, \n\u001b[1;32m      3\u001b[0m                                  x_label\u001b[38;5;241m=\u001b[39mx_label, y_label\u001b[38;5;241m=\u001b[39mdep_var, correlation\u001b[38;5;241m=\u001b[39mcorrelation, \n\u001b[1;32m      4\u001b[0m                                  out_dir\u001b[38;5;241m=\u001b[39mout_dir, palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                  rows_per_fig\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cols_per_fig\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                  ylim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[161], line 76\u001b[0m, in \u001b[0;36mScatterplotGenerator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_palette()\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_scatter_with_kde\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_plots()\n",
      "Cell \u001b[0;32mIn[161], line 45\u001b[0m, in \u001b[0;36mScatterplotGenerator.plot_scatter_with_kde\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m sns\u001b[38;5;241m.\u001b[39mregplot(x\u001b[38;5;241m=\u001b[39mindependent_var, y\u001b[38;5;241m=\u001b[39mdependent_var, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe, ax\u001b[38;5;241m=\u001b[39maxes[current_ax])\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrelation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     r, p \u001b[38;5;241m=\u001b[39m \u001b[43mpearsonr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindependent_var\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdependent_var\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrelation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspearman\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     47\u001b[0m     r, p \u001b[38;5;241m=\u001b[39m spearmanr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe[independent_var], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe[dependent_var])\n",
      "File \u001b[0;32m/usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4454\u001b[0m, in \u001b[0;36mpearsonr\u001b[0;34m(x, y, alternative)\u001b[0m\n\u001b[1;32m   4449\u001b[0m ym \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mastype(dtype) \u001b[38;5;241m-\u001b[39m ymean\n\u001b[1;32m   4451\u001b[0m \u001b[38;5;66;03m# Unlike np.linalg.norm or the expression sqrt((xm*xm).sum()),\u001b[39;00m\n\u001b[1;32m   4452\u001b[0m \u001b[38;5;66;03m# scipy.linalg.norm(xm) does not overflow if xm is, for example,\u001b[39;00m\n\u001b[1;32m   4453\u001b[0m \u001b[38;5;66;03m# [-5e210, 5e210, 3e200, -3e200]\u001b[39;00m\n\u001b[0;32m-> 4454\u001b[0m normxm \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4455\u001b[0m normym \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mnorm(ym)\n\u001b[1;32m   4457\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-13\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/lib/python3.10/site-packages/scipy/linalg/_misc.py:145\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(a, ord, axis, keepdims, check_finite)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Differs from numpy only in non-finite handling and the use of blas.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_finite:\n\u001b[0;32m--> 145\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray_chkfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "File \u001b[0;32m/usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/lib/python3.10/site-packages/numpy/lib/function_base.py:627\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    625\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dep_var in dep_vars:\n",
    "    generator = ScatterplotGenerator(dataframe=data_df, data_dict={dep_var: independent_variable_list}, \n",
    "                                 x_label=x_label, y_label=dep_var, correlation=correlation, \n",
    "                                 out_dir=out_dir, palette='Blues',\n",
    "                                 rows_per_fig=1, cols_per_fig=2,\n",
    "                                 ylim=None)\n",
    "    generator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Run ANCOVA-Style Analysis Using Correlation\n",
    "- AKA a 'Delta-R Analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannotations.Annotator import Annotator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class ANCOVACorrelation:\n",
    "    def __init__(self, df, dependent_variable, independent_variable, categorical_variable, spearman=False, out_dir=None, n_permutations=10000):\n",
    "        self.df = df\n",
    "        self.dependent_variable = dependent_variable\n",
    "        self.independent_variable = independent_variable\n",
    "        self.categorical_variable = categorical_variable\n",
    "        self.out_dir = out_dir\n",
    "        # Initialize a dictionary to store dataframes for each category\n",
    "        self.category_dataframes = {}\n",
    "        \n",
    "        # Initialize a dictionary to store observed correlations\n",
    "        self.observed_correlations = {}\n",
    "        \n",
    "        # Call the segregator function to split the data\n",
    "        self.segregate_data()\n",
    "        self.spearman=spearman\n",
    "        \n",
    "        self.n_permutations=n_permutations\n",
    "    \n",
    "    def segregate_data(self):\n",
    "        # Group the data by unique values of the categorical variable\n",
    "        unique_categories = self.df[self.categorical_variable].unique()\n",
    "        \n",
    "        # Create separate dataframes for each category\n",
    "        for category in unique_categories:\n",
    "            self.category_dataframes[category] = self.df[self.df[self.categorical_variable] == category]\n",
    "    \n",
    "    def calculate_correlation(self, category):\n",
    "        # Calculate the correlation between independent and dependent variables for a given category\n",
    "        category_df = self.category_dataframes.get(category)\n",
    "        if category_df is not None:\n",
    "            if self.spearman:\n",
    "                correlation = category_df[self.independent_variable].corr(category_df[self.dependent_variable], method='spearman')\n",
    "            else:\n",
    "                correlation = category_df[self.independent_variable].corr(category_df[self.dependent_variable])\n",
    "            self.observed_correlations[category] = correlation\n",
    "        else:\n",
    "            print(f\"Category '{category}' not found in the data.\")\n",
    "\n",
    "            \n",
    "    def calculate_observed_r_values(self):\n",
    "        # Calculate observed r values for each category and store them\n",
    "        self.observed_correlations = {}\n",
    "        for category in self.category_dataframes.keys():\n",
    "            correlation = self.calculate_correlation(category)\n",
    "            if correlation is not None:\n",
    "                self.observed_correlations[category] = correlation\n",
    "\n",
    "    def permute_and_calculate_correlations(self):\n",
    "        # Initialize a dictionary to store permuted correlations for each category\n",
    "        self.permuted_correlations = {category: [] for category in self.category_dataframes.keys()}\n",
    "\n",
    "        for _ in tqdm(range(self.n_permutations), desc=\"Permutations\"):\n",
    "            # Create a copy of the original data to permute\n",
    "            permuted_data = self.df.copy()\n",
    "\n",
    "            # Loop through each category's dataframe\n",
    "            for category, category_df in self.category_dataframes.items():\n",
    "                # Permute the outcomes (dependent variable) within the category's dataframe\n",
    "                category_outcomes = category_df[self.dependent_variable].values\n",
    "                random.shuffle(category_outcomes)\n",
    "                permuted_data.loc[category_df.index, self.dependent_variable] = category_outcomes\n",
    "\n",
    "                # Calculate and store the correlation with the independent variable\n",
    "                correlation = category_df[self.independent_variable].corr(permuted_data.loc[category_df.index, self.dependent_variable])\n",
    "                self.permuted_correlations[category].append(correlation)\n",
    "                \n",
    "    def calculate_p_values(self):\n",
    "        '''\n",
    "        This calculate a two-tailed p-value.\n",
    "        '''\n",
    "        # Initialize a dictionary to store p-values for each category\n",
    "        self.p_values = {category: None for category in self.category_dataframes.keys()}\n",
    "\n",
    "        for category in self.category_dataframes.keys():\n",
    "            observed_val = self.observed_correlations[category]\n",
    "            permuted_dist = self.permuted_correlations[category]\n",
    "\n",
    "            # Calculate the p-value\n",
    "            p_value = np.mean(np.array(np.abs(permuted_dist)) > np.abs(observed_val))\n",
    "\n",
    "            self.p_values[category] = p_value\n",
    "            \n",
    "    def calculate_delta_r(self):\n",
    "        # Initialize a dictionary to store Delta-R values and their significance for each category combination\n",
    "        self.delta_r_values = {}\n",
    "        \n",
    "        # Get all unique combinations of categories\n",
    "        category_combinations = list(itertools.combinations(self.category_dataframes.keys(), 2))\n",
    "\n",
    "        for category1, category2 in category_combinations:\n",
    "            observed_val1 = self.observed_correlations[category1]\n",
    "            observed_val2 = self.observed_correlations[category2]\n",
    "            observed_delta_r = observed_val1 - observed_val2\n",
    "\n",
    "            # Get the permuted distributions for both categories\n",
    "            permuted_dist1 = self.permuted_correlations[category1]\n",
    "            permuted_dist2 = self.permuted_correlations[category2]\n",
    "\n",
    "            delta_r = observed_val1 - observed_val2\n",
    "            delta_r_permuted = np.array(permuted_dist1) - np.array(permuted_dist2)\n",
    "\n",
    "            # Calculate the significance using a two-tailed test\n",
    "            p_value = np.mean(np.abs(delta_r_permuted) > np.abs(delta_r))\n",
    "            # Store the Delta-R value and its significance\n",
    "            self.delta_r_values[(category1, category2)] = {\n",
    "                'delta_r': delta_r,\n",
    "                'p_value': p_value\n",
    "            }\n",
    "\n",
    "    def plot_correlations(self):\n",
    "        # Convert observed correlations data to a DataFrame\n",
    "        observed_data = pd.DataFrame({\n",
    "            'Category': self.observed_correlations.keys(),\n",
    "            'Correlation': self.observed_correlations.values()\n",
    "        })\n",
    "\n",
    "        # Set style and increase font size\n",
    "        sns.set_style(\"white\")\n",
    "        # sns.set(font_scale=1)\n",
    "\n",
    "        # Create the bar plot using the observed data\n",
    "        sns.barplot(x='Category', y='Correlation', data=observed_data, palette='tab10')\n",
    "        sns.despine()\n",
    "        # # Add p-value annotations using Annotator\n",
    "        p_values = [self.delta_r_values[comb]['p_value'] for comb in self.delta_r_values]\n",
    "        combinations = [f'{comb[0]} vs {comb[1]}' for comb in self.delta_r_values]\n",
    "        data = pd.DataFrame({'Combination': combinations, 'p-value': p_values})\n",
    "        # annotator = Annotator(ax=ax, data=data, x='Combination', y='p-value', loc='outside', fontsize=12)\n",
    "\n",
    "        # Save the figure if out_dir is provided\n",
    "        if self.out_dir:\n",
    "            plt.savefig(f\"{self.out_dir}/delta_correlation_plot.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{self.out_dir}/delta_correlation_plot.svg\", bbox_inches='tight')\n",
    "            print(f'Saved to {self.out_dir}/delta_correlation_plot.svg')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        return observed_data, data\n",
    "        \n",
    "    def run(self):\n",
    "        self.segregate_data()\n",
    "        self.calculate_observed_r_values()\n",
    "        self.permute_and_calculate_correlations()\n",
    "        self.calculate_p_values()\n",
    "        self.calculate_delta_r()\n",
    "        single_data, delta_data = self.plot_correlations()\n",
    "        return single_data, delta_data\n",
    "\n",
    "    def create_combined_plot(self, group_variable, dependent_variable):\n",
    "        sns.set()\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=len(self.df[group_variable].unique()), sharex=True)\n",
    "        ax[-1].set_xlabel(dependent_variable)\n",
    "\n",
    "        for i, group_val in enumerate(self.df[group_variable].unique()):\n",
    "            group_data = self.df[self.df[group_variable] == group_val][dependent_variable]\n",
    "            jittered_group_data = group_data + 0.1 * (2 * random.random() - 1)  # Add small vertical jitter\n",
    "            ax[i].scatter(jittered_group_data, [0] * len(group_data))\n",
    "            sns.kdeplot(group_data, ax=ax[i], shade=False, legend=False)\n",
    "            ax[i].set_yticks([])\n",
    "            ax[i].set_ylim(-0.01)\n",
    "            ax[i].set_ylabel(f'{group_variable} ' + str(group_val))\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_correlations_with_lmplot(self, palette='Tab10'):\n",
    "        # Using seaborn's lmplot to plot linear regression lines for each category\n",
    "        if self.spearman:\n",
    "            print('Have rank-transformed data for visualization of Spearman correlation.')\n",
    "            self.df = self.df.rank()\n",
    "        lm = sns.lmplot(x=self.independent_variable, y=self.dependent_variable, \n",
    "                        hue=self.categorical_variable, data=self.df, \n",
    "                        aspect=1.5, height=5, palette=palette, legend=False, ci=0)\n",
    "\n",
    "        # Enhancements for better readability\n",
    "        lm.set_xlabels(f\"{self.independent_variable}\")\n",
    "        lm.set_ylabels(f\"{self.dependent_variable}\")\n",
    "        plt.title(\"Correlation Split by Category\")\n",
    "        plt.legend()\n",
    "        # Save the figure if out_dir is provided\n",
    "        if self.out_dir:\n",
    "            plt.savefig(f\"{self.out_dir}/delta_correlation_plot.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{self.out_dir}/delta_correlation_plot.svg\", bbox_inches='tight')\n",
    "            print(f'Saved to {self.out_dir}/delta_correlation_plot.svg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Subiculum_T_By_Origin_Group_',\n",
       "       'Z_Scored_Subiculum_Connectivity_T', 'Subiculum_Connectivity_T',\n",
       "       'Amnesia_Lesion_T_Map', 'Memory_Network_T', 'Z_Scored_Memory_Network_R',\n",
       "       'Memory_Network_R', 'Subiculum_Grey_Matter', 'Subiculum_White_Matter',\n",
       "       'Subiculum_CSF', 'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Z_Score_Sign',\n",
       "       'Subiculum_Group_By_Inflection_Point', 'Cognitive_Outcome',\n",
       "       'Estimated_Outcome', 'Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline__Lower_is_Better_',\n",
       "       'Min_Max_Normalized_Baseline', 'MinMaxNormBaseline_Higher_is_Better',\n",
       "       'ROI_to_Alz_Max', 'ROI_to_PD_Max', 'Standardzied_AD_Max',\n",
       "       'Standardized_PD_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter Variables\n",
    "- dependent_variable: the name of the dependent variable\n",
    "- independent_variable: the name of the independent variable\n",
    "- categorical_variable: the column containing categorical information. This may be strings or numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Subiculum_T_By_Origin_Group_',\n",
       "       'Z_Scored_Subiculum_Connectivity_T', 'Subiculum_Connectivity_T',\n",
       "       'Amnesia_Lesion_T_Map', 'Memory_Network_T', 'Z_Scored_Memory_Network_R',\n",
       "       'Memory_Network_R', 'Subiculum_Grey_Matter', 'Subiculum_White_Matter',\n",
       "       'Subiculum_CSF', 'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Z_Score_Sign',\n",
       "       'Subiculum_Group_By_Inflection_Point', 'Cognitive_Outcome',\n",
       "       'Estimated_Outcome', 'Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline__Lower_is_Better_',\n",
       "       'Min_Max_Normalized_Baseline', 'MinMaxNormBaseline_Higher_is_Better',\n",
       "       'ROI_to_Alz_Max', 'ROI_to_PD_Max', 'Standardzied_AD_Max',\n",
       "       'Standardized_PD_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable = 'Z_Scored_Percent_Cognitive_Improvement'\n",
    "independent_variable = 'Memory_Network_T'\n",
    "categorical_variable = 'City'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do You Want to Run A Spearman Correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = data_df.loc[:, ['SubcortexSurfaceVentricle', 'SubcortexCSF',\n",
    "#        'SubcortexGM', 'SubcortexWM' ,    'ventricle_eh' ]]\n",
    "# melted_df = test_df.melt(id_vars=[\"ventricle_eh\"], var_name=\"Measurement\", value_name=\"Value\")\n",
    "# # melted_df['Value'] = melted_df.apply(lambda row: row['Value'] if 'csf' in row['Measurement'].lower() else row['Value'] * -1, axis=1)\n",
    "# melted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Run the Correlational ANCOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutations: 100%|██████████| 10000/10000 [00:09<00:00, 1082.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/supplements/various_correlations/delta_correlation_plot.svg\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Combination</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toronto vs Wurzburg</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Combination  p-value\n",
       "0  Toronto vs Wurzburg    0.025"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_r = ANCOVACorrelation(df=data_df, dependent_variable=dependent_variable, independent_variable=independent_variable, categorical_variable=categorical_variable, out_dir=out_dir, spearman=spearman,\n",
    "                            n_permutations=10000)\n",
    "single_data, delta_data = delta_r.run()\n",
    "delta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cu135/Dropbox (Partners HealthCare)/studies/atrophy_seeds_2023/Figures/correlation_to_qualitative_grade/radiologist/vent.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.join(out_dir, 'vent.csv'))\n",
    "delta_data.to_csv(os.path.join(out_dir, 'vent.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_r.plot_correlations_with_lmplot(palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Delta Scatterplot (Pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the class and its methods\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "import seaborn as sns\n",
    "class ScatterWithConfidence:\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_analytic_confidence_interval(x, y, x_vals):\n",
    "        slope, intercept = np.polyfit(x, y, 1)\n",
    "        y_fit = slope * x_vals + intercept\n",
    "        \n",
    "        residuals = y - (slope * x + intercept)\n",
    "        stderr = np.sqrt(np.sum(residuals**2) / (len(y) - 2))\n",
    "        \n",
    "        t_value = t.ppf(0.975, df=len(x)-2)\n",
    "        ci = t_value * stderr * np.sqrt(1/len(x) + (x_vals - np.mean(x))**2 / np.sum((x - np.mean(x))**2))\n",
    "        \n",
    "        upper_bound = y_fit + ci\n",
    "        lower_bound = y_fit - ci\n",
    "        \n",
    "        return y_fit, lower_bound, upper_bound\n",
    "\n",
    "    def permute_data_and_difference_in_pearson_r(self, x_one, x_two, split_by, split_value, n_permutations=1000, permute_columns=[]):\n",
    "        original_diff = self.data_df[self.data_df[split_by] < split_value][x_one].corr(self.data_df[self.data_df[split_by] < split_value][x_two]) - \\\n",
    "                       self.data_df[self.data_df[split_by] >= split_value][x_one].corr(self.data_df[self.data_df[split_by] >= split_value][x_two])\n",
    "\n",
    "        permuted_diffs = []\n",
    "\n",
    "        for _ in range(n_permutations):\n",
    "            permuted_df = self.data_df.copy()\n",
    "            for column in permute_columns:\n",
    "                permuted_df[column] = np.random.permutation(permuted_df[column].values)\n",
    "            \n",
    "            diff = permuted_df[permuted_df[split_by] < split_value][x_one].corr(permuted_df[permuted_df[split_by] < split_value][x_two]) - \\\n",
    "                   permuted_df[permuted_df[split_by] >= split_value][x_one].corr(permuted_df[permuted_df[split_by] >= split_value][x_two])\n",
    "            \n",
    "            permuted_diffs.append(diff)\n",
    "\n",
    "        p_value = np.mean([diff <= original_diff for diff in permuted_diffs])\n",
    "        return original_diff, p_value\n",
    "\n",
    "    def plot_with_analytic_ci_manual_pvalue(self, x_one, x_two, \n",
    "                                            split_by, split_value, \n",
    "                                            x_label='X1', y_label='X2', \n",
    "                                            upper_split_legend='Above Split', lower_split_legend='Below Split',\n",
    "                                            alpha=0.3, manual_p_value=None, permute_column=None, \n",
    "                                            save=False, out_dir=None,\n",
    "                                            colour1='red', colour2='blue'):\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        \n",
    "        group1 = self.data_df[self.data_df[split_by] < split_value]\n",
    "        group2 = self.data_df[self.data_df[split_by] >= split_value]\n",
    "\n",
    "        ax.scatter(group1[x_one], group1[x_two], color=colour1, label=lower_split_legend, s=40, alpha=alpha, marker='o')\n",
    "        ax.scatter(group2[x_one], group2[x_two], color=colour2, label=upper_split_legend, s=40, alpha=alpha, marker='o')\n",
    "        \n",
    "        x_vals = np.linspace(self.data_df[x_one].min(), self.data_df[x_one].max(), 400)\n",
    "        \n",
    "        for group, color in [(group1, colour1), (group2, colour2)]:\n",
    "            y_fit, lower_bound, upper_bound = self.compute_analytic_confidence_interval(group[x_one], group[x_two], x_vals)\n",
    "            ax.plot(x_vals, y_fit, color=color)\n",
    "            # ax.fill_between(x_vals, lower_bound, upper_bound, color=color, alpha=alpha/4)\n",
    "        \n",
    "        if manual_p_value is None:\n",
    "            if permute_column:\n",
    "                rho, manual_p_value = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, n_permutations=10000, permute_columns=[permute_column])\n",
    "            else:\n",
    "                rho, manual_p_value = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, n_permutations=10000, permute_columns=[x_one, x_two, split_by])\n",
    "        \n",
    "        ax.set_title(f\"\\u0394 r = {rho:.2f} , p = {manual_p_value:.4f}\")\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.legend(loc='best', frameon=False)\n",
    "        ax.grid(False)\n",
    "        sns.despine(ax=ax)\n",
    "        \n",
    "        if save and out_dir is not None:\n",
    "            plt.savefig(f\"{out_dir}/scatter_with_polyfit_and_analytic_ci.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{out_dir}/scatter_with_polyfit_and_analytic_ci.svg\", bbox_inches='tight')\n",
    "            print(f'Saved to {out_dir}/scatter_with_polyfit_and_analytic_ci.svg')\n",
    "        return fig\n",
    "    \n",
    "class DeltaCorrelation(ScatterWithConfidence):\n",
    "    def __init__(self, data_df):\n",
    "        super().__init__(data_df)\n",
    "\n",
    "    def plot_histogram_of_delta_r(self, x_one, x_two, split_by, split_value, n_permutations=1000, \n",
    "                                permute_columns=[], bins=50, one_tail=False, color_palette='dark'):\n",
    "        # Generate the empirical distribution of delta_r\n",
    "        delta_rs = []\n",
    "        for _ in range(n_permutations):\n",
    "            permuted_df = self.data_df.copy()\n",
    "            for column in permute_columns:\n",
    "                permuted_df[column] = np.random.permutation(permuted_df[column].values)\n",
    "\n",
    "            delta_r = permuted_df[permuted_df[split_by] < split_value][x_one].corr(permuted_df[permuted_df[split_by] < split_value][x_two]) - \\\n",
    "                    permuted_df[permuted_df[split_by] >= split_value][x_one].corr(permuted_df[permuted_df[split_by] >= split_value][x_two])\n",
    "            delta_rs.append(delta_r)\n",
    "\n",
    "        # Calculate the observed delta_r\n",
    "        observed_delta_r, _ = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, permute_columns=permute_columns)\n",
    "\n",
    "        if one_tail:\n",
    "            observed_delta_r = np.abs(observed_delta_r)\n",
    "            delta_rs = np.abs(delta_rs)\n",
    "\n",
    "        # Calculate p-value\n",
    "        if one_tail:\n",
    "            p_value = np.mean([delta_r >= observed_delta_r for delta_r in delta_rs])\n",
    "        else:\n",
    "            p_value = np.mean([delta_r <= observed_delta_r for delta_r in delta_rs])\n",
    "\n",
    "        # Generate the displot (KDE + Histogram) using Seaborn\n",
    "        sns.set_palette(color_palette)\n",
    "        current_palette = sns.color_palette(color_palette)\n",
    "        chosen_color = current_palette[4]\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        g = sns.displot(delta_rs, kde=True, bins=bins, label=\"Empirical $\\\\Delta r$ Distribution\", element=\"step\",color='blue', alpha=.6)\n",
    "        plt.axvline(x=observed_delta_r, color='red', linestyle='-', linewidth=1.5, label=f\"Observed $\\\\Delta r$\", alpha=0.6)\n",
    "        plt.title(f\"$\\\\Delta r$ = {observed_delta_r}, p = {p_value}\")\n",
    "        plt.xlabel(\"$\\\\Delta r$\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "        \n",
    "        fig = g.fig\n",
    "        \n",
    "        fig.savefig(f\"{out_dir}/hist_kde.png\", bbox_inches='tight')\n",
    "        fig.savefig(f\"{out_dir}/hist_kde.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/hist_kde.svg')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.loc[:, ['TOTAL11', 'CerebellumSBM',\n",
    "       'CerebellumCSF', 'CerebellumGM', 'CerebellumWM', 'FrontalSurface',\n",
    "       'FrontalCSF', 'FrontalGM', 'FrontalWM', 'MTLCSF', 'MTLGM', 'MTLWM',\n",
    "       'OccipitalSurface', 'OccipitalCSF', 'OccipitalGM', 'OccipitalWM',\n",
    "       'ParietalSurface', 'ParietalCSF', 'ParietalGM', 'ParietalWM',\n",
    "       'SubcortexSurfaceVentricle', 'SubcortexCSF',\n",
    "       'SubcortexGM', 'SubcortexWM', 'temp_ins_csf', 'temp_ins_gm',\n",
    "       'temp_ins_wm', 'temp_ins_surface']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data To Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Variables\n",
    "dependent_variable = 'Z_Scored_Percent_Cognitive_Improvement'\n",
    "independent_variable = 'Z_Scored_Subiculum_Connectivity'\n",
    "split_by_var = 'Cohort' # This is the column which contains the values you are going to split the data by \n",
    "split_value_var = 1.5 # This is the value which will be used to determine how rows of the given column split the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels for Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Variables\n",
    "x_label = 'Standardized Subiculum Connectivity'\n",
    "y_label = 'Standardized Percent Improvement'\n",
    "legend_string_for_lower_split='Alzheimer Disease'\n",
    "legend_string_for_upper_split='Parkinson Disease'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_sctr = ScatterWithConfidence(data_df)\n",
    "delta_sctr_plot = delta_sctr.plot_with_analytic_ci_manual_pvalue(\n",
    "                                            x_one=independent_variable, x_two=dependent_variable,\n",
    "                                            split_by=split_by_var, split_value=split_value_var,\n",
    "                                            x_label=x_label, y_label=y_label,\n",
    "                                            upper_split_legend=legend_string_for_upper_split, lower_split_legend=legend_string_for_lower_split,\n",
    "                                            alpha=0.3, manual_p_value=None, permute_column=None,\n",
    "                                            save=True, out_dir=out_dir,\n",
    "                                            colour1='#FF0000', colour2='#0000FF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object from the new Seaborn-based class\n",
    "histogram_plotter_seaborn = DeltaCorrelation(data_df)\n",
    "\n",
    "# Generate the histogram using Seaborn\n",
    "fig = histogram_plotter_seaborn.plot_histogram_of_delta_r(x_one=independent_variable, x_two=dependent_variable, \n",
    "                                                    split_by=split_by_var, split_value=split_value_var, \n",
    "                                                    n_permutations=10000, \n",
    "                                                    permute_columns=[independent_variable, dependent_variable, split_by_var], \n",
    "                                                    bins=50,\n",
    "                                                    one_tail=True, \n",
    "                                                    color_palette='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix and Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_correlation_heatmap(dataframe):\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = dataframe.corr()\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(corr_matrix, annot=False, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={'shrink': .5})\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title('Correlation Heatmap')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sorted_columns = sorted(data_df.columns)\n",
    "data_df_sorted = data_df[sorted_columns]\n",
    "plot_correlation_heatmap(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def perform_cca(data_df, set1_columns, set2_columns):\n",
    "    \"\"\"\n",
    "    Perform Canonical Correlation Analysis (CCA) between two sets of columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        data_df (pd.DataFrame): The DataFrame containing the data.\n",
    "        set1_columns (list): List of column names for the first set of variables.\n",
    "        set2_columns (list): List of column names for the second set of variables.\n",
    "\n",
    "    Returns:\n",
    "        cca (CCA): The CCA model object.\n",
    "        cca_results (dict): A dictionary containing the CCA results.\n",
    "    \"\"\"\n",
    "    # Extract the specified sets of columns from the DataFrame\n",
    "    set1_data = data_df[set1_columns]\n",
    "    set2_data = data_df[set2_columns]\n",
    "\n",
    "    # Initialize the CCA model\n",
    "    cca = CCA(n_components=min(len(set1_columns), len(set2_columns)))\n",
    "\n",
    "    # Fit the CCA model to the data\n",
    "    cca.fit(set1_data, set2_data)\n",
    "\n",
    "    # Transform the data using CCA\n",
    "    set1_transformed, set2_transformed = cca.transform(set1_data, set2_data)\n",
    "\n",
    "    # Calculate canonical correlations from singular values\n",
    "    canonical_correlations = cca.singular_values_\n",
    "\n",
    "    # Store the CCA results in a dictionary\n",
    "    cca_results = {\n",
    "        'Canonical Correlations': canonical_correlations,\n",
    "        'Set 1 Transformed': set1_transformed,\n",
    "        'Set 2 Transformed': set2_transformed\n",
    "    }\n",
    "\n",
    "    return cca, cca_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define 2 lists\n",
    "- 1 is the first set to CCA\n",
    "- 2 is the second set to CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templist = []\n",
    "for val in data_df.columns:\n",
    "    if 'CSF' in val:\n",
    "        templist.insert(-1, val)\n",
    "print(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['frontal', 'temporal', 'parietal', 'occipital',\n",
    "       'cerebellum', 'Mesial_Temporal', 'ventricle']\n",
    "list2 = ['frontal_eh', 'temporal_eh', 'parietal_eh',\n",
    "       'occipital_eh', 'cerebellum_eh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def perform_cca(data_df, set1_columns, set2_columns):\n",
    "    \"\"\"\n",
    "    Perform Canonical Correlation Analysis (CCA) between two sets of columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        data_df (pd.DataFrame): The DataFrame containing the data.\n",
    "        set1_columns (list): List of column names for the first set of variables.\n",
    "        set2_columns (list): List of column names for the second set of variables.\n",
    "\n",
    "    Returns:\n",
    "        cca (CCA): The CCA model object.\n",
    "        cca_results (dict): A dictionary containing the CCA results.\n",
    "    \"\"\"\n",
    "    # Extract the specified sets of columns from the DataFrame\n",
    "    set1_data = data_df[set1_columns]\n",
    "    set2_data = data_df[set2_columns]\n",
    "\n",
    "    # Initialize the CCA model\n",
    "    cca = CCA(n_components=min(len(set1_columns), len(set2_columns)))\n",
    "\n",
    "    # Fit the CCA model to the data\n",
    "    cca.fit(set1_data, set2_data)\n",
    "\n",
    "    # Calculate canonical correlations using the score method\n",
    "    canonical_correlations = cca.score(set1_data, set2_data)\n",
    "\n",
    "    # Store the CCA results in a dictionary\n",
    "    cca_results = {\n",
    "        'Canonical Correlations': canonical_correlations\n",
    "    }\n",
    "\n",
    "    return cca, cca_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca, cca_results = perform_cca(data_df, set1_columns=list1, set2_columns=list2)\n",
    "cca_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
