{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the CSV\n",
    "spreadsheet_dir = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list_proper_subjects.csv'\n",
    "out_dir = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/analyses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "if os.path.exists(out_dir) != True:\n",
    "    os.makedirs(out_dir)\n",
    "    print('Made directory', out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Z-Scored Percent Cognitive Improvement</th>\n",
       "      <th>Percent Cognitive Improvement</th>\n",
       "      <th>Z-Scored Subiculum Connectivity</th>\n",
       "      <th>Subiculum Connectivity</th>\n",
       "      <th>Amnesia Lesion T Map</th>\n",
       "      <th>Memory Network T</th>\n",
       "      <th>Memory Network R</th>\n",
       "      <th>Subiculum Grey Matter</th>\n",
       "      <th>...</th>\n",
       "      <th>Standardized Age</th>\n",
       "      <th>Standardized Percent Improvement</th>\n",
       "      <th>Standardized Subiculum Connectivity</th>\n",
       "      <th>Standardized Subiculum Grey Matter</th>\n",
       "      <th>Standardized Subiculum White Matter</th>\n",
       "      <th>Standardized Subiculum CSF</th>\n",
       "      <th>Standardized Subiculum Total</th>\n",
       "      <th>Disease.1</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>Inclusion Cohort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>0.494596</td>\n",
       "      <td>0.418688</td>\n",
       "      <td>1.646994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>-1.179578</td>\n",
       "      <td>1.970962</td>\n",
       "      <td>0.663981</td>\n",
       "      <td>1.773701</td>\n",
       "      <td>1.789087</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>0.502192</td>\n",
       "      <td>0.417569</td>\n",
       "      <td>-0.865158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-1.635523</td>\n",
       "      <td>-0.125938</td>\n",
       "      <td>-1.611307</td>\n",
       "      <td>-0.272553</td>\n",
       "      <td>-1.093456</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>0.581148</td>\n",
       "      <td>0.500706</td>\n",
       "      <td>0.319460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.524419</td>\n",
       "      <td>0.862866</td>\n",
       "      <td>-0.734373</td>\n",
       "      <td>-0.463295</td>\n",
       "      <td>0.143505</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>0.520518</td>\n",
       "      <td>0.455778</td>\n",
       "      <td>-0.440643</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-0.857915</td>\n",
       "      <td>0.228406</td>\n",
       "      <td>-0.052972</td>\n",
       "      <td>-0.579749</td>\n",
       "      <td>0.126248</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>0.491742</td>\n",
       "      <td>0.455764</td>\n",
       "      <td>-0.328427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>-1.055024</td>\n",
       "      <td>0.322072</td>\n",
       "      <td>-0.128024</td>\n",
       "      <td>-1.856148</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>37</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.158855</td>\n",
       "      <td>-2.158273</td>\n",
       "      <td>-0.690244</td>\n",
       "      <td>17.978233</td>\n",
       "      <td>-0.243491</td>\n",
       "      <td>0.103399</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.346526</td>\n",
       "      <td>-0.694444</td>\n",
       "      <td>-1.171189</td>\n",
       "      <td>14.611144</td>\n",
       "      <td>-0.639372</td>\n",
       "      <td>-0.329546</td>\n",
       "      <td>-0.052089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.872505</td>\n",
       "      <td>-4.225352</td>\n",
       "      <td>-0.179834</td>\n",
       "      <td>21.551613</td>\n",
       "      <td>-0.293216</td>\n",
       "      <td>0.094811</td>\n",
       "      <td>0.203586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.393141</td>\n",
       "      <td>-2.836879</td>\n",
       "      <td>-0.372845</td>\n",
       "      <td>20.200341</td>\n",
       "      <td>-0.493639</td>\n",
       "      <td>-0.114161</td>\n",
       "      <td>0.108190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.191585</td>\n",
       "      <td>14.468351</td>\n",
       "      <td>-0.463787</td>\n",
       "      <td>-0.119392</td>\n",
       "      <td>0.069417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   Age  Z-Scored Percent Cognitive Improvement  \\\n",
       "0       101  62.0                                0.314066   \n",
       "1       102  77.0                                0.013999   \n",
       "2       103  76.0                               -0.841572   \n",
       "3       104  65.0                               -1.855477   \n",
       "4       105  50.0                                0.533109   \n",
       "..      ...   ...                                     ...   \n",
       "73       37  64.0                               -0.158855   \n",
       "74       38   NaN                                0.346526   \n",
       "75       39   NaN                               -0.872505   \n",
       "76       43   NaN                               -0.393141   \n",
       "77       44   NaN                                0.586280   \n",
       "\n",
       "    Percent Cognitive Improvement  Z-Scored Subiculum Connectivity  \\\n",
       "0                      -21.428571                        -1.282630   \n",
       "1                      -36.363636                        -1.760917   \n",
       "2                      -78.947368                        -0.595369   \n",
       "3                     -129.411765                        -0.945206   \n",
       "4                      -10.526316                        -1.151973   \n",
       "..                            ...                              ...   \n",
       "73                      -2.158273                        -0.690244   \n",
       "74                      -0.694444                        -1.171189   \n",
       "75                      -4.225352                        -0.179834   \n",
       "76                      -2.836879                        -0.372845   \n",
       "77                       0.000000                        -1.191585   \n",
       "\n",
       "    Subiculum Connectivity  Amnesia Lesion T Map  Memory Network T  \\\n",
       "0                56.864683              0.447264          0.494596   \n",
       "1                52.970984              0.436157          0.502192   \n",
       "2                62.459631              0.497749          0.581148   \n",
       "3                59.611631              0.432617          0.520518   \n",
       "4                57.928350              0.193389          0.491742   \n",
       "..                     ...                   ...               ...   \n",
       "73               17.978233             -0.243491          0.103399   \n",
       "74               14.611144             -0.639372         -0.329546   \n",
       "75               21.551613             -0.293216          0.094811   \n",
       "76               20.200341             -0.493639         -0.114161   \n",
       "77               14.468351             -0.463787         -0.119392   \n",
       "\n",
       "    Memory Network R  Subiculum Grey Matter  ...  Standardized Age  \\\n",
       "0           0.418688               1.646994  ...          0.289341   \n",
       "1           0.417569              -0.865158  ...         -0.016378   \n",
       "2           0.500706               0.319460  ...         -0.888064   \n",
       "3           0.455778              -0.440643  ...         -1.921065   \n",
       "4           0.455764              -0.328427  ...          0.512509   \n",
       "..               ...                    ...  ...               ...   \n",
       "73          0.185311                    NaN  ...               NaN   \n",
       "74         -0.052089                    NaN  ...               NaN   \n",
       "75          0.203586                    NaN  ...               NaN   \n",
       "76          0.108190                    NaN  ...               NaN   \n",
       "77          0.069417                    NaN  ...               NaN   \n",
       "\n",
       "    Standardized Percent Improvement  Standardized Subiculum Connectivity  \\\n",
       "0                           0.289341                            -1.179578   \n",
       "1                          -0.016378                            -1.635523   \n",
       "2                          -0.888064                            -0.524419   \n",
       "3                          -1.921065                            -0.857915   \n",
       "4                           0.512509                            -1.055024   \n",
       "..                               ...                                  ...   \n",
       "73                               NaN                                  NaN   \n",
       "74                               NaN                                  NaN   \n",
       "75                               NaN                                  NaN   \n",
       "76                               NaN                                  NaN   \n",
       "77                               NaN                                  NaN   \n",
       "\n",
       "   Standardized Subiculum Grey Matter  Standardized Subiculum White Matter  \\\n",
       "0                            1.970962                             0.663981   \n",
       "1                           -0.125938                            -1.611307   \n",
       "2                            0.862866                            -0.734373   \n",
       "3                            0.228406                            -0.052972   \n",
       "4                            0.322072                            -0.128024   \n",
       "..                                ...                                  ...   \n",
       "73                                NaN                                  NaN   \n",
       "74                                NaN                                  NaN   \n",
       "75                                NaN                                  NaN   \n",
       "76                                NaN                                  NaN   \n",
       "77                                NaN                                  NaN   \n",
       "\n",
       "    Standardized Subiculum CSF  Standardized Subiculum Total  Disease.1  \\\n",
       "0                     1.773701                      1.789087  Alzheimer   \n",
       "1                    -0.272553                     -1.093456  Alzheimer   \n",
       "2                    -0.463295                      0.143505  Alzheimer   \n",
       "3                    -0.579749                      0.126248  Alzheimer   \n",
       "4                    -1.856148                      0.144737  Alzheimer   \n",
       "..                         ...                           ...        ...   \n",
       "73                         NaN                           NaN  Parkinson   \n",
       "74                         NaN                           NaN  Parkinson   \n",
       "75                         NaN                           NaN  Parkinson   \n",
       "76                         NaN                           NaN  Parkinson   \n",
       "77                         NaN                           NaN  Parkinson   \n",
       "\n",
       "    Cohort  Inclusion Cohort  \n",
       "0        1                 1  \n",
       "1        1                 1  \n",
       "2        1                 1  \n",
       "3        1                 1  \n",
       "4        1                 1  \n",
       "..     ...               ...  \n",
       "73       0                 1  \n",
       "74       0                 1  \n",
       "75       0                 1  \n",
       "76       0                 1  \n",
       "77       0                 1  \n",
       "\n",
       "[78 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User Input\n",
    "## Input the sheet you would like to use below\n",
    "sheet_name = 'study_results'\n",
    "\n",
    "if os.path.basename(spreadsheet_dir).split('.')[1] == 'csv':\n",
    "    data_df = pd.read_csv(spreadsheet_dir)\n",
    "else:\n",
    "    data_df = pd.read_excel(spreadsheet_dir, sheet_name=sheet_name)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Rows by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_rows_to_remove = [11, 47, 48, 49]\n",
    "# data_df = data_df.drop(index=list_of_rows_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take Slice of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Z-Scored Percent Cognitive Improvement</th>\n",
       "      <th>Percent Cognitive Improvement</th>\n",
       "      <th>Z-Scored Subiculum Connectivity</th>\n",
       "      <th>Subiculum Connectivity</th>\n",
       "      <th>Amnesia Lesion T Map</th>\n",
       "      <th>Memory Network T</th>\n",
       "      <th>Memory Network R</th>\n",
       "      <th>Subiculum Grey Matter</th>\n",
       "      <th>...</th>\n",
       "      <th>Standardized Age</th>\n",
       "      <th>Standardized Percent Improvement</th>\n",
       "      <th>Standardized Subiculum Connectivity</th>\n",
       "      <th>Standardized Subiculum Grey Matter</th>\n",
       "      <th>Standardized Subiculum White Matter</th>\n",
       "      <th>Standardized Subiculum CSF</th>\n",
       "      <th>Standardized Subiculum Total</th>\n",
       "      <th>Disease.1</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>Inclusion Cohort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>0.494596</td>\n",
       "      <td>0.418688</td>\n",
       "      <td>1.646994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>-1.179578</td>\n",
       "      <td>1.970962</td>\n",
       "      <td>0.663981</td>\n",
       "      <td>1.773701</td>\n",
       "      <td>1.789087</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>0.502192</td>\n",
       "      <td>0.417569</td>\n",
       "      <td>-0.865158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-1.635523</td>\n",
       "      <td>-0.125938</td>\n",
       "      <td>-1.611307</td>\n",
       "      <td>-0.272553</td>\n",
       "      <td>-1.093456</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>0.581148</td>\n",
       "      <td>0.500706</td>\n",
       "      <td>0.319460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.524419</td>\n",
       "      <td>0.862866</td>\n",
       "      <td>-0.734373</td>\n",
       "      <td>-0.463295</td>\n",
       "      <td>0.143505</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>0.520518</td>\n",
       "      <td>0.455778</td>\n",
       "      <td>-0.440643</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-0.857915</td>\n",
       "      <td>0.228406</td>\n",
       "      <td>-0.052972</td>\n",
       "      <td>-0.579749</td>\n",
       "      <td>0.126248</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>0.491742</td>\n",
       "      <td>0.455764</td>\n",
       "      <td>-0.328427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>-1.055024</td>\n",
       "      <td>0.322072</td>\n",
       "      <td>-0.128024</td>\n",
       "      <td>-1.856148</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>37</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.158855</td>\n",
       "      <td>-2.158273</td>\n",
       "      <td>-0.690244</td>\n",
       "      <td>17.978233</td>\n",
       "      <td>-0.243491</td>\n",
       "      <td>0.103399</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.346526</td>\n",
       "      <td>-0.694444</td>\n",
       "      <td>-1.171189</td>\n",
       "      <td>14.611144</td>\n",
       "      <td>-0.639372</td>\n",
       "      <td>-0.329546</td>\n",
       "      <td>-0.052089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.872505</td>\n",
       "      <td>-4.225352</td>\n",
       "      <td>-0.179834</td>\n",
       "      <td>21.551613</td>\n",
       "      <td>-0.293216</td>\n",
       "      <td>0.094811</td>\n",
       "      <td>0.203586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.393141</td>\n",
       "      <td>-2.836879</td>\n",
       "      <td>-0.372845</td>\n",
       "      <td>20.200341</td>\n",
       "      <td>-0.493639</td>\n",
       "      <td>-0.114161</td>\n",
       "      <td>0.108190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.191585</td>\n",
       "      <td>14.468351</td>\n",
       "      <td>-0.463787</td>\n",
       "      <td>-0.119392</td>\n",
       "      <td>0.069417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   Age  Z-Scored Percent Cognitive Improvement  \\\n",
       "0       101  62.0                                0.314066   \n",
       "1       102  77.0                                0.013999   \n",
       "2       103  76.0                               -0.841572   \n",
       "3       104  65.0                               -1.855477   \n",
       "4       105  50.0                                0.533109   \n",
       "..      ...   ...                                     ...   \n",
       "73       37  64.0                               -0.158855   \n",
       "74       38   NaN                                0.346526   \n",
       "75       39   NaN                               -0.872505   \n",
       "76       43   NaN                               -0.393141   \n",
       "77       44   NaN                                0.586280   \n",
       "\n",
       "    Percent Cognitive Improvement  Z-Scored Subiculum Connectivity  \\\n",
       "0                      -21.428571                        -1.282630   \n",
       "1                      -36.363636                        -1.760917   \n",
       "2                      -78.947368                        -0.595369   \n",
       "3                     -129.411765                        -0.945206   \n",
       "4                      -10.526316                        -1.151973   \n",
       "..                            ...                              ...   \n",
       "73                      -2.158273                        -0.690244   \n",
       "74                      -0.694444                        -1.171189   \n",
       "75                      -4.225352                        -0.179834   \n",
       "76                      -2.836879                        -0.372845   \n",
       "77                       0.000000                        -1.191585   \n",
       "\n",
       "    Subiculum Connectivity  Amnesia Lesion T Map  Memory Network T  \\\n",
       "0                56.864683              0.447264          0.494596   \n",
       "1                52.970984              0.436157          0.502192   \n",
       "2                62.459631              0.497749          0.581148   \n",
       "3                59.611631              0.432617          0.520518   \n",
       "4                57.928350              0.193389          0.491742   \n",
       "..                     ...                   ...               ...   \n",
       "73               17.978233             -0.243491          0.103399   \n",
       "74               14.611144             -0.639372         -0.329546   \n",
       "75               21.551613             -0.293216          0.094811   \n",
       "76               20.200341             -0.493639         -0.114161   \n",
       "77               14.468351             -0.463787         -0.119392   \n",
       "\n",
       "    Memory Network R  Subiculum Grey Matter  ...  Standardized Age  \\\n",
       "0           0.418688               1.646994  ...          0.289341   \n",
       "1           0.417569              -0.865158  ...         -0.016378   \n",
       "2           0.500706               0.319460  ...         -0.888064   \n",
       "3           0.455778              -0.440643  ...         -1.921065   \n",
       "4           0.455764              -0.328427  ...          0.512509   \n",
       "..               ...                    ...  ...               ...   \n",
       "73          0.185311                    NaN  ...               NaN   \n",
       "74         -0.052089                    NaN  ...               NaN   \n",
       "75          0.203586                    NaN  ...               NaN   \n",
       "76          0.108190                    NaN  ...               NaN   \n",
       "77          0.069417                    NaN  ...               NaN   \n",
       "\n",
       "    Standardized Percent Improvement  Standardized Subiculum Connectivity  \\\n",
       "0                           0.289341                            -1.179578   \n",
       "1                          -0.016378                            -1.635523   \n",
       "2                          -0.888064                            -0.524419   \n",
       "3                          -1.921065                            -0.857915   \n",
       "4                           0.512509                            -1.055024   \n",
       "..                               ...                                  ...   \n",
       "73                               NaN                                  NaN   \n",
       "74                               NaN                                  NaN   \n",
       "75                               NaN                                  NaN   \n",
       "76                               NaN                                  NaN   \n",
       "77                               NaN                                  NaN   \n",
       "\n",
       "   Standardized Subiculum Grey Matter  Standardized Subiculum White Matter  \\\n",
       "0                            1.970962                             0.663981   \n",
       "1                           -0.125938                            -1.611307   \n",
       "2                            0.862866                            -0.734373   \n",
       "3                            0.228406                            -0.052972   \n",
       "4                            0.322072                            -0.128024   \n",
       "..                                ...                                  ...   \n",
       "73                                NaN                                  NaN   \n",
       "74                                NaN                                  NaN   \n",
       "75                                NaN                                  NaN   \n",
       "76                                NaN                                  NaN   \n",
       "77                                NaN                                  NaN   \n",
       "\n",
       "    Standardized Subiculum CSF  Standardized Subiculum Total  Disease.1  \\\n",
       "0                     1.773701                      1.789087  Alzheimer   \n",
       "1                    -0.272553                     -1.093456  Alzheimer   \n",
       "2                    -0.463295                      0.143505  Alzheimer   \n",
       "3                    -0.579749                      0.126248  Alzheimer   \n",
       "4                    -1.856148                      0.144737  Alzheimer   \n",
       "..                         ...                           ...        ...   \n",
       "73                         NaN                           NaN  Parkinson   \n",
       "74                         NaN                           NaN  Parkinson   \n",
       "75                         NaN                           NaN  Parkinson   \n",
       "76                         NaN                           NaN  Parkinson   \n",
       "77                         NaN                           NaN  Parkinson   \n",
       "\n",
       "    Cohort  Inclusion Cohort  \n",
       "0        1                 1  \n",
       "1        1                 1  \n",
       "2        1                 1  \n",
       "3        1                 1  \n",
       "4        1                 1  \n",
       "..     ...               ...  \n",
       "73       0                 1  \n",
       "74       0                 1  \n",
       "75       0                 1  \n",
       "76       0                 1  \n",
       "77       0                 1  \n",
       "\n",
       "[78 rows x 24 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#always run this\n",
    "# trim the dataframe as needed\n",
    "row_to_stop_at = None\n",
    "col_to_stop_at = None\n",
    "\n",
    "#----------------------------------------------------------------Do Not Touch----------------------------------------------------------------\n",
    "if row_to_stop_at is not None:\n",
    "    pass\n",
    "else:\n",
    "    row_to_stop_at = pd.IndexSlice[:]\n",
    "    \n",
    "if col_to_stop_at is not None:\n",
    "    pass\n",
    "else:\n",
    "    col_to_stop_at = pd.IndexSlice[:]\n",
    "    \n",
    "data_df = data_df.loc[row_to_stop_at, col_to_stop_at].copy()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df['Cognitive Status'] = np.where(data_df['Cognitive Status'] == 'MCI', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_nans = True\n",
    "column_to_drop_from = 'Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Z-Scored Percent Cognitive Improvement</th>\n",
       "      <th>Percent Cognitive Improvement</th>\n",
       "      <th>Z-Scored Subiculum Connectivity</th>\n",
       "      <th>Subiculum Connectivity</th>\n",
       "      <th>Amnesia Lesion T Map</th>\n",
       "      <th>Memory Network T</th>\n",
       "      <th>Memory Network R</th>\n",
       "      <th>Subiculum Grey Matter</th>\n",
       "      <th>...</th>\n",
       "      <th>Standardized Age</th>\n",
       "      <th>Standardized Percent Improvement</th>\n",
       "      <th>Standardized Subiculum Connectivity</th>\n",
       "      <th>Standardized Subiculum Grey Matter</th>\n",
       "      <th>Standardized Subiculum White Matter</th>\n",
       "      <th>Standardized Subiculum CSF</th>\n",
       "      <th>Standardized Subiculum Total</th>\n",
       "      <th>Disease.1</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>Inclusion Cohort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>0.494596</td>\n",
       "      <td>0.418688</td>\n",
       "      <td>1.646994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>-1.179578</td>\n",
       "      <td>1.970962</td>\n",
       "      <td>0.663981</td>\n",
       "      <td>1.773701</td>\n",
       "      <td>1.789087</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>0.502192</td>\n",
       "      <td>0.417569</td>\n",
       "      <td>-0.865158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-1.635523</td>\n",
       "      <td>-0.125938</td>\n",
       "      <td>-1.611307</td>\n",
       "      <td>-0.272553</td>\n",
       "      <td>-1.093456</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>0.581148</td>\n",
       "      <td>0.500706</td>\n",
       "      <td>0.319460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.524419</td>\n",
       "      <td>0.862866</td>\n",
       "      <td>-0.734373</td>\n",
       "      <td>-0.463295</td>\n",
       "      <td>0.143505</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>0.520518</td>\n",
       "      <td>0.455778</td>\n",
       "      <td>-0.440643</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-0.857915</td>\n",
       "      <td>0.228406</td>\n",
       "      <td>-0.052972</td>\n",
       "      <td>-0.579749</td>\n",
       "      <td>0.126248</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>0.491742</td>\n",
       "      <td>0.455764</td>\n",
       "      <td>-0.328427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>-1.055024</td>\n",
       "      <td>0.322072</td>\n",
       "      <td>-0.128024</td>\n",
       "      <td>-1.856148</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>30</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.106772</td>\n",
       "      <td>-1.388889</td>\n",
       "      <td>-0.590767</td>\n",
       "      <td>18.674670</td>\n",
       "      <td>-0.553686</td>\n",
       "      <td>-0.190240</td>\n",
       "      <td>0.055350</td>\n",
       "      <td>-8.205764</td>\n",
       "      <td>...</td>\n",
       "      <td>1.034438</td>\n",
       "      <td>1.034438</td>\n",
       "      <td>-1.099623</td>\n",
       "      <td>-3.303201</td>\n",
       "      <td>2.909124</td>\n",
       "      <td>-3.303201</td>\n",
       "      <td>2.775495</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>31</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.086636</td>\n",
       "      <td>1.449275</td>\n",
       "      <td>-1.065220</td>\n",
       "      <td>15.353030</td>\n",
       "      <td>-0.505577</td>\n",
       "      <td>-0.179074</td>\n",
       "      <td>0.022968</td>\n",
       "      <td>9.086053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009451</td>\n",
       "      <td>-0.009451</td>\n",
       "      <td>-1.142557</td>\n",
       "      <td>0.791574</td>\n",
       "      <td>-0.144002</td>\n",
       "      <td>0.791574</td>\n",
       "      <td>-0.544369</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>33</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.103418</td>\n",
       "      <td>-1.398601</td>\n",
       "      <td>-1.108473</td>\n",
       "      <td>15.050219</td>\n",
       "      <td>-0.659502</td>\n",
       "      <td>-0.363412</td>\n",
       "      <td>-0.058739</td>\n",
       "      <td>8.416993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.522107</td>\n",
       "      <td>-0.522107</td>\n",
       "      <td>-0.811944</td>\n",
       "      <td>0.633137</td>\n",
       "      <td>0.522049</td>\n",
       "      <td>0.633137</td>\n",
       "      <td>-1.339117</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>36</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-0.379443</td>\n",
       "      <td>-2.797203</td>\n",
       "      <td>-0.775406</td>\n",
       "      <td>17.382020</td>\n",
       "      <td>-0.527690</td>\n",
       "      <td>-0.175144</td>\n",
       "      <td>0.043586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>37</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.158855</td>\n",
       "      <td>-2.158273</td>\n",
       "      <td>-0.690244</td>\n",
       "      <td>17.978233</td>\n",
       "      <td>-0.243491</td>\n",
       "      <td>0.103399</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   Age  Z-Scored Percent Cognitive Improvement  \\\n",
       "0       101  62.0                                0.314066   \n",
       "1       102  77.0                                0.013999   \n",
       "2       103  76.0                               -0.841572   \n",
       "3       104  65.0                               -1.855477   \n",
       "4       105  50.0                                0.533109   \n",
       "..      ...   ...                                     ...   \n",
       "68       30  58.0                                0.106772   \n",
       "69       31  64.0                                1.086636   \n",
       "70       33  60.0                                0.103418   \n",
       "72       36  52.0                               -0.379443   \n",
       "73       37  64.0                               -0.158855   \n",
       "\n",
       "    Percent Cognitive Improvement  Z-Scored Subiculum Connectivity  \\\n",
       "0                      -21.428571                        -1.282630   \n",
       "1                      -36.363636                        -1.760917   \n",
       "2                      -78.947368                        -0.595369   \n",
       "3                     -129.411765                        -0.945206   \n",
       "4                      -10.526316                        -1.151973   \n",
       "..                            ...                              ...   \n",
       "68                      -1.388889                        -0.590767   \n",
       "69                       1.449275                        -1.065220   \n",
       "70                      -1.398601                        -1.108473   \n",
       "72                      -2.797203                        -0.775406   \n",
       "73                      -2.158273                        -0.690244   \n",
       "\n",
       "    Subiculum Connectivity  Amnesia Lesion T Map  Memory Network T  \\\n",
       "0                56.864683              0.447264          0.494596   \n",
       "1                52.970984              0.436157          0.502192   \n",
       "2                62.459631              0.497749          0.581148   \n",
       "3                59.611631              0.432617          0.520518   \n",
       "4                57.928350              0.193389          0.491742   \n",
       "..                     ...                   ...               ...   \n",
       "68               18.674670             -0.553686         -0.190240   \n",
       "69               15.353030             -0.505577         -0.179074   \n",
       "70               15.050219             -0.659502         -0.363412   \n",
       "72               17.382020             -0.527690         -0.175144   \n",
       "73               17.978233             -0.243491          0.103399   \n",
       "\n",
       "    Memory Network R  Subiculum Grey Matter  ...  Standardized Age  \\\n",
       "0           0.418688               1.646994  ...          0.289341   \n",
       "1           0.417569              -0.865158  ...         -0.016378   \n",
       "2           0.500706               0.319460  ...         -0.888064   \n",
       "3           0.455778              -0.440643  ...         -1.921065   \n",
       "4           0.455764              -0.328427  ...          0.512509   \n",
       "..               ...                    ...  ...               ...   \n",
       "68          0.055350              -8.205764  ...          1.034438   \n",
       "69          0.022968               9.086053  ...         -0.009451   \n",
       "70         -0.058739               8.416993  ...         -0.522107   \n",
       "72          0.043586                    NaN  ...               NaN   \n",
       "73          0.185311                    NaN  ...               NaN   \n",
       "\n",
       "    Standardized Percent Improvement  Standardized Subiculum Connectivity  \\\n",
       "0                           0.289341                            -1.179578   \n",
       "1                          -0.016378                            -1.635523   \n",
       "2                          -0.888064                            -0.524419   \n",
       "3                          -1.921065                            -0.857915   \n",
       "4                           0.512509                            -1.055024   \n",
       "..                               ...                                  ...   \n",
       "68                          1.034438                            -1.099623   \n",
       "69                         -0.009451                            -1.142557   \n",
       "70                         -0.522107                            -0.811944   \n",
       "72                               NaN                                  NaN   \n",
       "73                               NaN                                  NaN   \n",
       "\n",
       "   Standardized Subiculum Grey Matter  Standardized Subiculum White Matter  \\\n",
       "0                            1.970962                             0.663981   \n",
       "1                           -0.125938                            -1.611307   \n",
       "2                            0.862866                            -0.734373   \n",
       "3                            0.228406                            -0.052972   \n",
       "4                            0.322072                            -0.128024   \n",
       "..                                ...                                  ...   \n",
       "68                          -3.303201                             2.909124   \n",
       "69                           0.791574                            -0.144002   \n",
       "70                           0.633137                             0.522049   \n",
       "72                                NaN                                  NaN   \n",
       "73                                NaN                                  NaN   \n",
       "\n",
       "    Standardized Subiculum CSF  Standardized Subiculum Total  Disease.1  \\\n",
       "0                     1.773701                      1.789087  Alzheimer   \n",
       "1                    -0.272553                     -1.093456  Alzheimer   \n",
       "2                    -0.463295                      0.143505  Alzheimer   \n",
       "3                    -0.579749                      0.126248  Alzheimer   \n",
       "4                    -1.856148                      0.144737  Alzheimer   \n",
       "..                         ...                           ...        ...   \n",
       "68                   -3.303201                      2.775495  Parkinson   \n",
       "69                    0.791574                     -0.544369  Parkinson   \n",
       "70                    0.633137                     -1.339117  Parkinson   \n",
       "72                         NaN                           NaN  Parkinson   \n",
       "73                         NaN                           NaN  Parkinson   \n",
       "\n",
       "    Cohort  Inclusion Cohort  \n",
       "0        1                 1  \n",
       "1        1                 1  \n",
       "2        1                 1  \n",
       "3        1                 1  \n",
       "4        1                 1  \n",
       "..     ...               ...  \n",
       "68       0                 1  \n",
       "69       0                 1  \n",
       "70       0                 1  \n",
       "72       0                 1  \n",
       "73       0                 1  \n",
       "\n",
       "[72 rows x 24 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if drop_nans:\n",
    "    if column_to_drop_from == None:\n",
    "        data_df.dropna(inplace=True)\n",
    "    else:\n",
    "        data_df.dropna(subset=[column_to_drop_from], inplace=True)\n",
    "# data_df = data_df.fillna(method='pad')\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = data_df.iloc[:35, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dataframe by Value in a Given Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_split_by = 'Cohort'\n",
    "value_to_split_by = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Z-Scored Percent Cognitive Improvement</th>\n",
       "      <th>Percent Cognitive Improvement</th>\n",
       "      <th>Subiculum Connectivity</th>\n",
       "      <th>Amnesia Lesion T Map</th>\n",
       "      <th>Memory Network T</th>\n",
       "      <th>Memory Network R</th>\n",
       "      <th>Subiculum Grey Matter</th>\n",
       "      <th>Subiculum White Matter</th>\n",
       "      <th>...</th>\n",
       "      <th>Standardized Age</th>\n",
       "      <th>Standardized Percent Improvement</th>\n",
       "      <th>Standardized Subiculum Connectivity</th>\n",
       "      <th>Standardized Subiculum Grey Matter</th>\n",
       "      <th>Standardized Subiculum White Matter</th>\n",
       "      <th>Standardized Subiculum CSF</th>\n",
       "      <th>Standardized Subiculum Total</th>\n",
       "      <th>Disease.1</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>Inclusion Cohort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>0.494596</td>\n",
       "      <td>0.418688</td>\n",
       "      <td>1.646994</td>\n",
       "      <td>0.510111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>-1.179578</td>\n",
       "      <td>1.970962</td>\n",
       "      <td>0.663981</td>\n",
       "      <td>1.773701</td>\n",
       "      <td>1.789087</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>0.502192</td>\n",
       "      <td>0.417569</td>\n",
       "      <td>-0.865158</td>\n",
       "      <td>-1.615736</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-1.635523</td>\n",
       "      <td>-0.125938</td>\n",
       "      <td>-1.611307</td>\n",
       "      <td>-0.272553</td>\n",
       "      <td>-1.093456</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>0.581148</td>\n",
       "      <td>0.500706</td>\n",
       "      <td>0.319460</td>\n",
       "      <td>-0.796399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.524419</td>\n",
       "      <td>0.862866</td>\n",
       "      <td>-0.734373</td>\n",
       "      <td>-0.463295</td>\n",
       "      <td>0.143505</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>0.520518</td>\n",
       "      <td>0.455778</td>\n",
       "      <td>-0.440643</td>\n",
       "      <td>-0.159752</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-0.857915</td>\n",
       "      <td>0.228406</td>\n",
       "      <td>-0.052972</td>\n",
       "      <td>-0.579749</td>\n",
       "      <td>0.126248</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>0.491742</td>\n",
       "      <td>0.455764</td>\n",
       "      <td>-0.328427</td>\n",
       "      <td>-0.229875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>-1.055024</td>\n",
       "      <td>0.322072</td>\n",
       "      <td>-0.128024</td>\n",
       "      <td>-1.856148</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.028151</td>\n",
       "      <td>-38.461538</td>\n",
       "      <td>63.323903</td>\n",
       "      <td>0.513828</td>\n",
       "      <td>0.595253</td>\n",
       "      <td>0.522375</td>\n",
       "      <td>1.161078</td>\n",
       "      <td>-0.468849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059322</td>\n",
       "      <td>-0.059322</td>\n",
       "      <td>-0.423214</td>\n",
       "      <td>1.565367</td>\n",
       "      <td>-0.383797</td>\n",
       "      <td>-1.269291</td>\n",
       "      <td>0.852276</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.435498</td>\n",
       "      <td>-15.384615</td>\n",
       "      <td>53.317851</td>\n",
       "      <td>0.249080</td>\n",
       "      <td>0.505159</td>\n",
       "      <td>0.463887</td>\n",
       "      <td>-2.045541</td>\n",
       "      <td>-1.348159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413060</td>\n",
       "      <td>0.413060</td>\n",
       "      <td>-1.594906</td>\n",
       "      <td>-1.111207</td>\n",
       "      <td>-1.324921</td>\n",
       "      <td>-0.015839</td>\n",
       "      <td>-1.601922</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>-29.166667</td>\n",
       "      <td>57.979468</td>\n",
       "      <td>0.247062</td>\n",
       "      <td>0.538243</td>\n",
       "      <td>0.496959</td>\n",
       "      <td>-0.393004</td>\n",
       "      <td>-1.278036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130943</td>\n",
       "      <td>0.130943</td>\n",
       "      <td>-1.049038</td>\n",
       "      <td>0.268170</td>\n",
       "      <td>-1.249868</td>\n",
       "      <td>1.096779</td>\n",
       "      <td>-0.593001</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.133118</td>\n",
       "      <td>-30.434783</td>\n",
       "      <td>66.950749</td>\n",
       "      <td>0.514661</td>\n",
       "      <td>0.662954</td>\n",
       "      <td>0.589282</td>\n",
       "      <td>-0.295610</td>\n",
       "      <td>0.526719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104985</td>\n",
       "      <td>0.104985</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.349466</td>\n",
       "      <td>0.681757</td>\n",
       "      <td>-0.231823</td>\n",
       "      <td>0.669228</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.955451</td>\n",
       "      <td>-84.615385</td>\n",
       "      <td>69.267271</td>\n",
       "      <td>0.530628</td>\n",
       "      <td>0.706532</td>\n",
       "      <td>0.657522</td>\n",
       "      <td>1.332578</td>\n",
       "      <td>0.246226</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.004087</td>\n",
       "      <td>-1.004087</td>\n",
       "      <td>0.272745</td>\n",
       "      <td>1.708518</td>\n",
       "      <td>0.381545</td>\n",
       "      <td>0.771513</td>\n",
       "      <td>1.429771</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>111.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.581744</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>83.621480</td>\n",
       "      <td>0.514182</td>\n",
       "      <td>0.714419</td>\n",
       "      <td>0.689681</td>\n",
       "      <td>0.128905</td>\n",
       "      <td>-0.094242</td>\n",
       "      <td>...</td>\n",
       "      <td>1.580895</td>\n",
       "      <td>1.580895</td>\n",
       "      <td>1.953598</td>\n",
       "      <td>0.703809</td>\n",
       "      <td>0.017144</td>\n",
       "      <td>-0.852239</td>\n",
       "      <td>0.501588</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>113.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-0.460891</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>69.758721</td>\n",
       "      <td>0.497508</td>\n",
       "      <td>0.614933</td>\n",
       "      <td>0.544905</td>\n",
       "      <td>-0.579325</td>\n",
       "      <td>-0.775177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500213</td>\n",
       "      <td>-0.500213</td>\n",
       "      <td>0.330293</td>\n",
       "      <td>0.112648</td>\n",
       "      <td>-0.711660</td>\n",
       "      <td>-0.465016</td>\n",
       "      <td>-0.365578</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.420542</td>\n",
       "      <td>-16.129032</td>\n",
       "      <td>67.432125</td>\n",
       "      <td>0.461019</td>\n",
       "      <td>0.604464</td>\n",
       "      <td>0.537383</td>\n",
       "      <td>-1.324607</td>\n",
       "      <td>-1.297412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397822</td>\n",
       "      <td>0.397822</td>\n",
       "      <td>0.057852</td>\n",
       "      <td>-0.509441</td>\n",
       "      <td>-1.270606</td>\n",
       "      <td>-0.479645</td>\n",
       "      <td>-1.148309</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>115.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.225336</td>\n",
       "      <td>-48.275862</td>\n",
       "      <td>71.155530</td>\n",
       "      <td>0.227943</td>\n",
       "      <td>0.568978</td>\n",
       "      <td>0.539048</td>\n",
       "      <td>-1.155225</td>\n",
       "      <td>-1.005846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260221</td>\n",
       "      <td>-0.260221</td>\n",
       "      <td>0.493857</td>\n",
       "      <td>-0.368058</td>\n",
       "      <td>-0.958544</td>\n",
       "      <td>2.220871</td>\n",
       "      <td>-0.854939</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>116.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>-36.842105</td>\n",
       "      <td>61.952527</td>\n",
       "      <td>0.485376</td>\n",
       "      <td>0.593887</td>\n",
       "      <td>0.528984</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>-0.029655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026173</td>\n",
       "      <td>-0.026173</td>\n",
       "      <td>-0.583800</td>\n",
       "      <td>0.600422</td>\n",
       "      <td>0.086271</td>\n",
       "      <td>1.571485</td>\n",
       "      <td>0.472621</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>118.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.681812</td>\n",
       "      <td>-3.125000</td>\n",
       "      <td>72.547473</td>\n",
       "      <td>0.247475</td>\n",
       "      <td>0.544013</td>\n",
       "      <td>0.567287</td>\n",
       "      <td>-0.414177</td>\n",
       "      <td>-1.014150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664014</td>\n",
       "      <td>0.664014</td>\n",
       "      <td>0.656851</td>\n",
       "      <td>0.250497</td>\n",
       "      <td>-0.967432</td>\n",
       "      <td>-1.197583</td>\n",
       "      <td>-0.429060</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>119.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-0.887835</td>\n",
       "      <td>-81.250000</td>\n",
       "      <td>62.118622</td>\n",
       "      <td>0.503218</td>\n",
       "      <td>0.626241</td>\n",
       "      <td>0.546249</td>\n",
       "      <td>-1.214509</td>\n",
       "      <td>-0.248329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.935198</td>\n",
       "      <td>-0.935198</td>\n",
       "      <td>-0.564350</td>\n",
       "      <td>-0.417542</td>\n",
       "      <td>-0.147775</td>\n",
       "      <td>0.026612</td>\n",
       "      <td>-0.383452</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>120.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.186501</td>\n",
       "      <td>-27.777778</td>\n",
       "      <td>60.940570</td>\n",
       "      <td>0.284504</td>\n",
       "      <td>0.419123</td>\n",
       "      <td>0.378698</td>\n",
       "      <td>-0.084940</td>\n",
       "      <td>-0.248329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159373</td>\n",
       "      <td>0.159373</td>\n",
       "      <td>-0.702298</td>\n",
       "      <td>0.525312</td>\n",
       "      <td>-0.147775</td>\n",
       "      <td>-0.637402</td>\n",
       "      <td>0.274165</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.657244</td>\n",
       "      <td>-4.347826</td>\n",
       "      <td>71.223601</td>\n",
       "      <td>0.483995</td>\n",
       "      <td>0.684051</td>\n",
       "      <td>0.636102</td>\n",
       "      <td>-2.671196</td>\n",
       "      <td>-0.928342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638983</td>\n",
       "      <td>0.638983</td>\n",
       "      <td>0.501828</td>\n",
       "      <td>-1.633443</td>\n",
       "      <td>-0.875591</td>\n",
       "      <td>-0.336516</td>\n",
       "      <td>-1.685742</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>122.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-2.068210</td>\n",
       "      <td>-140.000000</td>\n",
       "      <td>68.856418</td>\n",
       "      <td>0.512125</td>\n",
       "      <td>0.672395</td>\n",
       "      <td>0.605023</td>\n",
       "      <td>-1.870865</td>\n",
       "      <td>-0.825924</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.137806</td>\n",
       "      <td>-2.137806</td>\n",
       "      <td>0.224634</td>\n",
       "      <td>-0.965405</td>\n",
       "      <td>-0.765974</td>\n",
       "      <td>-0.539593</td>\n",
       "      <td>-1.151390</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>123.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.840498</td>\n",
       "      <td>54.545455</td>\n",
       "      <td>64.525483</td>\n",
       "      <td>0.507649</td>\n",
       "      <td>0.638420</td>\n",
       "      <td>0.561814</td>\n",
       "      <td>3.146027</td>\n",
       "      <td>0.141963</td>\n",
       "      <td>...</td>\n",
       "      <td>1.844523</td>\n",
       "      <td>1.844523</td>\n",
       "      <td>-0.282511</td>\n",
       "      <td>3.222209</td>\n",
       "      <td>0.269953</td>\n",
       "      <td>-0.457272</td>\n",
       "      <td>2.415888</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>124.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.367883</td>\n",
       "      <td>-18.750000</td>\n",
       "      <td>60.270662</td>\n",
       "      <td>0.460080</td>\n",
       "      <td>0.521640</td>\n",
       "      <td>0.451422</td>\n",
       "      <td>-2.097414</td>\n",
       "      <td>1.341443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344171</td>\n",
       "      <td>0.344171</td>\n",
       "      <td>-0.780743</td>\n",
       "      <td>-1.154505</td>\n",
       "      <td>1.553753</td>\n",
       "      <td>-0.572291</td>\n",
       "      <td>0.164460</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>125.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-37.500000</td>\n",
       "      <td>71.676110</td>\n",
       "      <td>0.479488</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.625960</td>\n",
       "      <td>-1.843340</td>\n",
       "      <td>-0.194814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039640</td>\n",
       "      <td>-0.039640</td>\n",
       "      <td>0.554816</td>\n",
       "      <td>-0.942430</td>\n",
       "      <td>-0.090498</td>\n",
       "      <td>-0.251901</td>\n",
       "      <td>-0.713801</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>126.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-1.264551</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>76.111573</td>\n",
       "      <td>0.477498</td>\n",
       "      <td>0.683299</td>\n",
       "      <td>0.628300</td>\n",
       "      <td>0.481432</td>\n",
       "      <td>1.703132</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.319009</td>\n",
       "      <td>-1.319009</td>\n",
       "      <td>1.074201</td>\n",
       "      <td>0.998064</td>\n",
       "      <td>1.940868</td>\n",
       "      <td>1.606478</td>\n",
       "      <td>1.907422</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>127.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-0.791810</td>\n",
       "      <td>-76.470588</td>\n",
       "      <td>67.387791</td>\n",
       "      <td>0.501404</td>\n",
       "      <td>0.631439</td>\n",
       "      <td>0.571945</td>\n",
       "      <td>-2.189516</td>\n",
       "      <td>-0.853605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.837364</td>\n",
       "      <td>-0.837364</td>\n",
       "      <td>0.052661</td>\n",
       "      <td>-1.231383</td>\n",
       "      <td>-0.795600</td>\n",
       "      <td>0.045256</td>\n",
       "      <td>-1.355393</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>128.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.326026</td>\n",
       "      <td>-20.833333</td>\n",
       "      <td>75.530777</td>\n",
       "      <td>0.511383</td>\n",
       "      <td>0.658438</td>\n",
       "      <td>0.602109</td>\n",
       "      <td>0.129964</td>\n",
       "      <td>0.890254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301526</td>\n",
       "      <td>0.301526</td>\n",
       "      <td>1.006191</td>\n",
       "      <td>0.704693</td>\n",
       "      <td>1.070847</td>\n",
       "      <td>-1.051299</td>\n",
       "      <td>1.159822</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>129.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>-35.714286</td>\n",
       "      <td>77.657909</td>\n",
       "      <td>0.496203</td>\n",
       "      <td>0.704721</td>\n",
       "      <td>0.660007</td>\n",
       "      <td>0.868894</td>\n",
       "      <td>0.095829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003086</td>\n",
       "      <td>-0.003086</td>\n",
       "      <td>1.255274</td>\n",
       "      <td>1.321480</td>\n",
       "      <td>0.220577</td>\n",
       "      <td>-0.914194</td>\n",
       "      <td>1.059361</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>130.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.601088</td>\n",
       "      <td>-7.142857</td>\n",
       "      <td>65.993904</td>\n",
       "      <td>0.510956</td>\n",
       "      <td>0.639063</td>\n",
       "      <td>0.574723</td>\n",
       "      <td>-1.118172</td>\n",
       "      <td>1.888590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581768</td>\n",
       "      <td>0.581768</td>\n",
       "      <td>-0.110561</td>\n",
       "      <td>-0.337130</td>\n",
       "      <td>2.139363</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>1.100038</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>131.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-0.059061</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>71.997755</td>\n",
       "      <td>0.458336</td>\n",
       "      <td>0.627875</td>\n",
       "      <td>0.566249</td>\n",
       "      <td>-0.349600</td>\n",
       "      <td>-0.079479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090814</td>\n",
       "      <td>-0.090814</td>\n",
       "      <td>0.592480</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.032944</td>\n",
       "      <td>-0.579175</td>\n",
       "      <td>0.232872</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>133.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>74.948112</td>\n",
       "      <td>0.503698</td>\n",
       "      <td>0.671496</td>\n",
       "      <td>0.623739</td>\n",
       "      <td>-0.495692</td>\n",
       "      <td>-0.328602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855919</td>\n",
       "      <td>0.855919</td>\n",
       "      <td>0.937962</td>\n",
       "      <td>0.182456</td>\n",
       "      <td>-0.233691</td>\n",
       "      <td>-0.487963</td>\n",
       "      <td>-0.018588</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>134.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.335524</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>72.265483</td>\n",
       "      <td>0.402215</td>\n",
       "      <td>0.681575</td>\n",
       "      <td>0.648231</td>\n",
       "      <td>0.655049</td>\n",
       "      <td>-0.910811</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330038</td>\n",
       "      <td>1.330038</td>\n",
       "      <td>0.623830</td>\n",
       "      <td>1.142983</td>\n",
       "      <td>-0.856828</td>\n",
       "      <td>-2.251975</td>\n",
       "      <td>0.262455</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>135.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-0.173870</td>\n",
       "      <td>-45.714286</td>\n",
       "      <td>63.361510</td>\n",
       "      <td>0.481574</td>\n",
       "      <td>0.599392</td>\n",
       "      <td>0.549793</td>\n",
       "      <td>-1.849692</td>\n",
       "      <td>-0.003820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207785</td>\n",
       "      <td>-0.207785</td>\n",
       "      <td>-0.418810</td>\n",
       "      <td>-0.947732</td>\n",
       "      <td>0.113922</td>\n",
       "      <td>-0.370649</td>\n",
       "      <td>-0.589920</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>137.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-0.807926</td>\n",
       "      <td>-77.272727</td>\n",
       "      <td>65.596015</td>\n",
       "      <td>0.486037</td>\n",
       "      <td>0.612991</td>\n",
       "      <td>0.550762</td>\n",
       "      <td>-0.978432</td>\n",
       "      <td>0.596843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.853784</td>\n",
       "      <td>-0.853784</td>\n",
       "      <td>-0.157153</td>\n",
       "      <td>-0.220488</td>\n",
       "      <td>0.756810</td>\n",
       "      <td>-0.458133</td>\n",
       "      <td>0.318540</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>138.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.553251</td>\n",
       "      <td>-9.523810</td>\n",
       "      <td>80.611188</td>\n",
       "      <td>0.471613</td>\n",
       "      <td>0.689179</td>\n",
       "      <td>0.655872</td>\n",
       "      <td>-1.214509</td>\n",
       "      <td>0.399390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533031</td>\n",
       "      <td>0.533031</td>\n",
       "      <td>1.601098</td>\n",
       "      <td>-0.417542</td>\n",
       "      <td>0.545477</td>\n",
       "      <td>-1.119852</td>\n",
       "      <td>0.049207</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>139.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-1.382736</td>\n",
       "      <td>-105.882353</td>\n",
       "      <td>61.050691</td>\n",
       "      <td>0.520899</td>\n",
       "      <td>0.630829</td>\n",
       "      <td>0.553971</td>\n",
       "      <td>-0.422646</td>\n",
       "      <td>2.569525</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.439420</td>\n",
       "      <td>-1.439420</td>\n",
       "      <td>-0.689403</td>\n",
       "      <td>0.243428</td>\n",
       "      <td>2.868167</td>\n",
       "      <td>0.807654</td>\n",
       "      <td>1.959809</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>140.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.109898</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>72.686647</td>\n",
       "      <td>0.415110</td>\n",
       "      <td>0.665685</td>\n",
       "      <td>0.627668</td>\n",
       "      <td>-3.151819</td>\n",
       "      <td>-0.132072</td>\n",
       "      <td>...</td>\n",
       "      <td>1.100162</td>\n",
       "      <td>1.100162</td>\n",
       "      <td>0.673148</td>\n",
       "      <td>-2.034620</td>\n",
       "      <td>-0.023345</td>\n",
       "      <td>-0.271692</td>\n",
       "      <td>-1.433666</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>141.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.744598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.829337</td>\n",
       "      <td>0.503535</td>\n",
       "      <td>0.699511</td>\n",
       "      <td>0.662179</td>\n",
       "      <td>-0.457581</td>\n",
       "      <td>-1.729225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727982</td>\n",
       "      <td>0.727982</td>\n",
       "      <td>1.158250</td>\n",
       "      <td>0.214267</td>\n",
       "      <td>-1.732774</td>\n",
       "      <td>-0.916776</td>\n",
       "      <td>-0.931979</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>142.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>69.979040</td>\n",
       "      <td>0.521941</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.527993</td>\n",
       "      <td>-0.963611</td>\n",
       "      <td>0.010021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841704</td>\n",
       "      <td>0.841704</td>\n",
       "      <td>0.356092</td>\n",
       "      <td>-0.208117</td>\n",
       "      <td>0.128735</td>\n",
       "      <td>0.535738</td>\n",
       "      <td>-0.064813</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>143.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>66.339918</td>\n",
       "      <td>0.569043</td>\n",
       "      <td>0.677359</td>\n",
       "      <td>0.594520</td>\n",
       "      <td>-1.813699</td>\n",
       "      <td>-0.323066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855919</td>\n",
       "      <td>0.855919</td>\n",
       "      <td>-0.070043</td>\n",
       "      <td>-0.917687</td>\n",
       "      <td>-0.227766</td>\n",
       "      <td>1.002412</td>\n",
       "      <td>-0.782213</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>144.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-0.491801</td>\n",
       "      <td>-61.538462</td>\n",
       "      <td>58.899731</td>\n",
       "      <td>0.470849</td>\n",
       "      <td>0.509424</td>\n",
       "      <td>0.445631</td>\n",
       "      <td>-0.785760</td>\n",
       "      <td>0.349566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.531705</td>\n",
       "      <td>-0.531705</td>\n",
       "      <td>-0.941277</td>\n",
       "      <td>-0.059664</td>\n",
       "      <td>0.492150</td>\n",
       "      <td>-0.198837</td>\n",
       "      <td>0.265537</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>145.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>85.082502</td>\n",
       "      <td>0.420971</td>\n",
       "      <td>0.674168</td>\n",
       "      <td>0.637427</td>\n",
       "      <td>-0.697893</td>\n",
       "      <td>-0.588797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841704</td>\n",
       "      <td>0.841704</td>\n",
       "      <td>2.124682</td>\n",
       "      <td>0.013679</td>\n",
       "      <td>-0.512177</td>\n",
       "      <td>2.644521</td>\n",
       "      <td>-0.310109</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>146.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.351301</td>\n",
       "      <td>-54.545455</td>\n",
       "      <td>46.331586</td>\n",
       "      <td>0.469571</td>\n",
       "      <td>0.592629</td>\n",
       "      <td>0.519801</td>\n",
       "      <td>-1.833813</td>\n",
       "      <td>1.215036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.388559</td>\n",
       "      <td>-0.388559</td>\n",
       "      <td>-2.412986</td>\n",
       "      <td>-0.934477</td>\n",
       "      <td>1.418460</td>\n",
       "      <td>0.411253</td>\n",
       "      <td>0.233488</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>147.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.648924</td>\n",
       "      <td>-4.761905</td>\n",
       "      <td>69.447270</td>\n",
       "      <td>0.530166</td>\n",
       "      <td>0.668692</td>\n",
       "      <td>0.595269</td>\n",
       "      <td>-0.195039</td>\n",
       "      <td>-1.363845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630506</td>\n",
       "      <td>0.630506</td>\n",
       "      <td>0.293822</td>\n",
       "      <td>0.433412</td>\n",
       "      <td>-1.341709</td>\n",
       "      <td>-0.290623</td>\n",
       "      <td>-0.535067</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>148.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-3.428250</td>\n",
       "      <td>-207.692308</td>\n",
       "      <td>75.739873</td>\n",
       "      <td>0.489891</td>\n",
       "      <td>0.650916</td>\n",
       "      <td>0.592976</td>\n",
       "      <td>-0.588853</td>\n",
       "      <td>-0.047185</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.523461</td>\n",
       "      <td>-3.523461</td>\n",
       "      <td>1.030675</td>\n",
       "      <td>0.104695</td>\n",
       "      <td>0.067508</td>\n",
       "      <td>0.609166</td>\n",
       "      <td>0.115154</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>149.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.063636</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>62.007555</td>\n",
       "      <td>0.480527</td>\n",
       "      <td>0.635260</td>\n",
       "      <td>0.575416</td>\n",
       "      <td>-0.740238</td>\n",
       "      <td>0.897635</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.114310</td>\n",
       "      <td>-1.114310</td>\n",
       "      <td>-0.577356</td>\n",
       "      <td>-0.021667</td>\n",
       "      <td>1.078747</td>\n",
       "      <td>0.473782</td>\n",
       "      <td>0.658134</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>150.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.018573</td>\n",
       "      <td>13.636364</td>\n",
       "      <td>73.488381</td>\n",
       "      <td>0.461720</td>\n",
       "      <td>0.701090</td>\n",
       "      <td>0.655481</td>\n",
       "      <td>-2.789764</td>\n",
       "      <td>-0.078556</td>\n",
       "      <td>...</td>\n",
       "      <td>1.007117</td>\n",
       "      <td>1.007117</td>\n",
       "      <td>0.767030</td>\n",
       "      <td>-1.732412</td>\n",
       "      <td>0.033932</td>\n",
       "      <td>1.582098</td>\n",
       "      <td>-1.187137</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   Age  Z-Scored Percent Cognitive Improvement  \\\n",
       "0     101.0  62.0                                0.314066   \n",
       "1     102.0  77.0                                0.013999   \n",
       "2     103.0  76.0                               -0.841572   \n",
       "3     104.0  65.0                               -1.855477   \n",
       "4     105.0  50.0                                0.533109   \n",
       "5     106.0  66.0                               -0.028151   \n",
       "6     107.0  64.0                                0.435498   \n",
       "7     108.0  60.0                                0.158596   \n",
       "8     109.0  72.0                                0.133118   \n",
       "9     110.0  72.0                               -0.955451   \n",
       "10    111.0  62.0                                1.581744   \n",
       "11    113.0  69.0                               -0.460891   \n",
       "12    114.0  67.0                                0.420542   \n",
       "13    115.0  60.0                               -0.225336   \n",
       "14    116.0  67.0                                0.004385   \n",
       "15    118.0  52.0                                0.681812   \n",
       "16    119.0  75.0                               -0.887835   \n",
       "17    120.0  68.0                                0.186501   \n",
       "18    121.0  72.0                                0.657244   \n",
       "19    122.0  58.0                               -2.068210   \n",
       "20    123.0  47.0                                1.840498   \n",
       "21    124.0  61.0                                0.367883   \n",
       "22    125.0  73.0                               -0.008833   \n",
       "23    126.0  69.0                               -1.264551   \n",
       "24    127.0  74.0                               -0.791810   \n",
       "25    128.0  72.0                                0.326026   \n",
       "26    129.0  69.0                                0.027045   \n",
       "27    130.0  66.0                                0.601088   \n",
       "28    131.0  68.0                               -0.059061   \n",
       "29    133.0  74.0                                0.870170   \n",
       "30    134.0  66.0                                1.335524   \n",
       "31    135.0  57.0                               -0.173870   \n",
       "32    137.0  57.0                               -0.807926   \n",
       "33    138.0  72.0                                0.553251   \n",
       "34    139.0  58.0                               -1.382736   \n",
       "35    140.0  73.0                                1.109898   \n",
       "36    141.0  72.0                                0.744598   \n",
       "37    142.0  77.0                                0.856218   \n",
       "38    143.0  71.0                                0.870170   \n",
       "39    144.0  79.0                               -0.491801   \n",
       "40    145.0  74.0                                0.856218   \n",
       "41    146.0  76.0                               -0.351301   \n",
       "42    147.0  59.0                                0.648924   \n",
       "43    148.0  51.0                               -3.428250   \n",
       "44    149.0  77.0                               -1.063636   \n",
       "45    150.0  71.0                                1.018573   \n",
       "\n",
       "    Percent Cognitive Improvement  Subiculum Connectivity  \\\n",
       "0                      -21.428571               56.864683   \n",
       "1                      -36.363636               52.970984   \n",
       "2                      -78.947368               62.459631   \n",
       "3                     -129.411765               59.611631   \n",
       "4                      -10.526316               57.928350   \n",
       "5                      -38.461538               63.323903   \n",
       "6                      -15.384615               53.317851   \n",
       "7                      -29.166667               57.979468   \n",
       "8                      -30.434783               66.950749   \n",
       "9                      -84.615385               69.267271   \n",
       "10                      41.666667               83.621480   \n",
       "11                     -60.000000               69.758721   \n",
       "12                     -16.129032               67.432125   \n",
       "13                     -48.275862               71.155530   \n",
       "14                     -36.842105               61.952527   \n",
       "15                      -3.125000               72.547473   \n",
       "16                     -81.250000               62.118622   \n",
       "17                     -27.777778               60.940570   \n",
       "18                      -4.347826               71.223601   \n",
       "19                    -140.000000               68.856418   \n",
       "20                      54.545455               64.525483   \n",
       "21                     -18.750000               60.270662   \n",
       "22                     -37.500000               71.676110   \n",
       "23                    -100.000000               76.111573   \n",
       "24                     -76.470588               67.387791   \n",
       "25                     -20.833333               75.530777   \n",
       "26                     -35.714286               77.657909   \n",
       "27                      -7.142857               65.993904   \n",
       "28                     -40.000000               71.997755   \n",
       "29                       6.250000               74.948112   \n",
       "30                      29.411765               72.265483   \n",
       "31                     -45.714286               63.361510   \n",
       "32                     -77.272727               65.596015   \n",
       "33                      -9.523810               80.611188   \n",
       "34                    -105.882353               61.050691   \n",
       "35                      18.181818               72.686647   \n",
       "36                       0.000000               76.829337   \n",
       "37                       5.555556               69.979040   \n",
       "38                       6.250000               66.339918   \n",
       "39                     -61.538462               58.899731   \n",
       "40                       5.555556               85.082502   \n",
       "41                     -54.545455               46.331586   \n",
       "42                      -4.761905               69.447270   \n",
       "43                    -207.692308               75.739873   \n",
       "44                     -90.000000               62.007555   \n",
       "45                      13.636364               73.488381   \n",
       "\n",
       "    Amnesia Lesion T Map  Memory Network T  Memory Network R  \\\n",
       "0               0.447264          0.494596          0.418688   \n",
       "1               0.436157          0.502192          0.417569   \n",
       "2               0.497749          0.581148          0.500706   \n",
       "3               0.432617          0.520518          0.455778   \n",
       "4               0.193389          0.491742          0.455764   \n",
       "5               0.513828          0.595253          0.522375   \n",
       "6               0.249080          0.505159          0.463887   \n",
       "7               0.247062          0.538243          0.496959   \n",
       "8               0.514661          0.662954          0.589282   \n",
       "9               0.530628          0.706532          0.657522   \n",
       "10              0.514182          0.714419          0.689681   \n",
       "11              0.497508          0.614933          0.544905   \n",
       "12              0.461019          0.604464          0.537383   \n",
       "13              0.227943          0.568978          0.539048   \n",
       "14              0.485376          0.593887          0.528984   \n",
       "15              0.247475          0.544013          0.567287   \n",
       "16              0.503218          0.626241          0.546249   \n",
       "17              0.284504          0.419123          0.378698   \n",
       "18              0.483995          0.684051          0.636102   \n",
       "19              0.512125          0.672395          0.605023   \n",
       "20              0.507649          0.638420          0.561814   \n",
       "21              0.460080          0.521640          0.451422   \n",
       "22              0.479488          0.682800          0.625960   \n",
       "23              0.477498          0.683299          0.628300   \n",
       "24              0.501404          0.631439          0.571945   \n",
       "25              0.511383          0.658438          0.602109   \n",
       "26              0.496203          0.704721          0.660007   \n",
       "27              0.510956          0.639063          0.574723   \n",
       "28              0.458336          0.627875          0.566249   \n",
       "29              0.503698          0.671496          0.623739   \n",
       "30              0.402215          0.681575          0.648231   \n",
       "31              0.481574          0.599392          0.549793   \n",
       "32              0.486037          0.612991          0.550762   \n",
       "33              0.471613          0.689179          0.655872   \n",
       "34              0.520899          0.630829          0.553971   \n",
       "35              0.415110          0.665685          0.627668   \n",
       "36              0.503535          0.699511          0.662179   \n",
       "37              0.521941          0.594444          0.527993   \n",
       "38              0.569043          0.677359          0.594520   \n",
       "39              0.470849          0.509424          0.445631   \n",
       "40              0.420971          0.674168          0.637427   \n",
       "41              0.469571          0.592629          0.519801   \n",
       "42              0.530166          0.668692          0.595269   \n",
       "43              0.489891          0.650916          0.592976   \n",
       "44              0.480527          0.635260          0.575416   \n",
       "45              0.461720          0.701090          0.655481   \n",
       "\n",
       "    Subiculum Grey Matter  Subiculum White Matter  ...  Standardized Age  \\\n",
       "0                1.646994                0.510111  ...          0.289341   \n",
       "1               -0.865158               -1.615736  ...         -0.016378   \n",
       "2                0.319460               -0.796399  ...         -0.888064   \n",
       "3               -0.440643               -0.159752  ...         -1.921065   \n",
       "4               -0.328427               -0.229875  ...          0.512509   \n",
       "5                1.161078               -0.468849  ...         -0.059322   \n",
       "6               -2.045541               -1.348159  ...          0.413060   \n",
       "7               -0.393004               -1.278036  ...          0.130943   \n",
       "8               -0.295610                0.526719  ...          0.104985   \n",
       "9                1.332578                0.246226  ...         -1.004087   \n",
       "10               0.128905               -0.094242  ...          1.580895   \n",
       "11              -0.579325               -0.775177  ...         -0.500213   \n",
       "12              -1.324607               -1.297412  ...          0.397822   \n",
       "13              -1.155225               -1.005846  ...         -0.260221   \n",
       "14               0.005044               -0.029655  ...         -0.026173   \n",
       "15              -0.414177               -1.014150  ...          0.664014   \n",
       "16              -1.214509               -0.248329  ...         -0.935198   \n",
       "17              -0.084940               -0.248329  ...          0.159373   \n",
       "18              -2.671196               -0.928342  ...          0.638983   \n",
       "19              -1.870865               -0.825924  ...         -2.137806   \n",
       "20               3.146027                0.141963  ...          1.844523   \n",
       "21              -2.097414                1.341443  ...          0.344171   \n",
       "22              -1.843340               -0.194814  ...         -0.039640   \n",
       "23               0.481432                1.703132  ...         -1.319009   \n",
       "24              -2.189516               -0.853605  ...         -0.837364   \n",
       "25               0.129964                0.890254  ...          0.301526   \n",
       "26               0.868894                0.095829  ...         -0.003086   \n",
       "27              -1.118172                1.888590  ...          0.581768   \n",
       "28              -0.349600               -0.079479  ...         -0.090814   \n",
       "29              -0.495692               -0.328602  ...          0.855919   \n",
       "30               0.655049               -0.910811  ...          1.330038   \n",
       "31              -1.849692               -0.003820  ...         -0.207785   \n",
       "32              -0.978432                0.596843  ...         -0.853784   \n",
       "33              -1.214509                0.399390  ...          0.533031   \n",
       "34              -0.422646                2.569525  ...         -1.439420   \n",
       "35              -3.151819               -0.132072  ...          1.100162   \n",
       "36              -0.457581               -1.729225  ...          0.727982   \n",
       "37              -0.963611                0.010021  ...          0.841704   \n",
       "38              -1.813699               -0.323066  ...          0.855919   \n",
       "39              -0.785760                0.349566  ...         -0.531705   \n",
       "40              -0.697893               -0.588797  ...          0.841704   \n",
       "41              -1.833813                1.215036  ...         -0.388559   \n",
       "42              -0.195039               -1.363845  ...          0.630506   \n",
       "43              -0.588853               -0.047185  ...         -3.523461   \n",
       "44              -0.740238                0.897635  ...         -1.114310   \n",
       "45              -2.789764               -0.078556  ...          1.007117   \n",
       "\n",
       "    Standardized Percent Improvement Standardized Subiculum Connectivity  \\\n",
       "0                           0.289341                           -1.179578   \n",
       "1                          -0.016378                           -1.635523   \n",
       "2                          -0.888064                           -0.524419   \n",
       "3                          -1.921065                           -0.857915   \n",
       "4                           0.512509                           -1.055024   \n",
       "5                          -0.059322                           -0.423214   \n",
       "6                           0.413060                           -1.594906   \n",
       "7                           0.130943                           -1.049038   \n",
       "8                           0.104985                            0.001484   \n",
       "9                          -1.004087                            0.272745   \n",
       "10                          1.580895                            1.953598   \n",
       "11                         -0.500213                            0.330293   \n",
       "12                          0.397822                            0.057852   \n",
       "13                         -0.260221                            0.493857   \n",
       "14                         -0.026173                           -0.583800   \n",
       "15                          0.664014                            0.656851   \n",
       "16                         -0.935198                           -0.564350   \n",
       "17                          0.159373                           -0.702298   \n",
       "18                          0.638983                            0.501828   \n",
       "19                         -2.137806                            0.224634   \n",
       "20                          1.844523                           -0.282511   \n",
       "21                          0.344171                           -0.780743   \n",
       "22                         -0.039640                            0.554816   \n",
       "23                         -1.319009                            1.074201   \n",
       "24                         -0.837364                            0.052661   \n",
       "25                          0.301526                            1.006191   \n",
       "26                         -0.003086                            1.255274   \n",
       "27                          0.581768                           -0.110561   \n",
       "28                         -0.090814                            0.592480   \n",
       "29                          0.855919                            0.937962   \n",
       "30                          1.330038                            0.623830   \n",
       "31                         -0.207785                           -0.418810   \n",
       "32                         -0.853784                           -0.157153   \n",
       "33                          0.533031                            1.601098   \n",
       "34                         -1.439420                           -0.689403   \n",
       "35                          1.100162                            0.673148   \n",
       "36                          0.727982                            1.158250   \n",
       "37                          0.841704                            0.356092   \n",
       "38                          0.855919                           -0.070043   \n",
       "39                         -0.531705                           -0.941277   \n",
       "40                          0.841704                            2.124682   \n",
       "41                         -0.388559                           -2.412986   \n",
       "42                          0.630506                            0.293822   \n",
       "43                         -3.523461                            1.030675   \n",
       "44                         -1.114310                           -0.577356   \n",
       "45                          1.007117                            0.767030   \n",
       "\n",
       "    Standardized Subiculum Grey Matter  Standardized Subiculum White Matter  \\\n",
       "0                             1.970962                             0.663981   \n",
       "1                            -0.125938                            -1.611307   \n",
       "2                             0.862866                            -0.734373   \n",
       "3                             0.228406                            -0.052972   \n",
       "4                             0.322072                            -0.128024   \n",
       "5                             1.565367                            -0.383797   \n",
       "6                            -1.111207                            -1.324921   \n",
       "7                             0.268170                            -1.249868   \n",
       "8                             0.349466                             0.681757   \n",
       "9                             1.708518                             0.381545   \n",
       "10                            0.703809                             0.017144   \n",
       "11                            0.112648                            -0.711660   \n",
       "12                           -0.509441                            -1.270606   \n",
       "13                           -0.368058                            -0.958544   \n",
       "14                            0.600422                             0.086271   \n",
       "15                            0.250497                            -0.967432   \n",
       "16                           -0.417542                            -0.147775   \n",
       "17                            0.525312                            -0.147775   \n",
       "18                           -1.633443                            -0.875591   \n",
       "19                           -0.965405                            -0.765974   \n",
       "20                            3.222209                             0.269953   \n",
       "21                           -1.154505                             1.553753   \n",
       "22                           -0.942430                            -0.090498   \n",
       "23                            0.998064                             1.940868   \n",
       "24                           -1.231383                            -0.795600   \n",
       "25                            0.704693                             1.070847   \n",
       "26                            1.321480                             0.220577   \n",
       "27                           -0.337130                             2.139363   \n",
       "28                            0.304400                             0.032944   \n",
       "29                            0.182456                            -0.233691   \n",
       "30                            1.142983                            -0.856828   \n",
       "31                           -0.947732                             0.113922   \n",
       "32                           -0.220488                             0.756810   \n",
       "33                           -0.417542                             0.545477   \n",
       "34                            0.243428                             2.868167   \n",
       "35                           -2.034620                            -0.023345   \n",
       "36                            0.214267                            -1.732774   \n",
       "37                           -0.208117                             0.128735   \n",
       "38                           -0.917687                            -0.227766   \n",
       "39                           -0.059664                             0.492150   \n",
       "40                            0.013679                            -0.512177   \n",
       "41                           -0.934477                             1.418460   \n",
       "42                            0.433412                            -1.341709   \n",
       "43                            0.104695                             0.067508   \n",
       "44                           -0.021667                             1.078747   \n",
       "45                           -1.732412                             0.033932   \n",
       "\n",
       "    Standardized Subiculum CSF  Standardized Subiculum Total  Disease.1  \\\n",
       "0                     1.773701                      1.789087  Alzheimer   \n",
       "1                    -0.272553                     -1.093456  Alzheimer   \n",
       "2                    -0.463295                      0.143505  Alzheimer   \n",
       "3                    -0.579749                      0.126248  Alzheimer   \n",
       "4                    -1.856148                      0.144737  Alzheimer   \n",
       "5                    -1.269291                      0.852276  Alzheimer   \n",
       "6                    -0.015839                     -1.601922  Alzheimer   \n",
       "7                     1.096779                     -0.593001  Alzheimer   \n",
       "8                    -0.231823                      0.669228  Alzheimer   \n",
       "9                     0.771513                      1.429771  Alzheimer   \n",
       "10                   -0.852239                      0.501588  Alzheimer   \n",
       "11                   -0.465016                     -0.365578  Alzheimer   \n",
       "12                   -0.479645                     -1.148309  Alzheimer   \n",
       "13                    2.220871                     -0.854939  Alzheimer   \n",
       "14                    1.571485                      0.472621  Alzheimer   \n",
       "15                   -1.197583                     -0.429060  Alzheimer   \n",
       "16                    0.026612                     -0.383452  Alzheimer   \n",
       "17                   -0.637402                      0.274165  Alzheimer   \n",
       "18                   -0.336516                     -1.685742  Alzheimer   \n",
       "19                   -0.539593                     -1.151390  Alzheimer   \n",
       "20                   -0.457272                      2.415888  Alzheimer   \n",
       "21                   -0.572291                      0.164460  Alzheimer   \n",
       "22                   -0.251901                     -0.713801  Alzheimer   \n",
       "23                    1.606478                      1.907422  Alzheimer   \n",
       "24                    0.045256                     -1.355393  Alzheimer   \n",
       "25                   -1.051299                      1.159822  Alzheimer   \n",
       "26                   -0.914194                      1.059361  Alzheimer   \n",
       "27                   -0.100741                      1.100038  Alzheimer   \n",
       "28                   -0.579175                      0.232872  Alzheimer   \n",
       "29                   -0.487963                     -0.018588  Alzheimer   \n",
       "30                   -2.251975                      0.262455  Alzheimer   \n",
       "31                   -0.370649                     -0.589920  Alzheimer   \n",
       "32                   -0.458133                      0.318540  Alzheimer   \n",
       "33                   -1.119852                      0.049207  Alzheimer   \n",
       "34                    0.807654                      1.959809  Alzheimer   \n",
       "35                   -0.271692                     -1.433666  Alzheimer   \n",
       "36                   -0.916776                     -0.931979  Alzheimer   \n",
       "37                    0.535738                     -0.064813  Alzheimer   \n",
       "38                    1.002412                     -0.782213  Alzheimer   \n",
       "39                   -0.198837                      0.265537  Alzheimer   \n",
       "40                    2.644521                     -0.310109  Alzheimer   \n",
       "41                    0.411253                      0.233488  Alzheimer   \n",
       "42                   -0.290623                     -0.535067  Alzheimer   \n",
       "43                    0.609166                      0.115154  Alzheimer   \n",
       "44                    0.473782                      0.658134  Alzheimer   \n",
       "45                    1.582098                     -1.187137  Alzheimer   \n",
       "\n",
       "    Cohort  Inclusion Cohort  \n",
       "0      1.0               1.0  \n",
       "1      1.0               1.0  \n",
       "2      1.0               1.0  \n",
       "3      1.0               1.0  \n",
       "4      1.0               1.0  \n",
       "5      1.0               1.0  \n",
       "6      1.0               1.0  \n",
       "7      1.0               1.0  \n",
       "8      1.0               1.0  \n",
       "9      1.0               1.0  \n",
       "10     1.0               1.0  \n",
       "11     1.0               1.0  \n",
       "12     1.0               1.0  \n",
       "13     1.0               1.0  \n",
       "14     1.0               1.0  \n",
       "15     1.0               1.0  \n",
       "16     1.0               1.0  \n",
       "17     1.0               1.0  \n",
       "18     1.0               1.0  \n",
       "19     1.0               1.0  \n",
       "20     1.0               1.0  \n",
       "21     1.0               1.0  \n",
       "22     1.0               1.0  \n",
       "23     1.0               1.0  \n",
       "24     1.0               1.0  \n",
       "25     1.0               1.0  \n",
       "26     1.0               1.0  \n",
       "27     1.0               1.0  \n",
       "28     1.0               1.0  \n",
       "29     1.0               1.0  \n",
       "30     1.0               1.0  \n",
       "31     1.0               1.0  \n",
       "32     1.0               1.0  \n",
       "33     1.0               1.0  \n",
       "34     1.0               1.0  \n",
       "35     1.0               1.0  \n",
       "36     1.0               1.0  \n",
       "37     1.0               1.0  \n",
       "38     1.0               1.0  \n",
       "39     1.0               1.0  \n",
       "40     1.0               1.0  \n",
       "41     1.0               1.0  \n",
       "42     1.0               1.0  \n",
       "43     1.0               1.0  \n",
       "44     1.0               1.0  \n",
       "45     1.0               1.0  \n",
       "\n",
       "[46 rows x 23 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df[data_df[column_to_split_by] == value_to_split_by]\n",
    "dropped_df = data_df[data_df[column_to_split_by] != value_to_split_by]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Rows by Value of a Given Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Age'\n",
    "value = 65\n",
    "drop_greater_or_equal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_greater_or_equal:\n",
    "    data_df = data_df[data_df[column] <= value]\n",
    "else:\n",
    "    data_df = data_df[data_df[column] >= value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>34.0</td>\n",
       "      <td>117.470588</td>\n",
       "      <td>34.299249</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>110.750000</td>\n",
       "      <td>126.500000</td>\n",
       "      <td>139.500000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>34.0</td>\n",
       "      <td>71.735294</td>\n",
       "      <td>3.933607</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>68.250000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z-Scored Percent Cognitive Improvement</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.074955</td>\n",
       "      <td>0.760665</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-0.433494</td>\n",
       "      <td>0.114844</td>\n",
       "      <td>0.643205</td>\n",
       "      <td>1.335524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percent Cognitive Improvement</th>\n",
       "      <td>34.0</td>\n",
       "      <td>-31.621665</td>\n",
       "      <td>38.757632</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-58.636364</td>\n",
       "      <td>-29.106280</td>\n",
       "      <td>-0.177305</td>\n",
       "      <td>29.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z-Scored Subiculum Connectivity</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.133261</td>\n",
       "      <td>1.024626</td>\n",
       "      <td>-2.576474</td>\n",
       "      <td>-0.626785</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.734739</td>\n",
       "      <td>2.183535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum Connectivity</th>\n",
       "      <td>34.0</td>\n",
       "      <td>64.425914</td>\n",
       "      <td>14.745831</td>\n",
       "      <td>15.519790</td>\n",
       "      <td>61.966284</td>\n",
       "      <td>67.409958</td>\n",
       "      <td>72.581356</td>\n",
       "      <td>85.082502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amnesia Lesion T Map</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.405642</td>\n",
       "      <td>0.239716</td>\n",
       "      <td>-0.520786</td>\n",
       "      <td>0.441702</td>\n",
       "      <td>0.480007</td>\n",
       "      <td>0.503456</td>\n",
       "      <td>0.569043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memory Network T</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.581327</td>\n",
       "      <td>0.180462</td>\n",
       "      <td>-0.168692</td>\n",
       "      <td>0.592943</td>\n",
       "      <td>0.633350</td>\n",
       "      <td>0.680521</td>\n",
       "      <td>0.706532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memory Network R</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.541191</td>\n",
       "      <td>0.129461</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>0.520445</td>\n",
       "      <td>0.573334</td>\n",
       "      <td>0.628142</td>\n",
       "      <td>0.662179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum Grey Matter</th>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.278204</td>\n",
       "      <td>2.158913</td>\n",
       "      <td>-3.151819</td>\n",
       "      <td>-1.214509</td>\n",
       "      <td>-0.537509</td>\n",
       "      <td>0.272086</td>\n",
       "      <td>9.615373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum White Matter</th>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.131562</td>\n",
       "      <td>0.946388</td>\n",
       "      <td>-1.854694</td>\n",
       "      <td>-0.791093</td>\n",
       "      <td>-0.177283</td>\n",
       "      <td>0.323731</td>\n",
       "      <td>1.913677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum CSF</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1.273195</td>\n",
       "      <td>1.831018</td>\n",
       "      <td>-1.421123</td>\n",
       "      <td>0.430795</td>\n",
       "      <td>0.774300</td>\n",
       "      <td>1.606668</td>\n",
       "      <td>9.615373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum Total</th>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.188262</td>\n",
       "      <td>1.460615</td>\n",
       "      <td>-2.144075</td>\n",
       "      <td>-1.104652</td>\n",
       "      <td>-0.296989</td>\n",
       "      <td>0.303696</td>\n",
       "      <td>5.194540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Age</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.024226</td>\n",
       "      <td>0.768795</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-0.472299</td>\n",
       "      <td>-0.004488</td>\n",
       "      <td>0.624679</td>\n",
       "      <td>1.330038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Percent Improvement</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.024226</td>\n",
       "      <td>0.768795</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-0.472299</td>\n",
       "      <td>-0.004488</td>\n",
       "      <td>0.624679</td>\n",
       "      <td>1.330038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum Connectivity</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.140363</td>\n",
       "      <td>0.989747</td>\n",
       "      <td>-2.412986</td>\n",
       "      <td>-0.574105</td>\n",
       "      <td>0.165298</td>\n",
       "      <td>0.743559</td>\n",
       "      <td>2.124682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum Grey Matter</th>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.046168</td>\n",
       "      <td>0.934875</td>\n",
       "      <td>-2.034620</td>\n",
       "      <td>-0.644489</td>\n",
       "      <td>-0.003994</td>\n",
       "      <td>0.581644</td>\n",
       "      <td>1.708518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum White Matter</th>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.049479</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>-1.732774</td>\n",
       "      <td>-0.728695</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>0.464498</td>\n",
       "      <td>2.139363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum CSF</th>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.090876</td>\n",
       "      <td>0.988079</td>\n",
       "      <td>-2.251975</td>\n",
       "      <td>-0.622989</td>\n",
       "      <td>-0.272122</td>\n",
       "      <td>0.458150</td>\n",
       "      <td>2.644521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum Total</th>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>0.905173</td>\n",
       "      <td>-1.685742</td>\n",
       "      <td>-0.765110</td>\n",
       "      <td>0.134876</td>\n",
       "      <td>0.611756</td>\n",
       "      <td>1.907422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cohort</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.287902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inclusion Cohort</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        count        mean        std  \\\n",
       "subject                                  34.0  117.470588  34.299249   \n",
       "Age                                      34.0   71.735294   3.933607   \n",
       "Z-Scored Percent Cognitive Improvement   34.0    0.074955   0.760665   \n",
       "Percent Cognitive Improvement            34.0  -31.621665  38.757632   \n",
       "Z-Scored Subiculum Connectivity          34.0    0.133261   1.024626   \n",
       "Subiculum Connectivity                   34.0   64.425914  14.745831   \n",
       "Amnesia Lesion T Map                     34.0    0.405642   0.239716   \n",
       "Memory Network T                         34.0    0.581327   0.180462   \n",
       "Memory Network R                         34.0    0.541191   0.129461   \n",
       "Subiculum Grey Matter                    34.0   -0.278204   2.158913   \n",
       "Subiculum White Matter                   34.0   -0.131562   0.946388   \n",
       "Subiculum CSF                            34.0    1.273195   1.831018   \n",
       "Subiculum Total                          34.0   -0.188262   1.460615   \n",
       "Standardized Age                         34.0    0.024226   0.768795   \n",
       "Standardized Percent Improvement         34.0    0.024226   0.768795   \n",
       "Standardized Subiculum Connectivity      34.0    0.140363   0.989747   \n",
       "Standardized Subiculum Grey Matter       34.0   -0.046168   0.934875   \n",
       "Standardized Subiculum White Matter      34.0   -0.049479   0.927022   \n",
       "Standardized Subiculum CSF               34.0   -0.090876   0.988079   \n",
       "Standardized Subiculum Total             34.0   -0.002490   0.905173   \n",
       "Cohort                                   34.0    0.911765   0.287902   \n",
       "Inclusion Cohort                         34.0    1.000000   0.000000   \n",
       "\n",
       "                                               min         25%         50%  \\\n",
       "subject                                   7.000000  110.750000  126.500000   \n",
       "Age                                      65.000000   68.250000   72.000000   \n",
       "Z-Scored Percent Cognitive Improvement   -1.855477   -0.433494    0.114844   \n",
       "Percent Cognitive Improvement          -129.411765  -58.636364  -29.106280   \n",
       "Z-Scored Subiculum Connectivity          -2.576474   -0.626785    0.238806   \n",
       "Subiculum Connectivity                   15.519790   61.966284   67.409958   \n",
       "Amnesia Lesion T Map                     -0.520786    0.441702    0.480007   \n",
       "Memory Network T                         -0.168692    0.592943    0.633350   \n",
       "Memory Network R                          0.051213    0.520445    0.573334   \n",
       "Subiculum Grey Matter                    -3.151819   -1.214509   -0.537509   \n",
       "Subiculum White Matter                   -1.854694   -0.791093   -0.177283   \n",
       "Subiculum CSF                            -1.421123    0.430795    0.774300   \n",
       "Subiculum Total                          -2.144075   -1.104652   -0.296989   \n",
       "Standardized Age                         -1.921065   -0.472299   -0.004488   \n",
       "Standardized Percent Improvement         -1.921065   -0.472299   -0.004488   \n",
       "Standardized Subiculum Connectivity      -2.412986   -0.574105    0.165298   \n",
       "Standardized Subiculum Grey Matter       -2.034620   -0.644489   -0.003994   \n",
       "Standardized Subiculum White Matter      -1.732774   -0.728695   -0.071735   \n",
       "Standardized Subiculum CSF               -2.251975   -0.622989   -0.272122   \n",
       "Standardized Subiculum Total             -1.685742   -0.765110    0.134876   \n",
       "Cohort                                    0.000000    1.000000    1.000000   \n",
       "Inclusion Cohort                          1.000000    1.000000    1.000000   \n",
       "\n",
       "                                               75%         max  \n",
       "subject                                 139.500000  150.000000  \n",
       "Age                                      74.000000   79.000000  \n",
       "Z-Scored Percent Cognitive Improvement    0.643205    1.335524  \n",
       "Percent Cognitive Improvement            -0.177305   29.411765  \n",
       "Z-Scored Subiculum Connectivity           0.734739    2.183535  \n",
       "Subiculum Connectivity                   72.581356   85.082502  \n",
       "Amnesia Lesion T Map                      0.503456    0.569043  \n",
       "Memory Network T                          0.680521    0.706532  \n",
       "Memory Network R                          0.628142    0.662179  \n",
       "Subiculum Grey Matter                     0.272086    9.615373  \n",
       "Subiculum White Matter                    0.323731    1.913677  \n",
       "Subiculum CSF                             1.606668    9.615373  \n",
       "Subiculum Total                           0.303696    5.194540  \n",
       "Standardized Age                          0.624679    1.330038  \n",
       "Standardized Percent Improvement          0.624679    1.330038  \n",
       "Standardized Subiculum Connectivity       0.743559    2.124682  \n",
       "Standardized Subiculum Grey Matter        0.581644    1.708518  \n",
       "Standardized Subiculum White Matter       0.464498    2.139363  \n",
       "Standardized Subiculum CSF                0.458150    2.644521  \n",
       "Standardized Subiculum Total              0.611756    1.907422  \n",
       "Cohort                                    1.000000    1.000000  \n",
       "Inclusion Cohort                          1.000000    1.000000  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stats = data_df.describe().transpose()\n",
    "descriptive_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to Max Value in Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dz = data_df.pop('One Hot Disease'\n",
    "# )\n",
    "# age = data_df.pop('Age'\n",
    "# )\n",
    "# for col in data_df.columns.to_list():\n",
    "#     try:\n",
    "#         df_1[col] = (data_df[col])/(data_df[col].max())\n",
    "#     except:\n",
    "#         print(f'Cannot normalize {col}')\n",
    "# data_df['Age'] = age\n",
    "# data_df['One Hot Disease'] = dz\n",
    "# data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set New Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df['autocog_status'] = np.where(data_df['Total'] < 82/100, 1, 0)\n",
    "# data_df['ace_status'] = np.where(data_df['Total'] < 82/100, 1, 0)\n",
    "# data_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>38.0</td>\n",
       "      <td>59.605263</td>\n",
       "      <td>53.200559</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>38.0</td>\n",
       "      <td>56.605263</td>\n",
       "      <td>5.668974</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z-Scored Percent Cognitive Improvement</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>1.141568</td>\n",
       "      <td>-3.428250</td>\n",
       "      <td>-0.170116</td>\n",
       "      <td>0.236331</td>\n",
       "      <td>0.586280</td>\n",
       "      <td>1.840498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percent Cognitive Improvement</th>\n",
       "      <td>38.0</td>\n",
       "      <td>-17.508793</td>\n",
       "      <td>46.404404</td>\n",
       "      <td>-207.692308</td>\n",
       "      <td>-14.170040</td>\n",
       "      <td>-2.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z-Scored Subiculum Connectivity</th>\n",
       "      <td>38.0</td>\n",
       "      <td>-0.090110</td>\n",
       "      <td>0.970677</td>\n",
       "      <td>-1.718309</td>\n",
       "      <td>-0.842041</td>\n",
       "      <td>-0.374693</td>\n",
       "      <td>0.601037</td>\n",
       "      <td>2.357590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum Connectivity</th>\n",
       "      <td>38.0</td>\n",
       "      <td>39.643198</td>\n",
       "      <td>22.347554</td>\n",
       "      <td>15.050219</td>\n",
       "      <td>19.702550</td>\n",
       "      <td>30.195647</td>\n",
       "      <td>60.855684</td>\n",
       "      <td>83.621480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amnesia Lesion T Map</th>\n",
       "      <td>38.0</td>\n",
       "      <td>-0.071698</td>\n",
       "      <td>0.421511</td>\n",
       "      <td>-0.659502</td>\n",
       "      <td>-0.450701</td>\n",
       "      <td>-0.231959</td>\n",
       "      <td>0.397718</td>\n",
       "      <td>0.530166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memory Network T</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.230360</td>\n",
       "      <td>0.332163</td>\n",
       "      <td>-0.363412</td>\n",
       "      <td>-0.067954</td>\n",
       "      <td>0.156938</td>\n",
       "      <td>0.542570</td>\n",
       "      <td>0.714419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memory Network R</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.314031</td>\n",
       "      <td>0.213850</td>\n",
       "      <td>-0.058739</td>\n",
       "      <td>0.112791</td>\n",
       "      <td>0.284450</td>\n",
       "      <td>0.528526</td>\n",
       "      <td>0.689681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum Grey Matter</th>\n",
       "      <td>36.0</td>\n",
       "      <td>3.271270</td>\n",
       "      <td>4.754863</td>\n",
       "      <td>-8.205764</td>\n",
       "      <td>-0.416294</td>\n",
       "      <td>2.513490</td>\n",
       "      <td>7.140009</td>\n",
       "      <td>11.802522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum White Matter</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0.256448</td>\n",
       "      <td>1.465262</td>\n",
       "      <td>-1.780761</td>\n",
       "      <td>-0.652276</td>\n",
       "      <td>-0.025503</td>\n",
       "      <td>0.703705</td>\n",
       "      <td>5.376292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum CSF</th>\n",
       "      <td>36.0</td>\n",
       "      <td>3.906899</td>\n",
       "      <td>4.249554</td>\n",
       "      <td>-8.205764</td>\n",
       "      <td>0.699193</td>\n",
       "      <td>2.851770</td>\n",
       "      <td>7.140009</td>\n",
       "      <td>11.802522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subiculum Total</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1.467113</td>\n",
       "      <td>2.455339</td>\n",
       "      <td>-2.061456</td>\n",
       "      <td>-0.346240</td>\n",
       "      <td>1.306597</td>\n",
       "      <td>2.775744</td>\n",
       "      <td>8.959061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Age</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.068012</td>\n",
       "      <td>1.215697</td>\n",
       "      <td>-3.523461</td>\n",
       "      <td>-0.282257</td>\n",
       "      <td>0.246865</td>\n",
       "      <td>0.542009</td>\n",
       "      <td>1.844523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Percent Improvement</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.068012</td>\n",
       "      <td>1.215697</td>\n",
       "      <td>-3.523461</td>\n",
       "      <td>-0.282257</td>\n",
       "      <td>0.246865</td>\n",
       "      <td>0.542009</td>\n",
       "      <td>1.844523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum Connectivity</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.062590</td>\n",
       "      <td>0.960296</td>\n",
       "      <td>-1.594906</td>\n",
       "      <td>-0.850444</td>\n",
       "      <td>-0.276023</td>\n",
       "      <td>0.677657</td>\n",
       "      <td>2.297968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum Grey Matter</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0.106524</td>\n",
       "      <td>1.093581</td>\n",
       "      <td>-3.303201</td>\n",
       "      <td>-0.454561</td>\n",
       "      <td>0.241616</td>\n",
       "      <td>0.650805</td>\n",
       "      <td>3.222209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum White Matter</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.013502</td>\n",
       "      <td>1.055374</td>\n",
       "      <td>-1.397867</td>\n",
       "      <td>-0.704442</td>\n",
       "      <td>-0.098309</td>\n",
       "      <td>0.557532</td>\n",
       "      <td>2.909124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum CSF</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0.027245</td>\n",
       "      <td>1.063257</td>\n",
       "      <td>-3.303201</td>\n",
       "      <td>-0.547767</td>\n",
       "      <td>0.083153</td>\n",
       "      <td>0.787437</td>\n",
       "      <td>2.220871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized Subiculum Total</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0.074989</td>\n",
       "      <td>1.062024</td>\n",
       "      <td>-1.601922</td>\n",
       "      <td>-0.590727</td>\n",
       "      <td>0.036220</td>\n",
       "      <td>0.403873</td>\n",
       "      <td>2.775495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cohort</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.495355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inclusion Cohort</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        count       mean        std  \\\n",
       "subject                                  38.0  59.605263  53.200559   \n",
       "Age                                      38.0  56.605263   5.668974   \n",
       "Z-Scored Percent Cognitive Improvement   38.0   0.009785   1.141568   \n",
       "Percent Cognitive Improvement            38.0 -17.508793  46.404404   \n",
       "Z-Scored Subiculum Connectivity          38.0  -0.090110   0.970677   \n",
       "Subiculum Connectivity                   38.0  39.643198  22.347554   \n",
       "Amnesia Lesion T Map                     38.0  -0.071698   0.421511   \n",
       "Memory Network T                         38.0   0.230360   0.332163   \n",
       "Memory Network R                         38.0   0.314031   0.213850   \n",
       "Subiculum Grey Matter                    36.0   3.271270   4.754863   \n",
       "Subiculum White Matter                   36.0   0.256448   1.465262   \n",
       "Subiculum CSF                            36.0   3.906899   4.249554   \n",
       "Subiculum Total                          36.0   1.467113   2.455339   \n",
       "Standardized Age                         36.0  -0.068012   1.215697   \n",
       "Standardized Percent Improvement         36.0  -0.068012   1.215697   \n",
       "Standardized Subiculum Connectivity      36.0  -0.062590   0.960296   \n",
       "Standardized Subiculum Grey Matter       36.0   0.106524   1.093581   \n",
       "Standardized Subiculum White Matter      36.0  -0.013502   1.055374   \n",
       "Standardized Subiculum CSF               36.0   0.027245   1.063257   \n",
       "Standardized Subiculum Total             36.0   0.074989   1.062024   \n",
       "Cohort                                   38.0   0.394737   0.495355   \n",
       "Inclusion Cohort                         38.0   1.000000   0.000000   \n",
       "\n",
       "                                               min        25%        50%  \\\n",
       "subject                                   1.000000  15.500000  30.500000   \n",
       "Age                                      46.000000  52.000000  58.000000   \n",
       "Z-Scored Percent Cognitive Improvement   -3.428250  -0.170116   0.236331   \n",
       "Percent Cognitive Improvement          -207.692308 -14.170040  -2.142857   \n",
       "Z-Scored Subiculum Connectivity          -1.718309  -0.842041  -0.374693   \n",
       "Subiculum Connectivity                   15.050219  19.702550  30.195647   \n",
       "Amnesia Lesion T Map                     -0.659502  -0.450701  -0.231959   \n",
       "Memory Network T                         -0.363412  -0.067954   0.156938   \n",
       "Memory Network R                         -0.058739   0.112791   0.284450   \n",
       "Subiculum Grey Matter                    -8.205764  -0.416294   2.513490   \n",
       "Subiculum White Matter                   -1.780761  -0.652276  -0.025503   \n",
       "Subiculum CSF                            -8.205764   0.699193   2.851770   \n",
       "Subiculum Total                          -2.061456  -0.346240   1.306597   \n",
       "Standardized Age                         -3.523461  -0.282257   0.246865   \n",
       "Standardized Percent Improvement         -3.523461  -0.282257   0.246865   \n",
       "Standardized Subiculum Connectivity      -1.594906  -0.850444  -0.276023   \n",
       "Standardized Subiculum Grey Matter       -3.303201  -0.454561   0.241616   \n",
       "Standardized Subiculum White Matter      -1.397867  -0.704442  -0.098309   \n",
       "Standardized Subiculum CSF               -3.303201  -0.547767   0.083153   \n",
       "Standardized Subiculum Total             -1.601922  -0.590727   0.036220   \n",
       "Cohort                                    0.000000   0.000000   0.000000   \n",
       "Inclusion Cohort                          1.000000   1.000000   1.000000   \n",
       "\n",
       "                                               75%         max  \n",
       "subject                                 114.000000  148.000000  \n",
       "Age                                      61.000000   64.000000  \n",
       "Z-Scored Percent Cognitive Improvement    0.586280    1.840498  \n",
       "Percent Cognitive Improvement             0.000000   54.545455  \n",
       "Z-Scored Subiculum Connectivity           0.601037    2.357590  \n",
       "Subiculum Connectivity                   60.855684   83.621480  \n",
       "Amnesia Lesion T Map                      0.397718    0.530166  \n",
       "Memory Network T                          0.542570    0.714419  \n",
       "Memory Network R                          0.528526    0.689681  \n",
       "Subiculum Grey Matter                     7.140009   11.802522  \n",
       "Subiculum White Matter                    0.703705    5.376292  \n",
       "Subiculum CSF                             7.140009   11.802522  \n",
       "Subiculum Total                           2.775744    8.959061  \n",
       "Standardized Age                          0.542009    1.844523  \n",
       "Standardized Percent Improvement          0.542009    1.844523  \n",
       "Standardized Subiculum Connectivity       0.677657    2.297968  \n",
       "Standardized Subiculum Grey Matter        0.650805    3.222209  \n",
       "Standardized Subiculum White Matter       0.557532    2.909124  \n",
       "Standardized Subiculum CSF                0.787437    2.220871  \n",
       "Standardized Subiculum Total              0.403873    2.775495  \n",
       "Cohort                                    1.000000    1.000000  \n",
       "Inclusion Cohort                          1.000000    1.000000  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stats = data_df.describe().transpose()\n",
    "descriptive_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.rename(columns={\"Convolutional Neural Network Equivalent\": \"Paper Test\"}, inplace=True)\n",
    "data_df.rename(columns={\"Convolutional Neural Network\": \"RoCA\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannotations.Annotator import Annotator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "def plot_with_annotation(dataframe, col1, col2, xlabel, ylabel, test_type, colours):\n",
    "    # Validate the test type\n",
    "    valid_tests = ['t-test_ind', 't-test_paired', 'Mann-Whitney', 'Wilcoxon']\n",
    "    if test_type not in valid_tests:\n",
    "        raise ValueError(f\"Invalid test type. Choose from: {', '.join(valid_tests)}\")\n",
    "    \n",
    "    # Extract series\n",
    "    series1 = dataframe[col1]\n",
    "    series2 = dataframe[col2]\n",
    "    \n",
    "    # Prepare data\n",
    "    data = pd.DataFrame({'Group': [col1] * len(series1) + [col2] * len(series2),\n",
    "                         'Value': series1.tolist() + series2.tolist()})\n",
    "    \n",
    "    # Pairs for comparison\n",
    "    pairs = [(col1, col2)]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 2.5))\n",
    "    \n",
    "    sns.set_palette(colours, 2, desat=1)\n",
    "    \n",
    "    # Plot with seaborn\n",
    "    sns.boxplot(x='Group', y='Value', data=data, ax=ax)\n",
    "    \n",
    "    # Add annotations\n",
    "    annotator = Annotator(ax=ax,\n",
    "                          data=data,\n",
    "                          x='Group',\n",
    "                          y='Value',\n",
    "                          pairs=pairs,\n",
    "                          test=test_type,\n",
    "                          text_format='full',\n",
    "                          loc='inside',\n",
    "                          verbose=2)\n",
    "    \n",
    "    # Configure and annotate\n",
    "    _, corrected_results = annotator.configure(test=test_type, comparisons_correction=\"bonferroni\").apply_and_annotate()\n",
    "    \n",
    "    # Label and show plot\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_col_name = 'Paper Test'\n",
    "group2_col_name = 'RoCA'\n",
    "x_label = 'Test Type'\n",
    "y_label = 'Test Score'\n",
    "test_type = 't-test_paired'\n",
    "#----------------------------------------------------------------\n",
    "plt = plot_with_annotation(dataframe=data_df, \n",
    "                           col1=group1_col_name, \n",
    "                           col2=group2_col_name,\n",
    "                           xlabel=x_label, \n",
    "                           ylabel=y_label, \n",
    "                           test_type=test_type,\n",
    "                           colours='tab10')\n",
    "\n",
    "os.makedirs(os.path.join(out_dir, 'distribution_figures'), exist_ok=True)\n",
    "plt.savefig(os.path.join(out_dir, f'distribution_figures/{group1_col_name}_{test_type}_{group2_col_name}.png'))\n",
    "plt.savefig(os.path.join(out_dir, f'distribution_figures/{group1_col_name}_{test_type}_{group2_col_name}.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, f'distribution_figures/{group1_col_name}_{test_type}_{group2_col_name}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Multiple Distribution Tests and Correct for Multiple Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a detailed description of the `plot_distribution_tests` function.\n",
    "\n",
    "### `plot_distribution_tests` Function\n",
    "\n",
    "This function creates a plot of distribution tests with optional annotations for multiple key-value pair comparisons.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- `dataframe`: A pandas DataFrame containing the data for comparison.\n",
    "- `comparisons_dict`: A dictionary mapping comparisons with keys and values representing column names in the DataFrame.\n",
    "- `test_type`: Specifies the statistical method used for testing. Available options are:\n",
    "  - `'t-test_ind'`: Independent t-test\n",
    "  - `'t-test_paired'`: Paired t-test\n",
    "  - `'Mann-Whitney'`: Mann-Whitney U test\n",
    "  - `'Wilcoxon'`: Wilcoxon signed-rank test\n",
    "- `correction_method`: Specifies the method used to correct for multiple comparisons. Available options are:\n",
    "  - `'bonferroni'`: Bonferroni correction\n",
    "  - `'holm'`: Holm's correction\n",
    "  - `'fdr_bh'`: Benjamini-Hochberg false discovery rate correction\n",
    "- `key_label`: A string label for the keys in the plot.\n",
    "- `value_label`: A string label for the values in the plot.\n",
    "- `x_label`: Label for the x-axis.\n",
    "- `y_label`: Label for the y-axis.\n",
    "- `display`: Controls how the keys and values are displayed. Available options are:\n",
    "  - `'split_pairs'`: Displays keys and values side by side, with keys on the left and values on the right.\n",
    "  - `'group_pairs'`: Groups the key-value pairs together.\n",
    "- `full_legend_patches`: A boolean that controls the appearance of the legend. If `True`, full key-value pairs are shown in the legend; if `False`, only keys are shown.\n",
    "\n",
    "#### Returns\n",
    "\n",
    "- A matplotlib plot object.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "comparisons_dict = {\n",
    "    'Attention': 'Attention.1',\n",
    "    'Memory': 'Memory.1',\n",
    "    'Language': 'Language.1',\n",
    "    'Fluency': 'Fluency.1',\n",
    "    'Visuospatial': 'Visuospatial.1',\n",
    "}\n",
    "test_type = 'Mann-Whitney'\n",
    "correction_method = 'bonferroni'\n",
    "key_label = 'Keys'\n",
    "value_label = 'Values'\n",
    "x_label = 'Metrics'\n",
    "y_label = 'Values'\n",
    "\n",
    "plt = plot_distribution_tests(dataframe=data_df,\n",
    "                              comparisons_dict=comparisons_dict, \n",
    "                              test_type=test_type, \n",
    "                              correction_method=correction_method,\n",
    "                              key_label=key_label,\n",
    "                              value_label=value_label,\n",
    "                              x_label=x_label,\n",
    "                              y_label=y_label)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Notes\n",
    "\n",
    "- The `test_type` and `correction_method` must be chosen from the available options; otherwise, a `ValueError` will be raised.\n",
    "- The function supports customization of the plot appearance, including color mapping, annotations, and more, to suit various use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannotations.Annotator import Annotator\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_distribution_tests(dataframe, comparisons_dict, test_type, correction_method, key_label, value_label, x_label, y_label, display='split_pairs', full_legend_patches=False):\n",
    "    valid_tests = ['t-test_ind', 't-test_paired', 'Mann-Whitney', 'Wilcoxon']\n",
    "    valid_corrections = ['bonferroni', 'holm', 'fdr_bh']\n",
    "    if test_type not in valid_tests:\n",
    "        raise ValueError(f\"Invalid test type. Choose from: {', '.join(valid_tests)}\")\n",
    "    if correction_method not in valid_corrections:\n",
    "        raise ValueError(f\"Invalid correction method. Choose from: {', '.join(valid_corrections)}\")\n",
    "\n",
    "    # Create a color palette with enough unique colors\n",
    "    colors = sns.color_palette(\"tab10\", len(comparisons_dict))\n",
    "\n",
    "    # Create a new DataFrame to hold the data for plotting\n",
    "    data = pd.DataFrame(columns=['GroupType', 'Metric', 'Value'])\n",
    "    pairs = []\n",
    "    # Create legend patches\n",
    "    legend_patches = []\n",
    "\n",
    "    # Iterate through comparisons_dict\n",
    "    for idx, (key, value) in enumerate(comparisons_dict.items()):\n",
    "        key_series = dataframe[key]\n",
    "        value_series = dataframe[value]\n",
    "        color = colors[idx]\n",
    "        temp_data = pd.DataFrame({\n",
    "            'GroupType': [f'Key'] * len(key_series) + [f'Value'] * len(value_series),\n",
    "            'Metric': [key] * len(key_series) + [value] * len(value_series),\n",
    "            'Value': key_series.tolist() + value_series.tolist(),\n",
    "            'Color': [color] * (len(key_series) + len(value_series))\n",
    "        })\n",
    "        data = pd.concat([data, temp_data], axis=0)\n",
    "        pairs.append(((key, 'Key'), (value, 'Value')))\n",
    "        \n",
    "        if full_legend_patches:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{key} vs {value}'))\n",
    "        else:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{value}'))\n",
    "\n",
    "    # Map colour\n",
    "    palette_mapping = {row['Metric']: row['Color'] for _, row in data.iterrows()}\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(len(comparisons_dict)*1.3, 5))\n",
    "    # Add annotations\n",
    "    if display=='group_pairs':\n",
    "        # Plot with seaborn (split image)\n",
    "        sns.set_style('white')\n",
    "        palette = sns.color_palette('tab10', 2, desat=1)  # Define palette with 2 colors\n",
    "        sns.set_palette(palette)\n",
    "        ax = sns.boxplot(x='Metric', y='Value', hue='GroupType', data=data, ax=ax, palette=palette)\n",
    "        \n",
    "        # Define the Legend\n",
    "        key_color = palette[0]  # Color for the 'Key' group\n",
    "        value_color = palette[1]  # Color for the 'Value' group\n",
    "        legend_patches = [mpatches.Patch(color=key_color, label=key_label),\n",
    "                        mpatches.Patch(color=value_color, label=value_label)]\n",
    "        \n",
    "        # Add annotations\n",
    "        annotator = Annotator(ax=ax,\n",
    "                              data=data,\n",
    "                              x='Metric',\n",
    "                              y='Value',\n",
    "                              hue='GroupType',\n",
    "                              pairs=pairs,\n",
    "                              test=test_type,\n",
    "                              text_format='full',\n",
    "                              loc='inside',\n",
    "                              verbose=2)\n",
    "        x_labels = ['Question ' + pair[0][0].split('_')[2] for pair in pairs]\n",
    "        result = [' '] * (len(x_labels) * 2)\n",
    "        result[0::2] = x_labels\n",
    "        x_labels = result\n",
    "        x_ticks = [i for i in range(0, len(pairs)*2)]\n",
    "        plt.xticks(ticks=x_ticks, labels=x_labels)\n",
    "    elif display=='split_pairs':\n",
    "        # Plot with seaborn (side-by-side)\n",
    "        ax = sns.boxplot(x='GroupType', y='Value', hue='Metric', data=data, ax=ax, palette={row['Metric']: row['Color'] for _, row in data.iterrows()})\n",
    "        # Adjust pairs for side-by-side\n",
    "        pairs = [(('Key', col1), ('Value', col2)) for col1, col2 in comparisons_dict.items()]\n",
    "        # Add annotations\n",
    "        annotator = Annotator(ax=ax,\n",
    "                              data=data,\n",
    "                              x='GroupType',\n",
    "                              y='Value',\n",
    "                              hue='Metric',\n",
    "                              pairs=pairs,\n",
    "                              test=test_type,\n",
    "                              text_format='full',\n",
    "                              loc='outside',\n",
    "                              verbose=1)\n",
    "        x_labels = [key_label] + [value_label]\n",
    "        plt.xticks(ticks=range(len(x_labels)), labels=x_labels)\n",
    "    else:\n",
    "        raise ValueError(f'display {display} not supported please choose \"split_pairs\" or \"group_pairs\"')\n",
    "\n",
    "    # Configure and annotate\n",
    "    _, corrected_results = annotator.configure(test=test_type, comparisons_correction=correction_method).apply_and_annotate()\n",
    "\n",
    "    # Label and show plot\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    plt.title(f'Distribution Test: {test_type} with {correction_method} correction')\n",
    "    plt.legend(handles=legend_patches, frameon=False, loc=(0.025, 0.05))\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_dict = {'ACE3_Question_2': 'Question_2', 'ACE3_Question_3': 'Question_3', 'ACE3_Question_4': 'Question_4', 'ACE3_Question_5': 'Question_5', 'ACE3_Question_6': 'Question_6', 'ACE3_Question_7': 'Question_7', 'ACE3_Question_8': 'Question_8', 'ACE3_Question_9': 'Question_9', 'ACE3_Question_10': 'Question_10', 'ACE3_Question_11': 'Question_11', 'ACE3_Question_12': 'Question_12', 'ACE3_Question_13': 'Question_13', 'ACE3_Question_14': 'Question_14', 'ACE3_Question_15': 'Question_15', 'ACE3_Question_16': 'Question_16', 'ACE3_Question_17': 'Question_17', 'ACE3_Question_18': 'Question_18', 'ACE3_Question_19': 'Question_19', 'ACE3_Question_20': 'Question_20'}\n",
    "test_type = 'Wilcoxon'\n",
    "correction_method = 'bonferroni'\n",
    "key_label = 'Paper'\n",
    "value_label = 'Autonomous'\n",
    "x_label = 'Question'\n",
    "y_label = 'Score'\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "plt = plot_distribution_tests(dataframe=data_df,\n",
    "                        comparisons_dict=comparisons_dict, \n",
    "                        test_type=test_type, \n",
    "                        correction_method=correction_method,\n",
    "                        key_label=key_label, \n",
    "                        value_label=value_label, \n",
    "                        x_label=x_label, \n",
    "                        y_label=y_label,\n",
    "                        display='group_pairs')\n",
    "plt\n",
    "\n",
    "os.makedirs(os.path.join(out_dir, 'distribution_figures'), exist_ok=True)\n",
    "plt.savefig(os.path.join(out_dir, f'distribution_figures/{test_type}_{correction_method}_multiple_comparison_barplot.png'))\n",
    "plt.savefig(os.path.join(out_dir, f'distribution_figures/{test_type}_{correction_method}_multiple_comparison_barplot_barplot.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, f'icc_figures/{test_type}_{correction_method}_multiple_comparison_barplot')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman/Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from natsort import index_natsorted\n",
    "\n",
    "# Correcting the function to remove natural sorting and ensure DataFrame is passed\n",
    "\n",
    "def generate_scatterplot(dataframe, data_dict, columns_per_row=4, x_label='xlabel', y_label='ylabel', correlation='pearson', palette='Greys'):\n",
    "    '''\n",
    "    Generate scatterplots with specified correlation coefficient annotated.\n",
    "\n",
    "    :param dataframe: DataFrame containing the data\n",
    "    :param data_dict: Dictionary where key is the dependent variable name, and value is a list of independent variable names\n",
    "    :param columns_per_row: Number of columns per row in the facet plot\n",
    "    :param x_label: Label for the x-axis\n",
    "    :param y_label: Label for the y-axis\n",
    "    :param correlation: method of correlation ('pearson', 'spearman', 'kendall')\n",
    "    :return: Facet plot\n",
    "    '''\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette(palette, 1, desat=1)\n",
    "    \n",
    "    for dependent_var, independent_vars in data_dict.items():\n",
    "        # Melt the DataFrame for each dependent variable\n",
    "        melted_df = pd.melt(dataframe, id_vars=[dependent_var], value_vars=independent_vars,\n",
    "                            var_name='independent_variable_name', value_name='independent_variable_units')\n",
    "\n",
    "        # Sort the DataFrame\n",
    "        melted_df = melted_df.sort_values(by='independent_variable_name')\n",
    "\n",
    "        # Begin plotting\n",
    "        facet_plot = sns.lmplot(y=dependent_var, x='independent_variable_units', data=melted_df,\n",
    "                                col='independent_variable_name', truncate=False, col_wrap=columns_per_row,\n",
    "                                facet_kws=dict(sharex=False, sharey=True))\n",
    "\n",
    "        # Calculate correlation for each cluster\n",
    "        grouped_df = melted_df.groupby('independent_variable_name')\n",
    "        \n",
    "        if correlation == 'pearson':\n",
    "            result = grouped_df.apply(lambda x: pearsonr(x['independent_variable_units'], x[dependent_var]))\n",
    "        elif correlation == 'spearman':\n",
    "            result = grouped_df.apply(lambda x: spearmanr(x['independent_variable_units'], x[dependent_var]))\n",
    "        elif correlation == 'kendall':\n",
    "            result = grouped_df.apply(lambda x: kendalltau(x['independent_variable_units'], x[dependent_var]))\n",
    "        else:\n",
    "            raise ValueError(f'Correlation {correlation} not specified, please select \"pearson\", \"kendall\" or \"spearman\"')\n",
    "\n",
    "        # Annotate with correlation\n",
    "        for ax in facet_plot.axes:\n",
    "            # Get the title of the current subplot\n",
    "            region = ax.get_title().split(\"=\")[-1].strip()\n",
    "            # Get the corresponding r, p values from the result\n",
    "            r, p = result[region]\n",
    "            ax.set_title(region)\n",
    "            ax.annotate(f\"r = {r:.2f}, p = {p:.5f}\", xy=(.5, 1.0), xycoords='axes fraction',\n",
    "                        xytext=(0, 0), textcoords='offset points', ha='center', va='top',\n",
    "                        bbox=dict(boxstyle='round,pad=0.0', alpha=0.0),\n",
    "                        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "            \n",
    "        facet_plot.set_xlabels(x_label)\n",
    "        facet_plot.set_ylabels(y_label)\n",
    "        return facet_plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_correlate_to = 'perc_improvement'\n",
    "columns_to_correlate_list = [\n",
    " 'Subiculum Connectivity',\n",
    "       'Amnesia Lesion Network Map Spatial Correlation',\n",
    "       'Memory Network T Spatial Correlation',\n",
    "       'Memory Network R Spatial Correlation'\n",
    "]\n",
    "x_label = 'Connectivity'\n",
    "y_label = 'Percent Improvement (MDRS)'\n",
    "correlation = 'pearson'\n",
    "\n",
    "#---------\n",
    "plot = generate_scatterplot(dataframe=data_df, \n",
    "                     data_dict={columns_to_correlate_to:columns_to_correlate_list}, \n",
    "                     x_label=x_label, \n",
    "                     y_label=y_label,\n",
    "                     correlation=correlation,\n",
    "                     palette='Greys')\n",
    "\n",
    "os.makedirs(os.path.join(out_dir, 'scatterplot_figures'), exist_ok=True)\n",
    "plot.savefig(os.path.join(out_dir, f'scatterplot_figures/{x_label}_to_{y_label}.png'))\n",
    "plot.savefig(os.path.join(out_dir, f'scatterplot_figures/{x_label}_to_{y_label}.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, f'scatterplot_figures/{x_label}_to_{y_label}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Split Between Correlation of Variables\n",
    "\n",
    "The ScatterWithConfidence class is designed to generate scatter plots with associated confidence intervals and permutation-based p-values. It creates plots of two variables and segregates them by a specified criterion, such as age. These plots are further enhanced by overlaying regression lines with their associated confidence intervals. The class can also perform permutation tests to compute a p-value, which is then prominently displayed in the plot title.\n",
    "\n",
    "Attributes:\n",
    "data_df (pandas.DataFrame): The main dataframe containing the data intended for plotting.\n",
    "Key Methods:\n",
    "1. compute_analytic_confidence_interval(x, y, x_vals)\n",
    "Purpose: Computes the regression line and its 95% confidence intervals using analytic methods.\n",
    "Parameters:\n",
    "x (pandas.Series): The x-values of the data.\n",
    "y (pandas.Series): The y-values of the data.\n",
    "x_vals (numpy.ndarray): The x-values where the regression line and confidence intervals should be computed.\n",
    "Returns:\n",
    "y_fit: Regression line values.\n",
    "lower_bound: Lower 95% confidence interval.\n",
    "upper_bound: Upper 95% confidence interval.\n",
    "2. permute_data_and_difference_in_pearson_r(...)\n",
    "Purpose: Computes the difference in Pearson R between two groups using permutation tests.\n",
    "Parameters:\n",
    "x_one, x_two (str): Column names of the two variables to be correlated.\n",
    "split_by (str): Column name used to segregate the data.\n",
    "split_value (float): The criterion value for data segregation.\n",
    "n_permutations (int, optional): Number of permutations to perform (default is 1000).\n",
    "permute_columns (list of str, optional): List of column names to be permuted.\n",
    "Returns:\n",
    "original_diff: Difference in Pearson R of the original data between two groups.\n",
    "p_value: Permutation-based p-value.\n",
    "3. plot_with_analytic_ci_manual_pvalue(...)\n",
    "Purpose: Generates a scatter plot with regression lines, confidence intervals, and the permutation-based p-value in the title.\n",
    "Parameters:\n",
    "x_one, x_two (str): Column names of the two variables for plotting.\n",
    "split_by (str): Column name for data segregation.\n",
    "... (Other parameters for customization, saving, etc.)\n",
    "Returns:\n",
    "A matplotlib scatter plot.\n",
    "To utilize this class:\n",
    "\n",
    "Initialize the class with your dataset.\n",
    "Use the methods described above to compute confidence intervals or perform permutation tests.\n",
    "Generate and visualize your enhanced scatter plot using the provided methods.\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "scatter_plotter = ScatterWithConfidence(data_df)\n",
    "scatter_plotter.plot_with_analytic_ci_manual_pvalue(...)\n",
    "Remember to replace ... with appropriate arguments as your situation demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the class and its methods\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "import seaborn as sns\n",
    "class ScatterWithConfidence:\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_analytic_confidence_interval(x, y, x_vals):\n",
    "        slope, intercept = np.polyfit(x, y, 1)\n",
    "        y_fit = slope * x_vals + intercept\n",
    "        \n",
    "        residuals = y - (slope * x + intercept)\n",
    "        stderr = np.sqrt(np.sum(residuals**2) / (len(y) - 2))\n",
    "        \n",
    "        t_value = t.ppf(0.975, df=len(x)-2)\n",
    "        ci = t_value * stderr * np.sqrt(1/len(x) + (x_vals - np.mean(x))**2 / np.sum((x - np.mean(x))**2))\n",
    "        \n",
    "        upper_bound = y_fit + ci\n",
    "        lower_bound = y_fit - ci\n",
    "        \n",
    "        return y_fit, lower_bound, upper_bound\n",
    "\n",
    "    def permute_data_and_difference_in_pearson_r(self, x_one, x_two, split_by, split_value, n_permutations=1000, permute_columns=[]):\n",
    "        original_diff = self.data_df[self.data_df[split_by] < split_value][x_one].corr(self.data_df[self.data_df[split_by] < split_value][x_two]) - \\\n",
    "                       self.data_df[self.data_df[split_by] >= split_value][x_one].corr(self.data_df[self.data_df[split_by] >= split_value][x_two])\n",
    "\n",
    "        permuted_diffs = []\n",
    "\n",
    "        for _ in range(n_permutations):\n",
    "            permuted_df = self.data_df.copy()\n",
    "            for column in permute_columns:\n",
    "                permuted_df[column] = np.random.permutation(permuted_df[column].values)\n",
    "            \n",
    "            diff = permuted_df[permuted_df[split_by] < split_value][x_one].corr(permuted_df[permuted_df[split_by] < split_value][x_two]) - \\\n",
    "                   permuted_df[permuted_df[split_by] >= split_value][x_one].corr(permuted_df[permuted_df[split_by] >= split_value][x_two])\n",
    "            \n",
    "            permuted_diffs.append(diff)\n",
    "\n",
    "        p_value = np.mean([diff <= original_diff for diff in permuted_diffs])\n",
    "        return original_diff, p_value\n",
    "\n",
    "    def plot_with_analytic_ci_manual_pvalue(self, x_one, x_two, \n",
    "                                            split_by, split_value, \n",
    "                                            x_label='X1', y_label='X2', \n",
    "                                            upper_split_legend='Above Split', lower_split_legend='Below Split',\n",
    "                                            alpha=0.3, manual_p_value=None, permute_column=None, \n",
    "                                            save=False, out_dir=None):\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        \n",
    "        group1 = self.data_df[self.data_df[split_by] < split_value]\n",
    "        group2 = self.data_df[self.data_df[split_by] >= split_value]\n",
    "        \n",
    "        ax.scatter(group1[x_one], group1[x_two], color='blue', label=lower_split_legend, s=40, alpha=alpha, marker='o')\n",
    "        ax.scatter(group2[x_one], group2[x_two], color='red', label=upper_split_legend, s=40, alpha=alpha, marker='o')\n",
    "        \n",
    "        x_vals = np.linspace(self.data_df[x_one].min(), self.data_df[x_one].max(), 400)\n",
    "        \n",
    "        for group, color in [(group1, 'blue'), (group2, 'red')]:\n",
    "            y_fit, lower_bound, upper_bound = self.compute_analytic_confidence_interval(group[x_one], group[x_two], x_vals)\n",
    "            ax.plot(x_vals, y_fit, color=color)\n",
    "            ax.fill_between(x_vals, lower_bound, upper_bound, color=color, alpha=alpha/7)\n",
    "        \n",
    "        if manual_p_value is None:\n",
    "            if permute_column:\n",
    "                rho, manual_p_value = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, n_permutations=10000, permute_columns=[permute_column])\n",
    "            else:\n",
    "                rho, manual_p_value = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, n_permutations=10000, permute_columns=[x_one, x_two, split_by])\n",
    "        \n",
    "        ax.set_title(f\"\\u0394 r = {rho:.2f} , p = {manual_p_value:.4f}\")\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.legend(loc='best', frameon=False)\n",
    "        ax.grid(False)\n",
    "        sns.despine(ax=ax)\n",
    "        \n",
    "        if save and out_dir is not None:\n",
    "            plt.savefig(f\"{out_dir}/scatter_with_polyfit_and_analytic_ci.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{out_dir}/scatter_with_polyfit_and_analytic_ci.svg\", bbox_inches='tight')\n",
    "            print(f'Saved to {out_dir}/scatter_with_polyfit_and_analytic_ci.svg')\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Z-Scored Percent Cognitive Improvement',\n",
       "       'Percent Cognitive Improvement', 'Z-Scored Subiculum Connectivity',\n",
       "       'Subiculum Connectivity', 'Amnesia Lesion T Map', 'Memory Network T',\n",
       "       'Memory Network R', 'Subiculum Grey Matter', 'Subiculum White Matter',\n",
       "       'Subiculum CSF', 'Subiculum Total', 'Disease', 'Standardized Age',\n",
       "       'Standardized Percent Improvement',\n",
       "       'Standardized Subiculum Connectivity',\n",
       "       'Standardized Subiculum Grey Matter',\n",
       "       'Standardized Subiculum White Matter', 'Standardized Subiculum CSF',\n",
       "       'Standardized Subiculum Total', 'Disease.1', 'Cohort',\n",
       "       'Inclusion Cohort'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/scatterplot_analysis/scatterplot_figures/delta_scatterplots/scatter_with_polyfit_and_analytic_ci.svg\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "delta_sctr = ScatterWithConfidence(data_df)\n",
    "delta_sctr_plot = delta_sctr.plot_with_analytic_ci_manual_pvalue(\n",
    "                                            x_one='Z-Scored Subiculum Connectivity', x_two='Z-Scored Percent Cognitive Improvement', \n",
    "                                            split_by='Cohort', split_value=0.5, \n",
    "                                            x_label='Standardized Subiculum Connectivity', y_label='Standardized Percent Improvement', \n",
    "                                            upper_split_legend='Alzheimer Disease', lower_split_legend='Parkinson Disease',\n",
    "                                            alpha=0.3, manual_p_value=None, permute_column=None, \n",
    "                                            save=True, out_dir='/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/scatterplot_analysis/scatterplot_figures/delta_scatterplots')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intraclass Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "def calculate_icc(series1, series2):\n",
    "    # Combine the series into a DataFrame\n",
    "    df = series1.to_frame(name='rating1')\n",
    "    df['rating2'] = series2\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    df_melted = df.melt(value_vars=['rating1', 'rating2'], var_name='rater', value_name='rating')\n",
    "\n",
    "    # Add a subject column\n",
    "    df_melted['subject'] = df_melted.groupby('rater').cumcount()\n",
    "\n",
    "    # Calculate ICC\n",
    "    icc_result = pg.intraclass_corr(data=df_melted, targets='subject', raters='rater', ratings='rating')\n",
    "    \n",
    "    # You can choose the ICC type you want, here I'm returning ICC(3,1)\n",
    "    return icc_result.set_index('Type').loc['ICC3', 'ICC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.loc[:34, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icc = calculate_icc(data_df['Total'], data_df['ACE_Total'])\n",
    "icc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapped ICC 95% Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pingouin import intraclass_corr\n",
    "from calvin_utils.statistical_utils.distribution_statistics import BootstrappedDistributionStatistics\n",
    "from calvin_utils.statistical_utils.resampling_functions import Bootstrap\n",
    "\n",
    "def calculate_icc(df, col1, col2):\n",
    "    # Select only the specified columns and rename them for compatibility with pingouin\n",
    "    data = df[[col1, col2]].rename(columns={col1: 'rating1', col2: 'rating2'})\n",
    "\n",
    "    # Reshape data for pingouin's intraclass_corr function\n",
    "    df_melted = data.melt(value_vars=['rating1', 'rating2'], var_name='rater', value_name='rating')\n",
    "    df_melted['subject'] = df_melted.groupby('rater').cumcount()\n",
    "\n",
    "    # Calculate ICC\n",
    "    icc_result = intraclass_corr(data=df_melted, targets='subject', raters='rater', ratings='rating')\n",
    "    \n",
    "    # Return ICC(3,1)\n",
    "    return icc_result.set_index('Type').loc['ICC3', 'ICC']\n",
    "\n",
    "def bootstrap_icc(data, col1, col2, bootstrap_samples=2500):\n",
    "    # Perform the bootstrap resampling using the Bootstrap class\n",
    "    bootstrap = Bootstrap(data=data, func=calculate_icc, func_args={'col1': col1, 'col2': col2}, bootstrap_samples=bootstrap_samples)\n",
    "    bootstrap_results = bootstrap.bootstrap_function()\n",
    "\n",
    "    # Calculate the confidence intervals using the BootstrappedDistributionStatistics class\n",
    "    distribution_statistics = BootstrappedDistributionStatistics(bootstrap_results)\n",
    "    lower_bound, upper_bound = distribution_statistics.percentile_ci(alpha=0.05)\n",
    "\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound, upper_bound = bootstrap_icc(data=data_df, col1='Total', col2='ACE_Total', bootstrap_samples=10000)\n",
    "print(lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the class\n",
    "scatter_plotter = ScatterWithConfidence(data_df)\n",
    "\n",
    "# Generate and display the plot\n",
    "fig = scatter_plotter.plot_with_analytic_ci_manual_pvalue(x_one=\"Subiculum Connectivity\",\n",
    "                                                         x_two=\"Z-Scored Cognitive Improvement By Group\",\n",
    "                                                         split_by=\"Cohort\",\n",
    "                                                         split_value=0.5,\n",
    "                                                         x_label=\"Subiculum Connectivity\",\n",
    "                                                         y_label=\"Z-Scored Percent Cognitive Improvement\",\n",
    "                                                         upper_split_legend='AD', lower_split_legend='PD',\n",
    "                                                         manual_p_value=None,\n",
    "                                                         alpha=0.7,\n",
    "                                                         save=True, \n",
    "                                                         out_dir=out_dir)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_icc_with_ci(icc, ci_lower, ci_upper):\n",
    "    plt.figure(figsize=(6.5, 2))\n",
    "    plt.errorbar(x=icc, y=0, xerr=[[icc - ci_lower], [ci_upper - icc]], fmt='o', color='#1f77b4', capsize=5)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.yticks([])\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xlabel('ICC')\n",
    "    plt.title('Intraclass Correlation Coefficient (ICC) with 95% Confidence Interval')\n",
    "    plt.grid(axis='x', linestyle='--')\n",
    "    return plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plot_icc_with_ci(icc=icc, ci_lower=lower_bound, ci_upper=upper_bound)\n",
    "os.makedirs(os.path.join(out_dir, 'icc_figures'), exist_ok=True)\n",
    "figure.savefig(os.path.join(out_dir, 'icc_figures/icc_barplot.png'))\n",
    "figure.savefig(os.path.join(out_dir, 'icc_figures/icc_barplot.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, 'icc_figures/icc_barplot')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a Bunch of ICCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_icc_forest(comparisons_dict, dataframe, bootstrap_samples=2500, full_legend_patches=False):\n",
    "    # figure = plt.figure(figsize=(4, len(comparisons_dict)*1.2))\n",
    "    figure = plt.figure(figsize=(4, 5))\n",
    "    \n",
    "    # Create a color palette with enough unique colors\n",
    "    colors = sns.color_palette(\"tab10\", len(comparisons_dict))\n",
    "    \n",
    "    # Create legend patches\n",
    "    legend_patches = []\n",
    "    \n",
    "    # Iterate through the dictionary and plot ICC for each comparison\n",
    "    for idx, (col1_name, col2_name) in enumerate(comparisons_dict.items()):\n",
    "        # Calculate ICC\n",
    "        icc_value = calculate_icc(dataframe, col1_name, col2_name)\n",
    "\n",
    "        # Bootstrap 95% confidence interval\n",
    "        ci_lower, ci_upper = bootstrap_icc(dataframe, col1_name, col2_name, bootstrap_samples=bootstrap_samples)\n",
    "\n",
    "        # Plot ICC with confidence interval\n",
    "        plt.errorbar(x=icc_value, y=idx, xerr=[[icc_value - ci_lower], [ci_upper - icc_value]], fmt='o', color=colors[idx], capsize=5)\n",
    "        \n",
    "        # Add legend patch\n",
    "        if full_legend_patches:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{col1_name} vs {col2_name}'))\n",
    "        else:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{col1_name}'))\n",
    "\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(-1, len(comparisons_dict))\n",
    "    plt.yticks([])\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xlabel('ICC')\n",
    "    plt.title('Intraclass Correlation Coefficients (ICC) with 95% Confidence Intervals')\n",
    "    plt.grid(axis='x', linestyle='--')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(handles=legend_patches, frameon=False, loc=(0.05, 0.1))\n",
    "    return figure\n",
    "\n",
    "# Example usage\n",
    "# comparisons_dict = {\n",
    "#     'Total': 'Total.1',\n",
    "#     'Score1': 'Score2'\n",
    "# }\n",
    "\n",
    "# plot_icc_forest(comparisons_dict, dataframe=data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_dict = {'Question_2': 'ACE3_Question_2',\n",
    "    'Question_3': 'ACE3_Question_3',\n",
    "    'Question_4': 'ACE3_Question_4',\n",
    "    'Question_5': 'ACE3_Question_5',\n",
    "    'Question_6': 'ACE3_Question_6',\n",
    "    'Question_7': 'ACE3_Question_7',\n",
    "    'Question_8': 'ACE3_Question_8',\n",
    "    'Question_9': 'ACE3_Question_9',\n",
    "    'Question_10': 'ACE3_Question_10',\n",
    "    'Question_11': 'ACE3_Question_11',\n",
    "    'Question_12': 'ACE3_Question_12',\n",
    "    'Question_13': 'ACE3_Question_13',\n",
    "    'Question_14': 'ACE3_Question_14',\n",
    "    'Question_15': 'ACE3_Question_15',\n",
    "    'Question_16': 'ACE3_Question_16',\n",
    "    'Question_17': 'ACE3_Question_17',\n",
    "    'Question_18': 'ACE3_Question_18',\n",
    "    'Question_19': 'ACE3_Question_19',\n",
    "    'Question_20': 'ACE3_Question_20'\n",
    "}  \n",
    "bootstrap_samples = 5000\n",
    "#-----------------------------------------------------------------------------\n",
    "multi_icc_fig = plot_icc_forest(comparisons_dict, dataframe=data_df, bootstrap_samples=bootstrap_samples)\n",
    "\n",
    "os.makedirs(os.path.join(out_dir, 'icc_figures'), exist_ok=True)\n",
    "multi_icc_fig.savefig(os.path.join(out_dir, 'icc_figures/icc_multipl_comparison_barplot.png'))\n",
    "multi_icc_fig.savefig(os.path.join(out_dir, 'icc_figures/icc_multipl_comparison_barplot_barplot.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, 'icc_figures/icc_multipl_comparison_barplot')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_icc_fig.savefig(os.path.join(out_dir, 'icc_figures/all_qeustions_icc_multipl_comparison_barplot.png'))\n",
    "multi_icc_fig.savefig(os.path.join(out_dir, 'icc_figures/all_qeustions_icc_multipl_comparison_barplot_barplot.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, 'icc_figures/all_qeustions_icc_multipl_comparison_barplot')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_var_column = 'Total'\n",
    "dependent_var_column = 'Cognitive Status'\n",
    "positive_outcome = 1\n",
    "negative_outcome = 0\n",
    "\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "data_df = data_df.loc[:, [independent_var_column, dependent_var_column]].copy()\n",
    "data_df['one_hot_status'] = np.where(data_df[dependent_var_column] == positive_outcome, 0, 1)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "# assuming 'one_hot_status' is the target with MCI encoded as 1 and intact as 0, and 'Question_16' is the score\n",
    "y_true = data_df['one_hot_status']\n",
    "scores = data_df[independent_var_column]\n",
    "\n",
    "# calculate the false positive rate and true positive rate for all thresholds of the classification\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "# calculate the AUC (Area Under Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Create a DataFrame to store FPR, TPR, and corresponding thresholds\n",
    "df_roc = pd.DataFrame({\n",
    "    \"FPR\": fpr,\n",
    "    \"TPR\": tpr,\n",
    "    \"Threshold\": thresholds\n",
    "})\n",
    "\n",
    "# Plot\n",
    "sns.set_style(\"white\")\n",
    "roc = plt.figure(figsize=(7, 6))\n",
    "sns.lineplot(x=\"FPR\", y=\"TPR\", data=df_roc, marker='o', color=\"#1f77b4\")\n",
    "\n",
    "\n",
    "plt.title(f'AUC = {roc_auc:.2f}', fontsize=16)\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "if not os.path.exists(out_dir + '/roc'):\n",
    "    os.makedirs(out_dir + '/roc')\n",
    "    \n",
    "roc.savefig(out_dir + '/roc/roc.png')\n",
    "roc.savefig(out_dir + '/roc/roc.svg')\n",
    "print('saved to: ', out_dir + '/roc/roc.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap Confidence Intervals for the AUC Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.distribution_statistics import bootstrap_distribution_statistics\n",
    "from calvin_utils.statistical_utils.classification_statistics import compute_auc\n",
    "# Use the bootstrap function\n",
    "roc_func_args = {'y_true_variable': 'one_hot_status', 'independent_variable': f'{independent_var_column}'}\n",
    "result_df = bootstrap_distribution_statistics(data_df, compute_auc, roc_func_args, bootstrap_samples=10000)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapped ROC Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot CIs with Observed Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from numpy import interp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def plot_bootstrapped_roc(data, y_true_variable, independent_variable, n_bootstraps=2000):\n",
    "    \"\"\"\n",
    "    Plot ROC curve with bootstrapped confidence intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with the data.\n",
    "    - y_true_variable: string with the name of the column containing the true binary labels.\n",
    "    - independent_variable: string with the name of the column containing the independent variable (classifier scores).\n",
    "    - n_bootstraps: number of bootstrap samples to use for generating confidence intervals.\n",
    "    \"\"\"\n",
    "    # Real values\n",
    "    y_true = data[y_true_variable]\n",
    "    scores = data[independent_variable]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "    # Calculate the AUC (Area Under Curve)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Create a DataFrame to store FPR, TPR, and corresponding thresholds\n",
    "    df_roc = pd.DataFrame({\n",
    "        \"FPR\": fpr,\n",
    "        \"TPR\": tpr,\n",
    "        \"Threshold\": thresholds\n",
    "    })\n",
    "\n",
    "    # Empirical values\n",
    "    roc_auc_bootstraps = []\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    # Generate bootstrap samples and calculate ROC curve for each\n",
    "    for i in tqdm(range(n_bootstraps)):\n",
    "        sample = data.sample(frac=1, replace=True)\n",
    "        y_true_sample = sample[y_true_variable]\n",
    "        scores_sample = sample[independent_variable]\n",
    "        fpr, tpr, thresholds = roc_curve(y_true_sample, scores_sample)\n",
    "        tpr = interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        roc_auc_bootstraps.append(tpr)\n",
    "\n",
    "    roc_auc_bootstraps = np.array(roc_auc_bootstraps)\n",
    "\n",
    "    # Calculate mean and confidence intervals\n",
    "    roc_auc_mean = roc_auc_bootstraps.mean(axis=0)\n",
    "    ci_lower = np.percentile(roc_auc_bootstraps, 2.5, axis=0)\n",
    "    ci_upper = np.percentile(roc_auc_bootstraps, 97.5, axis=0)\n",
    "\n",
    "    # Compute AUC using mean ROC curve\n",
    "    roc_auc = auc(base_fpr, roc_auc_mean)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    sns.set_style(\"white\")\n",
    "    sns.lineplot(x=\"FPR\", y=\"TPR\", data=df_roc, marker='o', color=\"darkblue\", ci=None)\n",
    "    ax.fill_between(base_fpr, ci_lower, ci_upper, color='blue', alpha=0.15)\n",
    "    ax.plot(base_fpr, ci_lower, 'b-', alpha=0.05)\n",
    "    ax.plot(base_fpr, ci_upper, 'b-', alpha=0.05)\n",
    "\n",
    "    ax.set_title('AUC = %0.2f' % roc_auc, fontsize=16)\n",
    "    ax.plot([0, 1], [0, 1],'--', color='#ADD8E6')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=13)\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=13)\n",
    "    \n",
    "    sns.despine()\n",
    "    if not os.path.exists(out_dir + '/roc'):\n",
    "        os.makedirs(out_dir + '/roc')\n",
    "    fig.savefig(out_dir + f'/roc/bootstrapped_roc_observed_plotted.png')\n",
    "    fig.savefig(out_dir + f'/roc/bootstrapped_roc_observed_plotted.svg')\n",
    "    print('Saved to: ', out_dir + f'/roc/bootstrapped_roc_observed_plotted.svg')\n",
    "    \n",
    "plot_bootstrapped_roc(data_df, 'one_hot_status', f'{independent_var_column}', 10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapped Presenting Mean Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def plot_bootstrapped_roc(data, y_true_variable, independent_variable, n_bootstraps=2000):\n",
    "    \"\"\"\n",
    "    Plot ROC curve with bootstrapped confidence intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with the data.\n",
    "    - y_true_variable: string with the name of the column containing the true binary labels.\n",
    "    - independent_variable: string with the name of the column containing the independent variable (classifier scores).\n",
    "    - n_bootstraps: number of bootstrap samples to use for generating confidence intervals.\n",
    "    \"\"\"\n",
    "    roc_auc_bootstraps = []\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    # Generate bootstrap samples and calculate ROC curve for each\n",
    "    for i in tqdm(range(n_bootstraps)):\n",
    "        sample = data.sample(frac=1, replace=True)\n",
    "        y_true = sample[y_true_variable]\n",
    "        scores = sample[independent_variable]\n",
    "\n",
    "        # Exclude rows with NaN values\n",
    "        mask = ~(y_true.isna() | scores.isna())\n",
    "        y_true = y_true[mask]\n",
    "        scores = scores[mask]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "        # Interpolate the ROC curve to ensure it starts from (0,0)\n",
    "        tpr = np.interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "\n",
    "        roc_auc_bootstraps.append(tpr)\n",
    "\n",
    "    roc_auc_bootstraps = np.array(roc_auc_bootstraps)\n",
    "\n",
    "    # Calculate mean and confidence intervals, ignoring NaN values\n",
    "    roc_auc_mean = np.nanmean(roc_auc_bootstraps, axis=0)\n",
    "    ci_lower = np.percentile(roc_auc_bootstraps, 2.5, axis=0)\n",
    "    ci_upper = np.percentile(roc_auc_bootstraps, 97.5, axis=0)\n",
    "\n",
    "    # Compute AUC using mean ROC curve\n",
    "    roc_auc = auc(base_fpr, roc_auc_mean)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(base_fpr, roc_auc_mean, '#1f77b4')\n",
    "    ax.fill_between(base_fpr, ci_lower, ci_upper, color='#1f77b4', alpha=0.25)\n",
    "    ax.plot(base_fpr, ci_lower, 'b-', alpha=0.05)\n",
    "    ax.plot(base_fpr, ci_upper, 'b-', alpha=0.05)\n",
    "\n",
    "    ax.set_title('AUC = %0.2f' % roc_auc, fontsize=16)\n",
    "    ax.plot([0, 1], [0, 1],'--', color='#ff7f0e')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=13)\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=13)\n",
    "\n",
    "    sns.despine()\n",
    "    if not os.path.exists(out_dir + '/roc'):\n",
    "        os.makedirs(out_dir + '/roc')\n",
    "    plt.savefig(out_dir + f'/roc/bootstrapped_roc_mean_plotted.png')\n",
    "    plt.savefig(out_dir + f'/roc/bootstrapped_roc_mean_plotted.svg')\n",
    "    print('Saved to: ', out_dir + f'/roc/bootstrapped_roc_mean_plotted.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bootstrapped_roc(data_df, 'one_hot_status', f'{independent_var_column}', 10000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Accuracy Curves Across Thresolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_outcome = 'mci'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "data_df = data_df.loc[:, [independent_var_column, dependent_var_column]]\n",
    "data_df['one_hot_status'] = np.where(data_df[dependent_var_column] == positive_outcome, 0, 1)\n",
    "\n",
    "scores = data_df[independent_var_column]\n",
    "y_true = data_df['one_hot_status']\n",
    "\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "from calvin_utils.statistical_utils.distribution_statistics import bootstrap_distribution_statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "def compute_accuracy(sample, threshold, y_true_variable, independent_variable):\n",
    "    \"\"\"\n",
    "    Computes the accuracy for a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - sample: DataFrame with the data.\n",
    "    - threshold: float with the threshold to use for classifying the scores.\n",
    "    - y_true_variable: string with the name of the column containing the true binary labels.\n",
    "    - independent_variable: string with the name of the column containing the independent variable (classifier scores).\n",
    "\n",
    "    Returns:\n",
    "    - Scalar with the accuracy.\n",
    "    \"\"\"\n",
    "    y_true = sample[y_true_variable]\n",
    "    scores = sample[independent_variable]\n",
    "    \n",
    "    predictions = [0 if score <= threshold else 1 for score in scores]\n",
    "    return accuracy_score(y_true, predictions)\n",
    "\n",
    "def point_accuracy_and_distribution(data, scores_var, y_true_var, bootstrap_samples=1000):\n",
    "    # Extract the scores and true labels\n",
    "    scores = data[scores_var]\n",
    "    y_true = data[y_true_var]\n",
    "\n",
    "    # Calculate accuracies for all thresholds\n",
    "    thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "    accuracies = []\n",
    "\n",
    "    bootstrap_results = {}\n",
    "    for threshold in thresholds_unique:\n",
    "        predictions = [0 if score <= threshold else 1 for score in scores]\n",
    "        accuracy = accuracy_score(y_true, predictions)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # Bootstrap Distribution\n",
    "        func_args = {'threshold': threshold, 'y_true_variable': y_true_var, 'independent_variable': scores_var}\n",
    "        bootstrap_results[f'{threshold}'] = bootstrap_distribution_statistics(data, compute_accuracy, func_args, bootstrap_samples)\n",
    "    # Create a DataFrame to store thresholds and corresponding accuracies\n",
    "    df_accuracies = pd.DataFrame({\n",
    "        \"Threshold\": thresholds_unique,\n",
    "        \"Accuracy\": accuracies\n",
    "    })\n",
    "    \n",
    "    return df_accuracies, bootstrap_results\n",
    "\n",
    "def plot_accuracy_with_ci(df_accuracies, bootstrap_results, output_dir):\n",
    "    \"\"\"\n",
    "    Plots accuracy as a function of threshold with 95% confidence intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - df_accuracies: DataFrame with 'Threshold' and 'Accuracy' columns.\n",
    "    - bootstrap_results: Dictionary with keys as thresholds and values as DataFrames containing bootstrapped statistics.\n",
    "    - output_dir: String with the path to the output directory.\n",
    "\n",
    "    Returns:\n",
    "    - No return value. The plot is saved to the output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the lower and upper bounds of the 95% confidence interval for each threshold\n",
    "    lower_bounds = [bootstrap_results[str(threshold)].loc['Bootstrapped 95-CIs', '2.5th Percentile'] for threshold in df_accuracies[\"Threshold\"]]\n",
    "    upper_bounds = [bootstrap_results[str(threshold)].loc['Bootstrapped 95-CIs', '97.5th Percentile'] for threshold in df_accuracies[\"Threshold\"]]\n",
    "\n",
    "    # Plot accuracy as a function of threshold\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the accuracies \n",
    "    ax.plot(df_accuracies[\"Threshold\"], df_accuracies[\"Accuracy\"], '-o', color=\"darkblue\")\n",
    "\n",
    "    # Plot the 95% confidence intervals with lighter shade\n",
    "    ax.fill_between(df_accuracies[\"Threshold\"], lower_bounds, upper_bounds, color='blue', alpha=0.05)\n",
    "\n",
    "    # Labels\n",
    "    ax.set_ylabel('Accuracy', fontsize=13)\n",
    "    ax.set_xlabel('Threshold', fontsize=13)\n",
    "\n",
    "    sns.despine()\n",
    "\n",
    "    # Check if the directory exists and create it if it doesn't\n",
    "    if not os.path.exists(output_dir + '/accuracy_curves'):\n",
    "        os.makedirs(output_dir + '/accuracy_curves')\n",
    "\n",
    "    # Save the figure before calling show()\n",
    "    fig.savefig(output_dir + f'/accuracy_curves/accuracy_curves_rct_group.svg')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracies, bootstrap_results = point_accuracy_and_distribution(data_df, scores_var=f'{independent_var_column}', y_true_var='one_hot_status', bootstrap_samples=10000)\n",
    "display(bootstrap_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# df_accuracies, bootstrap_results = point_accuracy_and_distribution(data_df, scores_var=f'{independent_var_column}', y_true_var='one_hot_status', bootstrap_samples=1000)\n",
    "plot_accuracy_with_ci(df_accuracies, bootstrap_results, output_dir=out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Displaying bootstrap results of threshold with highest accuracy')\n",
    "bootstrap_results[str(df_accuracies.loc[df_accuracies['Accuracy'].idxmax(), 'Threshold'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sensitivity and Specificity Across Different Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_outcome = 'MCI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "data_df = df_1.copy().loc[:, [independent_var_column, dependent_var_column]]\n",
    "data_df['one_hot_status'] = np.where(data_df[dependent_var_column] == positive_outcome, 1, 0)\n",
    "\n",
    "scores = data_df[independent_var_column]\n",
    "y_true = data_df['one_hot_status']\n",
    "\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "# Unique thresholds\n",
    "thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "\n",
    "for threshold in thresholds_unique:\n",
    "    # Compute predictions based on threshold\n",
    "    predictions = [1 if score <= threshold else 0 for score in scores]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()\n",
    "    \n",
    "    # Compute sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Append to lists\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "\n",
    "# Create a DataFrame to store thresholds, sensitivities, and specificities\n",
    "df_threshold_metrics = pd.DataFrame({\n",
    "    \"Threshold\": thresholds_unique,\n",
    "    \"Sensitivity\": sensitivities,\n",
    "    \"Specificity\": specificities\n",
    "})\n",
    "\n",
    "# Plot sensitivity and specificity as functions of threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Sensitivity\n",
    "sns.lineplot(x=\"Threshold\", y=\"Sensitivity\", data=df_threshold_metrics, marker='o', color=\"lightblue\", label=\"Sensitivity\")\n",
    "\n",
    "# Specificity\n",
    "sns.lineplot(x=\"Threshold\", y=\"Specificity\", data=df_threshold_metrics, marker='o', color=\"darkblue\", label=\"Specificity\")\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/sensitivity_specificity_curves'):\n",
    "    os.makedirs(out_dir + '/sensitivity_specificity_curves')\n",
    "plt.savefig(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.png')\n",
    "plt.savefig(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.svg')\n",
    "df_threshold_metrics.to_csv(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves_values.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "bootstrap_iterations = 10000\n",
    "\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "lower_sensitivity_bounds = []\n",
    "upper_sensitivity_bounds = []\n",
    "lower_specificity_bounds = []\n",
    "upper_specificity_bounds = []\n",
    "\n",
    "for threshold in thresholds_unique:\n",
    "    bootstrap_sensitivities = []\n",
    "    bootstrap_specificities = []\n",
    "\n",
    "    for i in range(bootstrap_iterations):\n",
    "        # Resample data with replacement\n",
    "        resampled_scores, resampled_y_true = resample(scores, y_true)\n",
    "        \n",
    "        # Compute predictions based on threshold\n",
    "        predictions = [1 if score <= threshold else 0 for score in resampled_scores]\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        try:\n",
    "            tn, fp, fn, tp = confusion_matrix(resampled_y_true, predictions).ravel()\n",
    "        except:\n",
    "            continue \n",
    "        if tp + fn != 0:\n",
    "            sensitivity = tp / (tp + fn)\n",
    "        else:\n",
    "            sensitivity = np.nan\n",
    "\n",
    "        if tn + fp != 0:\n",
    "            specificity = tn / (tn + fp)\n",
    "        else:\n",
    "            specificity = np.nan\n",
    "        # # Compute sensitivity and specificity\n",
    "        # sensitivity = tp / (tp + fn)\n",
    "        # specificity = tn / (tn + fp)\n",
    "\n",
    "        # Append to lists\n",
    "        bootstrap_sensitivities.append(sensitivity)\n",
    "        bootstrap_specificities.append(specificity)\n",
    "\n",
    "    # Append mean values to lists\n",
    "    # sensitivities.append(np.mean(bootstrap_sensitivities))\n",
    "    # specificities.append(np.mean(bootstrap_specificities))\n",
    "    sensitivities.append(np.nanmean(bootstrap_sensitivities))\n",
    "    specificities.append(np.nanmean(bootstrap_specificities))\n",
    "    \n",
    "    # Compute 95% confidence intervals for sensitivity and specificity\n",
    "    # lower_sensitivity, upper_sensitivity = np.percentile(bootstrap_sensitivities, [2.5, 97.5])\n",
    "    # lower_specificity, upper_specificity = np.percentile(bootstrap_specificities, [2.5, 97.5])\n",
    "    # Compute 95% confidence intervals for sensitivity and specificity\n",
    "    lower_sensitivity, upper_sensitivity = np.nanpercentile(bootstrap_sensitivities, [2.5, 97.5])\n",
    "    lower_specificity, upper_specificity = np.nanpercentile(bootstrap_specificities, [2.5, 97.5])\n",
    "    \n",
    "    lower_sensitivity_bounds.append(lower_sensitivity)\n",
    "    upper_sensitivity_bounds.append(upper_sensitivity)\n",
    "    lower_specificity_bounds.append(lower_specificity)\n",
    "    upper_specificity_bounds.append(upper_specificity)\n",
    "    \n",
    "confidence_interval_df = pd.DataFrame({\n",
    "    'threshold': thresholds_unique,\n",
    "    'sensitivity': sensitivities,\n",
    "    'lower_sensitivity_bound': lower_sensitivity_bounds,\n",
    "    'upper_sensitivity_bound': upper_sensitivity_bounds,\n",
    "    'specificity': specificities,\n",
    "    'lower_specificity_bound': lower_specificity_bounds,\n",
    "    'upper_specificity_bound': upper_specificity_bounds,\n",
    "})\n",
    "confidence_interval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Merge the two dataframes on the 'Threshold' column\n",
    "df_threshold_metrics = df_threshold_metrics.merge(confidence_interval_df, how='inner', left_on='Threshold', right_on='threshold')\n",
    "\n",
    "# Plot sensitivity and specificity as functions of threshold\n",
    "plt.figure(figsize=(15.5, 6))\n",
    "\n",
    "# Sensitivity\n",
    "sns.lineplot(x=\"Threshold\", y=\"Sensitivity\", data=df_threshold_metrics, marker='o', color=\"#1f77b4\", label=\"Sensitivity\")\n",
    "plt.fill_between(df_threshold_metrics['Threshold'], df_threshold_metrics['lower_sensitivity_bound'], df_threshold_metrics['upper_sensitivity_bound'], color='#1f77b4', alpha=0.25)\n",
    "\n",
    "# Specificity\n",
    "sns.lineplot(x=\"Threshold\", y=\"Specificity\", data=df_threshold_metrics, marker='o', color=\"#ff7f0e\", label=\"Specificity\")\n",
    "plt.fill_between(df_threshold_metrics['Threshold'], df_threshold_metrics['lower_specificity_bound'], df_threshold_metrics['upper_specificity_bound'], color='#ff7f0e', alpha=0.25)\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend(loc=(0.9, 0.5), frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/sensitivity_specificity_curves'):\n",
    "    os.makedirs(out_dir + '/sensitivity_specificity_curves')\n",
    "plt.savefig(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.png')\n",
    "plt.savefig(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.svg')\n",
    "df_threshold_metrics.to_csv(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves_values.csv')\n",
    "print('Saved to: ', out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.svg')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate PPV and NPV Across All Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "data_df = data_df.loc[:, [independent_var_column, dependent_var_column]]\n",
    "data_df['one_hot_status'] = np.where(data_df[dependent_var_column] == positive_outcome, 1, 0)\n",
    "\n",
    "scores = data_df[independent_var_column]\n",
    "y_true = data_df['one_hot_status']\n",
    "\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Unique thresholds\n",
    "thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "\n",
    "ppv_list = []\n",
    "npv_list = []\n",
    "\n",
    "for threshold in thresholds_unique:\n",
    "    # Compute predictions based on threshold\n",
    "    predictions = [1 if score <= threshold else 0 for score in scores]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()\n",
    "    \n",
    "    # Compute Positive Predictive Value (PPV) and Negative Predictive Value (NPV)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "    npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "    \n",
    "    # Append to lists\n",
    "    ppv_list.append(ppv)\n",
    "    npv_list.append(npv)\n",
    "\n",
    "# Create a DataFrame to store thresholds, PPVs, and NPVs\n",
    "df_ppv_npv = pd.DataFrame({\n",
    "    \"Threshold\": thresholds_unique,\n",
    "    \"Positive Predictive Value\": ppv_list,\n",
    "    \"Negative Predictive Value\": npv_list\n",
    "})\n",
    "\n",
    "# Plot PPV and NPV as functions of threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# PPV\n",
    "sns.lineplot(x=\"Threshold\", y=\"Positive Predictive Value\", data=df_ppv_npv, marker='o', color=\"lightblue\", label=\"PPV\")\n",
    "\n",
    "# NPV\n",
    "sns.lineplot(x=\"Threshold\", y=\"Negative Predictive Value\", data=df_ppv_npv, marker='o', color=\"darkblue\", label=\"NPV\")\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/ppv_npv_curves'):\n",
    "    os.makedirs(out_dir + '/ppv_npv_curves')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.png')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.svg')\n",
    "df_ppv_npv.to_csv(out_dir + f'/ppv_npv_curves/ppv_npv_curves.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "bootstrap_samples = 10000  # number of bootstrap samples to generate\n",
    "\n",
    "# Unique thresholds\n",
    "thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "\n",
    "ppv_list = []\n",
    "npv_list = []\n",
    "ppv_lower = []\n",
    "ppv_upper = []\n",
    "npv_lower = []\n",
    "npv_upper = []\n",
    "\n",
    "y_true_array = y_true.to_numpy()\n",
    "scores_array = scores.to_numpy()\n",
    "for threshold in thresholds_unique:\n",
    "    # Compute predictions based on threshold\n",
    "    predictions = [1 if score <= threshold else 0 for score in scores_array]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Compute Positive Predictive Value (PPV) and Negative Predictive Value (NPV)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "    npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "\n",
    "    # Bootstrap samples\n",
    "    ppv_bootstrap = []\n",
    "    npv_bootstrap = []\n",
    "\n",
    "    # Generate bootstrap samples\n",
    "    for _ in range(bootstrap_samples):\n",
    "        # Create bootstrap sample\n",
    "        bootstrap_indices = np.random.choice(np.arange(len(predictions)), size=len(predictions), replace=True)\n",
    "        bootstrap_predictions = [predictions[i] for i in bootstrap_indices]\n",
    "        bootstrap_y_true = [y_true_array[i] for i in bootstrap_indices]\n",
    "\n",
    "        \n",
    "        # Compute confusion matrix for bootstrap sample\n",
    "        tn, fp, fn, tp = confusion_matrix(bootstrap_y_true, bootstrap_predictions).ravel()\n",
    "        \n",
    "        # Compute PPV and NPV for bootstrap sample\n",
    "        ppv_bootstrap_sample = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "        npv_bootstrap_sample = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "\n",
    "        # Append to lists\n",
    "        ppv_bootstrap.append(ppv_bootstrap_sample)\n",
    "        npv_bootstrap.append(npv_bootstrap_sample)\n",
    "\n",
    "    # Compute 95% confidence intervals\n",
    "    ppv_lower.append(np.percentile(ppv_bootstrap, 2.5))\n",
    "    ppv_upper.append(np.percentile(ppv_bootstrap, 97.5))\n",
    "    npv_lower.append(np.percentile(npv_bootstrap, 2.5))\n",
    "    npv_upper.append(np.percentile(npv_bootstrap, 97.5))\n",
    "    \n",
    "    # Append to lists\n",
    "    ppv_list.append(ppv)\n",
    "    npv_list.append(npv)\n",
    "\n",
    "# Create a DataFrame to store thresholds, PPVs, and NPVs\n",
    "df_ppv_npv = pd.DataFrame({\n",
    "    \"Threshold\": thresholds_unique,\n",
    "    \"Positive Predictive Value\": ppv_list,\n",
    "    \"PPV Lower\": ppv_lower,\n",
    "    \"PPV Upper\": ppv_upper,\n",
    "    \"Negative Predictive Value\": npv_list,\n",
    "    \"NPV Lower\": npv_lower,\n",
    "    \"NPV Upper\": npv_upper\n",
    "})\n",
    "\n",
    "# Plot PPV and NPV as functions of threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# PPV\n",
    "plt.plot(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"Positive Predictive Value\"], '-o', color=\"lightblue\", label=\"PPV\")\n",
    "plt.fill_between(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"PPV Lower\"], df_ppv_npv[\"PPV Upper\"], color='lightblue', alpha=0.25)\n",
    "\n",
    "# NPV\n",
    "plt.plot(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"Negative Predictive Value\"], '-o', color=\"darkblue\", label=\"NPV\")\n",
    "plt.fill_between(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"NPV Lower\"], df_ppv_npv[\"NPV Upper\"], color='darkblue', alpha=0.05)\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/ppv_npv_curves'):\n",
    "    os.makedirs(out_dir + '/ppv_npv_curves')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.png')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.svg')\n",
    "df_ppv_npv.to_csv(out_dir + f'/ppv_npv_curves/ppv_npv_curves.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "bootstrap_samples = 10000  # number of bootstrap samples to generate\n",
    "\n",
    "# Unique thresholds\n",
    "thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "\n",
    "ppv_list = []\n",
    "npv_list = []\n",
    "ppv_lower = []\n",
    "ppv_upper = []\n",
    "npv_lower = []\n",
    "npv_upper = []\n",
    "\n",
    "y_true_array = y_true.to_numpy()\n",
    "scores_array = scores.to_numpy()\n",
    "for threshold in thresholds_unique:\n",
    "    # Compute predictions based on threshold\n",
    "    predictions = [1 if score <= threshold else 0 for score in scores_array]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_array, predictions).ravel()\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Compute Positive Predictive Value (PPV) and Negative Predictive Value (NPV)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "    npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "\n",
    "    # Bootstrap samples\n",
    "    ppv_bootstrap = []\n",
    "    npv_bootstrap = []\n",
    "\n",
    "    # Generate bootstrap samples\n",
    "    for _ in range(bootstrap_samples):\n",
    "        # Create bootstrap sample\n",
    "        bootstrap_indices = np.random.choice(np.arange(len(predictions)), size=len(predictions), replace=True)\n",
    "        bootstrap_predictions = [predictions[i] for i in bootstrap_indices]\n",
    "        bootstrap_y_true = [y_true_array[i] for i in bootstrap_indices]\n",
    "\n",
    "        \n",
    "        # Compute confusion matrix for bootstrap sample\n",
    "        tn, fp, fn, tp = confusion_matrix(bootstrap_y_true, bootstrap_predictions).ravel()\n",
    "        \n",
    "        # Compute PPV and NPV for bootstrap sample\n",
    "        ppv_bootstrap_sample = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "        npv_bootstrap_sample = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "\n",
    "        # Append to lists\n",
    "        ppv_bootstrap.append(ppv_bootstrap_sample)\n",
    "        npv_bootstrap.append(npv_bootstrap_sample)\n",
    "\n",
    "    # Compute 95% confidence intervals\n",
    "    ppv_lower.append(np.percentile(ppv_bootstrap, 2.5))\n",
    "    ppv_upper.append(np.percentile(ppv_bootstrap, 97.5))\n",
    "    npv_lower.append(np.percentile(npv_bootstrap, 2.5))\n",
    "    npv_upper.append(np.percentile(npv_bootstrap, 97.5))\n",
    "    \n",
    "    # Append to lists\n",
    "    ppv_list.append(ppv)\n",
    "    npv_list.append(npv)\n",
    "\n",
    "# Create a DataFrame to store thresholds, PPVs, and NPVs\n",
    "df_ppv_npv = pd.DataFrame({\n",
    "    \"Threshold\": thresholds_unique,\n",
    "    \"Positive Predictive Value\": ppv_list,\n",
    "    \"PPV Lower\": ppv_lower,\n",
    "    \"PPV Upper\": ppv_upper,\n",
    "    \"Negative Predictive Value\": npv_list,\n",
    "    \"NPV Lower\": npv_lower,\n",
    "    \"NPV Upper\": npv_upper\n",
    "})\n",
    "\n",
    "# Plot PPV and NPV as functions of threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# PPV\n",
    "plt.plot(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"Positive Predictive Value\"], '-o', color=\"lightblue\", label=\"PPV\")\n",
    "plt.fill_between(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"PPV Lower\"], df_ppv_npv[\"PPV Upper\"], color='lightblue', alpha=0.25)\n",
    "\n",
    "# NPV\n",
    "plt.plot(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"Negative Predictive Value\"], '-o', color=\"darkblue\", label=\"NPV\")\n",
    "plt.fill_between(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"NPV Lower\"], df_ppv_npv[\"NPV Upper\"], color='darkblue', alpha=0.05)\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/ppv_npv_curves'):\n",
    "    os.makedirs(out_dir + '/ppv_npv_curves')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.png')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.svg')\n",
    "df_ppv_npv.to_csv(out_dir + f'/ppv_npv_curves/ppv_npv_curves.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_ppv_npv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Other Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num positive cases: ', np.count_nonzero(data_df['one_hot_status']))\n",
    "print('Num total cases: ', np.count_nonzero(data_df.index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62880161f19d28ddb8a8f59c63374d84ace356c39e36cc839cb3fb3bb03fb010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
