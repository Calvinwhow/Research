{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the CSV\n",
    "input_csv_path = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list_proper_subjects.csv'\n",
    "out_dir = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/analyses'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement', 'Z_Scored_Subiculum_Connectivity',\n",
       "       'Subiculum_Connectivity', 'Amnesia_Lesion_T_Map', 'Memory_Network_T',\n",
       "       'Z_Scored_Memory_Network_R', 'Memory_Network_R',\n",
       "       'Subiculum_Grey_Matter', 'Subiculum_White_Matter', 'Subiculum_CSF',\n",
       "       'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease', 'Subiculum_Group',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Inflection'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Z_Scored_Percent_Cognitive_Improvement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'City'  # The column you'd like to evaluate\n",
    "condition = 'not'  # Thecondition to check ('equal', 'above', 'below', 'not')\n",
    "value = 'Boston'  # The value to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Z_Scored_Percent_Cognitive_Improvement', 'Z_Scored_Subiculum_Connectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to standardize column Disease: Could not convert ParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinsonParkinson to numeric\n",
      "Unable to standardize column City: Could not convert BostonBostonBostonBostonBostonBostonBostonBostonBostonBostonBostonBostonBostonBostonBostonBoston to numeric\n",
      "Unable to standardize column Age_Group: Could not convert youngyoungyoungoldyoungoldyoungyoungyoungyoungyoungyoungyoungyoungyoungyoung to numeric\n",
      "Unable to standardize column Age_And_Disease: Could not convert Parkinson_youngParkinson_youngParkinson_youngParkinson_oldParkinson_youngParkinson_oldParkinson_youngParkinson_youngParkinson_youngParkinson_youngParkinson_youngParkinson_youngParkinson_youngParkinson_youngParkinson_youngParkinson_young to numeric\n",
      "Unable to standardize column Subiculum_Group: Could not convert highhighlowlowlowlowhighhighlowlowhighlowhighhighlowhigh to numeric\n",
      "Unable to standardize column Age_Disease_and_Cohort: Could not convert Parkinson_young_BostonParkinson_young_BostonParkinson_young_BostonParkinson_old_BostonParkinson_young_BostonParkinson_old_BostonParkinson_young_BostonParkinson_young_BostonParkinson_young_BostonParkinson_young_BostonParkinson_young_BostonParkinson_young_BostonParkinson_young_BostonParkinson_young_BostonParkinson_young_BostonParkinson_young_Boston to numeric\n",
      "Unable to standardize column Subiculum_Group_By_Inflection: Could not convert HighHighLowLowLowLowHighHighLowLowHighLowHighHighLowHigh to numeric\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity</th>\n",
       "      <th>Subiculum_Connectivity</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>Memory_Network_T</th>\n",
       "      <th>Z_Scored_Memory_Network_R</th>\n",
       "      <th>...</th>\n",
       "      <th>Standardized_Subiculum_Total</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>City</th>\n",
       "      <th>Inclusion_Cohort</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>Age_And_Disease</th>\n",
       "      <th>Subiculum_Group</th>\n",
       "      <th>Age_Disease_and_Cohort</th>\n",
       "      <th>Subiculum_Group_By_Inflection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-1.652802</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>-0.855337</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>-1.920852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>high</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-1.353144</td>\n",
       "      <td>0.800132</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.763335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>high</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-1.278229</td>\n",
       "      <td>0.509175</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>-0.1010</td>\n",
       "      <td>-0.494291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>low</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-1.053486</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>-1.8100</td>\n",
       "      <td>1.157517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>old</td>\n",
       "      <td>Parkinson_old</td>\n",
       "      <td>low</td>\n",
       "      <td>Parkinson_old_Boston</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-0.903657</td>\n",
       "      <td>0.654654</td>\n",
       "      <td>-0.855337</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>-1.0600</td>\n",
       "      <td>0.143907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>low</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.678913</td>\n",
       "      <td>-2.254918</td>\n",
       "      <td>1.250108</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-2.236068</td>\n",
       "      <td>-0.3380</td>\n",
       "      <td>0.350383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.840546</td>\n",
       "      <td>old</td>\n",
       "      <td>Parkinson_old</td>\n",
       "      <td>low</td>\n",
       "      <td>Parkinson_old_Boston</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-0.004682</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.651127</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.651127</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>high</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.145147</td>\n",
       "      <td>-1.091089</td>\n",
       "      <td>-1.908060</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.461708</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.461708</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>high</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.220061</td>\n",
       "      <td>-0.509175</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.4290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.272289</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.272289</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>low</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.444805</td>\n",
       "      <td>-0.072739</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>-0.082871</td>\n",
       "      <td>Boston</td>\n",
       "      <td>-0.082871</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>low</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.519720</td>\n",
       "      <td>1.963961</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0.106548</td>\n",
       "      <td>Boston</td>\n",
       "      <td>0.106548</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>high</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.669549</td>\n",
       "      <td>-0.072739</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.5740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0.295967</td>\n",
       "      <td>Boston</td>\n",
       "      <td>0.295967</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>low</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.894292</td>\n",
       "      <td>-0.509175</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>0.864223</td>\n",
       "      <td>Boston</td>\n",
       "      <td>0.864223</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>high</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.119036</td>\n",
       "      <td>1.382047</td>\n",
       "      <td>-1.908060</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>1.243061</td>\n",
       "      <td>Boston</td>\n",
       "      <td>1.243061</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>high</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.193951</td>\n",
       "      <td>0.654654</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.7560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>1.432479</td>\n",
       "      <td>Boston</td>\n",
       "      <td>1.432479</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>low</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.718352</td>\n",
       "      <td>-1.236568</td>\n",
       "      <td>2.302831</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parkinson</td>\n",
       "      <td>2.568992</td>\n",
       "      <td>Boston</td>\n",
       "      <td>2.568992</td>\n",
       "      <td>young</td>\n",
       "      <td>Parkinson_young</td>\n",
       "      <td>high</td>\n",
       "      <td>Parkinson_young_Boston</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject       Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "80  -1.652802  0.218218                                 -0.855337   \n",
       "82  -1.353144  0.800132                                  0.197386   \n",
       "83  -1.278229  0.509175                                  0.197386   \n",
       "84  -1.053486 -0.218218                                  0.197386   \n",
       "85  -0.903657  0.654654                                 -0.855337   \n",
       "86  -0.678913 -2.254918                                  1.250108   \n",
       "88  -0.004682 -0.218218                                  0.197386   \n",
       "89   0.145147 -1.091089                                 -1.908060   \n",
       "90   0.220061 -0.509175                                  0.197386   \n",
       "91   0.444805 -0.072739                                  0.197386   \n",
       "92   0.519720  1.963961                                  0.197386   \n",
       "93   0.669549 -0.072739                                  0.197386   \n",
       "96   0.894292 -0.509175                                  0.197386   \n",
       "98   1.119036  1.382047                                 -1.908060   \n",
       "99   1.193951  0.654654                                  0.197386   \n",
       "105  1.718352 -1.236568                                  2.302831   \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "80                                     0.25                       0.447214   \n",
       "82                                     0.00                       0.447214   \n",
       "83                                     0.00                       0.447214   \n",
       "84                                     0.00                       0.447214   \n",
       "85                                     0.25                       0.447214   \n",
       "86                                    -0.25                      -2.236068   \n",
       "88                                     0.00                            NaN   \n",
       "89                                     0.50                            NaN   \n",
       "90                                     0.00                            NaN   \n",
       "91                                     0.00                            NaN   \n",
       "92                                     0.00                            NaN   \n",
       "93                                     0.00                            NaN   \n",
       "96                                     0.00                            NaN   \n",
       "98                                     0.50                            NaN   \n",
       "99                                     0.00                            NaN   \n",
       "105                                   -0.50                            NaN   \n",
       "\n",
       "     Z_Scored_Subiculum_Connectivity  Subiculum_Connectivity  \\\n",
       "80                            0.0987               -1.920852   \n",
       "82                            0.1170                0.763335   \n",
       "83                           -0.1010               -0.494291   \n",
       "84                           -1.8100                1.157517   \n",
       "85                           -1.0600                0.143907   \n",
       "86                           -0.3380                0.350383   \n",
       "88                            0.7900                     NaN   \n",
       "89                            0.3710                     NaN   \n",
       "90                           -0.4290                     NaN   \n",
       "91                           -0.1190                     NaN   \n",
       "92                            1.1700                     NaN   \n",
       "93                           -0.5740                     NaN   \n",
       "96                            0.1900                     NaN   \n",
       "98                            0.3900                     NaN   \n",
       "99                           -0.7560                     NaN   \n",
       "105                           2.0800                     NaN   \n",
       "\n",
       "     Amnesia_Lesion_T_Map  Memory_Network_T  Z_Scored_Memory_Network_R  ...  \\\n",
       "80                    NaN               NaN                        NaN  ...   \n",
       "82                    NaN               NaN                        NaN  ...   \n",
       "83                    NaN               NaN                        NaN  ...   \n",
       "84                    NaN               NaN                        NaN  ...   \n",
       "85                    NaN               NaN                        NaN  ...   \n",
       "86                    NaN               NaN                        NaN  ...   \n",
       "88                    NaN               NaN                        NaN  ...   \n",
       "89                    NaN               NaN                        NaN  ...   \n",
       "90                    NaN               NaN                        NaN  ...   \n",
       "91                    NaN               NaN                        NaN  ...   \n",
       "92                    NaN               NaN                        NaN  ...   \n",
       "93                    NaN               NaN                        NaN  ...   \n",
       "96                    NaN               NaN                        NaN  ...   \n",
       "98                    NaN               NaN                        NaN  ...   \n",
       "99                    NaN               NaN                        NaN  ...   \n",
       "105                   NaN               NaN                        NaN  ...   \n",
       "\n",
       "     Standardized_Subiculum_Total    Disease    Cohort    City  \\\n",
       "80                            NaN  Parkinson -0.840546  Boston   \n",
       "82                            NaN  Parkinson -0.840546  Boston   \n",
       "83                            NaN  Parkinson -0.840546  Boston   \n",
       "84                            NaN  Parkinson -0.840546  Boston   \n",
       "85                            NaN  Parkinson -0.840546  Boston   \n",
       "86                            NaN  Parkinson -0.840546  Boston   \n",
       "88                            NaN  Parkinson -0.651127  Boston   \n",
       "89                            NaN  Parkinson -0.461708  Boston   \n",
       "90                            NaN  Parkinson -0.272289  Boston   \n",
       "91                            NaN  Parkinson -0.082871  Boston   \n",
       "92                            NaN  Parkinson  0.106548  Boston   \n",
       "93                            NaN  Parkinson  0.295967  Boston   \n",
       "96                            NaN  Parkinson  0.864223  Boston   \n",
       "98                            NaN  Parkinson  1.243061  Boston   \n",
       "99                            NaN  Parkinson  1.432479  Boston   \n",
       "105                           NaN  Parkinson  2.568992  Boston   \n",
       "\n",
       "     Inclusion_Cohort  Age_Group  Age_And_Disease  Subiculum_Group  \\\n",
       "80          -0.840546      young  Parkinson_young             high   \n",
       "82          -0.840546      young  Parkinson_young             high   \n",
       "83          -0.840546      young  Parkinson_young              low   \n",
       "84          -0.840546        old    Parkinson_old              low   \n",
       "85          -0.840546      young  Parkinson_young              low   \n",
       "86          -0.840546        old    Parkinson_old              low   \n",
       "88          -0.651127      young  Parkinson_young             high   \n",
       "89          -0.461708      young  Parkinson_young             high   \n",
       "90          -0.272289      young  Parkinson_young              low   \n",
       "91          -0.082871      young  Parkinson_young              low   \n",
       "92           0.106548      young  Parkinson_young             high   \n",
       "93           0.295967      young  Parkinson_young              low   \n",
       "96           0.864223      young  Parkinson_young             high   \n",
       "98           1.243061      young  Parkinson_young             high   \n",
       "99           1.432479      young  Parkinson_young              low   \n",
       "105          2.568992      young  Parkinson_young             high   \n",
       "\n",
       "     Age_Disease_and_Cohort  Subiculum_Group_By_Inflection  \n",
       "80   Parkinson_young_Boston                           High  \n",
       "82   Parkinson_young_Boston                           High  \n",
       "83   Parkinson_young_Boston                            Low  \n",
       "84     Parkinson_old_Boston                            Low  \n",
       "85   Parkinson_young_Boston                            Low  \n",
       "86     Parkinson_old_Boston                            Low  \n",
       "88   Parkinson_young_Boston                           High  \n",
       "89   Parkinson_young_Boston                           High  \n",
       "90   Parkinson_young_Boston                            Low  \n",
       "91   Parkinson_young_Boston                            Low  \n",
       "92   Parkinson_young_Boston                           High  \n",
       "93   Parkinson_young_Boston                            Low  \n",
       "96   Parkinson_young_Boston                           High  \n",
       "98   Parkinson_young_Boston                           High  \n",
       "99   Parkinson_young_Boston                            Low  \n",
       "105  Parkinson_young_Boston                           High  \n",
       "\n",
       "[16 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def generate_scatterplot(dataframe, data_dict, correlation, palette, out_dir):\n",
    "    \"\"\"\n",
    "    Generates a 3D scatter plot from the given DataFrame and saves it to the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas DataFrame containing the data.\n",
    "    - data_dict: Dictionary with one key-value pair, where the key is the dependent variable \n",
    "                 and the value is a list of independent variables (length should be 2 for 3D scatter).\n",
    "    - x_label, y_label, z_label: Labels for the x, y, and z axes.\n",
    "    - correlation: Whether to display correlation information on the plot (True/False).\n",
    "    - palette: Color palette for the plot.\n",
    "    - out_dir: Directory path where the plot image will be saved.\n",
    "    \"\"\"\n",
    "    dependent_var = list(data_dict.keys())[0]\n",
    "    independent_vars = data_dict[dependent_var]\n",
    "\n",
    "    if len(independent_vars) != 2:\n",
    "        raise ValueError(\"Independent variable list must contain exactly two elements for 3D scatter plot.\")\n",
    "\n",
    "    fig = px.scatter_3d(dataframe, x=independent_vars[0], y=independent_vars[1], z=dependent_var,\n",
    "                        color=dependent_var, color_continuous_scale=palette)\n",
    "\n",
    "    # Set the labels\n",
    "    fig.update_layout(scene=dict(\n",
    "        xaxis_title=independent_vars[0],\n",
    "        yaxis_title=independent_vars[1],\n",
    "        zaxis_title=dependent_var\n",
    "    ))\n",
    "\n",
    "    # Optionally, add correlation info as annotation\n",
    "    if correlation:\n",
    "        # Compute and display correlation (requires additional implementation)\n",
    "        pass\n",
    "\n",
    "    # Save the plot to the output directory\n",
    "    fig.write_image(f\"{out_dir}/3d_scatter_plot.png\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement', 'Z_Scored_Subiculum_Connectivity',\n",
       "       'Subiculum_Connectivity', 'Amnesia_Lesion_T_Map', 'Memory_Network_T',\n",
       "       'Z_Scored_Memory_Network_R', 'Memory_Network_R',\n",
       "       'Subiculum_Grey_Matter', 'Subiculum_White_Matter', 'Subiculum_CSF',\n",
       "       'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease', 'Subiculum_Group',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Inflection'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'Z_Scored_Percent_Cognitive_Improvement': ['Z_Scored_Subiculum_Connectivity', 'Age']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Z_Scored_Subiculum_Connectivity=%{x}<br>Age=%{y}<br>Z_Scored_Percent_Cognitive_Improvement=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           0.25,
           0,
           0,
           0,
           0.25,
           -0.25,
           0,
           0.5,
           0,
           0,
           0,
           0,
           0,
           0.5,
           0,
           -0.5
          ],
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "scene": "scene",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0.0987,
          0.117,
          -0.101,
          -1.81,
          -1.06,
          -0.338,
          0.79,
          0.371,
          -0.429,
          -0.119,
          1.17,
          -0.574,
          0.19,
          0.39,
          -0.756,
          2.08
         ],
         "y": [
          0.21821789023599236,
          0.8001322641986387,
          0.5091750772173156,
          -0.21821789023599236,
          0.6546536707079771,
          -2.2549181991052545,
          -0.21821789023599236,
          -1.0910894511799618,
          -0.5091750772173156,
          -0.07273929674533079,
          1.9639610121239315,
          -0.07273929674533079,
          -0.5091750772173156,
          1.382046638161285,
          0.6546536707079771,
          -1.2365680446706235
         ],
         "z": [
          0.25,
          0,
          0,
          0,
          0.25,
          -0.25,
          0,
          0.5,
          0,
          0,
          0,
          0,
          0,
          0.5,
          0,
          -0.5
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "Z_Scored_Percent_Cognitive_Improvement"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(255,245,240)"
          ],
          [
           0.125,
           "rgb(254,224,210)"
          ],
          [
           0.25,
           "rgb(252,187,161)"
          ],
          [
           0.375,
           "rgb(252,146,114)"
          ],
          [
           0.5,
           "rgb(251,106,74)"
          ],
          [
           0.625,
           "rgb(239,59,44)"
          ],
          [
           0.75,
           "rgb(203,24,29)"
          ],
          [
           0.875,
           "rgb(165,15,21)"
          ],
          [
           1,
           "rgb(103,0,13)"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "scene": {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "Z_Scored_Subiculum_Connectivity"
          }
         },
         "yaxis": {
          "title": {
           "text": "Age"
          }
         },
         "zaxis": {
          "title": {
           "text": "Z_Scored_Percent_Cognitive_Improvement"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "plot = generate_scatterplot(dataframe=data_df, \n",
    "                            data_dict=data_dict,\n",
    "                            correlation=True,\n",
    "                            palette='Reds',\n",
    "                            out_dir=out_dir)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input columns to test like this:\n",
    "- group1_col_name = 'Alzheimer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_col_name = 'Wurzburg'\n",
    "group2_col_name = 'Boston'\n",
    "x_label = 'Disease'\n",
    "y_label = 'Age'\n",
    "test_type = 't-test_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seabor as sns\n",
    "from calvin_utils.statistical_utils.distribution_statistics import plot_with_annotation\n",
    "plt = plot_with_annotation(dataframe=data_df, \n",
    "                           col1=group1_col_name, \n",
    "                           col2=group2_col_name,\n",
    "                           xlabel=x_label, \n",
    "                           ylabel=y_label, \n",
    "                           test_type=test_type,\n",
    "                           colours=sns.set_palette(['red', 'blue'])\n",
    ")\n",
    "\n",
    "os.makedirs(os.path.join(out_dir, 'distribution_figures/validation'), exist_ok=True)\n",
    "plt.savefig(os.path.join(out_dir, f'distribution_figures/validation/{x_label}_{test_type}_{y_label}.png'))\n",
    "plt.savefig(os.path.join(out_dir, f'distribution_figures/validation/{x_label}_{test_type}_{y_label}.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, f'distribution_figures/validation/{x_label}_{test_type}_{y_label}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Multiple Distribution Tests and Correct for Multiple Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a detailed description of the `plot_distribution_tests` function.\n",
    "\n",
    "### `plot_distribution_tests` Function\n",
    "\n",
    "This function creates a plot of distribution tests with optional annotations for multiple key-value pair comparisons.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- `dataframe`: A pandas DataFrame containing the data for comparison.\n",
    "- `comparisons_dict`: A dictionary mapping comparisons with keys and values representing column names in the DataFrame.\n",
    "- `test_type`: Specifies the statistical method used for testing. Available options are:\n",
    "  - `'t-test_ind'`: Independent t-test\n",
    "  - `'t-test_paired'`: Paired t-test\n",
    "  - `'Mann-Whitney'`: Mann-Whitney U test\n",
    "  - `'Wilcoxon'`: Wilcoxon signed-rank test\n",
    "- `correction_method`: Specifies the method used to correct for multiple comparisons. Available options are:\n",
    "  - `'bonferroni'`: Bonferroni correction\n",
    "  - `'holm'`: Holm's correction\n",
    "  - `'fdr_bh'`: Benjamini-Hochberg false discovery rate correction\n",
    "- `key_label`: A string label for the keys in the plot.\n",
    "- `value_label`: A string label for the values in the plot.\n",
    "- `x_label`: Label for the x-axis.\n",
    "- `y_label`: Label for the y-axis.\n",
    "- `display`: Controls how the keys and values are displayed. Available options are:\n",
    "  - `'split_pairs'`: Displays keys and values side by side, with keys on the left and values on the right.\n",
    "  - `'group_pairs'`: Groups the key-value pairs together.\n",
    "- `full_legend_patches`: A boolean that controls the appearance of the legend. If `True`, full key-value pairs are shown in the legend; if `False`, only keys are shown.\n",
    "\n",
    "#### Returns\n",
    "\n",
    "- A matplotlib plot object.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "comparisons_dict = {\n",
    "    'Attention': 'Attention.1',\n",
    "    'Memory': 'Memory.1',\n",
    "    'Language': 'Language.1',\n",
    "    'Fluency': 'Fluency.1',\n",
    "    'Visuospatial': 'Visuospatial.1',\n",
    "}\n",
    "test_type = 'Mann-Whitney'\n",
    "correction_method = 'bonferroni'\n",
    "key_label = 'Keys'\n",
    "value_label = 'Values'\n",
    "x_label = 'Metrics'\n",
    "y_label = 'Values'\n",
    "\n",
    "plt = plot_distribution_tests(dataframe=data_df,\n",
    "                              comparisons_dict=comparisons_dict, \n",
    "                              test_type=test_type, \n",
    "                              correction_method=correction_method,\n",
    "                              key_label=key_label,\n",
    "                              value_label=value_label,\n",
    "                              x_label=x_label,\n",
    "                              y_label=y_label)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Notes\n",
    "\n",
    "- The `test_type` and `correction_method` must be chosen from the available options; otherwise, a `ValueError` will be raised.\n",
    "- The function supports customization of the plot appearance, including color mapping, annotations, and more, to suit various use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannotations.Annotator import Annotator\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_distribution_tests(dataframe, comparisons_dict, test_type, correction_method, key_label, value_label, x_label, y_label, display='split_pairs', full_legend_patches=False):\n",
    "    valid_tests = ['t-test_ind', 't-test_paired', 'Mann-Whitney', 'Wilcoxon']\n",
    "    valid_corrections = ['bonferroni', 'holm', 'fdr_bh']\n",
    "    if test_type not in valid_tests:\n",
    "        raise ValueError(f\"Invalid test type. Choose from: {', '.join(valid_tests)}\")\n",
    "    if correction_method not in valid_corrections:\n",
    "        raise ValueError(f\"Invalid correction method. Choose from: {', '.join(valid_corrections)}\")\n",
    "\n",
    "    # Create a color palette with enough unique colors\n",
    "    colors = sns.color_palette(\"tab10\", len(comparisons_dict))\n",
    "\n",
    "    # Create a new DataFrame to hold the data for plotting\n",
    "    data = pd.DataFrame(columns=['GroupType', 'Metric', 'Value'])\n",
    "    pairs = []\n",
    "    # Create legend patches\n",
    "    legend_patches = []\n",
    "\n",
    "    # Iterate through comparisons_dict\n",
    "    for idx, (key, value) in enumerate(comparisons_dict.items()):\n",
    "        key_series = dataframe[key]\n",
    "        value_series = dataframe[value]\n",
    "        color = colors[idx]\n",
    "        temp_data = pd.DataFrame({\n",
    "            'GroupType': [f'Key'] * len(key_series) + [f'Value'] * len(value_series),\n",
    "            'Metric': [key] * len(key_series) + [value] * len(value_series),\n",
    "            'Value': key_series.tolist() + value_series.tolist(),\n",
    "            'Color': [color] * (len(key_series) + len(value_series))\n",
    "        })\n",
    "        data = pd.concat([data, temp_data], axis=0)\n",
    "        pairs.append(((key, 'Key'), (value, 'Value')))\n",
    "        \n",
    "        if full_legend_patches:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{key} vs {value}'))\n",
    "        else:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{value}'))\n",
    "\n",
    "    # Map colour\n",
    "    palette_mapping = {row['Metric']: row['Color'] for _, row in data.iterrows()}\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(len(comparisons_dict)*1.3, 5))\n",
    "    # Add annotations\n",
    "    if display=='group_pairs':\n",
    "        # Plot with seaborn (split image)\n",
    "        sns.set_style('white')\n",
    "        palette = sns.color_palette('tab10', 2, desat=1)  # Define palette with 2 colors\n",
    "        sns.set_palette(palette)\n",
    "        ax = sns.boxplot(x='Metric', y='Value', hue='GroupType', data=data, ax=ax, palette=palette)\n",
    "        \n",
    "        # Define the Legend\n",
    "        key_color = palette[0]  # Color for the 'Key' group\n",
    "        value_color = palette[1]  # Color for the 'Value' group\n",
    "        legend_patches = [mpatches.Patch(color=key_color, label=key_label),\n",
    "                        mpatches.Patch(color=value_color, label=value_label)]\n",
    "        \n",
    "        # Add annotations\n",
    "        annotator = Annotator(ax=ax,\n",
    "                              data=data,\n",
    "                              x='Metric',\n",
    "                              y='Value',\n",
    "                              hue='GroupType',\n",
    "                              pairs=pairs,\n",
    "                              test=test_type,\n",
    "                              text_format='full',\n",
    "                              loc='inside',\n",
    "                              verbose=2)\n",
    "        x_labels = ['Question ' + pair[0][0].split('_')[2] for pair in pairs]\n",
    "        result = [' '] * (len(x_labels) * 2)\n",
    "        result[0::2] = x_labels\n",
    "        x_labels = result\n",
    "        x_ticks = [i for i in range(0, len(pairs)*2)]\n",
    "        plt.xticks(ticks=x_ticks, labels=x_labels)\n",
    "    elif display=='split_pairs':\n",
    "        # Plot with seaborn (side-by-side)\n",
    "        ax = sns.boxplot(x='GroupType', y='Value', hue='Metric', data=data, ax=ax, palette={row['Metric']: row['Color'] for _, row in data.iterrows()})\n",
    "        # Adjust pairs for side-by-side\n",
    "        pairs = [(('Key', col1), ('Value', col2)) for col1, col2 in comparisons_dict.items()]\n",
    "        # Add annotations\n",
    "        annotator = Annotator(ax=ax,\n",
    "                              data=data,\n",
    "                              x='GroupType',\n",
    "                              y='Value',\n",
    "                              hue='Metric',\n",
    "                              pairs=pairs,\n",
    "                              test=test_type,\n",
    "                              text_format='full',\n",
    "                              loc='outside',\n",
    "                              verbose=1)\n",
    "        x_labels = [key_label] + [value_label]\n",
    "        plt.xticks(ticks=range(len(x_labels)), labels=x_labels)\n",
    "    else:\n",
    "        raise ValueError(f'display {display} not supported please choose \"split_pairs\" or \"group_pairs\"')\n",
    "\n",
    "    # Configure and annotate\n",
    "    _, corrected_results = annotator.configure(test=test_type, comparisons_correction=correction_method).apply_and_annotate()\n",
    "\n",
    "    # Label and show plot\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    plt.title(f'Distribution Test: {test_type} with {correction_method} correction')\n",
    "    plt.legend(handles=legend_patches, frameon=False, loc=(0.025, 0.05))\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_dict = {'ACE3_Question_2': 'Question_2', 'ACE3_Question_3': 'Question_3', 'ACE3_Question_4': 'Question_4', 'ACE3_Question_5': 'Question_5', 'ACE3_Question_6': 'Question_6', 'ACE3_Question_7': 'Question_7', 'ACE3_Question_8': 'Question_8', 'ACE3_Question_9': 'Question_9', 'ACE3_Question_10': 'Question_10', 'ACE3_Question_11': 'Question_11', 'ACE3_Question_12': 'Question_12', 'ACE3_Question_13': 'Question_13', 'ACE3_Question_14': 'Question_14', 'ACE3_Question_15': 'Question_15', 'ACE3_Question_16': 'Question_16', 'ACE3_Question_17': 'Question_17', 'ACE3_Question_18': 'Question_18', 'ACE3_Question_19': 'Question_19', 'ACE3_Question_20': 'Question_20'}\n",
    "test_type = 'Wilcoxon'\n",
    "correction_method = 'bonferroni'\n",
    "key_label = 'Paper'\n",
    "value_label = 'Autonomous'\n",
    "x_label = 'Question'\n",
    "y_label = 'Score'\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "plt = plot_distribution_tests(dataframe=data_df,\n",
    "                        comparisons_dict=comparisons_dict, \n",
    "                        test_type=test_type, \n",
    "                        correction_method=correction_method,\n",
    "                        key_label=key_label, \n",
    "                        value_label=value_label, \n",
    "                        x_label=x_label, \n",
    "                        y_label=y_label,\n",
    "                        display='group_pairs')\n",
    "plt\n",
    "\n",
    "os.makedirs(os.path.join(out_dir, 'distribution_figures'), exist_ok=True)\n",
    "plt.savefig(os.path.join(out_dir, f'distribution_figures/{test_type}_{correction_method}_multiple_comparison_barplot.png'))\n",
    "plt.savefig(os.path.join(out_dir, f'distribution_figures/{test_type}_{correction_method}_multiple_comparison_barplot_barplot.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, f'icc_figures/{test_type}_{correction_method}_multiple_comparison_barplot')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman/Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[data_df['Age'] > 65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from natsort import index_natsorted\n",
    "\n",
    "# Correcting the function to remove natural sorting and ensure DataFrame is passed\n",
    "\n",
    "def generate_scatterplot(dataframe, data_dict, columns_per_row=4, x_label='xlabel', y_label='ylabel', correlation='pearson', palette='Greys'):\n",
    "    '''\n",
    "    Generate scatterplots with specified correlation coefficient annotated.\n",
    "\n",
    "    :param dataframe: DataFrame containing the data\n",
    "    :param data_dict: Dictionary where key is the dependent variable name, and value is a list of independent variable names\n",
    "    :param columns_per_row: Number of columns per row in the facet plot\n",
    "    :param x_label: Label for the x-axis\n",
    "    :param y_label: Label for the y-axis\n",
    "    :param correlation: method of correlation ('pearson', 'spearman', 'kendall')\n",
    "    :return: Facet plot\n",
    "    '''\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette(palette, 1, desat=1)\n",
    "    \n",
    "    for dependent_var, independent_vars in data_dict.items():\n",
    "        # Melt the DataFrame for each dependent variable\n",
    "        melted_df = pd.melt(dataframe, id_vars=[dependent_var], value_vars=independent_vars,\n",
    "                            var_name='independent_variable_name', value_name='independent_variable_units')\n",
    "\n",
    "        # Sort the DataFrame\n",
    "        melted_df = melted_df.sort_values(by='independent_variable_name')\n",
    "\n",
    "        # Begin plotting\n",
    "        facet_plot = sns.lmplot(y=dependent_var, x='independent_variable_units', data=melted_df,\n",
    "                                col='independent_variable_name', truncate=False, col_wrap=columns_per_row,\n",
    "                                facet_kws=dict(sharex=False, sharey=True))\n",
    "\n",
    "        # Calculate correlation for each cluster\n",
    "        grouped_df = melted_df.groupby('independent_variable_name')\n",
    "        \n",
    "        if correlation == 'pearson':\n",
    "            result = grouped_df.apply(lambda x: pearsonr(x['independent_variable_units'], x[dependent_var]))\n",
    "        elif correlation == 'spearman':\n",
    "            result = grouped_df.apply(lambda x: spearmanr(x['independent_variable_units'], x[dependent_var]))\n",
    "        elif correlation == 'kendall':\n",
    "            result = grouped_df.apply(lambda x: kendalltau(x['independent_variable_units'], x[dependent_var]))\n",
    "        else:\n",
    "            raise ValueError(f'Correlation {correlation} not specified, please select \"pearson\", \"kendall\" or \"spearman\"')\n",
    "\n",
    "        # Annotate with correlation\n",
    "        for ax in facet_plot.axes:\n",
    "            # Get the title of the current subplot\n",
    "            region = ax.get_title().split(\"=\")[-1].strip()\n",
    "            # Get the corresponding r, p values from the result\n",
    "            r, p = result[region]\n",
    "            ax.set_title(region)\n",
    "            ax.annotate(f\"r = {r:.2f}, p = {p:.5f}\", xy=(.5, 1.0), xycoords='axes fraction',\n",
    "                        xytext=(0, 0), textcoords='offset points', ha='center', va='top',\n",
    "                        bbox=dict(boxstyle='round,pad=0.0', alpha=0.0),\n",
    "                        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "            \n",
    "        facet_plot.set_xlabels(x_label)\n",
    "        facet_plot.set_ylabels(y_label)\n",
    "        return facet_plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_correlate_to = 'Z-Scored Percent Cognitive Improvement'\n",
    "columns_to_correlate_list = [\n",
    " 'Age']\n",
    "x_label = 'Connectivity'\n",
    "y_label = 'Standardized Improvement'\n",
    "correlation = 'pearson'\n",
    "\n",
    "#---------\n",
    "plot = generate_scatterplot(dataframe=data_df, \n",
    "                     data_dict={columns_to_correlate_to:columns_to_correlate_list}, \n",
    "                     x_label=x_label, \n",
    "                     y_label=y_label,\n",
    "                     correlation=correlation,\n",
    "                     palette='Greys')\n",
    "\n",
    "# os.makedirs(os.path.join(out_dir, 'scatterplot_figures'), exist_ok=True)\n",
    "# plot.savefig(os.path.join(out_dir, f'scatterplot_figures/{x_label}_to_{y_label}.png'))\n",
    "# plot.savefig(os.path.join(out_dir, f'scatterplot_figures/{x_label}_to_{y_label}.svg'))\n",
    "# print('Figure saved to: ', (os.path.join(out_dir, f'scatterplot_figures/{x_label}_to_{y_label}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Split Between Correlation of Variables**\n",
    "\n",
    "The ScatterWithConfidence class is designed to generate scatter plots with associated confidence intervals and permutation-based p-values. It creates plots of two variables and segregates them by a specified criterion, such as age. These plots are further enhanced by overlaying regression lines with their associated confidence intervals. The class can also perform permutation tests to compute a p-value, which is then prominently displayed in the plot title.\n",
    "\n",
    "Attributes:\n",
    "data_df (pandas.DataFrame): The main dataframe containing the data intended for plotting.\n",
    "Key Methods:\n",
    "1. compute_analytic_confidence_interval(x, y, x_vals)\n",
    "Purpose: Computes the regression line and its 95% confidence intervals using analytic methods.\n",
    "Parameters:\n",
    "x (pandas.Series): The x-values of the data.\n",
    "y (pandas.Series): The y-values of the data.\n",
    "x_vals (numpy.ndarray): The x-values where the regression line and confidence intervals should be computed.\n",
    "Returns:\n",
    "y_fit: Regression line values.\n",
    "lower_bound: Lower 95% confidence interval.\n",
    "upper_bound: Upper 95% confidence interval.\n",
    "2. permute_data_and_difference_in_pearson_r(...)\n",
    "Purpose: Computes the difference in Pearson R between two groups using permutation tests.\n",
    "Parameters:\n",
    "x_one, x_two (str): Column names of the two variables to be correlated.\n",
    "split_by (str): Column name used to segregate the data.\n",
    "split_value (float): The criterion value for data segregation.\n",
    "n_permutations (int, optional): Number of permutations to perform (default is 1000).\n",
    "permute_columns (list of str, optional): List of column names to be permuted.\n",
    "Returns:\n",
    "original_diff: Difference in Pearson R of the original data between two groups.\n",
    "p_value: Permutation-based p-value.\n",
    "3. plot_with_analytic_ci_manual_pvalue(...)\n",
    "Purpose: Generates a scatter plot with regression lines, confidence intervals, and the permutation-based p-value in the title.\n",
    "Parameters:\n",
    "x_one, x_two (str): Column names of the two variables for plotting.\n",
    "split_by (str): Column name for data segregation.\n",
    "... (Other parameters for customization, saving, etc.)\n",
    "Returns:\n",
    "A matplotlib scatter plot.\n",
    "To utilize this class:\n",
    "\n",
    "Initialize the class with your dataset.\n",
    "Use the methods described above to compute confidence intervals or perform permutation tests.\n",
    "Generate and visualize your enhanced scatter plot using the provided methods.\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "scatter_plotter = ScatterWithConfidence(data_df)\n",
    "scatter_plotter.plot_with_analytic_ci_manual_pvalue(...)\n",
    "Remember to replace ... with appropriate arguments as your situation demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the class and its methods\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "import seaborn as sns\n",
    "class ScatterWithConfidence:\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_analytic_confidence_interval(x, y, x_vals):\n",
    "        slope, intercept = np.polyfit(x, y, 1)\n",
    "        y_fit = slope * x_vals + intercept\n",
    "        \n",
    "        residuals = y - (slope * x + intercept)\n",
    "        stderr = np.sqrt(np.sum(residuals**2) / (len(y) - 2))\n",
    "        \n",
    "        t_value = t.ppf(0.975, df=len(x)-2)\n",
    "        ci = t_value * stderr * np.sqrt(1/len(x) + (x_vals - np.mean(x))**2 / np.sum((x - np.mean(x))**2))\n",
    "        \n",
    "        upper_bound = y_fit + ci\n",
    "        lower_bound = y_fit - ci\n",
    "        \n",
    "        return y_fit, lower_bound, upper_bound\n",
    "\n",
    "    def permute_data_and_difference_in_pearson_r(self, x_one, x_two, split_by, split_value, n_permutations=1000, permute_columns=[]):\n",
    "        original_diff = self.data_df[self.data_df[split_by] < split_value][x_one].corr(self.data_df[self.data_df[split_by] < split_value][x_two]) - \\\n",
    "                       self.data_df[self.data_df[split_by] >= split_value][x_one].corr(self.data_df[self.data_df[split_by] >= split_value][x_two])\n",
    "\n",
    "        permuted_diffs = []\n",
    "\n",
    "        for _ in range(n_permutations):\n",
    "            permuted_df = self.data_df.copy()\n",
    "            for column in permute_columns:\n",
    "                permuted_df[column] = np.random.permutation(permuted_df[column].values)\n",
    "            \n",
    "            diff = permuted_df[permuted_df[split_by] < split_value][x_one].corr(permuted_df[permuted_df[split_by] < split_value][x_two]) - \\\n",
    "                   permuted_df[permuted_df[split_by] >= split_value][x_one].corr(permuted_df[permuted_df[split_by] >= split_value][x_two])\n",
    "            \n",
    "            permuted_diffs.append(diff)\n",
    "\n",
    "        p_value = np.mean([diff <= original_diff for diff in permuted_diffs])\n",
    "        return original_diff, p_value\n",
    "\n",
    "    def plot_with_analytic_ci_manual_pvalue(self, x_one, x_two, \n",
    "                                            split_by, split_value, \n",
    "                                            x_label='X1', y_label='X2', \n",
    "                                            upper_split_legend='Above Split', lower_split_legend='Below Split',\n",
    "                                            alpha=0.3, manual_p_value=None, permute_column=None, \n",
    "                                            save=False, out_dir=None):\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        \n",
    "        group1 = self.data_df[self.data_df[split_by] < split_value]\n",
    "        group2 = self.data_df[self.data_df[split_by] >= split_value]\n",
    "        \n",
    "        ax.scatter(group1[x_one], group1[x_two], color='red', label=lower_split_legend, s=40, alpha=alpha, marker='o')\n",
    "        ax.scatter(group2[x_one], group2[x_two], color='blue', label=upper_split_legend, s=40, alpha=alpha, marker='o')\n",
    "        \n",
    "        x_vals = np.linspace(self.data_df[x_one].min(), self.data_df[x_one].max(), 400)\n",
    "        \n",
    "        for group, color in [(group1, 'red'), (group2, 'blue')]:\n",
    "            y_fit, lower_bound, upper_bound = self.compute_analytic_confidence_interval(group[x_one], group[x_two], x_vals)\n",
    "            ax.plot(x_vals, y_fit, color=color)\n",
    "            ax.fill_between(x_vals, lower_bound, upper_bound, color=color, alpha=alpha/7)\n",
    "        \n",
    "        if manual_p_value is None:\n",
    "            if permute_column:\n",
    "                rho, manual_p_value = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, n_permutations=10000, permute_columns=[permute_column])\n",
    "            else:\n",
    "                rho, manual_p_value = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, n_permutations=10000, permute_columns=[x_one, x_two, split_by])\n",
    "        \n",
    "        ax.set_title(f\"\\u0394 r = {rho:.2f} , p = {manual_p_value:.4f}\")\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.legend(loc='best', frameon=False)\n",
    "        ax.grid(False)\n",
    "        sns.despine(ax=ax)\n",
    "        \n",
    "        if save and out_dir is not None:\n",
    "            plt.savefig(f\"{out_dir}/scatter_with_polyfit_and_analytic_ci.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{out_dir}/scatter_with_polyfit_and_analytic_ci.svg\", bbox_inches='tight')\n",
    "            print(f'Saved to {out_dir}/scatter_with_polyfit_and_analytic_ci.svg')\n",
    "        return fig\n",
    "    \n",
    "class DeltaCorrelation(ScatterWithConfidence):\n",
    "    def __init__(self, data_df):\n",
    "        super().__init__(data_df)\n",
    "\n",
    "    def plot_histogram_of_delta_r(self, x_one, x_two, split_by, split_value, n_permutations=1000, \n",
    "                                permute_columns=[], bins=50, one_tail=False, color_palette='dark'):\n",
    "        # Generate the empirical distribution of delta_r\n",
    "        delta_rs = []\n",
    "        for _ in range(n_permutations):\n",
    "            permuted_df = self.data_df.copy()\n",
    "            for column in permute_columns:\n",
    "                permuted_df[column] = np.random.permutation(permuted_df[column].values)\n",
    "\n",
    "            delta_r = permuted_df[permuted_df[split_by] < split_value][x_one].corr(permuted_df[permuted_df[split_by] < split_value][x_two]) - \\\n",
    "                    permuted_df[permuted_df[split_by] >= split_value][x_one].corr(permuted_df[permuted_df[split_by] >= split_value][x_two])\n",
    "            delta_rs.append(delta_r)\n",
    "\n",
    "        # Calculate the observed delta_r\n",
    "        observed_delta_r, _ = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, permute_columns=permute_columns)\n",
    "\n",
    "        if one_tail:\n",
    "            observed_delta_r = np.abs(observed_delta_r)\n",
    "            delta_rs = np.abs(delta_rs)\n",
    "\n",
    "        # Calculate p-value\n",
    "        if one_tail:\n",
    "            p_value = np.mean([delta_r >= observed_delta_r for delta_r in delta_rs])\n",
    "        else:\n",
    "            p_value = np.mean([delta_r <= observed_delta_r for delta_r in delta_rs])\n",
    "\n",
    "        # Generate the displot (KDE + Histogram) using Seaborn\n",
    "        sns.set_palette(color_palette)\n",
    "        current_palette = sns.color_palette(color_palette)\n",
    "        chosen_color = current_palette[4]\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        g = sns.displot(delta_rs, kde=True, bins=bins, label=\"Empirical $\\\\Delta r$ Distribution\", element=\"step\",color='blue', alpha=.3)\n",
    "        plt.axvline(x=observed_delta_r, color='red', linestyle='-', linewidth=1.5, label=f\"Observed $\\\\Delta r$\", alpha=0.6)\n",
    "        plt.title(f\"$\\\\Delta r$ = {observed_delta_r}, p = {p_value}\")\n",
    "        plt.xlabel(\"$\\\\Delta r$\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "        \n",
    "        fig = g.fig\n",
    "        \n",
    "        fig.savefig(f\"{out_dir}/hist_kde.png\", bbox_inches='tight')\n",
    "        fig.savefig(f\"{out_dir}/hist_kde.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/hist_kde.svg')\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Variables\n",
    "variable_to_correlate_2 = 'Z-Scored Percent Cognitive Improvement'\n",
    "variable_to_correlate_1 = 'Age'\n",
    "split_by_var = 'Cohort' # This is the column which contains the values you are going to split the data by \n",
    "split_value_var = 1.5 # This is the value which will be used to determine how rows of the given column split the entire dataset\n",
    "\n",
    "# Plotting Variables\n",
    "x_label='Age'\n",
    "y_label='Standardized Percent Improvement'\n",
    "legend_string_for_lower_split='Alzheimer Disease'\n",
    "legend_string_for_upper_split='Parkinson Disease'\n",
    "out_dir='/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/final/figure_two'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_sctr = ScatterWithConfidence(data_df)\n",
    "delta_sctr_plot = delta_sctr.plot_with_analytic_ci_manual_pvalue(\n",
    "                                            x_one=variable_to_correlate_1, x_two=variable_to_correlate_2, \n",
    "                                            split_by=split_by_var, split_value=split_value_var, \n",
    "                                            x_label=x_label, y_label=y_label, \n",
    "                                            upper_split_legend=legend_string_for_upper_split, lower_split_legend=legend_string_for_lower_split,\n",
    "                                            alpha=0.3, manual_p_value=None, permute_column=None, \n",
    "                                            save=True, out_dir=out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of The Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutations_var = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object from the new Seaborn-based class\n",
    "histogram_plotter_seaborn = DeltaCorrelation(data_df)\n",
    "\n",
    "# Generate the histogram using Seaborn\n",
    "fig = histogram_plotter_seaborn.plot_histogram_of_delta_r(x_one=variable_to_correlate_1, x_two=variable_to_correlate_2, \n",
    "                                                    split_by=split_by_var, split_value=split_value_var, \n",
    "                                                    n_permutations=n_permutations_var, \n",
    "                                                    permute_columns=[variable_to_correlate_1, variable_to_correlate_2, split_by_var], \n",
    "                                                    bins=50,\n",
    "                                                    one_tail=True, \n",
    "                                                    color_palette='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intraclass Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "def calculate_icc(series1, series2):\n",
    "    # Combine the series into a DataFrame\n",
    "    df = series1.to_frame(name='rating1')\n",
    "    df['rating2'] = series2\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    df_melted = df.melt(value_vars=['rating1', 'rating2'], var_name='rater', value_name='rating')\n",
    "\n",
    "    # Add a subject column\n",
    "    df_melted['subject'] = df_melted.groupby('rater').cumcount()\n",
    "\n",
    "    # Calculate ICC\n",
    "    icc_result = pg.intraclass_corr(data=df_melted, targets='subject', raters='rater', ratings='rating')\n",
    "    \n",
    "    # You can choose the ICC type you want, here I'm returning ICC(3,1)\n",
    "    return icc_result.set_index('Type').loc['ICC3', 'ICC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icc = calculate_icc(data_df['Total'], data_df['ACE_Total'])\n",
    "icc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapped ICC 95% Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pingouin import intraclass_corr\n",
    "from calvin_utils.statistical_utils.distribution_statistics import BootstrappedDistributionStatistics\n",
    "from calvin_utils.statistical_utils.resampling_functions import Bootstrap\n",
    "\n",
    "def calculate_icc(df, col1, col2):\n",
    "    # Select only the specified columns and rename them for compatibility with pingouin\n",
    "    data = df[[col1, col2]].rename(columns={col1: 'rating1', col2: 'rating2'})\n",
    "\n",
    "    # Reshape data for pingouin's intraclass_corr function\n",
    "    df_melted = data.melt(value_vars=['rating1', 'rating2'], var_name='rater', value_name='rating')\n",
    "    df_melted['subject'] = df_melted.groupby('rater').cumcount()\n",
    "\n",
    "    # Calculate ICC\n",
    "    icc_result = intraclass_corr(data=df_melted, targets='subject', raters='rater', ratings='rating')\n",
    "    \n",
    "    # Return ICC(3,1)\n",
    "    return icc_result.set_index('Type').loc['ICC3', 'ICC']\n",
    "\n",
    "def bootstrap_icc(data, col1, col2, bootstrap_samples=2500):\n",
    "    # Perform the bootstrap resampling using the Bootstrap class\n",
    "    bootstrap = Bootstrap(data=data, func=calculate_icc, func_args={'col1': col1, 'col2': col2}, bootstrap_samples=bootstrap_samples)\n",
    "    bootstrap_results = bootstrap.bootstrap_function()\n",
    "\n",
    "    # Calculate the confidence intervals using the BootstrappedDistributionStatistics class\n",
    "    distribution_statistics = BootstrappedDistributionStatistics(bootstrap_results)\n",
    "    lower_bound, upper_bound = distribution_statistics.percentile_ci(alpha=0.05)\n",
    "\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound, upper_bound = bootstrap_icc(data=data_df, col1='Total', col2='ACE_Total', bootstrap_samples=10000)\n",
    "print(lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the class\n",
    "scatter_plotter = ScatterWithConfidence(data_df)\n",
    "\n",
    "# Generate and display the plot\n",
    "fig = scatter_plotter.plot_with_analytic_ci_manual_pvalue(x_one=\"Subiculum Connectivity\",\n",
    "                                                         x_two=\"Z-Scored Cognitive Improvement By Group\",\n",
    "                                                         split_by=\"Cohort\",\n",
    "                                                         split_value=0.5,\n",
    "                                                         x_label=\"Subiculum Connectivity\",\n",
    "                                                         y_label=\"Z-Scored Percent Cognitive Improvement\",\n",
    "                                                         upper_split_legend='AD', lower_split_legend='PD',\n",
    "                                                         manual_p_value=None,\n",
    "                                                         alpha=0.7,\n",
    "                                                         save=True, \n",
    "                                                         out_dir=out_dir)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_icc_with_ci(icc, ci_lower, ci_upper):\n",
    "    plt.figure(figsize=(6.5, 2))\n",
    "    plt.errorbar(x=icc, y=0, xerr=[[icc - ci_lower], [ci_upper - icc]], fmt='o', color='#1f77b4', capsize=5)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.yticks([])\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xlabel('ICC')\n",
    "    plt.title('Intraclass Correlation Coefficient (ICC) with 95% Confidence Interval')\n",
    "    plt.grid(axis='x', linestyle='--')\n",
    "    return plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plot_icc_with_ci(icc=icc, ci_lower=lower_bound, ci_upper=upper_bound)\n",
    "os.makedirs(os.path.join(out_dir, 'icc_figures'), exist_ok=True)\n",
    "figure.savefig(os.path.join(out_dir, 'icc_figures/icc_barplot.png'))\n",
    "figure.savefig(os.path.join(out_dir, 'icc_figures/icc_barplot.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, 'icc_figures/icc_barplot')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a Bunch of ICCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_icc_forest(comparisons_dict, dataframe, bootstrap_samples=2500, full_legend_patches=False):\n",
    "    # figure = plt.figure(figsize=(4, len(comparisons_dict)*1.2))\n",
    "    figure = plt.figure(figsize=(4, 5))\n",
    "    \n",
    "    # Create a color palette with enough unique colors\n",
    "    colors = sns.color_palette(\"tab10\", len(comparisons_dict))\n",
    "    \n",
    "    # Create legend patches\n",
    "    legend_patches = []\n",
    "    \n",
    "    # Iterate through the dictionary and plot ICC for each comparison\n",
    "    for idx, (col1_name, col2_name) in enumerate(comparisons_dict.items()):\n",
    "        # Calculate ICC\n",
    "        icc_value = calculate_icc(dataframe, col1_name, col2_name)\n",
    "\n",
    "        # Bootstrap 95% confidence interval\n",
    "        ci_lower, ci_upper = bootstrap_icc(dataframe, col1_name, col2_name, bootstrap_samples=bootstrap_samples)\n",
    "\n",
    "        # Plot ICC with confidence interval\n",
    "        plt.errorbar(x=icc_value, y=idx, xerr=[[icc_value - ci_lower], [ci_upper - icc_value]], fmt='o', color=colors[idx], capsize=5)\n",
    "        \n",
    "        # Add legend patch\n",
    "        if full_legend_patches:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{col1_name} vs {col2_name}'))\n",
    "        else:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{col1_name}'))\n",
    "\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(-1, len(comparisons_dict))\n",
    "    plt.yticks([])\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xlabel('ICC')\n",
    "    plt.title('Intraclass Correlation Coefficients (ICC) with 95% Confidence Intervals')\n",
    "    plt.grid(axis='x', linestyle='--')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(handles=legend_patches, frameon=False, loc=(0.05, 0.1))\n",
    "    return figure\n",
    "\n",
    "# Example usage\n",
    "# comparisons_dict = {\n",
    "#     'Total': 'Total.1',\n",
    "#     'Score1': 'Score2'\n",
    "# }\n",
    "\n",
    "# plot_icc_forest(comparisons_dict, dataframe=data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_dict = {'Question_2': 'ACE3_Question_2',\n",
    "    'Question_3': 'ACE3_Question_3',\n",
    "    'Question_4': 'ACE3_Question_4',\n",
    "    'Question_5': 'ACE3_Question_5',\n",
    "    'Question_6': 'ACE3_Question_6',\n",
    "    'Question_7': 'ACE3_Question_7',\n",
    "    'Question_8': 'ACE3_Question_8',\n",
    "    'Question_9': 'ACE3_Question_9',\n",
    "    'Question_10': 'ACE3_Question_10',\n",
    "    'Question_11': 'ACE3_Question_11',\n",
    "    'Question_12': 'ACE3_Question_12',\n",
    "    'Question_13': 'ACE3_Question_13',\n",
    "    'Question_14': 'ACE3_Question_14',\n",
    "    'Question_15': 'ACE3_Question_15',\n",
    "    'Question_16': 'ACE3_Question_16',\n",
    "    'Question_17': 'ACE3_Question_17',\n",
    "    'Question_18': 'ACE3_Question_18',\n",
    "    'Question_19': 'ACE3_Question_19',\n",
    "    'Question_20': 'ACE3_Question_20'\n",
    "}  \n",
    "bootstrap_samples = 5000\n",
    "#-----------------------------------------------------------------------------\n",
    "multi_icc_fig = plot_icc_forest(comparisons_dict, dataframe=data_df, bootstrap_samples=bootstrap_samples)\n",
    "\n",
    "os.makedirs(os.path.join(out_dir, 'icc_figures'), exist_ok=True)\n",
    "multi_icc_fig.savefig(os.path.join(out_dir, 'icc_figures/icc_multipl_comparison_barplot.png'))\n",
    "multi_icc_fig.savefig(os.path.join(out_dir, 'icc_figures/icc_multipl_comparison_barplot_barplot.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, 'icc_figures/icc_multipl_comparison_barplot')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_icc_fig.savefig(os.path.join(out_dir, 'icc_figures/all_qeustions_icc_multipl_comparison_barplot.png'))\n",
    "multi_icc_fig.savefig(os.path.join(out_dir, 'icc_figures/all_qeustions_icc_multipl_comparison_barplot_barplot.svg'))\n",
    "print('Figure saved to: ', (os.path.join(out_dir, 'icc_figures/all_qeustions_icc_multipl_comparison_barplot')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_var_column = 'Total'\n",
    "dependent_var_column = 'Cognitive Status'\n",
    "positive_outcome = 1\n",
    "negative_outcome = 0\n",
    "\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "data_df = data_df.loc[:, [independent_var_column, dependent_var_column]].copy()\n",
    "data_df['one_hot_status'] = np.where(data_df[dependent_var_column] == positive_outcome, 0, 1)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "# assuming 'one_hot_status' is the target with MCI encoded as 1 and intact as 0, and 'Question_16' is the score\n",
    "y_true = data_df['one_hot_status']\n",
    "scores = data_df[independent_var_column]\n",
    "\n",
    "# calculate the false positive rate and true positive rate for all thresholds of the classification\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "# calculate the AUC (Area Under Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Create a DataFrame to store FPR, TPR, and corresponding thresholds\n",
    "df_roc = pd.DataFrame({\n",
    "    \"FPR\": fpr,\n",
    "    \"TPR\": tpr,\n",
    "    \"Threshold\": thresholds\n",
    "})\n",
    "\n",
    "# Plot\n",
    "sns.set_style(\"white\")\n",
    "roc = plt.figure(figsize=(7, 6))\n",
    "sns.lineplot(x=\"FPR\", y=\"TPR\", data=df_roc, marker='o', color=\"#1f77b4\")\n",
    "\n",
    "\n",
    "plt.title(f'AUC = {roc_auc:.2f}', fontsize=16)\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "if not os.path.exists(out_dir + '/roc'):\n",
    "    os.makedirs(out_dir + '/roc')\n",
    "    \n",
    "roc.savefig(out_dir + '/roc/roc.png')\n",
    "roc.savefig(out_dir + '/roc/roc.svg')\n",
    "print('saved to: ', out_dir + '/roc/roc.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap Confidence Intervals for the AUC Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.distribution_statistics import bootstrap_distribution_statistics\n",
    "from calvin_utils.statistical_utils.classification_statistics import compute_auc\n",
    "# Use the bootstrap function\n",
    "roc_func_args = {'y_true_variable': 'one_hot_status', 'independent_variable': f'{independent_var_column}'}\n",
    "result_df = bootstrap_distribution_statistics(data_df, compute_auc, roc_func_args, bootstrap_samples=10000)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapped ROC Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot CIs with Observed Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from numpy import interp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def plot_bootstrapped_roc(data, y_true_variable, independent_variable, n_bootstraps=2000):\n",
    "    \"\"\"\n",
    "    Plot ROC curve with bootstrapped confidence intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with the data.\n",
    "    - y_true_variable: string with the name of the column containing the true binary labels.\n",
    "    - independent_variable: string with the name of the column containing the independent variable (classifier scores).\n",
    "    - n_bootstraps: number of bootstrap samples to use for generating confidence intervals.\n",
    "    \"\"\"\n",
    "    # Real values\n",
    "    y_true = data[y_true_variable]\n",
    "    scores = data[independent_variable]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "    # Calculate the AUC (Area Under Curve)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Create a DataFrame to store FPR, TPR, and corresponding thresholds\n",
    "    df_roc = pd.DataFrame({\n",
    "        \"FPR\": fpr,\n",
    "        \"TPR\": tpr,\n",
    "        \"Threshold\": thresholds\n",
    "    })\n",
    "\n",
    "    # Empirical values\n",
    "    roc_auc_bootstraps = []\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    # Generate bootstrap samples and calculate ROC curve for each\n",
    "    for i in tqdm(range(n_bootstraps)):\n",
    "        sample = data.sample(frac=1, replace=True)\n",
    "        y_true_sample = sample[y_true_variable]\n",
    "        scores_sample = sample[independent_variable]\n",
    "        fpr, tpr, thresholds = roc_curve(y_true_sample, scores_sample)\n",
    "        tpr = interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        roc_auc_bootstraps.append(tpr)\n",
    "\n",
    "    roc_auc_bootstraps = np.array(roc_auc_bootstraps)\n",
    "\n",
    "    # Calculate mean and confidence intervals\n",
    "    roc_auc_mean = roc_auc_bootstraps.mean(axis=0)\n",
    "    ci_lower = np.percentile(roc_auc_bootstraps, 2.5, axis=0)\n",
    "    ci_upper = np.percentile(roc_auc_bootstraps, 97.5, axis=0)\n",
    "\n",
    "    # Compute AUC using mean ROC curve\n",
    "    roc_auc = auc(base_fpr, roc_auc_mean)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    sns.set_style(\"white\")\n",
    "    sns.lineplot(x=\"FPR\", y=\"TPR\", data=df_roc, marker='o', color=\"darkblue\", ci=None)\n",
    "    ax.fill_between(base_fpr, ci_lower, ci_upper, color='blue', alpha=0.15)\n",
    "    ax.plot(base_fpr, ci_lower, 'b-', alpha=0.05)\n",
    "    ax.plot(base_fpr, ci_upper, 'b-', alpha=0.05)\n",
    "\n",
    "    ax.set_title('AUC = %0.2f' % roc_auc, fontsize=16)\n",
    "    ax.plot([0, 1], [0, 1],'--', color='#ADD8E6')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=13)\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=13)\n",
    "    \n",
    "    sns.despine()\n",
    "    if not os.path.exists(out_dir + '/roc'):\n",
    "        os.makedirs(out_dir + '/roc')\n",
    "    fig.savefig(out_dir + f'/roc/bootstrapped_roc_observed_plotted.png')\n",
    "    fig.savefig(out_dir + f'/roc/bootstrapped_roc_observed_plotted.svg')\n",
    "    print('Saved to: ', out_dir + f'/roc/bootstrapped_roc_observed_plotted.svg')\n",
    "    \n",
    "plot_bootstrapped_roc(data_df, 'one_hot_status', f'{independent_var_column}', 10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapped Presenting Mean Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def plot_bootstrapped_roc(data, y_true_variable, independent_variable, n_bootstraps=2000):\n",
    "    \"\"\"\n",
    "    Plot ROC curve with bootstrapped confidence intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with the data.\n",
    "    - y_true_variable: string with the name of the column containing the true binary labels.\n",
    "    - independent_variable: string with the name of the column containing the independent variable (classifier scores).\n",
    "    - n_bootstraps: number of bootstrap samples to use for generating confidence intervals.\n",
    "    \"\"\"\n",
    "    roc_auc_bootstraps = []\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    # Generate bootstrap samples and calculate ROC curve for each\n",
    "    for i in tqdm(range(n_bootstraps)):\n",
    "        sample = data.sample(frac=1, replace=True)\n",
    "        y_true = sample[y_true_variable]\n",
    "        scores = sample[independent_variable]\n",
    "\n",
    "        # Exclude rows with NaN values\n",
    "        mask = ~(y_true.isna() | scores.isna())\n",
    "        y_true = y_true[mask]\n",
    "        scores = scores[mask]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "        # Interpolate the ROC curve to ensure it starts from (0,0)\n",
    "        tpr = np.interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "\n",
    "        roc_auc_bootstraps.append(tpr)\n",
    "\n",
    "    roc_auc_bootstraps = np.array(roc_auc_bootstraps)\n",
    "\n",
    "    # Calculate mean and confidence intervals, ignoring NaN values\n",
    "    roc_auc_mean = np.nanmean(roc_auc_bootstraps, axis=0)\n",
    "    ci_lower = np.percentile(roc_auc_bootstraps, 2.5, axis=0)\n",
    "    ci_upper = np.percentile(roc_auc_bootstraps, 97.5, axis=0)\n",
    "\n",
    "    # Compute AUC using mean ROC curve\n",
    "    roc_auc = auc(base_fpr, roc_auc_mean)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(base_fpr, roc_auc_mean, '#1f77b4')\n",
    "    ax.fill_between(base_fpr, ci_lower, ci_upper, color='#1f77b4', alpha=0.25)\n",
    "    ax.plot(base_fpr, ci_lower, 'b-', alpha=0.05)\n",
    "    ax.plot(base_fpr, ci_upper, 'b-', alpha=0.05)\n",
    "\n",
    "    ax.set_title('AUC = %0.2f' % roc_auc, fontsize=16)\n",
    "    ax.plot([0, 1], [0, 1],'--', color='#ff7f0e')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=13)\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=13)\n",
    "\n",
    "    sns.despine()\n",
    "    if not os.path.exists(out_dir + '/roc'):\n",
    "        os.makedirs(out_dir + '/roc')\n",
    "    plt.savefig(out_dir + f'/roc/bootstrapped_roc_mean_plotted.png')\n",
    "    plt.savefig(out_dir + f'/roc/bootstrapped_roc_mean_plotted.svg')\n",
    "    print('Saved to: ', out_dir + f'/roc/bootstrapped_roc_mean_plotted.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bootstrapped_roc(data_df, 'one_hot_status', f'{independent_var_column}', 10000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Accuracy Curves Across Thresolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_outcome = 'mci'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "data_df = data_df.loc[:, [independent_var_column, dependent_var_column]]\n",
    "data_df['one_hot_status'] = np.where(data_df[dependent_var_column] == positive_outcome, 0, 1)\n",
    "\n",
    "scores = data_df[independent_var_column]\n",
    "y_true = data_df['one_hot_status']\n",
    "\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "from calvin_utils.statistical_utils.distribution_statistics import bootstrap_distribution_statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "def compute_accuracy(sample, threshold, y_true_variable, independent_variable):\n",
    "    \"\"\"\n",
    "    Computes the accuracy for a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - sample: DataFrame with the data.\n",
    "    - threshold: float with the threshold to use for classifying the scores.\n",
    "    - y_true_variable: string with the name of the column containing the true binary labels.\n",
    "    - independent_variable: string with the name of the column containing the independent variable (classifier scores).\n",
    "\n",
    "    Returns:\n",
    "    - Scalar with the accuracy.\n",
    "    \"\"\"\n",
    "    y_true = sample[y_true_variable]\n",
    "    scores = sample[independent_variable]\n",
    "    \n",
    "    predictions = [0 if score <= threshold else 1 for score in scores]\n",
    "    return accuracy_score(y_true, predictions)\n",
    "\n",
    "def point_accuracy_and_distribution(data, scores_var, y_true_var, bootstrap_samples=1000):\n",
    "    # Extract the scores and true labels\n",
    "    scores = data[scores_var]\n",
    "    y_true = data[y_true_var]\n",
    "\n",
    "    # Calculate accuracies for all thresholds\n",
    "    thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "    accuracies = []\n",
    "\n",
    "    bootstrap_results = {}\n",
    "    for threshold in thresholds_unique:\n",
    "        predictions = [0 if score <= threshold else 1 for score in scores]\n",
    "        accuracy = accuracy_score(y_true, predictions)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # Bootstrap Distribution\n",
    "        func_args = {'threshold': threshold, 'y_true_variable': y_true_var, 'independent_variable': scores_var}\n",
    "        bootstrap_results[f'{threshold}'] = bootstrap_distribution_statistics(data, compute_accuracy, func_args, bootstrap_samples)\n",
    "    # Create a DataFrame to store thresholds and corresponding accuracies\n",
    "    df_accuracies = pd.DataFrame({\n",
    "        \"Threshold\": thresholds_unique,\n",
    "        \"Accuracy\": accuracies\n",
    "    })\n",
    "    \n",
    "    return df_accuracies, bootstrap_results\n",
    "\n",
    "def plot_accuracy_with_ci(df_accuracies, bootstrap_results, output_dir):\n",
    "    \"\"\"\n",
    "    Plots accuracy as a function of threshold with 95% confidence intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - df_accuracies: DataFrame with 'Threshold' and 'Accuracy' columns.\n",
    "    - bootstrap_results: Dictionary with keys as thresholds and values as DataFrames containing bootstrapped statistics.\n",
    "    - output_dir: String with the path to the output directory.\n",
    "\n",
    "    Returns:\n",
    "    - No return value. The plot is saved to the output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the lower and upper bounds of the 95% confidence interval for each threshold\n",
    "    lower_bounds = [bootstrap_results[str(threshold)].loc['Bootstrapped 95-CIs', '2.5th Percentile'] for threshold in df_accuracies[\"Threshold\"]]\n",
    "    upper_bounds = [bootstrap_results[str(threshold)].loc['Bootstrapped 95-CIs', '97.5th Percentile'] for threshold in df_accuracies[\"Threshold\"]]\n",
    "\n",
    "    # Plot accuracy as a function of threshold\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the accuracies \n",
    "    ax.plot(df_accuracies[\"Threshold\"], df_accuracies[\"Accuracy\"], '-o', color=\"darkblue\")\n",
    "\n",
    "    # Plot the 95% confidence intervals with lighter shade\n",
    "    ax.fill_between(df_accuracies[\"Threshold\"], lower_bounds, upper_bounds, color='blue', alpha=0.05)\n",
    "\n",
    "    # Labels\n",
    "    ax.set_ylabel('Accuracy', fontsize=13)\n",
    "    ax.set_xlabel('Threshold', fontsize=13)\n",
    "\n",
    "    sns.despine()\n",
    "\n",
    "    # Check if the directory exists and create it if it doesn't\n",
    "    if not os.path.exists(output_dir + '/accuracy_curves'):\n",
    "        os.makedirs(output_dir + '/accuracy_curves')\n",
    "\n",
    "    # Save the figure before calling show()\n",
    "    fig.savefig(output_dir + f'/accuracy_curves/accuracy_curves_rct_group.svg')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracies, bootstrap_results = point_accuracy_and_distribution(data_df, scores_var=f'{independent_var_column}', y_true_var='one_hot_status', bootstrap_samples=10000)\n",
    "display(bootstrap_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# df_accuracies, bootstrap_results = point_accuracy_and_distribution(data_df, scores_var=f'{independent_var_column}', y_true_var='one_hot_status', bootstrap_samples=1000)\n",
    "plot_accuracy_with_ci(df_accuracies, bootstrap_results, output_dir=out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Displaying bootstrap results of threshold with highest accuracy')\n",
    "bootstrap_results[str(df_accuracies.loc[df_accuracies['Accuracy'].idxmax(), 'Threshold'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sensitivity and Specificity Across Different Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_outcome = 'MCI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "data_df = df_1.copy().loc[:, [independent_var_column, dependent_var_column]]\n",
    "data_df['one_hot_status'] = np.where(data_df[dependent_var_column] == positive_outcome, 1, 0)\n",
    "\n",
    "scores = data_df[independent_var_column]\n",
    "y_true = data_df['one_hot_status']\n",
    "\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "# Unique thresholds\n",
    "thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "\n",
    "for threshold in thresholds_unique:\n",
    "    # Compute predictions based on threshold\n",
    "    predictions = [1 if score <= threshold else 0 for score in scores]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()\n",
    "    \n",
    "    # Compute sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Append to lists\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "\n",
    "# Create a DataFrame to store thresholds, sensitivities, and specificities\n",
    "df_threshold_metrics = pd.DataFrame({\n",
    "    \"Threshold\": thresholds_unique,\n",
    "    \"Sensitivity\": sensitivities,\n",
    "    \"Specificity\": specificities\n",
    "})\n",
    "\n",
    "# Plot sensitivity and specificity as functions of threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Sensitivity\n",
    "sns.lineplot(x=\"Threshold\", y=\"Sensitivity\", data=df_threshold_metrics, marker='o', color=\"lightblue\", label=\"Sensitivity\")\n",
    "\n",
    "# Specificity\n",
    "sns.lineplot(x=\"Threshold\", y=\"Specificity\", data=df_threshold_metrics, marker='o', color=\"darkblue\", label=\"Specificity\")\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/sensitivity_specificity_curves'):\n",
    "    os.makedirs(out_dir + '/sensitivity_specificity_curves')\n",
    "plt.savefig(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.png')\n",
    "plt.savefig(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.svg')\n",
    "df_threshold_metrics.to_csv(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves_values.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "bootstrap_iterations = 10000\n",
    "\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "lower_sensitivity_bounds = []\n",
    "upper_sensitivity_bounds = []\n",
    "lower_specificity_bounds = []\n",
    "upper_specificity_bounds = []\n",
    "\n",
    "for threshold in thresholds_unique:\n",
    "    bootstrap_sensitivities = []\n",
    "    bootstrap_specificities = []\n",
    "\n",
    "    for i in range(bootstrap_iterations):\n",
    "        # Resample data with replacement\n",
    "        resampled_scores, resampled_y_true = resample(scores, y_true)\n",
    "        \n",
    "        # Compute predictions based on threshold\n",
    "        predictions = [1 if score <= threshold else 0 for score in resampled_scores]\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        try:\n",
    "            tn, fp, fn, tp = confusion_matrix(resampled_y_true, predictions).ravel()\n",
    "        except:\n",
    "            continue \n",
    "        if tp + fn != 0:\n",
    "            sensitivity = tp / (tp + fn)\n",
    "        else:\n",
    "            sensitivity = np.nan\n",
    "\n",
    "        if tn + fp != 0:\n",
    "            specificity = tn / (tn + fp)\n",
    "        else:\n",
    "            specificity = np.nan\n",
    "        # # Compute sensitivity and specificity\n",
    "        # sensitivity = tp / (tp + fn)\n",
    "        # specificity = tn / (tn + fp)\n",
    "\n",
    "        # Append to lists\n",
    "        bootstrap_sensitivities.append(sensitivity)\n",
    "        bootstrap_specificities.append(specificity)\n",
    "\n",
    "    # Append mean values to lists\n",
    "    # sensitivities.append(np.mean(bootstrap_sensitivities))\n",
    "    # specificities.append(np.mean(bootstrap_specificities))\n",
    "    sensitivities.append(np.nanmean(bootstrap_sensitivities))\n",
    "    specificities.append(np.nanmean(bootstrap_specificities))\n",
    "    \n",
    "    # Compute 95% confidence intervals for sensitivity and specificity\n",
    "    # lower_sensitivity, upper_sensitivity = np.percentile(bootstrap_sensitivities, [2.5, 97.5])\n",
    "    # lower_specificity, upper_specificity = np.percentile(bootstrap_specificities, [2.5, 97.5])\n",
    "    # Compute 95% confidence intervals for sensitivity and specificity\n",
    "    lower_sensitivity, upper_sensitivity = np.nanpercentile(bootstrap_sensitivities, [2.5, 97.5])\n",
    "    lower_specificity, upper_specificity = np.nanpercentile(bootstrap_specificities, [2.5, 97.5])\n",
    "    \n",
    "    lower_sensitivity_bounds.append(lower_sensitivity)\n",
    "    upper_sensitivity_bounds.append(upper_sensitivity)\n",
    "    lower_specificity_bounds.append(lower_specificity)\n",
    "    upper_specificity_bounds.append(upper_specificity)\n",
    "    \n",
    "confidence_interval_df = pd.DataFrame({\n",
    "    'threshold': thresholds_unique,\n",
    "    'sensitivity': sensitivities,\n",
    "    'lower_sensitivity_bound': lower_sensitivity_bounds,\n",
    "    'upper_sensitivity_bound': upper_sensitivity_bounds,\n",
    "    'specificity': specificities,\n",
    "    'lower_specificity_bound': lower_specificity_bounds,\n",
    "    'upper_specificity_bound': upper_specificity_bounds,\n",
    "})\n",
    "confidence_interval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Merge the two dataframes on the 'Threshold' column\n",
    "df_threshold_metrics = df_threshold_metrics.merge(confidence_interval_df, how='inner', left_on='Threshold', right_on='threshold')\n",
    "\n",
    "# Plot sensitivity and specificity as functions of threshold\n",
    "plt.figure(figsize=(15.5, 6))\n",
    "\n",
    "# Sensitivity\n",
    "sns.lineplot(x=\"Threshold\", y=\"Sensitivity\", data=df_threshold_metrics, marker='o', color=\"#1f77b4\", label=\"Sensitivity\")\n",
    "plt.fill_between(df_threshold_metrics['Threshold'], df_threshold_metrics['lower_sensitivity_bound'], df_threshold_metrics['upper_sensitivity_bound'], color='#1f77b4', alpha=0.25)\n",
    "\n",
    "# Specificity\n",
    "sns.lineplot(x=\"Threshold\", y=\"Specificity\", data=df_threshold_metrics, marker='o', color=\"#ff7f0e\", label=\"Specificity\")\n",
    "plt.fill_between(df_threshold_metrics['Threshold'], df_threshold_metrics['lower_specificity_bound'], df_threshold_metrics['upper_specificity_bound'], color='#ff7f0e', alpha=0.25)\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend(loc=(0.9, 0.5), frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/sensitivity_specificity_curves'):\n",
    "    os.makedirs(out_dir + '/sensitivity_specificity_curves')\n",
    "plt.savefig(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.png')\n",
    "plt.savefig(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.svg')\n",
    "df_threshold_metrics.to_csv(out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves_values.csv')\n",
    "print('Saved to: ', out_dir + f'/sensitivity_specificity_curves/sensitivity_specificity_curves.svg')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate PPV and NPV Across All Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "data_df = data_df.loc[:, [independent_var_column, dependent_var_column]]\n",
    "data_df['one_hot_status'] = np.where(data_df[dependent_var_column] == positive_outcome, 1, 0)\n",
    "\n",
    "scores = data_df[independent_var_column]\n",
    "y_true = data_df['one_hot_status']\n",
    "\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Unique thresholds\n",
    "thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "\n",
    "ppv_list = []\n",
    "npv_list = []\n",
    "\n",
    "for threshold in thresholds_unique:\n",
    "    # Compute predictions based on threshold\n",
    "    predictions = [1 if score <= threshold else 0 for score in scores]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()\n",
    "    \n",
    "    # Compute Positive Predictive Value (PPV) and Negative Predictive Value (NPV)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "    npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "    \n",
    "    # Append to lists\n",
    "    ppv_list.append(ppv)\n",
    "    npv_list.append(npv)\n",
    "\n",
    "# Create a DataFrame to store thresholds, PPVs, and NPVs\n",
    "df_ppv_npv = pd.DataFrame({\n",
    "    \"Threshold\": thresholds_unique,\n",
    "    \"Positive Predictive Value\": ppv_list,\n",
    "    \"Negative Predictive Value\": npv_list\n",
    "})\n",
    "\n",
    "# Plot PPV and NPV as functions of threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# PPV\n",
    "sns.lineplot(x=\"Threshold\", y=\"Positive Predictive Value\", data=df_ppv_npv, marker='o', color=\"lightblue\", label=\"PPV\")\n",
    "\n",
    "# NPV\n",
    "sns.lineplot(x=\"Threshold\", y=\"Negative Predictive Value\", data=df_ppv_npv, marker='o', color=\"darkblue\", label=\"NPV\")\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/ppv_npv_curves'):\n",
    "    os.makedirs(out_dir + '/ppv_npv_curves')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.png')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.svg')\n",
    "df_ppv_npv.to_csv(out_dir + f'/ppv_npv_curves/ppv_npv_curves.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "bootstrap_samples = 10000  # number of bootstrap samples to generate\n",
    "\n",
    "# Unique thresholds\n",
    "thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "\n",
    "ppv_list = []\n",
    "npv_list = []\n",
    "ppv_lower = []\n",
    "ppv_upper = []\n",
    "npv_lower = []\n",
    "npv_upper = []\n",
    "\n",
    "y_true_array = y_true.to_numpy()\n",
    "scores_array = scores.to_numpy()\n",
    "for threshold in thresholds_unique:\n",
    "    # Compute predictions based on threshold\n",
    "    predictions = [1 if score <= threshold else 0 for score in scores_array]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Compute Positive Predictive Value (PPV) and Negative Predictive Value (NPV)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "    npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "\n",
    "    # Bootstrap samples\n",
    "    ppv_bootstrap = []\n",
    "    npv_bootstrap = []\n",
    "\n",
    "    # Generate bootstrap samples\n",
    "    for _ in range(bootstrap_samples):\n",
    "        # Create bootstrap sample\n",
    "        bootstrap_indices = np.random.choice(np.arange(len(predictions)), size=len(predictions), replace=True)\n",
    "        bootstrap_predictions = [predictions[i] for i in bootstrap_indices]\n",
    "        bootstrap_y_true = [y_true_array[i] for i in bootstrap_indices]\n",
    "\n",
    "        \n",
    "        # Compute confusion matrix for bootstrap sample\n",
    "        tn, fp, fn, tp = confusion_matrix(bootstrap_y_true, bootstrap_predictions).ravel()\n",
    "        \n",
    "        # Compute PPV and NPV for bootstrap sample\n",
    "        ppv_bootstrap_sample = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "        npv_bootstrap_sample = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "\n",
    "        # Append to lists\n",
    "        ppv_bootstrap.append(ppv_bootstrap_sample)\n",
    "        npv_bootstrap.append(npv_bootstrap_sample)\n",
    "\n",
    "    # Compute 95% confidence intervals\n",
    "    ppv_lower.append(np.percentile(ppv_bootstrap, 2.5))\n",
    "    ppv_upper.append(np.percentile(ppv_bootstrap, 97.5))\n",
    "    npv_lower.append(np.percentile(npv_bootstrap, 2.5))\n",
    "    npv_upper.append(np.percentile(npv_bootstrap, 97.5))\n",
    "    \n",
    "    # Append to lists\n",
    "    ppv_list.append(ppv)\n",
    "    npv_list.append(npv)\n",
    "\n",
    "# Create a DataFrame to store thresholds, PPVs, and NPVs\n",
    "df_ppv_npv = pd.DataFrame({\n",
    "    \"Threshold\": thresholds_unique,\n",
    "    \"Positive Predictive Value\": ppv_list,\n",
    "    \"PPV Lower\": ppv_lower,\n",
    "    \"PPV Upper\": ppv_upper,\n",
    "    \"Negative Predictive Value\": npv_list,\n",
    "    \"NPV Lower\": npv_lower,\n",
    "    \"NPV Upper\": npv_upper\n",
    "})\n",
    "\n",
    "# Plot PPV and NPV as functions of threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# PPV\n",
    "plt.plot(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"Positive Predictive Value\"], '-o', color=\"lightblue\", label=\"PPV\")\n",
    "plt.fill_between(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"PPV Lower\"], df_ppv_npv[\"PPV Upper\"], color='lightblue', alpha=0.25)\n",
    "\n",
    "# NPV\n",
    "plt.plot(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"Negative Predictive Value\"], '-o', color=\"darkblue\", label=\"NPV\")\n",
    "plt.fill_between(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"NPV Lower\"], df_ppv_npv[\"NPV Upper\"], color='darkblue', alpha=0.05)\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/ppv_npv_curves'):\n",
    "    os.makedirs(out_dir + '/ppv_npv_curves')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.png')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.svg')\n",
    "df_ppv_npv.to_csv(out_dir + f'/ppv_npv_curves/ppv_npv_curves.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "bootstrap_samples = 10000  # number of bootstrap samples to generate\n",
    "\n",
    "# Unique thresholds\n",
    "thresholds_unique = sorted(list(set(scores)))  # get unique score values and sort them\n",
    "\n",
    "ppv_list = []\n",
    "npv_list = []\n",
    "ppv_lower = []\n",
    "ppv_upper = []\n",
    "npv_lower = []\n",
    "npv_upper = []\n",
    "\n",
    "y_true_array = y_true.to_numpy()\n",
    "scores_array = scores.to_numpy()\n",
    "for threshold in thresholds_unique:\n",
    "    # Compute predictions based on threshold\n",
    "    predictions = [1 if score <= threshold else 0 for score in scores_array]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_array, predictions).ravel()\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # Compute Positive Predictive Value (PPV) and Negative Predictive Value (NPV)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "    npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "\n",
    "    # Bootstrap samples\n",
    "    ppv_bootstrap = []\n",
    "    npv_bootstrap = []\n",
    "\n",
    "    # Generate bootstrap samples\n",
    "    for _ in range(bootstrap_samples):\n",
    "        # Create bootstrap sample\n",
    "        bootstrap_indices = np.random.choice(np.arange(len(predictions)), size=len(predictions), replace=True)\n",
    "        bootstrap_predictions = [predictions[i] for i in bootstrap_indices]\n",
    "        bootstrap_y_true = [y_true_array[i] for i in bootstrap_indices]\n",
    "\n",
    "        \n",
    "        # Compute confusion matrix for bootstrap sample\n",
    "        tn, fp, fn, tp = confusion_matrix(bootstrap_y_true, bootstrap_predictions).ravel()\n",
    "        \n",
    "        # Compute PPV and NPV for bootstrap sample\n",
    "        ppv_bootstrap_sample = tp / (tp + fp) if (tp + fp) != 0 else 0  # Prevent division by zero\n",
    "        npv_bootstrap_sample = tn / (tn + fn) if (tn + fn) != 0 else 0  # Prevent division by zero\n",
    "\n",
    "        # Append to lists\n",
    "        ppv_bootstrap.append(ppv_bootstrap_sample)\n",
    "        npv_bootstrap.append(npv_bootstrap_sample)\n",
    "\n",
    "    # Compute 95% confidence intervals\n",
    "    ppv_lower.append(np.percentile(ppv_bootstrap, 2.5))\n",
    "    ppv_upper.append(np.percentile(ppv_bootstrap, 97.5))\n",
    "    npv_lower.append(np.percentile(npv_bootstrap, 2.5))\n",
    "    npv_upper.append(np.percentile(npv_bootstrap, 97.5))\n",
    "    \n",
    "    # Append to lists\n",
    "    ppv_list.append(ppv)\n",
    "    npv_list.append(npv)\n",
    "\n",
    "# Create a DataFrame to store thresholds, PPVs, and NPVs\n",
    "df_ppv_npv = pd.DataFrame({\n",
    "    \"Threshold\": thresholds_unique,\n",
    "    \"Positive Predictive Value\": ppv_list,\n",
    "    \"PPV Lower\": ppv_lower,\n",
    "    \"PPV Upper\": ppv_upper,\n",
    "    \"Negative Predictive Value\": npv_list,\n",
    "    \"NPV Lower\": npv_lower,\n",
    "    \"NPV Upper\": npv_upper\n",
    "})\n",
    "\n",
    "# Plot PPV and NPV as functions of threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# PPV\n",
    "plt.plot(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"Positive Predictive Value\"], '-o', color=\"lightblue\", label=\"PPV\")\n",
    "plt.fill_between(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"PPV Lower\"], df_ppv_npv[\"PPV Upper\"], color='lightblue', alpha=0.25)\n",
    "\n",
    "# NPV\n",
    "plt.plot(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"Negative Predictive Value\"], '-o', color=\"darkblue\", label=\"NPV\")\n",
    "plt.fill_between(df_ppv_npv[\"Threshold\"], df_ppv_npv[\"NPV Lower\"], df_ppv_npv[\"NPV Upper\"], color='darkblue', alpha=0.05)\n",
    "\n",
    "# Labels\n",
    "plt.ylabel('Metric Value', fontsize=13)\n",
    "plt.xlabel('Threshold', fontsize=13)\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if not os.path.exists(out_dir + '/ppv_npv_curves'):\n",
    "    os.makedirs(out_dir + '/ppv_npv_curves')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.png')\n",
    "plt.savefig(out_dir + f'/ppv_npv_curves/ppv_npv_curves.svg')\n",
    "df_ppv_npv.to_csv(out_dir + f'/ppv_npv_curves/ppv_npv_curves.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_ppv_npv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Other Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num positive cases: ', np.count_nonzero(data_df['one_hot_status']))\n",
    "print('Num total cases: ', np.count_nonzero(data_df.index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62880161f19d28ddb8a8f59c63374d84ace356c39e36cc839cb3fb3bb03fb010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
