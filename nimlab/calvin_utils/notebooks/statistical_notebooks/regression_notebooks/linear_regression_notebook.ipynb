{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Any Kind of OLS Regression (ANOVA, GLM, etc.)\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "#### Last updated: July 6, 2023\n",
    "\n",
    "Use this to run/test a statistical model (e.g., regression or T-tests) on a spreadsheet.\n",
    "\n",
    "Notes:\n",
    "- To best use this notebook, you should be familar with GLM design and Contrast Matrix design. See this webpage to get started:\n",
    "[FSL's GLM page](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep Output Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Dropbox (Partners HealthCare)/studies/atrophy_seeds_2023/Figures/correlation_to_baseline_scores/regression_to_baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Dropbox (Partners HealthCare)/studies/atrophy_seeds_2023/metadata/master_list_z_only_unthresholded.csv'\n",
    "sheet = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>CerebellumSBM</th>\n",
       "      <th>CerebellumCSF</th>\n",
       "      <th>CerebellumGM</th>\n",
       "      <th>CerebellumWM</th>\n",
       "      <th>FrontalSurface</th>\n",
       "      <th>FrontalCSF</th>\n",
       "      <th>FrontalGM</th>\n",
       "      <th>FrontalWM</th>\n",
       "      <th>InsularSurface</th>\n",
       "      <th>...</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>Q11</th>\n",
       "      <th>Q12</th>\n",
       "      <th>Q14</th>\n",
       "      <th>TOTAL11</th>\n",
       "      <th>TOTALMOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4497.364934</td>\n",
       "      <td>-980.567621</td>\n",
       "      <td>-2977.241279</td>\n",
       "      <td>30.976181</td>\n",
       "      <td>-9854.189802</td>\n",
       "      <td>2592.141866</td>\n",
       "      <td>-15981.602420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2201.458697</td>\n",
       "      <td>18138.572720</td>\n",
       "      <td>11183.039810</td>\n",
       "      <td>36.083698</td>\n",
       "      <td>-9149.893654</td>\n",
       "      <td>26395.833840</td>\n",
       "      <td>16728.096790</td>\n",
       "      <td>4.145363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5422.499148</td>\n",
       "      <td>5613.118199</td>\n",
       "      <td>-9831.510960</td>\n",
       "      <td>104.795906</td>\n",
       "      <td>13796.400820</td>\n",
       "      <td>18396.096240</td>\n",
       "      <td>-4236.306518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>#</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.00</td>\n",
       "      <td>35.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10039.542490</td>\n",
       "      <td>-4899.249983</td>\n",
       "      <td>-6559.417793</td>\n",
       "      <td>252.720785</td>\n",
       "      <td>10684.093530</td>\n",
       "      <td>9280.278454</td>\n",
       "      <td>-16906.567050</td>\n",
       "      <td>2.313154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.67</td>\n",
       "      <td>17.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-807.610765</td>\n",
       "      <td>10722.003290</td>\n",
       "      <td>7145.779641</td>\n",
       "      <td>48.613606</td>\n",
       "      <td>-5685.897742</td>\n",
       "      <td>16940.901910</td>\n",
       "      <td>6925.515179</td>\n",
       "      <td>4.794878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-2512.170377</td>\n",
       "      <td>1056.173682</td>\n",
       "      <td>-2425.407108</td>\n",
       "      <td>92.413730</td>\n",
       "      <td>-4723.426224</td>\n",
       "      <td>6500.935230</td>\n",
       "      <td>-9211.785284</td>\n",
       "      <td>3.640567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>-3810.571029</td>\n",
       "      <td>-2017.963719</td>\n",
       "      <td>-2364.859302</td>\n",
       "      <td>72.367939</td>\n",
       "      <td>-5452.430173</td>\n",
       "      <td>-3952.113561</td>\n",
       "      <td>-4915.163107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1377.764614</td>\n",
       "      <td>301.016853</td>\n",
       "      <td>-1047.897930</td>\n",
       "      <td>257.644674</td>\n",
       "      <td>4570.889094</td>\n",
       "      <td>-3721.107713</td>\n",
       "      <td>-13588.501410</td>\n",
       "      <td>15.507745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>21528.922020</td>\n",
       "      <td>-10057.371230</td>\n",
       "      <td>-15527.412260</td>\n",
       "      <td>165.863003</td>\n",
       "      <td>24748.663740</td>\n",
       "      <td>-8417.850048</td>\n",
       "      <td>-17625.209260</td>\n",
       "      <td>2.043447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.33</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>-331.479770</td>\n",
       "      <td>6104.352219</td>\n",
       "      <td>-580.351404</td>\n",
       "      <td>22.831228</td>\n",
       "      <td>-1913.363895</td>\n",
       "      <td>-1692.824389</td>\n",
       "      <td>-8063.496605</td>\n",
       "      <td>3.357940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>9869.486016</td>\n",
       "      <td>-11122.342400</td>\n",
       "      <td>895.478236</td>\n",
       "      <td>86.666695</td>\n",
       "      <td>17608.845400</td>\n",
       "      <td>-20688.027740</td>\n",
       "      <td>-11112.854390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4717.085594</td>\n",
       "      <td>-16826.353680</td>\n",
       "      <td>-13884.914220</td>\n",
       "      <td>137.724410</td>\n",
       "      <td>10017.086990</td>\n",
       "      <td>-19402.827040</td>\n",
       "      <td>-11341.775500</td>\n",
       "      <td>6.575785</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.67</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>-1199.497876</td>\n",
       "      <td>19962.179850</td>\n",
       "      <td>14307.885460</td>\n",
       "      <td>81.470232</td>\n",
       "      <td>-6091.241026</td>\n",
       "      <td>16573.059910</td>\n",
       "      <td>14511.796900</td>\n",
       "      <td>12.376970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>27.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1505.740371</td>\n",
       "      <td>-1834.647957</td>\n",
       "      <td>-11441.559620</td>\n",
       "      <td>260.507399</td>\n",
       "      <td>11500.925660</td>\n",
       "      <td>11917.766050</td>\n",
       "      <td>-4810.739465</td>\n",
       "      <td>4.936501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.67</td>\n",
       "      <td>17.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-2138.697112</td>\n",
       "      <td>-11485.524060</td>\n",
       "      <td>-8944.047318</td>\n",
       "      <td>34.296010</td>\n",
       "      <td>3958.961232</td>\n",
       "      <td>-8319.639296</td>\n",
       "      <td>-8413.126989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>3786.179144</td>\n",
       "      <td>3432.225779</td>\n",
       "      <td>-4813.191631</td>\n",
       "      <td>112.079015</td>\n",
       "      <td>451.627930</td>\n",
       "      <td>1587.288273</td>\n",
       "      <td>-21575.294040</td>\n",
       "      <td>4.617155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.33</td>\n",
       "      <td>13.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>7603.158068</td>\n",
       "      <td>1516.198249</td>\n",
       "      <td>-1050.849621</td>\n",
       "      <td>65.192509</td>\n",
       "      <td>22814.774920</td>\n",
       "      <td>-10082.905480</td>\n",
       "      <td>-8241.878144</td>\n",
       "      <td>8.583321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4112.348942</td>\n",
       "      <td>4522.095084</td>\n",
       "      <td>8312.880945</td>\n",
       "      <td>91.675593</td>\n",
       "      <td>13779.898060</td>\n",
       "      <td>10242.476120</td>\n",
       "      <td>5966.339646</td>\n",
       "      <td>4.506109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>-4559.982453</td>\n",
       "      <td>2306.715670</td>\n",
       "      <td>6252.112247</td>\n",
       "      <td>31.608093</td>\n",
       "      <td>-3526.163105</td>\n",
       "      <td>-6144.537889</td>\n",
       "      <td>-4015.608531</td>\n",
       "      <td>10.554750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-7629.796469</td>\n",
       "      <td>1568.993475</td>\n",
       "      <td>-915.804551</td>\n",
       "      <td>35.703737</td>\n",
       "      <td>-10044.546640</td>\n",
       "      <td>-11087.465950</td>\n",
       "      <td>-17808.474990</td>\n",
       "      <td>2.150845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>5526.185830</td>\n",
       "      <td>-22452.092720</td>\n",
       "      <td>-14570.054120</td>\n",
       "      <td>52.295274</td>\n",
       "      <td>19140.690400</td>\n",
       "      <td>-17671.054580</td>\n",
       "      <td>-25126.580250</td>\n",
       "      <td>4.120685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.33</td>\n",
       "      <td>12.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>6835.044151</td>\n",
       "      <td>-13134.277280</td>\n",
       "      <td>-5918.253242</td>\n",
       "      <td>38.561811</td>\n",
       "      <td>10518.274600</td>\n",
       "      <td>-4042.628162</td>\n",
       "      <td>-180.395935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-7917.877719</td>\n",
       "      <td>-2829.108559</td>\n",
       "      <td>2091.997116</td>\n",
       "      <td>18.170561</td>\n",
       "      <td>-15082.353820</td>\n",
       "      <td>-4191.354112</td>\n",
       "      <td>-4036.167384</td>\n",
       "      <td>2.273662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>4871.868954</td>\n",
       "      <td>-5714.418470</td>\n",
       "      <td>-4327.271552</td>\n",
       "      <td>93.567752</td>\n",
       "      <td>3605.186881</td>\n",
       "      <td>-24796.708190</td>\n",
       "      <td>-14008.245160</td>\n",
       "      <td>11.832618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.67</td>\n",
       "      <td>13.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>584.186317</td>\n",
       "      <td>-593.786472</td>\n",
       "      <td>328.309038</td>\n",
       "      <td>120.186539</td>\n",
       "      <td>-4318.369043</td>\n",
       "      <td>8410.281330</td>\n",
       "      <td>-5545.971836</td>\n",
       "      <td>9.028558</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>140.358298</td>\n",
       "      <td>2790.885944</td>\n",
       "      <td>7229.608020</td>\n",
       "      <td>36.458639</td>\n",
       "      <td>3868.937890</td>\n",
       "      <td>16348.602200</td>\n",
       "      <td>7213.338886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.67</td>\n",
       "      <td>21.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>6842.889851</td>\n",
       "      <td>-1829.611557</td>\n",
       "      <td>-2845.852721</td>\n",
       "      <td>55.963174</td>\n",
       "      <td>32409.225480</td>\n",
       "      <td>-32696.493660</td>\n",
       "      <td>-19451.466500</td>\n",
       "      <td>2.145772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>-362.152382</td>\n",
       "      <td>-15498.240640</td>\n",
       "      <td>-8462.338620</td>\n",
       "      <td>22.204505</td>\n",
       "      <td>3227.715573</td>\n",
       "      <td>-36997.985770</td>\n",
       "      <td>-27442.767550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>-579.449091</td>\n",
       "      <td>4457.028976</td>\n",
       "      <td>4584.381484</td>\n",
       "      <td>150.486269</td>\n",
       "      <td>1724.975248</td>\n",
       "      <td>13969.310430</td>\n",
       "      <td>3174.852195</td>\n",
       "      <td>7.015039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>-39.253529</td>\n",
       "      <td>3875.987928</td>\n",
       "      <td>6736.100139</td>\n",
       "      <td>79.655200</td>\n",
       "      <td>-2528.287915</td>\n",
       "      <td>16161.459660</td>\n",
       "      <td>7905.077616</td>\n",
       "      <td>2.169010</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>-2726.979906</td>\n",
       "      <td>8123.317974</td>\n",
       "      <td>2699.443537</td>\n",
       "      <td>18.417506</td>\n",
       "      <td>-8983.663847</td>\n",
       "      <td>7647.710417</td>\n",
       "      <td>-1456.834638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>6642.389242</td>\n",
       "      <td>-5018.559162</td>\n",
       "      <td>-1857.476388</td>\n",
       "      <td>22.695699</td>\n",
       "      <td>26513.315130</td>\n",
       "      <td>-17404.061190</td>\n",
       "      <td>-13128.701330</td>\n",
       "      <td>2.448552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>634.183060</td>\n",
       "      <td>714.222526</td>\n",
       "      <td>-4729.156976</td>\n",
       "      <td>108.472697</td>\n",
       "      <td>8093.889995</td>\n",
       "      <td>269.409785</td>\n",
       "      <td>-22577.763390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>-2465.338531</td>\n",
       "      <td>1864.715981</td>\n",
       "      <td>-2686.673946</td>\n",
       "      <td>25.477848</td>\n",
       "      <td>-6713.509834</td>\n",
       "      <td>-2453.344085</td>\n",
       "      <td>6162.799090</td>\n",
       "      <td>2.881592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>3555.800211</td>\n",
       "      <td>-14753.220180</td>\n",
       "      <td>150.872161</td>\n",
       "      <td>242.753904</td>\n",
       "      <td>17148.432800</td>\n",
       "      <td>-799.463774</td>\n",
       "      <td>-15418.381470</td>\n",
       "      <td>23.334179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>3662.913619</td>\n",
       "      <td>-25114.113290</td>\n",
       "      <td>-21345.037160</td>\n",
       "      <td>102.211027</td>\n",
       "      <td>5618.037817</td>\n",
       "      <td>-15431.461800</td>\n",
       "      <td>-23032.918860</td>\n",
       "      <td>6.700692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.33</td>\n",
       "      <td>18.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>-8765.419115</td>\n",
       "      <td>14359.589150</td>\n",
       "      <td>6846.536868</td>\n",
       "      <td>58.914601</td>\n",
       "      <td>-16935.218110</td>\n",
       "      <td>-13903.760880</td>\n",
       "      <td>-296.867440</td>\n",
       "      <td>2.016903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2521.557181</td>\n",
       "      <td>-12100.739480</td>\n",
       "      <td>-417.469514</td>\n",
       "      <td>102.246471</td>\n",
       "      <td>11742.379690</td>\n",
       "      <td>-5088.720537</td>\n",
       "      <td>103.936259</td>\n",
       "      <td>2.703633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>-3491.481790</td>\n",
       "      <td>2887.488762</td>\n",
       "      <td>4124.295062</td>\n",
       "      <td>29.664265</td>\n",
       "      <td>-2090.066558</td>\n",
       "      <td>-243.428552</td>\n",
       "      <td>4357.417470</td>\n",
       "      <td>5.475521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.67</td>\n",
       "      <td>12.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4501.390785</td>\n",
       "      <td>-14086.366130</td>\n",
       "      <td>-10089.115110</td>\n",
       "      <td>58.658250</td>\n",
       "      <td>11756.146820</td>\n",
       "      <td>-29855.017680</td>\n",
       "      <td>-27764.291620</td>\n",
       "      <td>7.389536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.67</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>9090.831006</td>\n",
       "      <td>8203.018071</td>\n",
       "      <td>-3937.388504</td>\n",
       "      <td>244.496132</td>\n",
       "      <td>8036.921762</td>\n",
       "      <td>21071.482220</td>\n",
       "      <td>-4475.087802</td>\n",
       "      <td>22.125416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.33</td>\n",
       "      <td>10.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>4697.908300</td>\n",
       "      <td>6542.625302</td>\n",
       "      <td>-2982.377927</td>\n",
       "      <td>323.563475</td>\n",
       "      <td>12666.571450</td>\n",
       "      <td>19120.601390</td>\n",
       "      <td>-7222.117208</td>\n",
       "      <td>8.989416</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.67</td>\n",
       "      <td>20.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>1127.841143</td>\n",
       "      <td>9308.785805</td>\n",
       "      <td>8742.807780</td>\n",
       "      <td>55.187397</td>\n",
       "      <td>6025.086804</td>\n",
       "      <td>1165.535088</td>\n",
       "      <td>2027.875705</td>\n",
       "      <td>2.079455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.33</td>\n",
       "      <td>22.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>-2348.541294</td>\n",
       "      <td>3030.575610</td>\n",
       "      <td>1952.084806</td>\n",
       "      <td>335.655284</td>\n",
       "      <td>-3175.294289</td>\n",
       "      <td>-5612.834128</td>\n",
       "      <td>5312.765393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>7221.251020</td>\n",
       "      <td>-5021.810228</td>\n",
       "      <td>-1499.685479</td>\n",
       "      <td>33.856108</td>\n",
       "      <td>5405.563481</td>\n",
       "      <td>-2025.419643</td>\n",
       "      <td>-15187.370340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>-3961.355104</td>\n",
       "      <td>24031.910070</td>\n",
       "      <td>9109.204510</td>\n",
       "      <td>192.463318</td>\n",
       "      <td>-4703.781559</td>\n",
       "      <td>40740.635520</td>\n",
       "      <td>11328.841710</td>\n",
       "      <td>13.848285</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>5056.305591</td>\n",
       "      <td>3548.124638</td>\n",
       "      <td>-482.194832</td>\n",
       "      <td>116.596988</td>\n",
       "      <td>-2834.427986</td>\n",
       "      <td>2232.282375</td>\n",
       "      <td>-2153.935846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>26.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>5243.802177</td>\n",
       "      <td>-4984.295131</td>\n",
       "      <td>-5586.470819</td>\n",
       "      <td>109.097904</td>\n",
       "      <td>13527.445630</td>\n",
       "      <td>4125.723466</td>\n",
       "      <td>-13711.862090</td>\n",
       "      <td>4.460277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.67</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>-2735.743492</td>\n",
       "      <td>26412.015930</td>\n",
       "      <td>2040.272422</td>\n",
       "      <td>187.581870</td>\n",
       "      <td>-5532.505653</td>\n",
       "      <td>31899.326300</td>\n",
       "      <td>-504.884462</td>\n",
       "      <td>4.179206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.33</td>\n",
       "      <td>7.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1887.333795</td>\n",
       "      <td>1577.954777</td>\n",
       "      <td>3398.982892</td>\n",
       "      <td>27.574568</td>\n",
       "      <td>-990.640509</td>\n",
       "      <td>-7074.137496</td>\n",
       "      <td>-3076.111578</td>\n",
       "      <td>2.285043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject  CerebellumSBM  CerebellumCSF  CerebellumGM  CerebellumWM  \\\n",
       "0         1              0   -4497.364934   -980.567621  -2977.241279   \n",
       "1         2              0   -2201.458697  18138.572720  11183.039810   \n",
       "2         3              0    5422.499148   5613.118199  -9831.510960   \n",
       "3         4              0   10039.542490  -4899.249983  -6559.417793   \n",
       "4         5              0    -807.610765  10722.003290   7145.779641   \n",
       "5         6              0   -2512.170377   1056.173682  -2425.407108   \n",
       "6         7              0   -3810.571029  -2017.963719  -2364.859302   \n",
       "7         8              0   -1377.764614    301.016853  -1047.897930   \n",
       "8         9              0   21528.922020 -10057.371230 -15527.412260   \n",
       "9        10              0    -331.479770   6104.352219   -580.351404   \n",
       "10       11              0    9869.486016 -11122.342400    895.478236   \n",
       "11       12              0    4717.085594 -16826.353680 -13884.914220   \n",
       "12       13              0   -1199.497876  19962.179850  14307.885460   \n",
       "13       14              0    1505.740371  -1834.647957 -11441.559620   \n",
       "14       15              0   -2138.697112 -11485.524060  -8944.047318   \n",
       "15       16              0    3786.179144   3432.225779  -4813.191631   \n",
       "16       17              0    7603.158068   1516.198249  -1050.849621   \n",
       "17       18              0    4112.348942   4522.095084   8312.880945   \n",
       "18       19              0   -4559.982453   2306.715670   6252.112247   \n",
       "19       20              0   -7629.796469   1568.993475   -915.804551   \n",
       "20       21              0    5526.185830 -22452.092720 -14570.054120   \n",
       "21       22              0    6835.044151 -13134.277280  -5918.253242   \n",
       "22       23              0   -7917.877719  -2829.108559   2091.997116   \n",
       "23       24              0    4871.868954  -5714.418470  -4327.271552   \n",
       "24       25              0     584.186317   -593.786472    328.309038   \n",
       "25       26              0     140.358298   2790.885944   7229.608020   \n",
       "26       27              0    6842.889851  -1829.611557  -2845.852721   \n",
       "27       28              0    -362.152382 -15498.240640  -8462.338620   \n",
       "28       29              0    -579.449091   4457.028976   4584.381484   \n",
       "29       30              0     -39.253529   3875.987928   6736.100139   \n",
       "30       31              0   -2726.979906   8123.317974   2699.443537   \n",
       "31       32              0    6642.389242  -5018.559162  -1857.476388   \n",
       "32       33              0     634.183060    714.222526  -4729.156976   \n",
       "33       34              0   -2465.338531   1864.715981  -2686.673946   \n",
       "34       35              0    3555.800211 -14753.220180    150.872161   \n",
       "35       36              0    3662.913619 -25114.113290 -21345.037160   \n",
       "36       37              0   -8765.419115  14359.589150   6846.536868   \n",
       "37       38              0    2521.557181 -12100.739480   -417.469514   \n",
       "38       39              0   -3491.481790   2887.488762   4124.295062   \n",
       "39       40              0    4501.390785 -14086.366130 -10089.115110   \n",
       "40       41              0    9090.831006   8203.018071  -3937.388504   \n",
       "41       42              0    4697.908300   6542.625302  -2982.377927   \n",
       "42       43              0    1127.841143   9308.785805   8742.807780   \n",
       "43       44              0   -2348.541294   3030.575610   1952.084806   \n",
       "44       45              0    7221.251020  -5021.810228  -1499.685479   \n",
       "45       46              0   -3961.355104  24031.910070   9109.204510   \n",
       "46       47              0    5056.305591   3548.124638   -482.194832   \n",
       "47       48              0    5243.802177  -4984.295131  -5586.470819   \n",
       "48       49              0   -2735.743492  26412.015930   2040.272422   \n",
       "49       50              0    1887.333795   1577.954777   3398.982892   \n",
       "\n",
       "    FrontalSurface    FrontalCSF     FrontalGM     FrontalWM  InsularSurface  \\\n",
       "0        30.976181  -9854.189802   2592.141866 -15981.602420        0.000000   \n",
       "1        36.083698  -9149.893654  26395.833840  16728.096790        4.145363   \n",
       "2       104.795906  13796.400820  18396.096240  -4236.306518        0.000000   \n",
       "3       252.720785  10684.093530   9280.278454 -16906.567050        2.313154   \n",
       "4        48.613606  -5685.897742  16940.901910   6925.515179        4.794878   \n",
       "5        92.413730  -4723.426224   6500.935230  -9211.785284        3.640567   \n",
       "6        72.367939  -5452.430173  -3952.113561  -4915.163107        0.000000   \n",
       "7       257.644674   4570.889094  -3721.107713 -13588.501410       15.507745   \n",
       "8       165.863003  24748.663740  -8417.850048 -17625.209260        2.043447   \n",
       "9        22.831228  -1913.363895  -1692.824389  -8063.496605        3.357940   \n",
       "10       86.666695  17608.845400 -20688.027740 -11112.854390        0.000000   \n",
       "11      137.724410  10017.086990 -19402.827040 -11341.775500        6.575785   \n",
       "12       81.470232  -6091.241026  16573.059910  14511.796900       12.376970   \n",
       "13      260.507399  11500.925660  11917.766050  -4810.739465        4.936501   \n",
       "14       34.296010   3958.961232  -8319.639296  -8413.126989        0.000000   \n",
       "15      112.079015    451.627930   1587.288273 -21575.294040        4.617155   \n",
       "16       65.192509  22814.774920 -10082.905480  -8241.878144        8.583321   \n",
       "17       91.675593  13779.898060  10242.476120   5966.339646        4.506109   \n",
       "18       31.608093  -3526.163105  -6144.537889  -4015.608531       10.554750   \n",
       "19       35.703737 -10044.546640 -11087.465950 -17808.474990        2.150845   \n",
       "20       52.295274  19140.690400 -17671.054580 -25126.580250        4.120685   \n",
       "21       38.561811  10518.274600  -4042.628162   -180.395935        0.000000   \n",
       "22       18.170561 -15082.353820  -4191.354112  -4036.167384        2.273662   \n",
       "23       93.567752   3605.186881 -24796.708190 -14008.245160       11.832618   \n",
       "24      120.186539  -4318.369043   8410.281330  -5545.971836        9.028558   \n",
       "25       36.458639   3868.937890  16348.602200   7213.338886        0.000000   \n",
       "26       55.963174  32409.225480 -32696.493660 -19451.466500        2.145772   \n",
       "27       22.204505   3227.715573 -36997.985770 -27442.767550        0.000000   \n",
       "28      150.486269   1724.975248  13969.310430   3174.852195        7.015039   \n",
       "29       79.655200  -2528.287915  16161.459660   7905.077616        2.169010   \n",
       "30       18.417506  -8983.663847   7647.710417  -1456.834638        0.000000   \n",
       "31       22.695699  26513.315130 -17404.061190 -13128.701330        2.448552   \n",
       "32      108.472697   8093.889995    269.409785 -22577.763390        0.000000   \n",
       "33       25.477848  -6713.509834  -2453.344085   6162.799090        2.881592   \n",
       "34      242.753904  17148.432800   -799.463774 -15418.381470       23.334179   \n",
       "35      102.211027   5618.037817 -15431.461800 -23032.918860        6.700692   \n",
       "36       58.914601 -16935.218110 -13903.760880   -296.867440        2.016903   \n",
       "37      102.246471  11742.379690  -5088.720537    103.936259        2.703633   \n",
       "38       29.664265  -2090.066558   -243.428552   4357.417470        5.475521   \n",
       "39       58.658250  11756.146820 -29855.017680 -27764.291620        7.389536   \n",
       "40      244.496132   8036.921762  21071.482220  -4475.087802       22.125416   \n",
       "41      323.563475  12666.571450  19120.601390  -7222.117208        8.989416   \n",
       "42       55.187397   6025.086804   1165.535088   2027.875705        2.079455   \n",
       "43      335.655284  -3175.294289  -5612.834128   5312.765393        0.000000   \n",
       "44       33.856108   5405.563481  -2025.419643 -15187.370340        0.000000   \n",
       "45      192.463318  -4703.781559  40740.635520  11328.841710       13.848285   \n",
       "46      116.596988  -2834.427986   2232.282375  -2153.935846        0.000000   \n",
       "47      109.097904  13527.445630   4125.723466 -13711.862090        4.460277   \n",
       "48      187.581870  -5532.505653  31899.326300   -504.884462        4.179206   \n",
       "49       27.574568   -990.640509  -7074.137496  -3076.111578        2.285043   \n",
       "\n",
       "    ...   Q6   Q7   Q8   Q9  Q10  Q11  Q12  Q14  TOTAL11  TOTALMOD  \n",
       "0   ...  0.0  0.0    0  0.0  0.0  0.0  0.0  0.0     3.00      6.00  \n",
       "1   ...  0.0  0.0    5  0.0  0.0  0.0  0.0  0.0     9.00     13.00  \n",
       "2   ...  0.0  6.0    #  0.0  1.0  0.0  0.0  1.0    24.00     35.00  \n",
       "3   ...  0.0  0.0    5  0.0  0.0  0.0  0.0  1.0     9.67     17.67  \n",
       "4   ...  0.0  0.0    0  0.0  0.0  0.0  0.0  1.0     2.00      5.00  \n",
       "5   ...  0.0  0.0    0  0.0  0.0  1.0  0.0  0.0     2.33      4.33  \n",
       "6   ...  0.0  0.0    3  0.0  0.0  0.0  0.0  0.0     5.00      7.00  \n",
       "7   ...  0.0  1.0    3  0.0  0.0  1.0  1.0  1.0    12.00     22.00  \n",
       "8   ...  0.0  0.0    2  0.0  0.0  1.0  0.0  0.0    10.33     14.33  \n",
       "9   ...  0.0  0.0    2  0.0  0.0  0.0  0.0  0.0     7.00     14.00  \n",
       "10  ...  0.0  8.0    3  0.0  0.0  0.0  0.0  0.0    18.00     23.00  \n",
       "11  ...  1.0  0.0    3  0.0  0.0  0.0  0.0  1.0     7.67     14.67  \n",
       "12  ...  0.0  2.0    3  0.0  2.0  2.0  2.0  1.0    17.00     27.00  \n",
       "13  ...  0.0  1.0    2  0.0  0.0  0.0  0.0  1.0     9.67     17.67  \n",
       "14  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN      NaN       NaN  \n",
       "15  ...  0.0  0.0    4  0.0  0.0  0.0  0.0  0.0     9.33     13.33  \n",
       "16  ...  0.0  0.0    5  0.0  0.0  0.0  0.0  0.0     8.00      9.00  \n",
       "17  ...  0.0  2.0    5  0.0  1.0  0.0  0.0  0.0    13.00     20.00  \n",
       "18  ...  0.0  1.0    1  0.0  0.0  0.0  0.0  1.0     7.00     12.00  \n",
       "19  ...  0.0  0.0    3  0.0  0.0  0.0  0.0  0.0     5.00      6.00  \n",
       "20  ...  0.0  0.0    4  0.0  0.0  0.0  0.0  0.0     9.33     12.33  \n",
       "21  ...  0.0  0.0    3  0.0  0.0  0.0  0.0  1.0    10.00     16.00  \n",
       "22  ...  0.0  0.0    0  0.0  0.0  0.0  0.0  0.0     3.00      4.00  \n",
       "23  ...  0.0  1.0    3  0.0  0.0  0.0  0.0  0.0     7.67     13.67  \n",
       "24  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN      NaN       NaN  \n",
       "25  ...  1.0  1.0    0  0.0  0.0  1.0  1.0  2.0    13.67     21.67  \n",
       "26  ...  0.0  0.0    4  0.0  0.0  0.0  0.0  0.0     9.00     17.00  \n",
       "27  ...  0.0  0.0    0  0.0  0.0  0.0  0.0  0.0     0.00      0.00  \n",
       "28  ...  0.0  0.0    0  0.0  0.0  0.0  0.0  1.0     2.00      4.00  \n",
       "29  ...  1.0  0.0    7  0.0  0.0  0.0  0.0  0.0    14.00     23.00  \n",
       "30  ...  0.0  0.0    0  0.0  0.0  1.0  0.0  0.0     5.00      7.00  \n",
       "31  ...  0.0  3.0    9  0.0  0.0  0.0  0.0  2.0    18.00     28.00  \n",
       "32  ...  0.0  1.0    3  0.0  0.0  0.0  0.0  2.0    10.00     19.00  \n",
       "33  ...  0.0  1.0    1  0.0  0.0  0.0  0.0  0.0     4.00      6.00  \n",
       "34  ...  0.0  0.0    3  0.0  0.0  0.0  0.0  0.0     7.00     11.00  \n",
       "35  ...  0.0  1.0    3  0.0  0.0  0.0  0.0  0.0    11.33     18.33  \n",
       "36  ...  0.0  0.0    6  0.0  0.0  0.0  0.0  1.0    11.00     15.00  \n",
       "37  ...  0.0  0.0    3  0.0  0.0  0.0  0.0  2.0     9.00     14.00  \n",
       "38  ...  0.0  0.0    3  0.0  0.0  0.0  0.0  0.0     6.67     12.67  \n",
       "39  ...  0.0  0.0    6  0.0  0.0  0.0  0.0  1.0    10.67     16.67  \n",
       "40  ...  0.0  0.0    2  0.0  0.0  0.0  0.0  2.0     6.33     10.33  \n",
       "41  ...  1.0  2.0    5  0.0  0.0  0.0  0.0  2.0    11.67     20.67  \n",
       "42  ...  0.0  0.0    8  0.0  0.0  0.0  0.0  0.0    15.33     22.33  \n",
       "43  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN      NaN       NaN  \n",
       "44  ...  0.0  0.0    1  0.0  0.0  2.0  0.0  2.0     8.00     13.00  \n",
       "45  ...  1.0  5.0    7  1.0  0.0  1.0  0.0  1.0    23.00     32.00  \n",
       "46  ...  1.0  1.0    9  0.0  0.0  0.0  0.0  2.0    16.00     26.00  \n",
       "47  ...  0.0  0.0    #  0.0  0.0  0.0  0.0  0.0    13.67     14.67  \n",
       "48  ...  0.0  0.0    3  0.0  0.0  0.0  0.0  0.0     6.33      7.33  \n",
       "49  ...  0.0  0.0    1  0.0  0.0  0.0  0.0  1.0     4.00      8.00  \n",
       "\n",
       "[50 rows x 73 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Cerebellum_WM_Wscore', 'Cortex_WM_Wscore',\n",
       "       'Subcortex_WM_Wscore', 'Temporal_WM_Wscore', 'WholeBrain_WM_Wscore',\n",
       "       'MTL_WM_Wscore', 'Occipital_WM_Wscore', 'Frontal_WM_Wscore',\n",
       "       'Parietal_WM_Wscore', 'Insula_WM_Wscore', 'temp_ins_WM_Wscore',\n",
       "       'Cerebellum_GM_Wscore', 'Cortex_GM_Wscore', 'Subcortex_GM_Wscore',\n",
       "       'Temporal_GM_Wscore', 'WholeBrain_GM_Wscore', 'MTL_GM_Wscore',\n",
       "       'Occipital_GM_Wscore', 'Frontal_GM_Wscore', 'Parietal_GM_Wscore',\n",
       "       'Insula_GM_Wscore', 'temp_ins_GM_Wscore', 'Cerebellum_CSF_Wscore',\n",
       "       'Cortex_CSF_Wscore', 'Subcortex_CSF_Wscore', 'Temporal_CSF_Wscore',\n",
       "       'WholeBrain_CSF_Wscore', 'MTL_CSF_Wscore', 'Occipital_CSF_Wscore',\n",
       "       'Frontal_CSF_Wscore', 'Parietal_CSF_Wscore', 'Insula_CSF_Wscore',\n",
       "       'temp_ins_CSF_Wscore', 'frontal_eh', 'temporal_eh', 'parietal_eh',\n",
       "       'occipital_eh', 'cerebellum_eh', 'mesial_temporal_eh', 'ventricle_eh',\n",
       "       'Cerebellum_CT_Wscore', 'Cortex_CT_Wscore', 'Subcortex_CT_Wscore',\n",
       "       'Temporal_CT_Wscore', 'WholeBrain_CT_Wscore', 'MTL_CT_Wscore',\n",
       "       'Occipital_CT_Wscore', 'Frontal_CT_Wscore', 'Parietal_CT_Wscore',\n",
       "       'Insula_CT_Wscore', 'temp_ins_CT_Wscore', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5',\n",
       "       'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12', 'Q14', 'TOTAL11',\n",
       "       'TOTALMOD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Frontal_GM_Wscore', 'TOTALMOD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Cerebellum_WM_Wscore', 'Cortex_WM_Wscore',\n",
       "       'Subcortex_WM_Wscore', 'Temporal_WM_Wscore', 'WholeBrain_WM_Wscore',\n",
       "       'MTL_WM_Wscore', 'Occipital_WM_Wscore', 'Frontal_WM_Wscore',\n",
       "       'Parietal_WM_Wscore', 'Insula_WM_Wscore', 'temp_ins_WM_Wscore',\n",
       "       'Cerebellum_GM_Wscore', 'Cortex_GM_Wscore', 'Subcortex_GM_Wscore',\n",
       "       'Temporal_GM_Wscore', 'WholeBrain_GM_Wscore', 'MTL_GM_Wscore',\n",
       "       'Occipital_GM_Wscore', 'Frontal_GM_Wscore', 'Parietal_GM_Wscore',\n",
       "       'Insula_GM_Wscore', 'temp_ins_GM_Wscore', 'Cerebellum_CSF_Wscore',\n",
       "       'Cortex_CSF_Wscore', 'Subcortex_CSF_Wscore', 'Temporal_CSF_Wscore',\n",
       "       'WholeBrain_CSF_Wscore', 'MTL_CSF_Wscore', 'Occipital_CSF_Wscore',\n",
       "       'Frontal_CSF_Wscore', 'Parietal_CSF_Wscore', 'Insula_CSF_Wscore',\n",
       "       'temp_ins_CSF_Wscore', 'frontal_eh', 'temporal_eh', 'parietal_eh',\n",
       "       'occipital_eh', 'cerebellum_eh', 'mesial_temporal_eh', 'ventricle_eh',\n",
       "       'Cerebellum_CT_Wscore', 'Cortex_CT_Wscore', 'Subcortex_CT_Wscore',\n",
       "       'Temporal_CT_Wscore', 'WholeBrain_CT_Wscore', 'MTL_CT_Wscore',\n",
       "       'Occipital_CT_Wscore', 'Frontal_CT_Wscore', 'Parietal_CT_Wscore',\n",
       "       'Insula_CT_Wscore', 'temp_ins_CT_Wscore', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5',\n",
       "       'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12', 'Q14', 'TOTAL11',\n",
       "       'TOTALMOD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'City'  # The column you'd like to evaluate\n",
    "condition = 'not'  # The condition to check ('equal', 'above', 'below', 'not')\n",
    "value = 'Toronto' # The value to drop if found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T_Redone</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>...</th>\n",
       "      <th>Meaningful_Failure</th>\n",
       "      <th>Cognitive_Improve</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>21.150595</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>19.702349</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>23.231614</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>22.172312</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>21.546222</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.705128</td>\n",
       "      <td>-0.028151</td>\n",
       "      <td>-0.028151</td>\n",
       "      <td>-38.461538</td>\n",
       "      <td>-0.489205</td>\n",
       "      <td>-0.489205</td>\n",
       "      <td>23.553077</td>\n",
       "      <td>63.323903</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>15.816634</td>\n",
       "      <td>17.617107</td>\n",
       "      <td>-0.597423</td>\n",
       "      <td>-0.388183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.282051</td>\n",
       "      <td>0.435498</td>\n",
       "      <td>0.435498</td>\n",
       "      <td>-15.384615</td>\n",
       "      <td>-1.718309</td>\n",
       "      <td>-1.718309</td>\n",
       "      <td>19.831365</td>\n",
       "      <td>53.317851</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>15.524025</td>\n",
       "      <td>13.452311</td>\n",
       "      <td>-0.688373</td>\n",
       "      <td>-1.507246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.534722</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>-29.166667</td>\n",
       "      <td>-1.145694</td>\n",
       "      <td>-1.145694</td>\n",
       "      <td>21.565235</td>\n",
       "      <td>57.979468</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.816622</td>\n",
       "      <td>-0.816622</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>16.546984</td>\n",
       "      <td>13.932696</td>\n",
       "      <td>-0.370413</td>\n",
       "      <td>-1.378169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.557971</td>\n",
       "      <td>0.133118</td>\n",
       "      <td>0.133118</td>\n",
       "      <td>-30.434783</td>\n",
       "      <td>-0.043697</td>\n",
       "      <td>-0.043697</td>\n",
       "      <td>24.902068</td>\n",
       "      <td>66.950749</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.641086</td>\n",
       "      <td>-0.641086</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>19.669539</td>\n",
       "      <td>21.341523</td>\n",
       "      <td>0.600149</td>\n",
       "      <td>0.612551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.551282</td>\n",
       "      <td>-0.955451</td>\n",
       "      <td>-0.955451</td>\n",
       "      <td>-84.615385</td>\n",
       "      <td>0.240855</td>\n",
       "      <td>0.240855</td>\n",
       "      <td>25.763689</td>\n",
       "      <td>69.267271</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>18.295718</td>\n",
       "      <td>19.263977</td>\n",
       "      <td>0.173133</td>\n",
       "      <td>0.054323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>111</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>1.581744</td>\n",
       "      <td>1.581744</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>2.004069</td>\n",
       "      <td>2.004069</td>\n",
       "      <td>31.102681</td>\n",
       "      <td>83.621480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-1.289805</td>\n",
       "      <td>1.289805</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.92</td>\n",
       "      <td>17.022328</td>\n",
       "      <td>16.151031</td>\n",
       "      <td>-0.222665</td>\n",
       "      <td>-0.782112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>113</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>-0.460891</td>\n",
       "      <td>-0.460891</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.301223</td>\n",
       "      <td>0.301223</td>\n",
       "      <td>25.946482</td>\n",
       "      <td>69.758721</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.763198</td>\n",
       "      <td>0.763198</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>16.225059</td>\n",
       "      <td>17.818822</td>\n",
       "      <td>-0.470475</td>\n",
       "      <td>-0.333983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-0.295699</td>\n",
       "      <td>0.420542</td>\n",
       "      <td>0.420542</td>\n",
       "      <td>-16.129032</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>25.081114</td>\n",
       "      <td>67.432125</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.045371</td>\n",
       "      <td>-2.045371</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.16</td>\n",
       "      <td>15.295109</td>\n",
       "      <td>18.046377</td>\n",
       "      <td>-0.759525</td>\n",
       "      <td>-0.272840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>115</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.885057</td>\n",
       "      <td>-0.225336</td>\n",
       "      <td>-0.225336</td>\n",
       "      <td>-48.275862</td>\n",
       "      <td>0.472801</td>\n",
       "      <td>0.472801</td>\n",
       "      <td>26.466020</td>\n",
       "      <td>71.155530</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1.694300</td>\n",
       "      <td>-1.694300</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.24</td>\n",
       "      <td>18.623736</td>\n",
       "      <td>16.113043</td>\n",
       "      <td>0.275089</td>\n",
       "      <td>-0.792319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>116</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-0.675439</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>-36.842105</td>\n",
       "      <td>-0.657659</td>\n",
       "      <td>-0.657659</td>\n",
       "      <td>23.042999</td>\n",
       "      <td>61.952527</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>16.462336</td>\n",
       "      <td>18.503053</td>\n",
       "      <td>-0.396724</td>\n",
       "      <td>-0.150134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>118</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-0.057292</td>\n",
       "      <td>0.681812</td>\n",
       "      <td>0.681812</td>\n",
       "      <td>-3.125000</td>\n",
       "      <td>0.643782</td>\n",
       "      <td>0.643782</td>\n",
       "      <td>26.983748</td>\n",
       "      <td>72.547473</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.220907</td>\n",
       "      <td>-2.220907</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.842273</td>\n",
       "      <td>19.964170</td>\n",
       "      <td>-0.589454</td>\n",
       "      <td>0.242462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>119</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-1.489583</td>\n",
       "      <td>-0.887835</td>\n",
       "      <td>-0.887835</td>\n",
       "      <td>-81.250000</td>\n",
       "      <td>-0.637257</td>\n",
       "      <td>-0.637257</td>\n",
       "      <td>23.104777</td>\n",
       "      <td>62.118622</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>17.332886</td>\n",
       "      <td>19.198042</td>\n",
       "      <td>-0.126137</td>\n",
       "      <td>0.036607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>120</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-0.509259</td>\n",
       "      <td>0.186501</td>\n",
       "      <td>0.186501</td>\n",
       "      <td>-27.777778</td>\n",
       "      <td>-0.781964</td>\n",
       "      <td>-0.781964</td>\n",
       "      <td>22.666606</td>\n",
       "      <td>60.940570</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.236591</td>\n",
       "      <td>0.236591</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>10.126563</td>\n",
       "      <td>9.630690</td>\n",
       "      <td>-2.366028</td>\n",
       "      <td>-2.534098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>121</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.079710</td>\n",
       "      <td>0.657244</td>\n",
       "      <td>0.657244</td>\n",
       "      <td>-4.347826</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>26.491339</td>\n",
       "      <td>71.223601</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.641086</td>\n",
       "      <td>-0.641086</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>22.476761</td>\n",
       "      <td>23.985143</td>\n",
       "      <td>1.472698</td>\n",
       "      <td>1.322880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>122</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-2.566667</td>\n",
       "      <td>-2.068210</td>\n",
       "      <td>-2.068210</td>\n",
       "      <td>-140.000000</td>\n",
       "      <td>0.190387</td>\n",
       "      <td>0.190387</td>\n",
       "      <td>25.610874</td>\n",
       "      <td>68.856418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.763198</td>\n",
       "      <td>0.763198</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>21.466327</td>\n",
       "      <td>23.582758</td>\n",
       "      <td>1.158632</td>\n",
       "      <td>1.214761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>123</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.840498</td>\n",
       "      <td>1.840498</td>\n",
       "      <td>54.545455</td>\n",
       "      <td>-0.341607</td>\n",
       "      <td>-0.341607</td>\n",
       "      <td>24.230174</td>\n",
       "      <td>64.525483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>19.453283</td>\n",
       "      <td>21.676897</td>\n",
       "      <td>0.532931</td>\n",
       "      <td>0.702664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>124</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-0.343750</td>\n",
       "      <td>0.367883</td>\n",
       "      <td>0.367883</td>\n",
       "      <td>-18.750000</td>\n",
       "      <td>-0.864253</td>\n",
       "      <td>-0.864253</td>\n",
       "      <td>22.417436</td>\n",
       "      <td>60.270662</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>13.457540</td>\n",
       "      <td>15.819523</td>\n",
       "      <td>-1.330684</td>\n",
       "      <td>-0.871186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>125</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-0.687500</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-37.500000</td>\n",
       "      <td>0.536747</td>\n",
       "      <td>0.536747</td>\n",
       "      <td>26.659648</td>\n",
       "      <td>71.676110</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>22.443084</td>\n",
       "      <td>24.738357</td>\n",
       "      <td>1.462230</td>\n",
       "      <td>1.525265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>126</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-1.833333</td>\n",
       "      <td>-1.264551</td>\n",
       "      <td>-1.264551</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.081582</td>\n",
       "      <td>1.081582</td>\n",
       "      <td>28.309400</td>\n",
       "      <td>76.111573</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>20.186102</td>\n",
       "      <td>22.065363</td>\n",
       "      <td>0.760708</td>\n",
       "      <td>0.807044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>127</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-1.401961</td>\n",
       "      <td>-0.791810</td>\n",
       "      <td>-0.791810</td>\n",
       "      <td>-76.470588</td>\n",
       "      <td>0.009987</td>\n",
       "      <td>0.009987</td>\n",
       "      <td>25.064624</td>\n",
       "      <td>67.387791</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>16.996513</td>\n",
       "      <td>18.394062</td>\n",
       "      <td>-0.230689</td>\n",
       "      <td>-0.179419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>128</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.381944</td>\n",
       "      <td>0.326026</td>\n",
       "      <td>0.326026</td>\n",
       "      <td>-20.833333</td>\n",
       "      <td>1.010239</td>\n",
       "      <td>1.010239</td>\n",
       "      <td>28.093376</td>\n",
       "      <td>75.530777</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.816622</td>\n",
       "      <td>-0.816622</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>17.204945</td>\n",
       "      <td>18.264558</td>\n",
       "      <td>-0.165904</td>\n",
       "      <td>-0.214216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>129</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-0.654762</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>-35.714286</td>\n",
       "      <td>1.271528</td>\n",
       "      <td>1.271528</td>\n",
       "      <td>28.884554</td>\n",
       "      <td>77.657909</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>21.078380</td>\n",
       "      <td>22.489816</td>\n",
       "      <td>1.038049</td>\n",
       "      <td>0.921092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>130</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.130952</td>\n",
       "      <td>0.601088</td>\n",
       "      <td>0.601088</td>\n",
       "      <td>-7.142857</td>\n",
       "      <td>-0.161232</td>\n",
       "      <td>-0.161232</td>\n",
       "      <td>24.546173</td>\n",
       "      <td>65.993904</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.938734</td>\n",
       "      <td>0.938734</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.84</td>\n",
       "      <td>18.617648</td>\n",
       "      <td>20.415928</td>\n",
       "      <td>0.273197</td>\n",
       "      <td>0.363848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>131</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-0.733333</td>\n",
       "      <td>-0.059061</td>\n",
       "      <td>-0.059061</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>0.576257</td>\n",
       "      <td>0.576257</td>\n",
       "      <td>26.779282</td>\n",
       "      <td>71.997755</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.114480</td>\n",
       "      <td>-0.114480</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>18.410547</td>\n",
       "      <td>19.822564</td>\n",
       "      <td>0.208825</td>\n",
       "      <td>0.204413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>133</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.938667</td>\n",
       "      <td>0.938667</td>\n",
       "      <td>27.876656</td>\n",
       "      <td>74.948112</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>18.420784</td>\n",
       "      <td>19.612924</td>\n",
       "      <td>0.212007</td>\n",
       "      <td>0.148084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>134</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>1.335524</td>\n",
       "      <td>1.335524</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>0.609143</td>\n",
       "      <td>0.609143</td>\n",
       "      <td>26.878862</td>\n",
       "      <td>72.265483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>23.613479</td>\n",
       "      <td>26.048153</td>\n",
       "      <td>1.826016</td>\n",
       "      <td>1.877202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>135</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-0.838095</td>\n",
       "      <td>-0.173870</td>\n",
       "      <td>-0.173870</td>\n",
       "      <td>-45.714286</td>\n",
       "      <td>-0.484585</td>\n",
       "      <td>-0.484585</td>\n",
       "      <td>23.567065</td>\n",
       "      <td>63.361510</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.747514</td>\n",
       "      <td>-2.747514</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.101986</td>\n",
       "      <td>17.647381</td>\n",
       "      <td>-0.508729</td>\n",
       "      <td>-0.380049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>137</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.416667</td>\n",
       "      <td>-0.807926</td>\n",
       "      <td>-0.807926</td>\n",
       "      <td>-77.272727</td>\n",
       "      <td>-0.210108</td>\n",
       "      <td>-0.210108</td>\n",
       "      <td>24.398180</td>\n",
       "      <td>65.596015</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>16.837360</td>\n",
       "      <td>18.171505</td>\n",
       "      <td>-0.280158</td>\n",
       "      <td>-0.239219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>138</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.174603</td>\n",
       "      <td>0.553251</td>\n",
       "      <td>0.553251</td>\n",
       "      <td>-9.523810</td>\n",
       "      <td>1.634297</td>\n",
       "      <td>1.634297</td>\n",
       "      <td>29.983015</td>\n",
       "      <td>80.611188</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>20.531868</td>\n",
       "      <td>22.371502</td>\n",
       "      <td>0.868180</td>\n",
       "      <td>0.889302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>139</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-1.941176</td>\n",
       "      <td>-1.382736</td>\n",
       "      <td>-1.382736</td>\n",
       "      <td>-105.882353</td>\n",
       "      <td>-0.768437</td>\n",
       "      <td>-0.768437</td>\n",
       "      <td>22.707565</td>\n",
       "      <td>61.050691</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>17.957341</td>\n",
       "      <td>19.952870</td>\n",
       "      <td>0.067958</td>\n",
       "      <td>0.239426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>140</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.109898</td>\n",
       "      <td>1.109898</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>0.660878</td>\n",
       "      <td>0.660878</td>\n",
       "      <td>27.035513</td>\n",
       "      <td>72.686647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>24.418011</td>\n",
       "      <td>27.143129</td>\n",
       "      <td>2.076083</td>\n",
       "      <td>2.171417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>141</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744598</td>\n",
       "      <td>0.744598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.169749</td>\n",
       "      <td>1.169749</td>\n",
       "      <td>28.576370</td>\n",
       "      <td>76.829337</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>17.856530</td>\n",
       "      <td>19.670979</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.163683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>142</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.328286</td>\n",
       "      <td>0.328286</td>\n",
       "      <td>26.028429</td>\n",
       "      <td>69.979040</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.236591</td>\n",
       "      <td>0.236591</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>14.769545</td>\n",
       "      <td>16.131533</td>\n",
       "      <td>-0.922882</td>\n",
       "      <td>-0.787351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>143</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>-0.118729</td>\n",
       "      <td>-0.118729</td>\n",
       "      <td>24.674872</td>\n",
       "      <td>66.339918</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>20.179952</td>\n",
       "      <td>19.975031</td>\n",
       "      <td>0.758797</td>\n",
       "      <td>0.245380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>144</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-1.128205</td>\n",
       "      <td>-0.491801</td>\n",
       "      <td>-0.491801</td>\n",
       "      <td>-61.538462</td>\n",
       "      <td>-1.032653</td>\n",
       "      <td>-1.032653</td>\n",
       "      <td>21.907524</td>\n",
       "      <td>58.899731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>11.801552</td>\n",
       "      <td>14.042861</td>\n",
       "      <td>-1.845403</td>\n",
       "      <td>-1.348567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>145</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>2.183535</td>\n",
       "      <td>2.183535</td>\n",
       "      <td>31.646103</td>\n",
       "      <td>85.082502</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.236591</td>\n",
       "      <td>0.236591</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>21.327670</td>\n",
       "      <td>22.294652</td>\n",
       "      <td>1.115534</td>\n",
       "      <td>0.868652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>146</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.351301</td>\n",
       "      <td>-0.351301</td>\n",
       "      <td>-54.545455</td>\n",
       "      <td>-2.576474</td>\n",
       "      <td>-2.576474</td>\n",
       "      <td>17.232852</td>\n",
       "      <td>46.331586</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-1.465341</td>\n",
       "      <td>1.465341</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>16.512422</td>\n",
       "      <td>18.197520</td>\n",
       "      <td>-0.381156</td>\n",
       "      <td>-0.232229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>147</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-0.087302</td>\n",
       "      <td>0.648924</td>\n",
       "      <td>0.648924</td>\n",
       "      <td>-4.761905</td>\n",
       "      <td>0.262965</td>\n",
       "      <td>0.262965</td>\n",
       "      <td>25.830639</td>\n",
       "      <td>69.447270</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>19.159295</td>\n",
       "      <td>19.553377</td>\n",
       "      <td>0.441553</td>\n",
       "      <td>0.132084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>148</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-3.807692</td>\n",
       "      <td>-3.428250</td>\n",
       "      <td>-3.428250</td>\n",
       "      <td>-207.692308</td>\n",
       "      <td>1.035924</td>\n",
       "      <td>1.035924</td>\n",
       "      <td>28.171148</td>\n",
       "      <td>75.739873</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>19.340269</td>\n",
       "      <td>21.114119</td>\n",
       "      <td>0.497804</td>\n",
       "      <td>0.551449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>149</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>-1.063636</td>\n",
       "      <td>-1.063636</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>-0.650900</td>\n",
       "      <td>-0.650900</td>\n",
       "      <td>23.063466</td>\n",
       "      <td>62.007555</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-1.640876</td>\n",
       "      <td>1.640876</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>19.062652</td>\n",
       "      <td>20.569076</td>\n",
       "      <td>0.411514</td>\n",
       "      <td>0.404998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>150</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.018573</td>\n",
       "      <td>1.018573</td>\n",
       "      <td>13.636364</td>\n",
       "      <td>0.759359</td>\n",
       "      <td>0.759359</td>\n",
       "      <td>27.333715</td>\n",
       "      <td>73.488381</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>23.954833</td>\n",
       "      <td>26.691505</td>\n",
       "      <td>1.932117</td>\n",
       "      <td>2.050067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0       101  62.0                                 -0.392857   \n",
       "1       102  77.0                                 -0.666667   \n",
       "2       103  76.0                                 -1.447368   \n",
       "3       104  65.0                                 -2.372549   \n",
       "4       105  50.0                                 -0.192982   \n",
       "5       106  66.0                                 -0.705128   \n",
       "6       107  64.0                                 -0.282051   \n",
       "7       108  60.0                                 -0.534722   \n",
       "8       109  72.0                                 -0.557971   \n",
       "9       110  72.0                                 -1.551282   \n",
       "10      111  62.0                                  0.763889   \n",
       "11      113  69.0                                 -1.100000   \n",
       "12      114  67.0                                 -0.295699   \n",
       "13      115  60.0                                 -0.885057   \n",
       "14      116  67.0                                 -0.675439   \n",
       "15      118  52.0                                 -0.057292   \n",
       "16      119  75.0                                 -1.489583   \n",
       "17      120  68.0                                 -0.509259   \n",
       "18      121  72.0                                 -0.079710   \n",
       "19      122  58.0                                 -2.566667   \n",
       "20      123  47.0                                  1.000000   \n",
       "21      124  61.0                                 -0.343750   \n",
       "22      125  73.0                                 -0.687500   \n",
       "23      126  69.0                                 -1.833333   \n",
       "24      127  74.0                                 -1.401961   \n",
       "25      128  72.0                                 -0.381944   \n",
       "26      129  69.0                                 -0.654762   \n",
       "27      130  66.0                                 -0.130952   \n",
       "28      131  68.0                                 -0.733333   \n",
       "29      133  74.0                                  0.114583   \n",
       "30      134  66.0                                  0.539216   \n",
       "31      135  57.0                                 -0.838095   \n",
       "32      137  57.0                                 -1.416667   \n",
       "33      138  72.0                                 -0.174603   \n",
       "34      139  58.0                                 -1.941176   \n",
       "35      140  73.0                                  0.333333   \n",
       "36      141  72.0                                  0.000000   \n",
       "37      142  77.0                                  0.101852   \n",
       "38      143  71.0                                  0.114583   \n",
       "39      144  79.0                                 -1.128205   \n",
       "40      145  74.0                                  0.101852   \n",
       "41      146  76.0                                 -1.000000   \n",
       "42      147  59.0                                 -0.087302   \n",
       "43      148  51.0                                 -3.807692   \n",
       "44      149  77.0                                 -1.650000   \n",
       "45      150  71.0                                  0.250000   \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                            0.314066        \n",
       "1                                            0.013999        \n",
       "2                                           -0.841572        \n",
       "3                                           -1.855477        \n",
       "4                                            0.533109        \n",
       "5                                           -0.028151        \n",
       "6                                            0.435498        \n",
       "7                                            0.158596        \n",
       "8                                            0.133118        \n",
       "9                                           -0.955451        \n",
       "10                                           1.581744        \n",
       "11                                          -0.460891        \n",
       "12                                           0.420542        \n",
       "13                                          -0.225336        \n",
       "14                                           0.004385        \n",
       "15                                           0.681812        \n",
       "16                                          -0.887835        \n",
       "17                                           0.186501        \n",
       "18                                           0.657244        \n",
       "19                                          -2.068210        \n",
       "20                                           1.840498        \n",
       "21                                           0.367883        \n",
       "22                                          -0.008833        \n",
       "23                                          -1.264551        \n",
       "24                                          -0.791810        \n",
       "25                                           0.326026        \n",
       "26                                           0.027045        \n",
       "27                                           0.601088        \n",
       "28                                          -0.059061        \n",
       "29                                           0.870170        \n",
       "30                                           1.335524        \n",
       "31                                          -0.173870        \n",
       "32                                          -0.807926        \n",
       "33                                           0.553251        \n",
       "34                                          -1.382736        \n",
       "35                                           1.109898        \n",
       "36                                           0.744598        \n",
       "37                                           0.856218        \n",
       "38                                           0.870170        \n",
       "39                                          -0.491801        \n",
       "40                                           0.856218        \n",
       "41                                          -0.351301        \n",
       "42                                           0.648924        \n",
       "43                                          -3.428250        \n",
       "44                                          -1.063636        \n",
       "45                                           1.018573        \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                 0.314066                     -21.428571   \n",
       "1                                 0.013999                     -36.363636   \n",
       "2                                -0.841572                     -78.947368   \n",
       "3                                -1.855477                    -129.411765   \n",
       "4                                 0.533109                     -10.526316   \n",
       "5                                -0.028151                     -38.461538   \n",
       "6                                 0.435498                     -15.384615   \n",
       "7                                 0.158596                     -29.166667   \n",
       "8                                 0.133118                     -30.434783   \n",
       "9                                -0.955451                     -84.615385   \n",
       "10                                1.581744                      41.666667   \n",
       "11                               -0.460891                     -60.000000   \n",
       "12                                0.420542                     -16.129032   \n",
       "13                               -0.225336                     -48.275862   \n",
       "14                                0.004385                     -36.842105   \n",
       "15                                0.681812                      -3.125000   \n",
       "16                               -0.887835                     -81.250000   \n",
       "17                                0.186501                     -27.777778   \n",
       "18                                0.657244                      -4.347826   \n",
       "19                               -2.068210                    -140.000000   \n",
       "20                                1.840498                      54.545455   \n",
       "21                                0.367883                     -18.750000   \n",
       "22                               -0.008833                     -37.500000   \n",
       "23                               -1.264551                    -100.000000   \n",
       "24                               -0.791810                     -76.470588   \n",
       "25                                0.326026                     -20.833333   \n",
       "26                                0.027045                     -35.714286   \n",
       "27                                0.601088                      -7.142857   \n",
       "28                               -0.059061                     -40.000000   \n",
       "29                                0.870170                       6.250000   \n",
       "30                                1.335524                      29.411765   \n",
       "31                               -0.173870                     -45.714286   \n",
       "32                               -0.807926                     -77.272727   \n",
       "33                                0.553251                      -9.523810   \n",
       "34                               -1.382736                    -105.882353   \n",
       "35                                1.109898                      18.181818   \n",
       "36                                0.744598                       0.000000   \n",
       "37                                0.856218                       5.555556   \n",
       "38                                0.870170                       6.250000   \n",
       "39                               -0.491801                     -61.538462   \n",
       "40                                0.856218                       5.555556   \n",
       "41                               -0.351301                     -54.545455   \n",
       "42                                0.648924                      -4.761905   \n",
       "43                               -3.428250                    -207.692308   \n",
       "44                               -1.063636                     -90.000000   \n",
       "45                                1.018573                      13.636364   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                               -1.282630                          -1.282630   \n",
       "1                               -1.760917                          -1.760917   \n",
       "2                               -0.595369                          -0.595369   \n",
       "3                               -0.945206                          -0.945206   \n",
       "4                               -1.151973                          -1.151973   \n",
       "5                               -0.489205                          -0.489205   \n",
       "6                               -1.718309                          -1.718309   \n",
       "7                               -1.145694                          -1.145694   \n",
       "8                               -0.043697                          -0.043697   \n",
       "9                                0.240855                           0.240855   \n",
       "10                               2.004069                           2.004069   \n",
       "11                               0.301223                           0.301223   \n",
       "12                               0.015433                           0.015433   \n",
       "13                               0.472801                           0.472801   \n",
       "14                              -0.657659                          -0.657659   \n",
       "15                               0.643782                           0.643782   \n",
       "16                              -0.637257                          -0.637257   \n",
       "17                              -0.781964                          -0.781964   \n",
       "18                               0.481163                           0.481163   \n",
       "19                               0.190387                           0.190387   \n",
       "20                              -0.341607                          -0.341607   \n",
       "21                              -0.864253                          -0.864253   \n",
       "22                               0.536747                           0.536747   \n",
       "23                               1.081582                           1.081582   \n",
       "24                               0.009987                           0.009987   \n",
       "25                               1.010239                           1.010239   \n",
       "26                               1.271528                           1.271528   \n",
       "27                              -0.161232                          -0.161232   \n",
       "28                               0.576257                           0.576257   \n",
       "29                               0.938667                           0.938667   \n",
       "30                               0.609143                           0.609143   \n",
       "31                              -0.484585                          -0.484585   \n",
       "32                              -0.210108                          -0.210108   \n",
       "33                               1.634297                           1.634297   \n",
       "34                              -0.768437                          -0.768437   \n",
       "35                               0.660878                           0.660878   \n",
       "36                               1.169749                           1.169749   \n",
       "37                               0.328286                           0.328286   \n",
       "38                              -0.118729                          -0.118729   \n",
       "39                              -1.032653                          -1.032653   \n",
       "40                               2.183535                           2.183535   \n",
       "41                              -2.576474                          -2.576474   \n",
       "42                               0.262965                           0.262965   \n",
       "43                               1.035924                           1.035924   \n",
       "44                              -0.650900                          -0.650900   \n",
       "45                               0.759359                           0.759359   \n",
       "\n",
       "    Subiculum_Connectivity_T_Redone  Subiculum_Connectivity_T  ...  \\\n",
       "0                         21.150595                 56.864683  ...   \n",
       "1                         19.702349                 52.970984  ...   \n",
       "2                         23.231614                 62.459631  ...   \n",
       "3                         22.172312                 59.611631  ...   \n",
       "4                         21.546222                 57.928350  ...   \n",
       "5                         23.553077                 63.323903  ...   \n",
       "6                         19.831365                 53.317851  ...   \n",
       "7                         21.565235                 57.979468  ...   \n",
       "8                         24.902068                 66.950749  ...   \n",
       "9                         25.763689                 69.267271  ...   \n",
       "10                        31.102681                 83.621480  ...   \n",
       "11                        25.946482                 69.758721  ...   \n",
       "12                        25.081114                 67.432125  ...   \n",
       "13                        26.466020                 71.155530  ...   \n",
       "14                        23.042999                 61.952527  ...   \n",
       "15                        26.983748                 72.547473  ...   \n",
       "16                        23.104777                 62.118622  ...   \n",
       "17                        22.666606                 60.940570  ...   \n",
       "18                        26.491339                 71.223601  ...   \n",
       "19                        25.610874                 68.856418  ...   \n",
       "20                        24.230174                 64.525483  ...   \n",
       "21                        22.417436                 60.270662  ...   \n",
       "22                        26.659648                 71.676110  ...   \n",
       "23                        28.309400                 76.111573  ...   \n",
       "24                        25.064624                 67.387791  ...   \n",
       "25                        28.093376                 75.530777  ...   \n",
       "26                        28.884554                 77.657909  ...   \n",
       "27                        24.546173                 65.993904  ...   \n",
       "28                        26.779282                 71.997755  ...   \n",
       "29                        27.876656                 74.948112  ...   \n",
       "30                        26.878862                 72.265483  ...   \n",
       "31                        23.567065                 63.361510  ...   \n",
       "32                        24.398180                 65.596015  ...   \n",
       "33                        29.983015                 80.611188  ...   \n",
       "34                        22.707565                 61.050691  ...   \n",
       "35                        27.035513                 72.686647  ...   \n",
       "36                        28.576370                 76.829337  ...   \n",
       "37                        26.028429                 69.979040  ...   \n",
       "38                        24.674872                 66.339918  ...   \n",
       "39                        21.907524                 58.899731  ...   \n",
       "40                        31.646103                 85.082502  ...   \n",
       "41                        17.232852                 46.331586  ...   \n",
       "42                        25.830639                 69.447270  ...   \n",
       "43                        28.171148                 75.739873  ...   \n",
       "44                        23.063466                 62.007555  ...   \n",
       "45                        27.333715                 73.488381  ...   \n",
       "\n",
       "    Meaningful_Failure  Cognitive_Improve  Z_Scored_Cognitive_Baseline  \\\n",
       "0                  1.0                 No                     1.518764   \n",
       "1                  1.0                 No                     0.465551   \n",
       "2                  1.0                 No                    -0.061056   \n",
       "3                  1.0                 No                    -0.412127   \n",
       "4                  1.0                 No                    -0.061056   \n",
       "5                  1.0                 No                    -1.114269   \n",
       "6                  1.0                 No                    -1.114269   \n",
       "7                  1.0                 No                     0.816622   \n",
       "8                  1.0                 No                     0.641086   \n",
       "9                  1.0                 No                    -1.114269   \n",
       "10                 0.0                Yes                    -1.289805   \n",
       "11                 1.0                 No                    -0.763198   \n",
       "12                 1.0                 No                     2.045371   \n",
       "13                 1.0                 No                     1.694300   \n",
       "14                 1.0                 No                    -0.061056   \n",
       "15                 1.0                 No                     2.220907   \n",
       "16                 1.0                 No                    -0.587663   \n",
       "17                 1.0                 No                    -0.236591   \n",
       "18                 1.0                 No                     0.641086   \n",
       "19                 1.0                 No                    -0.763198   \n",
       "20                 0.0                Yes                     0.465551   \n",
       "21                 1.0                 No                    -0.587663   \n",
       "22                 1.0                 No                    -0.587663   \n",
       "23                 1.0                 No                     0.290015   \n",
       "24                 1.0                 No                    -0.412127   \n",
       "25                 1.0                 No                     0.816622   \n",
       "26                 1.0                 No                     1.518764   \n",
       "27                 1.0                 No                    -0.938734   \n",
       "28                 1.0                 No                     0.114480   \n",
       "29                 1.0                Yes                    -0.587663   \n",
       "30                 0.0                Yes                    -0.412127   \n",
       "31                 1.0                 No                     2.747514   \n",
       "32                 1.0                 No                     0.465551   \n",
       "33                 1.0                 No                     0.290015   \n",
       "34                 1.0                 No                    -0.412127   \n",
       "35                 0.0                Yes                     0.465551   \n",
       "36                 1.0                Yes                    -0.061056   \n",
       "37                 1.0                Yes                    -0.236591   \n",
       "38                 1.0                Yes                    -0.587663   \n",
       "39                 1.0                 No                    -1.114269   \n",
       "40                 1.0                Yes                    -0.236591   \n",
       "41                 1.0                 No                    -1.465341   \n",
       "42                 1.0                 No                     0.290015   \n",
       "43                 1.0                 No                    -1.114269   \n",
       "44                 1.0                 No                    -1.640876   \n",
       "45                 1.0                Yes                     0.465551   \n",
       "\n",
       "    Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                       -1.518764   \n",
       "1                                       -0.465551   \n",
       "2                                        0.061056   \n",
       "3                                        0.412127   \n",
       "4                                        0.061056   \n",
       "5                                        1.114269   \n",
       "6                                        1.114269   \n",
       "7                                       -0.816622   \n",
       "8                                       -0.641086   \n",
       "9                                        1.114269   \n",
       "10                                       1.289805   \n",
       "11                                       0.763198   \n",
       "12                                      -2.045371   \n",
       "13                                      -1.694300   \n",
       "14                                       0.061056   \n",
       "15                                      -2.220907   \n",
       "16                                       0.587663   \n",
       "17                                       0.236591   \n",
       "18                                      -0.641086   \n",
       "19                                       0.763198   \n",
       "20                                      -0.465551   \n",
       "21                                       0.587663   \n",
       "22                                       0.587663   \n",
       "23                                      -0.290015   \n",
       "24                                       0.412127   \n",
       "25                                      -0.816622   \n",
       "26                                      -1.518764   \n",
       "27                                       0.938734   \n",
       "28                                      -0.114480   \n",
       "29                                       0.587663   \n",
       "30                                       0.412127   \n",
       "31                                      -2.747514   \n",
       "32                                      -0.465551   \n",
       "33                                      -0.290015   \n",
       "34                                       0.412127   \n",
       "35                                      -0.465551   \n",
       "36                                       0.061056   \n",
       "37                                       0.236591   \n",
       "38                                       0.587663   \n",
       "39                                       1.114269   \n",
       "40                                       0.236591   \n",
       "41                                       1.465341   \n",
       "42                                      -0.290015   \n",
       "43                                       1.114269   \n",
       "44                                       1.640876   \n",
       "45                                      -0.465551   \n",
       "\n",
       "    Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                          0.72                                 0.28   \n",
       "1                          0.48                                 0.52   \n",
       "2                          0.36                                 0.64   \n",
       "3                          0.28                                 0.72   \n",
       "4                          0.36                                 0.64   \n",
       "5                          0.12                                 0.88   \n",
       "6                          0.12                                 0.88   \n",
       "7                          0.56                                 0.44   \n",
       "8                          0.52                                 0.48   \n",
       "9                          0.12                                 0.88   \n",
       "10                         0.08                                 0.92   \n",
       "11                         0.20                                 0.80   \n",
       "12                         0.84                                 0.16   \n",
       "13                         0.76                                 0.24   \n",
       "14                         0.36                                 0.64   \n",
       "15                         0.88                                 0.12   \n",
       "16                         0.24                                 0.76   \n",
       "17                         0.32                                 0.68   \n",
       "18                         0.52                                 0.48   \n",
       "19                         0.20                                 0.80   \n",
       "20                         0.48                                 0.52   \n",
       "21                         0.24                                 0.76   \n",
       "22                         0.24                                 0.76   \n",
       "23                         0.44                                 0.56   \n",
       "24                         0.28                                 0.72   \n",
       "25                         0.56                                 0.44   \n",
       "26                         0.72                                 0.28   \n",
       "27                         0.16                                 0.84   \n",
       "28                         0.40                                 0.60   \n",
       "29                         0.24                                 0.76   \n",
       "30                         0.28                                 0.72   \n",
       "31                         1.00                                 0.00   \n",
       "32                         0.48                                 0.52   \n",
       "33                         0.44                                 0.56   \n",
       "34                         0.28                                 0.72   \n",
       "35                         0.48                                 0.52   \n",
       "36                         0.36                                 0.64   \n",
       "37                         0.32                                 0.68   \n",
       "38                         0.24                                 0.76   \n",
       "39                         0.12                                 0.88   \n",
       "40                         0.32                                 0.68   \n",
       "41                         0.04                                 0.96   \n",
       "42                         0.44                                 0.56   \n",
       "43                         0.12                                 0.88   \n",
       "44                         0.00                                 1.00   \n",
       "45                         0.48                                 0.52   \n",
       "\n",
       "    ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0        12.222658      14.493929            -1.714513            -1.227368  \n",
       "1        14.020048      15.257338            -1.155843            -1.022243  \n",
       "2        15.118727      17.376384            -0.814348            -0.452865  \n",
       "3        13.112424      15.287916            -1.437954            -1.014027  \n",
       "4        15.086568      12.951426            -0.824344            -1.641831  \n",
       "5        15.816634      17.617107            -0.597423            -0.388183  \n",
       "6        15.524025      13.452311            -0.688373            -1.507246  \n",
       "7        16.546984      13.932696            -0.370413            -1.378169  \n",
       "8        19.669539      21.341523             0.600149             0.612551  \n",
       "9        18.295718      19.263977             0.173133             0.054323  \n",
       "10       17.022328      16.151031            -0.222665            -0.782112  \n",
       "11       16.225059      17.818822            -0.470475            -0.333983  \n",
       "12       15.295109      18.046377            -0.759525            -0.272840  \n",
       "13       18.623736      16.113043             0.275089            -0.792319  \n",
       "14       16.462336      18.503053            -0.396724            -0.150134  \n",
       "15       15.842273      19.964170            -0.589454             0.242462  \n",
       "16       17.332886      19.198042            -0.126137             0.036607  \n",
       "17       10.126563       9.630690            -2.366028            -2.534098  \n",
       "18       22.476761      23.985143             1.472698             1.322880  \n",
       "19       21.466327      23.582758             1.158632             1.214761  \n",
       "20       19.453283      21.676897             0.532931             0.702664  \n",
       "21       13.457540      15.819523            -1.330684            -0.871186  \n",
       "22       22.443084      24.738357             1.462230             1.525265  \n",
       "23       20.186102      22.065363             0.760708             0.807044  \n",
       "24       16.996513      18.394062            -0.230689            -0.179419  \n",
       "25       17.204945      18.264558            -0.165904            -0.214216  \n",
       "26       21.078380      22.489816             1.038049             0.921092  \n",
       "27       18.617648      20.415928             0.273197             0.363848  \n",
       "28       18.410547      19.822564             0.208825             0.204413  \n",
       "29       18.420784      19.612924             0.212007             0.148084  \n",
       "30       23.613479      26.048153             1.826016             1.877202  \n",
       "31       16.101986      17.647381            -0.508729            -0.380049  \n",
       "32       16.837360      18.171505            -0.280158            -0.239219  \n",
       "33       20.531868      22.371502             0.868180             0.889302  \n",
       "34       17.957341      19.952870             0.067958             0.239426  \n",
       "35       24.418011      27.143129             2.076083             2.171417  \n",
       "36       17.856530      19.670979             0.036624             0.163683  \n",
       "37       14.769545      16.131533            -0.922882            -0.787351  \n",
       "38       20.179952      19.975031             0.758797             0.245380  \n",
       "39       11.801552      14.042861            -1.845403            -1.348567  \n",
       "40       21.327670      22.294652             1.115534             0.868652  \n",
       "41       16.512422      18.197520            -0.381156            -0.232229  \n",
       "42       19.159295      19.553377             0.441553             0.132084  \n",
       "43       19.340269      21.114119             0.497804             0.551449  \n",
       "44       19.062652      20.569076             0.411514             0.404998  \n",
       "45       23.954833      26.691505             1.932117             2.050067  \n",
       "\n",
       "[46 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = None # ['Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group', 'Z_Scored_Subiculum_T_By_Origin_Group_'] #['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in data_df.columns:\n",
    "#     if 'CSF' and 'eh' not in col:\n",
    "#         data_df[col] = data_df[col] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # Desired value for the important value\n",
    "# desired_value = 24\n",
    "# values = data_df['Subiculum_Connectivity_T'].values\n",
    "# # Calculate the scaling factor\n",
    "# scaling_factor = desired_value / values[np.where(values == 64.52548319)]\n",
    "\n",
    "# # Scale all values\n",
    "# scaled_values = values * scaling_factor\n",
    "\n",
    "# # Verify mean and standard deviation\n",
    "# original_mean = np.mean(values)\n",
    "# original_std = np.std(values)\n",
    "# scaled_mean = np.mean(scaled_values)\n",
    "# scaled_std = np.std(scaled_values)\n",
    "\n",
    "# print(\"Original Mean:\", original_mean)\n",
    "# print(\"Original Standard Deviation:\", original_std)\n",
    "# print(\"Scaled Mean:\", scaled_mean)\n",
    "# print(\"Scaled Standard Deviation:\", scaled_std)\n",
    "\n",
    "# data_df['scaled_sbc'] = scaled_values\n",
    "data_df.to_excel('/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list_proper_subjects_scaled.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Define Your Formula\n",
    "\n",
    "This is the formula relating outcome to predictors, and takes the form:\n",
    "- y = B0 + B1 + B2 + B3 + . . . BN\n",
    "\n",
    "It is defined using the columns of your dataframe instead of the variables above:\n",
    "- 'Apples_Picked ~ hours_worked + owns_apple_picking_machine'\n",
    "\n",
    "____\n",
    "**ANOVA**\n",
    "- Tests differences in means for one categorical variable.\n",
    "- formula = 'Outcome ~ C(Group1)'\n",
    "\n",
    "**2-Way ANOVA**\n",
    "- Tests differences in means for two categorical variables without interaction.\n",
    "- formula = 'Outcome ~ C(Group1) + C(Group2)'\n",
    "\n",
    "**2-Way ANOVA with Interaction**\n",
    "- Tests for interaction effects between two categorical variables.\n",
    "- formula = 'Outcome ~ C(Group1) * C(Group2)'\n",
    "\n",
    "**ANCOVA**\n",
    "- Similar to ANOVA, but includes a covariate to control for its effect.\n",
    "- formula = 'Outcome ~ C(Group1) + Covariate'\n",
    "\n",
    "**2-Way ANCOVA**\n",
    "- Extends ANCOVA with two categorical variables and their interaction, controlling for a covariate.\n",
    "- formula = 'Outcome ~ C(Group1) * C(Group2) + Covariate'\n",
    "\n",
    "**Multiple Regression**\n",
    "- Assesses the impact of multiple predictors on an outcome.\n",
    "- formula = 'Outcome ~ Predictor1 + Predictor2'\n",
    "\n",
    "**Simple Linear Regression**\n",
    "- Assesses the impact of a single predictor on an outcome.\n",
    "- formula = 'Outcome ~ Predictor'\n",
    "\n",
    "**MANOVA**\n",
    "- Assesses multiple dependent variables across groups.\n",
    "- Note: Not typically set up with a formula in statsmodels. Requires specialized functions.\n",
    "\n",
    "____\n",
    "Use the printout below to design your formula. \n",
    "- Left of the \"~\" symbol is the thing to be predicted. \n",
    "- Right of the \"~\" symbol are the predictors. \n",
    "- \":\" indicates an interaction between two things. \n",
    "- \"*\" indicates and interactions AND it accounts for the simple effects too. \n",
    "- \"+\" indicates that you want to add another predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Cerebellum_WM_Wscore', 'Cortex_WM_Wscore',\n",
       "       'Subcortex_WM_Wscore', 'Temporal_WM_Wscore', 'WholeBrain_WM_Wscore',\n",
       "       'MTL_WM_Wscore', 'Occipital_WM_Wscore', 'Frontal_WM_Wscore',\n",
       "       'Parietal_WM_Wscore', 'Insula_WM_Wscore', 'temp_ins_WM_Wscore',\n",
       "       'Cerebellum_GM_Wscore', 'Cortex_GM_Wscore', 'Subcortex_GM_Wscore',\n",
       "       'Temporal_GM_Wscore', 'WholeBrain_GM_Wscore', 'MTL_GM_Wscore',\n",
       "       'Occipital_GM_Wscore', 'Frontal_GM_Wscore', 'Parietal_GM_Wscore',\n",
       "       'Insula_GM_Wscore', 'temp_ins_GM_Wscore', 'Cerebellum_CSF_Wscore',\n",
       "       'Cortex_CSF_Wscore', 'Subcortex_CSF_Wscore', 'Temporal_CSF_Wscore',\n",
       "       'WholeBrain_CSF_Wscore', 'MTL_CSF_Wscore', 'Occipital_CSF_Wscore',\n",
       "       'Frontal_CSF_Wscore', 'Parietal_CSF_Wscore', 'Insula_CSF_Wscore',\n",
       "       'temp_ins_CSF_Wscore', 'frontal_eh', 'temporal_eh', 'parietal_eh',\n",
       "       'occipital_eh', 'cerebellum_eh', 'mesial_temporal_eh', 'ventricle_eh',\n",
       "       'Cerebellum_CT_Wscore', 'Cortex_CT_Wscore', 'Subcortex_CT_Wscore',\n",
       "       'Temporal_CT_Wscore', 'WholeBrain_CT_Wscore', 'MTL_CT_Wscore',\n",
       "       'Occipital_CT_Wscore', 'Frontal_CT_Wscore', 'Parietal_CT_Wscore',\n",
       "       'Insula_CT_Wscore', 'temp_ins_CT_Wscore', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5',\n",
       "       'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12', 'Q14', 'TOTAL11',\n",
       "       'TOTALMOD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cerebellum_GM_Wscore', 'Cortex_GM_Wscore', 'Subcortex_GM_Wscore', 'Temporal_GM_Wscore', 'WholeBrain_GM_Wscore', 'MTL_GM_Wscore', 'Occipital_GM_Wscore', 'Frontal_GM_Wscore', 'Parietal_GM_Wscore', 'Insula_GM_Wscore', 'temp_ins_GM_Wscore']\n"
     ]
    }
   ],
   "source": [
    "vals_list = [val for val in data_df.columns if 'gm' in val.lower()]\n",
    "print(vals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"TOTALMOD ~ Frontal_GM_Wscore + Occipital_GM_Wscore + Parietal_GM_Wscore + temp_ins_GM_Wscore \"\n",
    "# \"Z_Scored_Percent_Cognitive_Improvement ~ Age_Disease_Cohort_Stim\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Visualize Your Design Matrix\n",
    "\n",
    "This is the explanatory variable half of your regression formula\n",
    "_______________________________________________________\n",
    "Create Design Matrix: Use the create_design_matrix method. You can provide a list of formula variables which correspond to column names in your dataframe.\n",
    "\n",
    "- design_matrix = palm.create_design_matrix(formula_vars=[\"var1\", \"var2\", \"var1*var2\"])\n",
    "- To include interaction terms, use * between variables, like \"var1*var2\".\n",
    "- By default, an intercept will be added unless you set intercept=False\n",
    "- **don't explicitly add the 'intercept' column. I'll do it for you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Frontal_GM_Wscore</th>\n",
       "      <th>Occipital_GM_Wscore</th>\n",
       "      <th>Parietal_GM_Wscore</th>\n",
       "      <th>temp_ins_GM_Wscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22609.050080</td>\n",
       "      <td>8754.361612</td>\n",
       "      <td>9947.996570</td>\n",
       "      <td>19024.984090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-4617.666275</td>\n",
       "      <td>-14293.557940</td>\n",
       "      <td>-10075.483750</td>\n",
       "      <td>-16210.506930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-43204.780170</td>\n",
       "      <td>-11917.171430</td>\n",
       "      <td>-21033.593490</td>\n",
       "      <td>2332.992165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-62992.950400</td>\n",
       "      <td>-26002.674290</td>\n",
       "      <td>-31401.277300</td>\n",
       "      <td>-29376.961170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23673.776970</td>\n",
       "      <td>6723.486988</td>\n",
       "      <td>19004.809470</td>\n",
       "      <td>2845.854587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11934.517970</td>\n",
       "      <td>9853.028019</td>\n",
       "      <td>10654.113210</td>\n",
       "      <td>4992.987068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33551.884340</td>\n",
       "      <td>23588.754200</td>\n",
       "      <td>30444.957830</td>\n",
       "      <td>12790.821490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-19491.371980</td>\n",
       "      <td>-2031.581497</td>\n",
       "      <td>-8085.494356</td>\n",
       "      <td>-3522.327339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7993.575143</td>\n",
       "      <td>15340.672670</td>\n",
       "      <td>17928.904070</td>\n",
       "      <td>22101.365220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-4504.586204</td>\n",
       "      <td>-11531.125930</td>\n",
       "      <td>-3974.399514</td>\n",
       "      <td>-4767.078647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7418.107911</td>\n",
       "      <td>2352.671971</td>\n",
       "      <td>17502.882450</td>\n",
       "      <td>11985.783540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-9610.575060</td>\n",
       "      <td>-2320.621156</td>\n",
       "      <td>-5627.133256</td>\n",
       "      <td>-11105.590440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-22131.055520</td>\n",
       "      <td>-1589.832770</td>\n",
       "      <td>3779.181542</td>\n",
       "      <td>-3395.022755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22952.131860</td>\n",
       "      <td>5533.193158</td>\n",
       "      <td>10750.348470</td>\n",
       "      <td>19103.909200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>38388.956560</td>\n",
       "      <td>-1097.724477</td>\n",
       "      <td>9764.933678</td>\n",
       "      <td>11639.480950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-72302.507930</td>\n",
       "      <td>-23280.903770</td>\n",
       "      <td>-39029.713540</td>\n",
       "      <td>-35967.266650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-486.158057</td>\n",
       "      <td>70.133499</td>\n",
       "      <td>-1238.358638</td>\n",
       "      <td>-4897.637297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-30503.968660</td>\n",
       "      <td>-18552.667580</td>\n",
       "      <td>-23393.478660</td>\n",
       "      <td>-21433.785400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>43463.416700</td>\n",
       "      <td>20786.851500</td>\n",
       "      <td>25693.064830</td>\n",
       "      <td>37218.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8825.601778</td>\n",
       "      <td>-5282.423396</td>\n",
       "      <td>-553.201231</td>\n",
       "      <td>12122.255450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1359.287119</td>\n",
       "      <td>-5422.757312</td>\n",
       "      <td>1495.019605</td>\n",
       "      <td>11534.994340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-4183.629577</td>\n",
       "      <td>-9394.633315</td>\n",
       "      <td>-4906.825439</td>\n",
       "      <td>-3644.847999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>40495.853060</td>\n",
       "      <td>14239.288260</td>\n",
       "      <td>23846.110440</td>\n",
       "      <td>40104.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-48693.635420</td>\n",
       "      <td>-18426.654830</td>\n",
       "      <td>-26242.888400</td>\n",
       "      <td>-37668.441890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-10944.898030</td>\n",
       "      <td>-20756.667670</td>\n",
       "      <td>-12965.166880</td>\n",
       "      <td>10.176988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>32939.175400</td>\n",
       "      <td>15168.580080</td>\n",
       "      <td>732.942213</td>\n",
       "      <td>32954.554260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27353.486210</td>\n",
       "      <td>9901.269701</td>\n",
       "      <td>22934.927900</td>\n",
       "      <td>29236.121130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>111990.898400</td>\n",
       "      <td>50960.179610</td>\n",
       "      <td>64674.542330</td>\n",
       "      <td>67126.293770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6189.125941</td>\n",
       "      <td>-11248.278870</td>\n",
       "      <td>-5338.128240</td>\n",
       "      <td>-2199.816088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-52074.400890</td>\n",
       "      <td>-12212.614840</td>\n",
       "      <td>-45596.060230</td>\n",
       "      <td>-23683.917960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3459.273310</td>\n",
       "      <td>10159.731350</td>\n",
       "      <td>15572.351780</td>\n",
       "      <td>12514.694430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-32129.810520</td>\n",
       "      <td>-8633.652295</td>\n",
       "      <td>-22746.960930</td>\n",
       "      <td>-18257.677510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-19155.741110</td>\n",
       "      <td>9703.933772</td>\n",
       "      <td>-4717.923539</td>\n",
       "      <td>2875.238589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-13249.944380</td>\n",
       "      <td>-12802.560670</td>\n",
       "      <td>-9178.346295</td>\n",
       "      <td>-11149.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>34308.818350</td>\n",
       "      <td>5655.558617</td>\n",
       "      <td>16077.604420</td>\n",
       "      <td>19299.431700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-3908.230134</td>\n",
       "      <td>2948.599892</td>\n",
       "      <td>7179.713491</td>\n",
       "      <td>-1928.865794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-5577.648075</td>\n",
       "      <td>-10208.265770</td>\n",
       "      <td>-20038.339830</td>\n",
       "      <td>-1250.419619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7357.574641</td>\n",
       "      <td>3239.360350</td>\n",
       "      <td>4444.309222</td>\n",
       "      <td>15035.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-79701.144780</td>\n",
       "      <td>-24990.288030</td>\n",
       "      <td>-56078.578060</td>\n",
       "      <td>-45962.744440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30980.710790</td>\n",
       "      <td>23277.976900</td>\n",
       "      <td>24527.394000</td>\n",
       "      <td>37937.372280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-3521.335346</td>\n",
       "      <td>-3659.147598</td>\n",
       "      <td>-1184.473527</td>\n",
       "      <td>5961.230159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12070.513620</td>\n",
       "      <td>2056.796378</td>\n",
       "      <td>10971.933090</td>\n",
       "      <td>13045.985070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5378.775526</td>\n",
       "      <td>-6597.714023</td>\n",
       "      <td>-1222.893081</td>\n",
       "      <td>11679.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>47219.862500</td>\n",
       "      <td>10651.254530</td>\n",
       "      <td>29008.987800</td>\n",
       "      <td>29936.236910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>51639.905420</td>\n",
       "      <td>6860.152452</td>\n",
       "      <td>26153.030100</td>\n",
       "      <td>33687.553470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>67291.997680</td>\n",
       "      <td>33351.979870</td>\n",
       "      <td>57602.046660</td>\n",
       "      <td>47564.633980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Intercept  Frontal_GM_Wscore  Occipital_GM_Wscore  Parietal_GM_Wscore  \\\n",
       "0         1.0       22609.050080          8754.361612         9947.996570   \n",
       "1         1.0       -4617.666275        -14293.557940       -10075.483750   \n",
       "2         1.0      -43204.780170        -11917.171430       -21033.593490   \n",
       "3         1.0      -62992.950400        -26002.674290       -31401.277300   \n",
       "4         1.0       23673.776970          6723.486988        19004.809470   \n",
       "5         1.0       11934.517970          9853.028019        10654.113210   \n",
       "6         1.0       33551.884340         23588.754200        30444.957830   \n",
       "7         1.0      -19491.371980         -2031.581497        -8085.494356   \n",
       "8         1.0        7993.575143         15340.672670        17928.904070   \n",
       "9         1.0       -4504.586204        -11531.125930        -3974.399514   \n",
       "10        1.0        7418.107911          2352.671971        17502.882450   \n",
       "11        1.0       -9610.575060         -2320.621156        -5627.133256   \n",
       "12        1.0      -22131.055520         -1589.832770         3779.181542   \n",
       "13        1.0       22952.131860          5533.193158        10750.348470   \n",
       "15        1.0       38388.956560         -1097.724477         9764.933678   \n",
       "16        1.0      -72302.507930        -23280.903770       -39029.713540   \n",
       "17        1.0        -486.158057            70.133499        -1238.358638   \n",
       "18        1.0      -30503.968660        -18552.667580       -23393.478660   \n",
       "19        1.0       43463.416700         20786.851500        25693.064830   \n",
       "20        1.0        8825.601778         -5282.423396         -553.201231   \n",
       "21        1.0        1359.287119         -5422.757312         1495.019605   \n",
       "22        1.0       -4183.629577         -9394.633315        -4906.825439   \n",
       "23        1.0       40495.853060         14239.288260        23846.110440   \n",
       "25        1.0      -48693.635420        -18426.654830       -26242.888400   \n",
       "26        1.0      -10944.898030        -20756.667670       -12965.166880   \n",
       "27        1.0       32939.175400         15168.580080          732.942213   \n",
       "28        1.0       27353.486210          9901.269701        22934.927900   \n",
       "29        1.0      111990.898400         50960.179610        64674.542330   \n",
       "31        1.0        6189.125941        -11248.278870        -5338.128240   \n",
       "32        1.0      -52074.400890        -12212.614840       -45596.060230   \n",
       "33        1.0        3459.273310         10159.731350        15572.351780   \n",
       "34        1.0      -32129.810520         -8633.652295       -22746.960930   \n",
       "35        1.0      -19155.741110          9703.933772        -4717.923539   \n",
       "36        1.0      -13249.944380        -12802.560670        -9178.346295   \n",
       "37        1.0       34308.818350          5655.558617        16077.604420   \n",
       "38        1.0       -3908.230134          2948.599892         7179.713491   \n",
       "39        1.0       -5577.648075        -10208.265770       -20038.339830   \n",
       "40        1.0        7357.574641          3239.360350         4444.309222   \n",
       "41        1.0      -79701.144780        -24990.288030       -56078.578060   \n",
       "42        1.0       30980.710790         23277.976900        24527.394000   \n",
       "44        1.0       -3521.335346         -3659.147598        -1184.473527   \n",
       "45        1.0       12070.513620          2056.796378        10971.933090   \n",
       "46        1.0        5378.775526         -6597.714023        -1222.893081   \n",
       "47        1.0       47219.862500         10651.254530        29008.987800   \n",
       "48        1.0       51639.905420          6860.152452        26153.030100   \n",
       "49        1.0       67291.997680         33351.979870        57602.046660   \n",
       "\n",
       "    temp_ins_GM_Wscore  \n",
       "0         19024.984090  \n",
       "1        -16210.506930  \n",
       "2          2332.992165  \n",
       "3        -29376.961170  \n",
       "4          2845.854587  \n",
       "5          4992.987068  \n",
       "6         12790.821490  \n",
       "7         -3522.327339  \n",
       "8         22101.365220  \n",
       "9         -4767.078647  \n",
       "10        11985.783540  \n",
       "11       -11105.590440  \n",
       "12        -3395.022755  \n",
       "13        19103.909200  \n",
       "15        11639.480950  \n",
       "16       -35967.266650  \n",
       "17        -4897.637297  \n",
       "18       -21433.785400  \n",
       "19        37218.073500  \n",
       "20        12122.255450  \n",
       "21        11534.994340  \n",
       "22        -3644.847999  \n",
       "23        40104.254600  \n",
       "25       -37668.441890  \n",
       "26           10.176988  \n",
       "27        32954.554260  \n",
       "28        29236.121130  \n",
       "29        67126.293770  \n",
       "31        -2199.816088  \n",
       "32       -23683.917960  \n",
       "33        12514.694430  \n",
       "34       -18257.677510  \n",
       "35         2875.238589  \n",
       "36       -11149.559000  \n",
       "37        19299.431700  \n",
       "38        -1928.865794  \n",
       "39        -1250.419619  \n",
       "40        15035.338700  \n",
       "41       -45962.744440  \n",
       "42        37937.372280  \n",
       "44         5961.230159  \n",
       "45        13045.985070  \n",
       "46        11679.951900  \n",
       "47        29936.236910  \n",
       "48        33687.553470  \n",
       "49        47564.633980  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the design matrix\n",
    "outcome_matrix, design_matrix = cal_palm.define_design_matrix(formula, data_df)\n",
    "design_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Visualize Your Dependent Variable\n",
    "\n",
    "I have generated this for you based on the formula you provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOTALMOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>18.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>12.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>22.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>26.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>7.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TOTALMOD\n",
       "0       6.00\n",
       "1      13.00\n",
       "2      35.00\n",
       "3      17.67\n",
       "4       5.00\n",
       "5       4.33\n",
       "6       7.00\n",
       "7      22.00\n",
       "8      14.33\n",
       "9      14.00\n",
       "10     23.00\n",
       "11     14.67\n",
       "12     27.00\n",
       "13     17.67\n",
       "15     13.33\n",
       "16      9.00\n",
       "17     20.00\n",
       "18     12.00\n",
       "19      6.00\n",
       "20     12.33\n",
       "21     16.00\n",
       "22      4.00\n",
       "23     13.67\n",
       "25     21.67\n",
       "26     17.00\n",
       "27      0.00\n",
       "28      4.00\n",
       "29     23.00\n",
       "31     28.00\n",
       "32     19.00\n",
       "33      6.00\n",
       "34     11.00\n",
       "35     18.33\n",
       "36     15.00\n",
       "37     14.00\n",
       "38     12.67\n",
       "39     16.67\n",
       "40     10.33\n",
       "41     20.67\n",
       "42     22.33\n",
       "44     13.00\n",
       "45     32.00\n",
       "46     26.00\n",
       "47     14.67\n",
       "48      7.33\n",
       "49      8.00"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Generate Contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a Contrast Matrix\n",
    "- This is different from the contrast matrices used in cell-means regressions such as in PALM, but it is much more powerful. \n",
    "\n",
    "\n",
    "\n",
    "For more information on contrast matrices, please refer to this: https://cran.r-project.org/web/packages/codingMatrices/vignettes/codingMatrices.pdf\n",
    "\n",
    "Generally, these drastically effect the results of ANOVA. However, they are mereley a nuisance for a regression.\n",
    "In essence, they assess the coefficients of a given\n",
    "\n",
    "________________________________________________________________\n",
    "A coding matrix (a contrast matrix if it sums to zero) is simply a way of defining what coefficients to evaluate and how to evaluate them. \n",
    "If a coefficient is set to 1 and everything else is set to zero, we are taking the mean of the coefficient's means and assessing if they significantly\n",
    "deviate from zero--IE we are checking if it had a significant impact on the ability to predict the depdendent variable.\n",
    "If a coefficient is set to 1, another is -1, and others are 0, we are assessing how the means of the two coefficients deviate from eachother. \n",
    "If several coefficients are 1 and several others are -1, we are assessing how the group-level means of the two coefficients deviate from eachother.\n",
    "If a group of coefficients are 1, a group is -1, and a group is 0, we are only assessing how the groups +1 and -1 have differing means. \n",
    "\n",
    "1: This value indicates that the corresponding variable's coefficient in the model is included in the contrast. It means you are interested in estimating the effect of that variable.\n",
    "\n",
    "0: This value indicates that the corresponding variable's coefficient in the model is not included in the contrast. It means you are not interested in estimating the effect of that variable.\n",
    "\n",
    "-1: This value indicates that the corresponding variable's coefficient in the model is included in the contrast, but with an opposite sign. It means you are interested in estimating the negative effect of that variable.\n",
    "\n",
    "----------------------------------------------------------------\n",
    "The contrast matrix is typically a matrix with dimensions (number of contrasts) x (number of regression coefficients). Each row of the contrast matrix represents a contrast or comparison you want to test.\n",
    "\n",
    "For example, let's say you have the following regression coefficients in your model:\n",
    "\n",
    "Intercept, Age, connectivity, Age_interaction_connectivity\n",
    "A contrast matric has dimensions of [n_predictors, n_experiments] where each experiment is a contrast\n",
    "\n",
    "If you want to test the hypothesis that the effect of Age is significant, you can set up a contrast matrix with a row that specifies this contrast (actually an averaging vector):\n",
    "```\n",
    "[0,1,0,0]. This is an averaging vector because it sums to 1\n",
    "```\n",
    "This contrast will test the coefficient corresponding to the Age variable against zero.\n",
    "\n",
    "\n",
    "If you want to test the hypothesis that the effect of Age is different from the effect of connectivity, you can set up a contrast matrix with two rows:\n",
    "```\n",
    "[0,1,−1,0]. This is a contrast because it sums to 0\n",
    "```\n",
    "\n",
    "Thus, if you want to see if any given effect is significant compared to the intercept (average), you can use the following contrast matrix:\n",
    "```\n",
    "[1,0,0,0]\n",
    "[-1,1,0,0]\n",
    "[-1,0,1,0]\n",
    "[-1,0,0,1] actually a coding matrix of averaging vectors\n",
    "```\n",
    "\n",
    "The first row tests the coefficient for Age against zero, and the second row tests the coefficient for connectivity against zero. The difference between the two coefficients can then be assessed.\n",
    "_____\n",
    "You can define any number of contrasts in the contrast matrix to test different hypotheses or comparisons of interest in your regression analysis.\n",
    "\n",
    "It's important to note that the specific contrasts you choose depend on your research questions and hypotheses. You should carefully consider the comparisons you want to make and design the contrast matrix accordingly.\n",
    "\n",
    "- Examples:\n",
    "    - [Two Sample T-Test](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM#Two-Group_Difference_.28Two-Sample_Unpaired_T-Test.29)\n",
    "    - [One Sample with Covariate](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM#Single-Group_Average_with_Additional_Covariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a basic contrast matrix set up to evaluate the significance of each variable.\n",
      "Here is an example of what your contrast matrix looks like as a dataframe: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Frontal_GM_Wscore</th>\n",
       "      <th>Occipital_GM_Wscore</th>\n",
       "      <th>Parietal_GM_Wscore</th>\n",
       "      <th>temp_ins_GM_Wscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept  Frontal_GM_Wscore  Occipital_GM_Wscore  Parietal_GM_Wscore  \\\n",
       "0          1                  0                    0                   0   \n",
       "1          0                  1                    0                   0   \n",
       "2          0                  0                    1                   0   \n",
       "3          0                  0                    0                   1   \n",
       "4          0                  0                    0                   0   \n",
       "\n",
       "   temp_ins_GM_Wscore  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the same contrast matrix, but as an array.\n",
      "Copy it into a cell below and edit it for more control over your analysis.\n",
      "[\n",
      "    [1, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 0],\n",
      "    [0, 0, 1, 0, 0],\n",
      "    [0, 0, 0, 1, 0],\n",
      "    [0, 0, 0, 0, 1],\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "contrast_matrix = cal_palm.generate_basic_contrast_matrix(design_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit Contrast Matrix Here\n",
    "- The generic contrast matrix will simply check if your Betas are significantly different from the intercept (average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptrn = oldhigh oldlow younghigh younglow\n",
    "# contrast_matrix = [\n",
    "#     [1, -1, 0, 0],\n",
    "#     [0, 0, 1, -1]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalize Contrast Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Frontal_GM_Wscore</th>\n",
       "      <th>Occipital_GM_Wscore</th>\n",
       "      <th>Parietal_GM_Wscore</th>\n",
       "      <th>temp_ins_GM_Wscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept  Frontal_GM_Wscore  Occipital_GM_Wscore  Parietal_GM_Wscore  \\\n",
       "0          1                  0                    0                   0   \n",
       "1          0                  1                    0                   0   \n",
       "2          0                  0                    1                   0   \n",
       "3          0                  0                    0                   1   \n",
       "4          0                  0                    0                   0   \n",
       "\n",
       "   temp_ins_GM_Wscore  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   1  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_matrix_df = cal_palm.finalize_contrast_matrix(design_matrix=design_matrix, \n",
    "                                                    contrast_matrix=contrast_matrix) \n",
    "contrast_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Define Exchangeability Blocks (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional - Exchangability Blocks\n",
    "- This is optional and for when you are doing a meta-analysis\n",
    "- Not yet implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is just an example, you will have to edit to adapt to your data, \n",
    "### but it should be integers, starting with 1,2,3....\n",
    "\n",
    "# coding_key = {\"Prosopagnosia_w_Yeo1000\": 1,\n",
    "#              \"Corbetta_Lesions\": 1,\n",
    "#              \"DBS_dataset\": 2\n",
    "#              }\n",
    "\n",
    "# eb_matrix = pd.DataFrame()\n",
    "# eb_matrix = clean_df['dataset'].replace(coding_key)\n",
    "# display(eb_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Run the Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Results Are Displayed Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Results: Ordinary least squares\n",
      "===================================================================\n",
      "Model:                OLS              Adj. R-squared:     0.033   \n",
      "Dependent Variable:   TOTALMOD         AIC:                322.2207\n",
      "Date:                 2024-04-25 10:57 BIC:                331.3639\n",
      "No. Observations:     46               Log-Likelihood:     -156.11 \n",
      "Df Model:             4                F-statistic:        1.385   \n",
      "Df Residuals:         41               Prob (F-statistic): 0.256   \n",
      "R-squared:            0.119            Scale:              58.242  \n",
      "-------------------------------------------------------------------\n",
      "                     Coef.  Std.Err.    t    P>|t|   [0.025  0.975]\n",
      "-------------------------------------------------------------------\n",
      "Intercept           14.1426   1.3199 10.7145 0.0000 11.4769 16.8083\n",
      "Frontal_GM_Wscore   -0.0002   0.0001 -1.6246 0.1119 -0.0004  0.0000\n",
      "Occipital_GM_Wscore -0.0002   0.0002 -0.8907 0.3783 -0.0005  0.0002\n",
      "Parietal_GM_Wscore   0.0001   0.0002  0.5750 0.5684 -0.0002  0.0004\n",
      "temp_ins_GM_Wscore   0.0002   0.0001  1.3874 0.1728 -0.0001  0.0005\n",
      "-------------------------------------------------------------------\n",
      "Omnibus:                1.825        Durbin-Watson:           1.656\n",
      "Prob(Omnibus):          0.402        Jarque-Bera (JB):        1.621\n",
      "Skew:                   0.447        Prob(JB):                0.445\n",
      "Kurtosis:               2.787        Condition No.:           58724\n",
      "===================================================================\n",
      "* The condition number is large (6e+04). This might indicate\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "# Fit the regression model\n",
    "model = sm.OLS(outcome_matrix, design_matrix)\n",
    "results = model.fit()\n",
    "print(results.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6220932405998316"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sqrt(.387)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the Regression as a Forest Plot\n",
    "- This will probably look poor if you ran a regression without standardizing your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /Users/cu135/Dropbox (Partners HealthCare)/studies/atrophy_seeds_2023/Figures/correlation_to_baseline_scores/regression_to_baseline as regression_forest_plot.svg and .png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import ForestPlot\n",
    "forest = ForestPlot(model=results, sig_digits=2, out_dir=out_dir, table=False)\n",
    "forest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize The Model's Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import model_diagnostics\n",
    "model_diagnostics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the Partial Regression Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import PartialRegressionPlot\n",
    "partial_plot = PartialRegressionPlot(model=results, design_matrix=design_matrix, out_dir=out_dir, palette='Reds')\n",
    "partial_plot = partial_plot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Run the Contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast Results Are Displayed Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_results = results.t_test(contrast_matrix_df)\n",
    "print(contrast_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Compare the Coefficient Between 2 Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_column = 'Age_Group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import RegressionAnalysis\n",
    "regression_test = RegressionAnalysis(outcome_df=outcome_matrix, design_df=design_matrix, groups_df=data_df[[groups_column]], N=10000, metric='similarity', two_tail=False, out_dir=out_dir)\n",
    "regression_test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - Compare Distribution of T Values Between Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_together=False\n",
    "groups_column = 'Age_Group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import BootstrappedRegressionAnalysis\n",
    "\n",
    "# Create an instance of BootstrappedRegressionAnalysis\n",
    "bootstrapped_regression_test = BootstrappedRegressionAnalysis(outcome_df=outcome_matrix, design_df=design_matrix, groups_df=data_df[[groups_column]], N=10000, out_dir=out_dir, plot_together=plot_together)\n",
    "\n",
    "# Run the bootstrapped regression analysis\n",
    "df1 = bootstrapped_regression_test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Predict Another Dataframe\n",
    "- Can use this to predict data from a second group, such as the 'other_df' defined in \"Step 01, Drop Rows Based on Value of a Column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "def calculate_ssr(observations, predictions):\n",
    "    \"\"\"\n",
    "    Calculate the regression sum of squares.\n",
    "    This is the sum of squared deviations, with deviation being Y_hat_i - Y_bar\n",
    "    \n",
    "    SSR is a measure used to quantify the variance in the observed data that is not explained by the model. \n",
    "    It is calculated as the sum of the squares of the differences between the observed values and the model's predictions. \n",
    "    The more the 'mean' predicts \n",
    "    A lower SSR indicates a better model fit, meaning the model's predictions are closer to the actual observations.\n",
    "    \n",
    "    SSR = Σ(y_hat - y_bar)^2 \n",
    "    \n",
    "    Parameters:\n",
    "    - observations (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated SSR.\n",
    "    \"\"\"\n",
    "    y_hat = observations\n",
    "    y_bar = np.mean(predictions)\n",
    "    ssr = np.sum((y_hat - y_bar) ** 2)\n",
    "    return ssr\n",
    "\n",
    "def calculate_sse(observations, predictions):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squares due to error (SSE).\n",
    "    \n",
    "    SSE is a measure of the total deviation of the response values from the fit to the response values. \n",
    "    It is calculated as the sum of the squares of the differences between the predicted values and the observed values. \n",
    "    A lower SSE indicates a model that more accurately fits the data.\n",
    "    \n",
    "    SSE = Σ(y - y_hat)^2\n",
    "    \n",
    "    Parameters:\n",
    "    - observations (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated SSE.\n",
    "    \"\"\"\n",
    "    y = observations\n",
    "    y_hat = predictions\n",
    "    sse = np.sum((y - y_hat) ** 2)\n",
    "    return sse\n",
    "\n",
    "def calculate_ssto(observations):\n",
    "    \"\"\"\n",
    "    Calculate the total sum of squares (SSTO).\n",
    "    \n",
    "    SSTO is a measure of the total variance in the observed data and is used as a comparative tool for model evaluation. \n",
    "    It is calculated as the sum of the squares of the differences between the observed values and their overall mean. \n",
    "    SSTO is used in the denominator of the coefficient of determination, R^2, which assesses the fit of the model.\n",
    "    \n",
    "    SSTO = Σ(y - y_bar)^2\n",
    "    \n",
    "    Parameters:\n",
    "    - observations (array-like): The actual observed outcomes.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated SSTO.\n",
    "    \"\"\"\n",
    "    y = observations\n",
    "    y_bar = np.mean(y)\n",
    "    ssto = np.sum((y - y_bar) ** 2)\n",
    "    return ssto\n",
    "\n",
    "\n",
    "def calculate_msr(ssr, num_regressors):\n",
    "    \"\"\"\n",
    "    Calculate the mean square due to regression (MSR).\n",
    "    \n",
    "    MSR is a measure of the variation explained by the independent variables in the model. It is calculated as the \n",
    "    sum of squared residuals (SSR) divided by the degrees of freedom, which is the number of independent variables (regressors) minus one.\n",
    "    A higher MSR indicates that the model explains a greater amount of variation in the outcome variable.\n",
    "    \n",
    "    MSR = SSR / (number of regressors - 1)\n",
    "    \n",
    "    Parameters:\n",
    "    - ssr (float): The sum of squared residuals from the regression model.\n",
    "    - num_regressors (int): The number of independent variables in the model.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated MSR.\n",
    "    \"\"\"\n",
    "    return ssr / (num_regressors - 1)\n",
    "\n",
    "def calculate_mse(sse, num_regressors, num_observations):\n",
    "    \"\"\"\n",
    "    Calculate the mean square error (MSE).\n",
    "    \n",
    "    MSE is a measure of the average of the squares of the errors, that is, the average squared difference between the observed actual outcomes and the outcomes predicted by the model. It is calculated as the sum of squared errors (SSE) divided by the degrees of freedom, which is the number of observations minus the number of regressors.\n",
    "    A lower MSE indicates a better fit of the model to the data.\n",
    "    \n",
    "    MSE = SSE / (number of observations - number of regressors)\n",
    "    \n",
    "    Parameters:\n",
    "    - sse (float): The sum of squared errors from the regression model.\n",
    "    - num_regressors (int): The number of independent variables in the model.\n",
    "    - num_observations (int): The number of observations in the data set.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated MSE.\n",
    "    \"\"\"\n",
    "    return sse / (num_observations - num_regressors)\n",
    "\n",
    "def calculate_f_stat(msr, mse, num_regressors, num_observations):\n",
    "    \"\"\"\n",
    "    Calculate the F-statistic.\n",
    "    \n",
    "    The F-statistic is used to compare statistical models that have been fitted to a data set in order to identify the model that best fits the population from which the data were sampled. It is the ratio of the mean square due to regression (MSR) to the mean square error (MSE).\n",
    "    \n",
    "    The F-statistic follows the F-distribution under the null hypothesis that the model with no independent variables fits the data as well as your model. A higher F-statistic implies that the null hypothesis is false, and your model adds value in explaining the variation in the data.\n",
    "    \n",
    "    F = MSR / MSE\n",
    "    \n",
    "    Parameters:\n",
    "    - msr (float): The mean square due to regression.\n",
    "    - mse (float): The mean square error.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated F-statistic.\n",
    "    - float: The p-value from the F-distribution.\n",
    "    \"\"\"\n",
    "    f_stat = msr / mse\n",
    "    # The degrees of freedom for the numerator (dfn) is the number of independent variables (regressors).\n",
    "    # The degrees of freedom for the denominator (dfd) is the total number of observations minus the number of independent variables minus 1.\n",
    "    # These values need to be defined or calculated outside of this function.\n",
    "    dfn = num_regressors - 1\n",
    "    dfd = num_observations - num_regressors \n",
    "    p_value = f.sf(f_stat, dfn, dfd)\n",
    "    return f_stat, p_value\n",
    "\n",
    "def run_goodness_of_fit(target_outcome_matrix, predictions, target_design_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the F-statistic and p-value for a linear regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - target_outcome_matrix (array-like): The actual observed outcomes (Y_actual).\n",
    "    - predictions (array-like): The outcomes predicted by the model (Y_hat).\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated F-statistic.\n",
    "    - float: The p-value from the F-distribution.\n",
    "    \"\"\"\n",
    "    # Calculate the regression sum of squares (SSR).\n",
    "    ssr = calculate_ssr(target_outcome_matrix, predictions)\n",
    "\n",
    "    # Calculate the sum of squares due to error (SSE).\n",
    "    sse = calculate_sse(target_outcome_matrix, predictions)\n",
    "\n",
    "    # Calculate the number of regressors and observations.\n",
    "    num_regressors = target_design_matrix.shape[1]\n",
    "    num_observations = len(target_outcome_matrix)\n",
    "\n",
    "    # Calculate the mean square due to regression (MSR).\n",
    "    msr = calculate_msr(ssr, num_regressors)\n",
    "\n",
    "    # Calculate the mean square error (MSE).\n",
    "    mse = calculate_mse(sse, num_regressors, num_observations)\n",
    "\n",
    "    # Calculate the F-statistic and p-value.\n",
    "    f_stat, p_value = calculate_f_stat(msr, mse, num_regressors, num_observations)\n",
    "\n",
    "    return f_stat, p_value\n",
    "\n",
    "def calculate_r_squared(observations, predictions):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared (coefficient of determination) value.\n",
    "\n",
    "    R-squared measures the proportion of the variance in the observed outcomes that is explained by the predictions.\n",
    "\n",
    "    R-squared = 1 - (SSE / SSTO)\n",
    "\n",
    "    Parameters:\n",
    "    - observations (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated R-squared value.\n",
    "    \"\"\"\n",
    "    # Calculate SSE (Sum of Squares of Errors)\n",
    "    sse = np.sum((observations - predictions) ** 2)\n",
    "\n",
    "    # Calculate SSTO (Total Sum of Squares)\n",
    "    y_mean = np.mean(observations)\n",
    "    ssto = np.sum((observations - y_mean) ** 2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r_squared = 1 - (sse / ssto)\n",
    "    \n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the design matrix\n",
    "target_outcome_matrix, target_design_matrix = cal_palm.define_design_matrix(formula, data_df)\n",
    "predictions = results.predict(target_design_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract F-Test for Goodness of Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_stat, p_value = run_goodness_of_fit(target_outcome_matrix=target_outcome_matrix.to_numpy().flatten(), \n",
    "                    predictions=predictions.to_numpy().flatten(), \n",
    "                    target_design_matrix=target_design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract R-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared = calculate_r_squared(target_outcome_matrix.to_numpy().flatten(), predictions.to_numpy().flatten())\n",
    "print(f\"R-squared: {r_squared:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(target_outcome_matrix, predictions)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(target_outcome_matrix, predictions)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot The Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_scatter_with_f_stat_in_title(target_outcome_matrix, predictions, f_stat, p_value, out_dir=None):\n",
    "    \"\"\"\n",
    "    Create a scatterplot of predicted vs. observed values with F-statistic and p-value in the title.\n",
    "\n",
    "    Parameters:\n",
    "    - target_outcome_matrix (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "    - f_stat (float): The F-statistic value.\n",
    "    - p_value (float): The p-value.\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "    # Create a scatterplot using Seaborn\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=predictions,\n",
    "                    y=target_outcome_matrix,\n",
    "                    label='Predicted vs. Observed')\n",
    "\n",
    "    # Add a diagonal line\n",
    "    xlim = plt.xlim()  # Get current X-axis limits\n",
    "    ylim = plt.ylim()  # Get current Y-axis limits\n",
    "    min_limit = min(xlim[0], ylim[0])\n",
    "    max_limit = max(xlim[1], ylim[1])\n",
    "    plt.plot([min_limit, max_limit], [min_limit, max_limit], linestyle='--', color='gray', label='Perfect Fit')\n",
    "\n",
    "    # Add labels and title with F-statistic and p-value\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Observed Values')\n",
    "    plt.title(f'Scatterplot of Predicted vs. Observed Values\\nF-statistic: {f_stat:.2f}, p-value: {p_value:.4f}')\n",
    "\n",
    "    # Set axis limits\n",
    "    plt.xlim(min_limit, max_limit)\n",
    "    plt.ylim(min_limit, max_limit)\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "    \n",
    "    if out_dir:\n",
    "        # Save the figure\n",
    "        plt.savefig(f\"{out_dir}/predicted_plot.png\", bbox_inches='tight')\n",
    "        plt.savefig(f\"{out_dir}/predicted_plot.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/predicted_plot.svg')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals(target_outcome_matrix, predictions, f_stat, p_value, out_dir=None):\n",
    "    \"\"\"\n",
    "    Create a scatterplot of residuals with F-statistic and p-value in the title and save it.\n",
    "\n",
    "    Parameters:\n",
    "    - target_outcome_matrix (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "    - f_stat (float): The F-statistic value.\n",
    "    - p_value (float): The p-value.\n",
    "    - out_dir (str, optional): The directory to save the plot. If None, the plot won't be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "    # Calculate residuals\n",
    "    residuals = target_outcome_matrix - predictions\n",
    "\n",
    "    # Create a scatterplot of residuals\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=predictions,\n",
    "                    y=residuals,\n",
    "                    label='Residuals vs. Predicted')\n",
    "\n",
    "    # Calculate y-axis limits\n",
    "    y_lim_min = min(-3, np.min(residuals) - 0.5)\n",
    "    y_lim_max = max(3, np.max(residuals) + 0.5)\n",
    "\n",
    "    # Set y-axis limits\n",
    "    plt.ylim(y_lim_min, y_lim_max)\n",
    "\n",
    "    # Add a horizontal line at y=0\n",
    "    plt.axhline(0, color='gray', linestyle='--', label='Zero Residual')\n",
    "\n",
    "    # Add labels and title with F-statistic and p-value\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'Scatterplot of Residuals vs. Predicted Values\\nF-statistic: {f_stat:.2f}, p-value: {p_value:.4f}')\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "\n",
    "    if out_dir:\n",
    "        # Save the figure\n",
    "        plt.savefig(f\"{out_dir}/residuals_plot.png\", bbox_inches='tight')\n",
    "        plt.savefig(f\"{out_dir}/residuals_plot.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/residuals_plot.svg')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with saving:\n",
    "# plot_residuals(target_outcome_matrix.to_numpy().flatten(), predictions.to_numpy().flatten(), f_stat, p_value, out_dir=\"your_output_directory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(target_outcome_matrix.to_numpy().flatten(), predictions.to_numpy().flatten(), f_stat, p_value, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_with_f_stat_in_title(target_outcome_matrix.to_numpy().flatten(), predictions.to_numpy().flatten(), f_stat, p_value, out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Visualize an ANCOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = 'Subiculum_Group_By_24'\n",
    "group_2  ='Age_Group'\n",
    "outcome_column = 'Z_Scored_Percent_Cognitive_Improvement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_outcomes(model, design_matrix):\n",
    "    \"\"\"\n",
    "    Predicts the outcomes for each row in the data DataFrame based on the given model and formula.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The fitted OLS model.\n",
    "    - data_df: DataFrame containing the data for prediction.\n",
    "\n",
    "    Returns:\n",
    "    - predictions: A DataFrame with predicted values for each row in the data_df.\n",
    "    \"\"\"\n",
    "    # Add a column for predictions to the data_df\n",
    "    \n",
    "    return model.predict(design_matrix)\n",
    "\n",
    "def calculate_average_predictions(data_df, predictions, group1_column, group2_column):\n",
    "    \"\"\"\n",
    "    Calculates average predictions for each combination of unique values in Group 1 and Group 2.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the 'Group 1' and 'Group 2' columns.\n",
    "    - predictions: Series containing the predicted values.\n",
    "    - group1_column: Name of the Group 1 column in data_df.\n",
    "    - group2_column: Name of the Group 2 column in data_df.\n",
    "\n",
    "    Returns:\n",
    "    - average_predictions_df: DataFrame with 'Group 1', 'Group 2', and 'Average Prediction' columns.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame to store the results\n",
    "    average_predictions_df = pd.DataFrame(columns=['Group 1', 'Group 2', 'Average Prediction'])\n",
    "\n",
    "    # Get unique values in Group 1 and Group 2\n",
    "    unique_group1_values = data_df[group1_column].unique()\n",
    "    unique_group2_values = data_df[group2_column].unique()\n",
    "\n",
    "    # Loop through unique values of Group 1 and Group 2\n",
    "    for group1_value in unique_group1_values:\n",
    "        for group2_value in unique_group2_values:\n",
    "            # Filter predictions based on the indices where Group 1 and Group 2 match\n",
    "            filtered_predictions = predictions[(data_df[group1_column] == group1_value) & (data_df[group2_column] == group2_value)]\n",
    "\n",
    "            # Calculate the average prediction for this combination\n",
    "            average_prediction = filtered_predictions.mean()\n",
    "\n",
    "            # Add the result to the DataFrame\n",
    "            average_predictions_df = average_predictions_df.append({'Group 1': group1_value, 'Group 2': group2_value, 'Average Prediction': average_prediction}, ignore_index=True)\n",
    "\n",
    "    return average_predictions_df\n",
    "\n",
    "def plot_grouped_barplot(data_df, out_dir):\n",
    "    \"\"\"\n",
    "    Plots a grouped barplot with 'Group 2' on the x-axis and 'Group 1' bars side-by-side.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the data for plotting.\n",
    "    \"\"\"\n",
    "    # Set the style for the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Create the barplot\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.barplot(x='Group 2', y='Average Prediction', hue='Group 1', data=data_df)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Group 2')\n",
    "    plt.ylabel('Average Prediction')\n",
    "    plt.title('Grouped Barplot of Average Predictions')\n",
    "\n",
    "    # Show the legend\n",
    "    plt.legend(title='Group 1', loc='upper right')\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"{out_dir}/estimated_marginal_mean.png\", bbox_inches='tight')\n",
    "    plt.savefig(f\"{out_dir}/estimated_marginal_mean.svg\", bbox_inches='tight')\n",
    "    print(f'Saved to {out_dir}/estimated_marginal_mean.svg')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "def calculate_average_actual(data_df, group1_column, group2_column, outcome_column):\n",
    "    \"\"\"\n",
    "    Calculates average actual outcomes for each combination of unique values in Group 1 and Group 2.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the data.\n",
    "    - group1_column: Name of the Group 1 column in data_df.\n",
    "    - group2_column: Name of the Group 2 column in data_df.\n",
    "    - outcome_column: Name of the column containing the actual outcomes.\n",
    "\n",
    "    Returns:\n",
    "    - average_actual_df: DataFrame with 'Group 1', 'Group 2', and 'Average Actual' columns.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame to store the results\n",
    "    average_actual_df = pd.DataFrame(columns=['Group 1', 'Group 2', 'Average Actual'])\n",
    "\n",
    "    # Get unique values in Group 1 and Group 2\n",
    "    unique_group1_values = data_df[group1_column].unique()\n",
    "    unique_group2_values = data_df[group2_column].unique()\n",
    "\n",
    "    # Loop through unique values of Group 1 and Group 2\n",
    "    for group1_value in unique_group1_values:\n",
    "        for group2_value in unique_group2_values:\n",
    "            # Filter data based on the indices where Group 1 and Group 2 match\n",
    "            filtered_data = data_df[(data_df[group1_column] == group1_value) & (data_df[group2_column] == group2_value)]\n",
    "\n",
    "            # Calculate the average actual outcome for this combination\n",
    "            average_actual = filtered_data[outcome_column].mean()\n",
    "\n",
    "            # Add the result to the DataFrame\n",
    "            average_actual_df = average_actual_df.append({'Group 1': group1_value, 'Group 2': group2_value, 'Average Actual': average_actual}, ignore_index=True)\n",
    "\n",
    "    return average_actual_df\n",
    "\n",
    "\n",
    "def plot_grouped_barplot_actual(data_df, out_dir):\n",
    "    \"\"\"\n",
    "    Plots a grouped barplot with 'Group 2' on the x-axis and 'Group 1' bars side-by-side for average actual outcomes.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the data for plotting.\n",
    "    \"\"\"\n",
    "    # Set the style for the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Create the barplot\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.barplot(x='Group 2', y='Average Actual', hue='Group 1', data=data_df)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Group 2')\n",
    "    plt.ylabel('Average Actual Outcome')\n",
    "    plt.title('Grouped Barplot of Average Actual Outcomes')\n",
    "\n",
    "    # Show the legend\n",
    "    plt.legend(title='Group 1', loc='upper right')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    out_dir = out_dir # Replace with your output directory\n",
    "    plt.savefig(f\"{out_dir}/actual_marginal_mean.png\", bbox_inches='tight')\n",
    "    plt.savefig(f\"{out_dir}/actual_marginal_mean.svg\", bbox_inches='tight')\n",
    "    print(f'Saved to {out_dir}/actual_marginal_mean.svg')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_grouped_error_bar_actual(data_df, group1_column, group2_column, outcome_column, out_dir):\n",
    "    \"\"\"\n",
    "    Plots a grouped error bar plot with 'Group 2' on the x-axis and 'Group 1' bars side-by-side for average actual outcomes with SEM.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the data for plotting.\n",
    "    - group1_column: Name of the Group 1 column in data_df.\n",
    "    - group2_column: Name of the Group 2 column in data_df.\n",
    "    - outcome_column: Name of the column containing the actual outcomes.\n",
    "    - out_dir: Directory where the plots will be saved.\n",
    "    \"\"\"\n",
    "    # Set the style for the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Calculate means and SEM\n",
    "    summary_df = data_df.groupby([group2_column, group1_column])[outcome_column].agg(['mean', 'sem']).reset_index()\n",
    "    summary_df.columns = [group2_column, group1_column, 'Average', 'SEM']\n",
    "\n",
    "    # Create the error bar plot\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.barplot(x=group2_column, y='Average', hue=group1_column, data=summary_df, ci=None, palette=\"muted\")\n",
    "    \n",
    "    # Add error bars\n",
    "    for i, row in summary_df.iterrows():\n",
    "        plt.errorbar(x=i, y=row['Average'], yerr=row['SEM'], fmt='none', capsize=5, color='black')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(group2_column)\n",
    "    plt.ylabel('Average Actual Outcome')\n",
    "    plt.title('Grouped Error Bar Plot of Average Actual Outcomes')\n",
    "\n",
    "    # Show the legend\n",
    "    plt.legend(title=group1_column, loc='upper right')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    if out_dir:\n",
    "        plt.savefig(f\"{out_dir}/actual_means_error_bars.png\", bbox_inches='tight')\n",
    "        plt.savefig(f\"{out_dir}/actual_means_error_bars.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/actual_means_error_bars.svg')\n",
    "\n",
    "def run_ancova_emm(model, df, design_matrix, group_1, group_2, outcome_column, out_dir=None):\n",
    "    \"\"\"\n",
    "    Executes the ANCOVA EMM analysis pipeline, generating predictions based on a given model and plotting \n",
    "    both the average predicted and actual outcomes for specified groupings within the data.\n",
    "\n",
    "    This function serves as a wrapper to streamline the process of predicting outcomes using the model,\n",
    "    calculating average predictions and actual outcomes for specified group categories, and then plotting\n",
    "    these averages for visual comparison.\n",
    "    \n",
    "    This is a type 1 EMM, which is techncially more accurate. It makes predictions on each observation, then averages them together. \n",
    "\n",
    "    Parameters:\n",
    "    - model: A fitted statistical model object that has a predict method. This model is used to generate \n",
    "             outcome predictions based on the design matrix provided.\n",
    "    - design_matrix: DataFrame, the design matrix containing data for prediction. This includes both \n",
    "                     the group categorization columns and any other predictors required by the model.\n",
    "    - group_1: String, the name of the first grouping variable in the design matrix. This categorizes data \n",
    "               into different groups for which EMMs are to be calculated.\n",
    "    - group_2: String, the name of the second grouping variable in the design matrix, used in conjunction \n",
    "               with group_1 to further categorize data.\n",
    "    - outcome_column: String, the name of the column in the design matrix that contains the actual outcome \n",
    "                      values. These are used for calculating and plotting actual means.\n",
    "    - out_dir: String, optional, the directory path where the plots will be saved. If None, plots will \n",
    "               not be saved to files.\n",
    "\n",
    "    Outputs:\n",
    "    The function generates two plots: one for the average predictions and another for the average actual \n",
    "    outcomes, grouped by the specified categorization variables. Optionally, these plots are saved to the \n",
    "    specified output directory.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    predictions = predict_outcomes(model=model, data_df=design_matrix)\n",
    "    average_predictions_df = calculate_average_predictions(df, predictions, group_1, group_2)\n",
    "    print(\"Estimated Marginal means below\")\n",
    "    plot_grouped_barplot(average_predictions_df, out_dir=out_dir)\n",
    "    print('Actual means below')\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_means_and_sem(data_df, group1_column, group2_column, outcome_column):\n",
    "    \"\"\"\n",
    "    Calculate means and standard errors of the mean for specified groupings.\n",
    "    \"\"\"\n",
    "    # Group data and calculate mean, standard deviation, and count of samples\n",
    "    summary_df = data_df.groupby([group2_column, group1_column])[outcome_column].agg(['mean', 'std', 'count']).reset_index()\n",
    "    summary_df.columns = [group2_column, group1_column, 'Average', 'STD', 'Count']\n",
    "\n",
    "    # Calculate SEM only for groups with more than one data point\n",
    "    summary_df['SEM'] = summary_df.apply(lambda row: row['STD'] / np.sqrt(row['Count']) if row['Count'] > 1 else np.nan, axis=1)\n",
    "\n",
    "    # Drop any rows with NaN values in 'Average'\n",
    "    summary_df.dropna(subset=['Average'], inplace=True)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def plot_interaction_error_bar(summary_df, group1_column, group2_column, out_dir=None):\n",
    "    \"\"\"\n",
    "    Plots an interaction error bar plot showing the relationship between two factors with SEM.\n",
    "    \"\"\"\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Unique categories in the second grouping variable\n",
    "    group2_categories = summary_df[group2_column].unique()\n",
    "\n",
    "    # Colors for each line\n",
    "    colors = sns.color_palette(\"tab10\", len(group2_categories))\n",
    "\n",
    "    for idx, category in enumerate(group2_categories):\n",
    "        # Subset for each category of group 2\n",
    "        subset = summary_df[summary_df[group2_column] == category]\n",
    "        # Plot points. If SEM is NaN, error bars are not drawn.\n",
    "        plt.errorbar(subset[group1_column], subset['Average'], capsize=6, yerr=subset['SEM'], fmt='-o', label=category, color=colors[idx])\n",
    "\n",
    "    plt.xlabel(group1_column)\n",
    "    plt.ylabel('Average Outcome')\n",
    "    plt.title('Interaction Plot of Average Outcomes with SEM')\n",
    "    plt.legend(title=group2_column, loc='upper right')\n",
    "    plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    # Save the figure if a directory is provided\n",
    "    if out_dir:\n",
    "        plt.savefig(f\"{out_dir}/interaction_means_error_bars.png\", bbox_inches='tight')\n",
    "        plt.savefig(f\"{out_dir}/interaction_means_error_bars.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/interaction_means_error_bars.svg')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = calculate_means_and_sem(data_df, group_1, group_2, outcome_column)\n",
    "\n",
    "# Plot\n",
    "plot_interaction_error_bar(summary_df, group_1, group_2, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "import pandas as pd\n",
    "def diagnose_data(data_df, group1_column, group2_column, outcome_column):\n",
    "    group2_categories = data_df[group2_column].unique()\n",
    "\n",
    "    for category in group2_categories:\n",
    "        subset = data_df[data_df[group2_column] == category]\n",
    "        print(f\"Category: {category}\")\n",
    "        groups = [subset[subset[group1_column] == group][outcome_column].values for group in subset[group1_column].unique()]\n",
    "        \n",
    "        for i, group in enumerate(subset[group1_column].unique()):\n",
    "            print(f\"  Group {group} size: {len(groups[i])}, values: {groups[i]}\")\n",
    "            \n",
    "        if any(len(group) <= 1 for group in groups):\n",
    "            print(\"  Issue: One or more groups have insufficient data points.\")\n",
    "        elif all(np.all(group == groups[0][0]) for group in groups):\n",
    "            print(\"  Issue: All values in the groups are the same.\")\n",
    "\n",
    "from scipy.stats import kruskal\n",
    "import pandas as pd\n",
    "\n",
    "def perform_kruskal_wallis_test(data_df, group1_column, group2_column, outcome_column):\n",
    "    \"\"\"\n",
    "    Performs the Kruskal-Wallis test to compare the distribution of outcome values across\n",
    "    multiple groups defined by group1_column within each level of group2_column.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the data.\n",
    "    - group1_column: String, the name of the first grouping column.\n",
    "    - group2_column: String, the name of the second grouping column.\n",
    "    - outcome_column: String, the name of the outcome column.\n",
    "\n",
    "    Returns:\n",
    "    - results: Dictionary containing the p-values of the Kruskal-Wallis test for each level of group2_column.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Get unique categories in the second grouping variable\n",
    "    group2_categories = data_df[group2_column].unique()\n",
    "\n",
    "    for category in group2_categories:\n",
    "        # Filter data for each category of group2\n",
    "        subset = data_df[data_df[group2_column] == category]\n",
    "\n",
    "        # Collect the data from each group in group1_column, removing NaN values\n",
    "        groups = [subset[subset[group1_column] == group][outcome_column].dropna().values for group in subset[group1_column].unique()]\n",
    "\n",
    "        # Check if all groups have data after removing NaNs\n",
    "        if all(len(group) > 0 for group in groups):\n",
    "            # Perform Kruskal-Wallis test\n",
    "            statistic, p_value = kruskal(*groups)\n",
    "            results[category] = p_value\n",
    "        else:\n",
    "            results[category] = 'Not enough data to perform test'\n",
    "\n",
    "    return results\n",
    "\n",
    "group_1 = 'Subiculum_Group_By_Inflection_Point'\n",
    "group_2 = 'Age_Disease_and_Cohort'\n",
    "results = perform_kruskal_wallis_test(data_df, group_1, group_2, outcome_column)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def compare_levels_across_cohorts(data_df, group_column, outcome_column):\n",
    "    \"\"\"\n",
    "    Compares data across all cohorts between two levels ('low' and 'high') using the Mann-Whitney U test.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the data.\n",
    "    - group_column: String, the column that specifies the subgroup level ('low' vs 'high').\n",
    "    - outcome_column: String, the column containing the data to compare.\n",
    "\n",
    "    Returns:\n",
    "    - result: A dictionary containing the U statistic and p-value of the test.\n",
    "    \"\"\"\n",
    "    # Filter data into two groups based on the levels\n",
    "    low_data = data_df[data_df[group_column] == 'low'][outcome_column].dropna()\n",
    "    high_data = data_df[data_df[group_column] == 'high'][outcome_column].dropna()\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    if len(low_data) > 0 and len(high_data) > 0:\n",
    "        statistic, p_value = mannwhitneyu(low_data, high_data, alternative='two-sided')\n",
    "        return {'U statistic': statistic, 'p-value': p_value}\n",
    "    else:\n",
    "        return {'Error': 'Insufficient data'}\n",
    "\n",
    "# Perform the test\n",
    "results = compare_levels_across_cohorts(data_df, cohort_column='City', group_column='Subiculum_Group_By_Inflection_Point', outcome_column='Z_Scored_Percent_Cognitive_Improvement')\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from calvin_utils.statistical_utils.statistical_measurements import run_ancova_emm\n",
    "plot_grouped_barplot_actual(calculate_average_actual(data_df, group_1, group_2, outcome_column),out_dir=out_dir)\n",
    "\n",
    "# run_ancova_emm(model=results, df=data_df, design_matrix=design_matrix, group_1=group_1, group_2=group_2, outcome_column=outcome_column, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Marginals Plot (AKA Profile Plot)\n",
    "- **this is pretty complex stuff, so be sure to read up on Marginals and Estimated Marginal Means**\n",
    "- This is a plot which generates an estimated marginal mean across a set number of categories/factors\n",
    "- Do not set any values to a list of strings. If you want to use categorical data, encode the categories as ordinal values and re-run the model. Then set the categories in the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_scenarios_dict = {'Age_Disease_and_Cohort_Labeled': [0,1,2,3,4,5,6,7], 'Z_Scored_Subiculum_T_By_Origin_Group_':['std']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import GLMMarginalsPlot\n",
    "factor_plot = GLMMarginalsPlot(formula, data_df, model=results, data_range=None, marginal_scenarios_dict=marginal_scenarios_dict, variance_bars='sem')\n",
    "factor_plot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the column names used to generate the original formula. The order of the list determines how variables are categorized in the following interaction plot. \n",
    "\n",
    "- list_of_categories = ['Subiculum_Group_By_Inflection', 'City', 'Age_Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_categories = ['City', 'Age_Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import EstimatedMarginalMeansANOVA\n",
    "\n",
    "interaction_plot = EstimatedMarginalMeansANOVA(formula, data_df, model=results)\n",
    "interaction_plot.extract_unique_variables()\n",
    "interaction_plot.create_emm_df()\n",
    "interaction_plot.define_design_matrix()\n",
    "interaction_plot.predict_emm_design_df()\n",
    "interaction_plot.create_interaction_plot_dynamic(interaction_plot.emm_df.dropna(), list_of_categories, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit DataFrame Contents for Plotting Purposes\n",
    "- After changing the contents of a category, you can create the above plot again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_df['City'] = np.where(data_df['City'] == 'Toronto', 'Alzheimer Toronto (N=46)', data_df['City'])\n",
    "data_df['City'] = np.where(data_df['City'] == 'Wurzburg', 'Parkinson Wurzburg (N=26)', data_df['City'])\n",
    "data_df['City'] = np.where(data_df['City'] == 'Parkinson Wurzburg (N=17)', 'Parkinson Boston (N=17)', data_df['City'])\n",
    "data_df['City'] = np.where(data_df['City'] == 'Queensland', 'Parkinson Queensland (N=59)', data_df['City'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Get Zero Points of Each Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero point is the point where the coefficient cross zero.\n",
    "When the response topology has a saddle point, the zero point indicates where the saddle is. \n",
    "Technically, the linear regression's formula does not have a saddle point. \n",
    "However, if you rotate the response topology which exhibits a saddle point by 45 degrees, you will observe a saddle point in orientation with the Cartesian plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': 64.35210627418724,\n",
       " 'scaled_sbc': 23.889596805387878,\n",
       " 'Age:scaled_sbc': -1540.986066364188}"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from calvin_utils.statistical_utils.calculus_utils import find_zero_point_of_coefficients\n",
    "find_zero_point_of_coefficients(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - Compare 2 Different Sets of Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Nested Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.api import anova_lm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "smaller_formula = 'TOTALMOD ~ Frontal_CSFCT + Parietal_CSFCT + Temp_Ins_CSFCT + Occipital_CSF_CT'\n",
    "\n",
    "larger_formula = 'TOTALMOD ~  FrontalCSF + OccipitalCSF + ParietalCSF + temp_ins_csf'\n",
    "\n",
    "#----------------------------------------------------------------DO NOT TOUCH!----------------------------------------------------------------\n",
    "table1 = anova_lm(smf.ols(smaller_formula, data=data_df).fit(), smf.ols(larger_formula, data=data_df).fit())\n",
    "print(table1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Un-nested Models\n",
    "- Need to employ permutation because there is no existing method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'CerebellumSBM', 'CerebellumCSF', 'CerebellumGM',\n",
       "       'CerebellumWM', 'FrontalSurface', 'FrontalCSF', 'FrontalGM',\n",
       "       'FrontalWM', 'InsularSurface', 'InsularCSF', 'InsularGM', 'InsularWM',\n",
       "       'MTLSurface', 'MTLCSF', 'MTLGM', 'MTLWM', 'OccipitalSurface',\n",
       "       'OccipitalCSF', 'OccipitalGM', 'OccipitalWM', 'ParietalSurface',\n",
       "       'ParietalCSF', 'ParietalGM', 'ParietalWM', 'TemporalSurface',\n",
       "       'TemporalCSF', 'TemporalGM', 'TemporalWM', 'SubcortexSurface',\n",
       "       'SubcortexSurfaceVentricle', 'SubcortexCSF', 'SubcortexGM',\n",
       "       'SubcortexWM', 'WholeBrainSurface', 'WholeBrainCSF', 'WholeBrainGM',\n",
       "       'WholeBrainWM', 'temp_ins_csf', 'temp_ins_gm', 'temp_ins_wm',\n",
       "       'temp_ins_surface', 'frontal', 'temporal', 'parietal', 'occipital',\n",
       "       'cerebellum', 'Mesial_Temporal', 'ventricle', 'cerebellar_vermis',\n",
       "       'cerebellar_fissures', 'frontal_eh', 'temporal_eh', 'parietal_eh',\n",
       "       'occipital_eh', 'cerebellum_eh', 'mesial_temporal_eh', 'ventricle_eh',\n",
       "       'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11',\n",
       "       'Q12', 'Q14', 'TOTAL11', 'TOTALMOD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FrontalSurface', 'InsularSurface', 'MTLSurface', 'OccipitalSurface', 'ParietalSurface', 'TemporalSurface', 'SubcortexSurface', 'SubcortexSurfaceVentricle', 'WholeBrainSurface', 'temp_ins_surface']\n"
     ]
    }
   ],
   "source": [
    "vals_list = [val for val in data_df.columns if 'surf' in val.lower()]\n",
    "print(vals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula1 = 'TOTALMOD ~  FrontalSurface + temp_ins_surface + ParietalSurface + OccipitalSurface + CerebellumSBM + SubcortexSurfaceVentricle + MTLSurface'\n",
    "\n",
    "formula2 = 'TOTALMOD ~  FrontalCSF + temp_ins_csf + ParietalCSF + OccipitalCSF + CerebellumCSF + SubcortexCSF + MTLCSF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 25/1000 [00:00<00:12, 81.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 86.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original R^2 Difference: -0.13312068507301866\n",
      "P-value from permutation test: 0.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Original R^2 values from the models\n",
    "r2_model1 = smf.ols(formula1, data=data_df).fit().rsquared\n",
    "r2_model2 = smf.ols(formula2, data=data_df).fit().rsquared\n",
    "\n",
    "# Difference in R^2 of original models\n",
    "original_diff = r2_model1 - r2_model2\n",
    "\n",
    "# Number of permutations\n",
    "n_permutations = 1000\n",
    "\n",
    "# Store differences from permutations\n",
    "perm_diffs = []\n",
    "\n",
    "for i in tqdm(range(n_permutations)):\n",
    "    # Permute the outcome variable\n",
    "    data_df_permuted = data_df.copy()\n",
    "    data_df_permuted['TOTALMOD'] = np.random.permutation(data_df_permuted['TOTALMOD'].values)\n",
    "    \n",
    "    # Fit the models to the permuted dataset and calculate R^2\n",
    "    perm_model1 = smf.ols(formula1, data=data_df_permuted).fit()\n",
    "    perm_model2 = smf.ols(formula2, data=data_df_permuted).fit()\n",
    "    \n",
    "    # Difference in R^2 for the permuted models\n",
    "    perm_diff = perm_model1.rsquared - perm_model2.rsquared\n",
    "    \n",
    "    # Store the difference\n",
    "    perm_diffs.append(perm_diff)\n",
    "\n",
    "# Calculate the p-value as the proportion of permuted differences\n",
    "# that are greater than or equal to the observed difference\n",
    "p_value = np.mean([abs(diff) >= abs(original_diff) for diff in perm_diffs])\n",
    "\n",
    "print(f\"Original R^2 Difference: {original_diff}\")\n",
    "print(f\"P-value from permutation test: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generate Comparison of Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame with the data\n",
    "data = {\n",
    "    'Imaging Method': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '14'],\n",
    "    'R-squared': [0.187, 0.535, 0.097, 0.163, 0.058, .039, .475, .627, 0.108, 0.146,  0.391, 0.156, 0.148]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_palette(\"tab10\")\n",
    "sns.barplot(x='R-squared', y='Imaging Method', data=df)\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xlim(0, 1)\n",
    "plt.title('R-squared Values of Questio by Surface Method')\n",
    "plt.xlabel('R-squared')\n",
    "plt.ylabel('ADAS-Cog Question')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Barplot Of Methods Within Groups\n",
    "- The groups to compare within are the top level keys of the dictionary. \n",
    "- The groups to compare across are the second-level keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Barplot Of Methods Within Groups\n",
    "- The groups to compare within are the top level keys of the dictionary. \n",
    "- The groups to compare across are the second-level keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Question 1': {'CSF': 0.126, 'Surface': 0.187, 'GM': 0.161},\n",
    "    'Question 2': {'CSF': 0.547, 'Surface': 0.535, 'GM': 0.075},\n",
    "    'Question 3': {'CSF': 0.297, 'Surface': 0.097, 'GM': 0.451},\n",
    "    'Question 4': {'CSF': 0.268, 'Surface': 0.163, 'GM': 0.260},\n",
    "    'Question 5': {'CSF': 0.120, 'Surface': 0.058, 'GM': 0.351},\n",
    "    'Question 6': {'CSF': 0.433, 'Surface': 0.039, 'GM': 0.140},\n",
    "    'Question 7': {'CSF': 0.840, 'Surface': 0.475, 'GM': 0.383},\n",
    "    'Question 8': {'CSF': 0.540, 'Surface': 0.627, 'GM': 0.349},\n",
    "    'Question 9': {'CSF': 0.190, 'Surface': 0.108, 'GM': 0.377},\n",
    "    'Question 10': {'CSF': 0.151, 'Surface': 0.146, 'GM': 0.475},\n",
    "    'Question 11': {'CSF': 0.596, 'Surface': 0.391, 'GM': 0.407},\n",
    "    'Question 12': {'CSF': 0.151, 'Surface': 0.156, 'GM': 0.475},\n",
    "    'Question 14': {'CSF': 0.479, 'Surface': 0.148, 'GM': 0.191},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Flattening the nested dictionary and creating a DataFrame\n",
    "# flat_data = [(question, method, score) for question, methods in data.items() for method, score in methods.items()]\n",
    "# df = pd.DataFrame(flat_data, columns=['Question', 'Imaging Method', 'R-squared'])\n",
    "\n",
    "# # Create the bar plot\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.set_palette(\"tab10\")\n",
    "# sns.barplot(x='Question', y='R-squared', hue='Imaging Method', data=df)\n",
    "\n",
    "# # Customizing the plot\n",
    "# plt.ylim(0, 1)\n",
    "# plt.title('R-squared Values by Question and Imaging Method')\n",
    "# plt.xlabel('Question')\n",
    "# plt.ylabel('R-squared')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n",
    "\n",
    "# Flattening the nested dictionary and creating a DataFrame\n",
    "flat_data = [(question, method, score) for question, methods in data.items() for method, score in methods.items()]\n",
    "df = pd.DataFrame(flat_data, columns=['Question', 'Imaging Method', 'R-squared'])\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_palette(\"tab10\")\n",
    "sns.barplot(x='R-squared', y='Question', hue='Imaging Method', data=df)\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xlim(0, 1)\n",
    "plt.title('R-squared Values by Question and Imaging Method')\n",
    "plt.ylabel('Question')\n",
    "plt.xlabel('R-squared')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2=pd.DataFrame(data)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.average(df_2.iloc[0, :]), \n",
    "np.std(df_2.iloc[0, :]),\n",
    "\n",
    "\n",
    "np.average(df_2.iloc[1, :]),\n",
    "np.std(df_2.iloc[1, :]),\n",
    "\n",
    "np.average(df_2.iloc[2, :]),\n",
    "np.std(df_2.iloc[2, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula\n",
    "data_df['Cognitive_Baseline'] = data_df['Cognitive_Baseline'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Assuming 'formula' is a string like 'y ~ x1 + x2' and data_df is your DataFrame\n",
    "result = smf.ols(formula=formula, data=data_df).fit()\n",
    "\n",
    "# To view the regression results summary\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate Uncertainty Around a Regression Metric with Bootstrapping\n",
    "- R-squared approach visualized below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original R^2: 0.11906772046052772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CI for R^2: (0.0330, 0.4868)\n",
      "Mean R^2: 0.21003866087317674 | Stdev: 0.12062512037264742 | StdErr R^2: 1.2062512037264741e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstraps = 10000\n",
    "r_squared_values = []\n",
    "\n",
    "# Original model\n",
    "model = sm.OLS(outcome_matrix, design_matrix)\n",
    "original_results = model.fit()\n",
    "print(\"Original R^2:\", original_results.rsquared)\n",
    "\n",
    "# Bootstrapping\n",
    "for _ in range(n_bootstraps):\n",
    "    # Resample the indices with replacement\n",
    "    boot_indices = np.random.choice(len(outcome_matrix), size=len(outcome_matrix), replace=True)\n",
    "    if isinstance(outcome_matrix, pd.DataFrame):\n",
    "        boot_outcome = outcome_matrix.iloc[boot_indices]\n",
    "    else:\n",
    "        boot_outcome = outcome_matrix[boot_indices]\n",
    "    \n",
    "    if isinstance(design_matrix, pd.DataFrame):\n",
    "        boot_design = design_matrix.iloc[boot_indices]\n",
    "    else:\n",
    "        boot_design = design_matrix[boot_indices]\n",
    "\n",
    "    # Fit the model to the bootstrapped sample\n",
    "    boot_model = sm.OLS(boot_outcome, boot_design)\n",
    "    boot_results = boot_model.fit()\n",
    "    \n",
    "    # Store the R-squared\n",
    "    r_squared_values.append(boot_results.rsquared)\n",
    "\n",
    "# Calculate the 95% confidence interval for R^2\n",
    "r_squared_lower = np.percentile(r_squared_values, 2.5)\n",
    "r_squared_upper = np.percentile(r_squared_values, 97.5)\n",
    "\n",
    "print(f\"95% CI for R^2: ({r_squared_lower:.4f}, {r_squared_upper:.4f})\")\n",
    "print(f\"Mean R^2: {np.mean(r_squared_values)} | Stdev: {np.std(r_squared_values)} | StdErr R^2: {np.std(r_squared_values)/len(r_squared_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy.\n",
    "\n",
    "-- Calvin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
