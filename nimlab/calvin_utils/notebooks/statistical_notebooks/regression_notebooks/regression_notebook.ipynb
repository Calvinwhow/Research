{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Any Kind of OLS Regression (ANOVA, GLM, etc.)\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "#### Last updated: July 6, 2023\n",
    "\n",
    "Use this to run/test a statistical model (e.g., regression or T-tests) on a spreadsheet.\n",
    "\n",
    "Notes:\n",
    "- To best use this notebook, you should be familar with GLM design and Contrast Matrix design. See this webpage to get started:\n",
    "[FSL's GLM page](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep Output Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/supplements/alzheimer_regressions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list_proper_subjects.xlsx'\n",
    "sheet = 'master_list_proper_subjects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>...</th>\n",
       "      <th>Estimated_Outcome</th>\n",
       "      <th>Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>211</td>\n",
       "      <td>58.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.415745</td>\n",
       "      <td>-0.189000</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>152</td>\n",
       "      <td>69.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.701419</td>\n",
       "      <td>-0.455000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>DECLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>208</td>\n",
       "      <td>79.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.929958</td>\n",
       "      <td>-0.669000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>DECLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>223</td>\n",
       "      <td>71.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.829972</td>\n",
       "      <td>-0.575000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>DECLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>214</td>\n",
       "      <td>76.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.244199</td>\n",
       "      <td>-0.958000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>DECLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0        101  62.0                                 -0.392857   \n",
       "1        102  77.0                                 -0.666667   \n",
       "2        103  76.0                                 -1.447368   \n",
       "3        104  65.0                                 -2.372549   \n",
       "4        105  50.0                                 -0.192982   \n",
       "..       ...   ...                                       ...   \n",
       "194      211  58.7                                       NaN   \n",
       "195      152  69.4                                       NaN   \n",
       "196      208  79.2                                       NaN   \n",
       "197      223  71.1                                       NaN   \n",
       "198      214  76.6                                       NaN   \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                             0.314066        \n",
       "1                                             0.013999        \n",
       "2                                            -0.841572        \n",
       "3                                            -1.855477        \n",
       "4                                             0.533109        \n",
       "..                                                 ...        \n",
       "194                                                NaN        \n",
       "195                                                NaN        \n",
       "196                                                NaN        \n",
       "197                                                NaN        \n",
       "198                                                NaN        \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                  0.314066                     -21.428571   \n",
       "1                                  0.013999                     -36.363636   \n",
       "2                                 -0.841572                     -78.947368   \n",
       "3                                 -1.855477                    -129.411765   \n",
       "4                                  0.533109                     -10.526316   \n",
       "..                                      ...                            ...   \n",
       "194                                     NaN                            NaN   \n",
       "195                                     NaN                            NaN   \n",
       "196                                     NaN                            NaN   \n",
       "197                                     NaN                            NaN   \n",
       "198                                     NaN                            NaN   \n",
       "\n",
       "     Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                                -1.282630                          -1.282630   \n",
       "1                                -1.760917                          -1.760917   \n",
       "2                                -0.595369                          -0.595369   \n",
       "3                                -0.945206                          -0.945206   \n",
       "4                                -1.151973                          -1.151973   \n",
       "..                                     ...                                ...   \n",
       "194                              -0.415745                          -0.189000   \n",
       "195                              -0.701419                          -0.455000   \n",
       "196                              -0.929958                          -0.669000   \n",
       "197                              -0.829972                          -0.575000   \n",
       "198                              -1.244199                          -0.958000   \n",
       "\n",
       "     Subiculum_Connectivity_T  Amnesia_Lesion_T_Map  ...  Estimated_Outcome  \\\n",
       "0                   56.864683              0.447264  ...                NaN   \n",
       "1                   52.970984              0.436157  ...                NaN   \n",
       "2                   62.459631              0.497749  ...                NaN   \n",
       "3                   59.611631              0.432617  ...                NaN   \n",
       "4                   57.928350              0.193389  ...                NaN   \n",
       "..                        ...                   ...  ...                ...   \n",
       "194                 19.900000                   NaN  ...               GOOD   \n",
       "195                 17.900000                   NaN  ...            DECLINE   \n",
       "196                 16.300000                   NaN  ...            DECLINE   \n",
       "197                 17.000000                   NaN  ...            DECLINE   \n",
       "198                 14.100000                   NaN  ...            DECLINE   \n",
       "\n",
       "     Cognitive_Baseline  Z_Scored_Cognitive_Baseline  \\\n",
       "0                    28                     1.518764   \n",
       "1                    22                     0.465551   \n",
       "2                    19                    -0.061056   \n",
       "3                    17                    -0.412127   \n",
       "4                    19                    -0.061056   \n",
       "..                  ...                          ...   \n",
       "194                 NaN                          NaN   \n",
       "195                 NaN                          NaN   \n",
       "196                 NaN                          NaN   \n",
       "197                 NaN                          NaN   \n",
       "198                 NaN                          NaN   \n",
       "\n",
       "     Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                        -1.518764   \n",
       "1                                        -0.465551   \n",
       "2                                         0.061056   \n",
       "3                                         0.412127   \n",
       "4                                         0.061056   \n",
       "..                                             ...   \n",
       "194                                            NaN   \n",
       "195                                            NaN   \n",
       "196                                            NaN   \n",
       "197                                            NaN   \n",
       "198                                            NaN   \n",
       "\n",
       "     Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                           0.72                                 0.28   \n",
       "1                           0.48                                 0.52   \n",
       "2                           0.36                                 0.64   \n",
       "3                           0.28                                 0.72   \n",
       "4                           0.36                                 0.64   \n",
       "..                           ...                                  ...   \n",
       "194                          NaN                                  NaN   \n",
       "195                          NaN                                  NaN   \n",
       "196                          NaN                                  NaN   \n",
       "197                          NaN                                  NaN   \n",
       "198                          NaN                                  NaN   \n",
       "\n",
       "     ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0         12.222658      14.493929            -1.714513            -1.227368  \n",
       "1         14.020048      15.257338            -1.155843            -1.022243  \n",
       "2         15.118727      17.376384            -0.814348            -0.452865  \n",
       "3         13.112424      15.287916            -1.437954            -1.014027  \n",
       "4         15.086568      12.951426            -0.824344            -1.641831  \n",
       "..              ...            ...                  ...                  ...  \n",
       "194             NaN            NaN                  NaN                  NaN  \n",
       "195             NaN            NaN                  NaN                  NaN  \n",
       "196             NaN            NaN                  NaN                  NaN  \n",
       "197             NaN            NaN                  NaN                  NaN  \n",
       "198             NaN            NaN                  NaN                  NaN  \n",
       "\n",
       "[199 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Subiculum_T_By_Origin_Group_',\n",
       "       'Z_Scored_Subiculum_Connectivity_T', 'Subiculum_Connectivity_T',\n",
       "       'Amnesia_Lesion_T_Map', 'Memory_Network_T', 'Z_Scored_Memory_Network_R',\n",
       "       'Memory_Network_R', 'Subiculum_Grey_Matter', 'Subiculum_White_Matter',\n",
       "       'Subiculum_CSF', 'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Z_Score_Sign',\n",
       "       'Subiculum_Group_By_Inflection_Point', 'Cognitive_Outcome',\n",
       "       'Estimated_Outcome', 'Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline__Lower_is_Better_',\n",
       "       'Min_Max_Normalized_Baseline', 'MinMaxNormBaseline_Higher_is_Better',\n",
       "       'ROI_to_Alz_Max', 'ROI_to_PD_Max', 'Standardzied_AD_Max',\n",
       "       'Standardized_PD_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Age', 'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group', 'Z_Scored_Subiculum_T_By_Origin_Group_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>...</th>\n",
       "      <th>Estimated_Outcome</th>\n",
       "      <th>Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>86</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598787</td>\n",
       "      <td>-0.099428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.087220</td>\n",
       "      <td>-0.621000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598787</td>\n",
       "      <td>-0.099428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598397</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>88</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.854050</td>\n",
       "      <td>2.637141</td>\n",
       "      <td>15.384615</td>\n",
       "      <td>0.269872</td>\n",
       "      <td>-0.207000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>89</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598787</td>\n",
       "      <td>-0.099428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.158639</td>\n",
       "      <td>-0.694000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>91</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.818759</td>\n",
       "      <td>0.535847</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0.469844</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0        101  62.0                                 -0.392857   \n",
       "1        102  77.0                                 -0.666667   \n",
       "2        103  76.0                                 -1.447368   \n",
       "3        104  65.0                                 -2.372549   \n",
       "4        105  50.0                                 -0.192982   \n",
       "..       ...   ...                                       ...   \n",
       "160       86  57.0                                       NaN   \n",
       "161       87  65.0                                       NaN   \n",
       "162       88  65.0                                       NaN   \n",
       "163       89  67.0                                       NaN   \n",
       "164       91  45.0                                       NaN   \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                             0.314066        \n",
       "1                                             0.013999        \n",
       "2                                            -0.841572        \n",
       "3                                            -1.855477        \n",
       "4                                             0.533109        \n",
       "..                                                 ...        \n",
       "160                                           0.598787        \n",
       "161                                           0.598787        \n",
       "162                                           5.854050        \n",
       "163                                           0.598787        \n",
       "164                                           1.818759        \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                  0.314066                     -21.428571   \n",
       "1                                  0.013999                     -36.363636   \n",
       "2                                 -0.841572                     -78.947368   \n",
       "3                                 -1.855477                    -129.411765   \n",
       "4                                  0.533109                     -10.526316   \n",
       "..                                      ...                            ...   \n",
       "160                               -0.099428                       0.000000   \n",
       "161                               -0.099428                       0.000000   \n",
       "162                                2.637141                      15.384615   \n",
       "163                               -0.099428                       0.000000   \n",
       "164                                0.535847                       3.571429   \n",
       "\n",
       "     Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                                -1.282630                          -1.282630   \n",
       "1                                -1.760917                          -1.760917   \n",
       "2                                -0.595369                          -0.595369   \n",
       "3                                -0.945206                          -0.945206   \n",
       "4                                -1.151973                          -1.151973   \n",
       "..                                     ...                                ...   \n",
       "160                              -0.087220                          -0.621000   \n",
       "161                               0.598397                           0.173000   \n",
       "162                               0.269872                          -0.207000   \n",
       "163                              -0.158639                          -0.694000   \n",
       "164                               0.469844                           0.019400   \n",
       "\n",
       "     Subiculum_Connectivity_T  Amnesia_Lesion_T_Map  ...  Estimated_Outcome  \\\n",
       "0                   56.864683              0.447264  ...                NaN   \n",
       "1                   52.970984              0.436157  ...                NaN   \n",
       "2                   62.459631              0.497749  ...                NaN   \n",
       "3                   59.611631              0.432617  ...                NaN   \n",
       "4                   57.928350              0.193389  ...                NaN   \n",
       "..                        ...                   ...  ...                ...   \n",
       "160                 22.200000                   NaN  ...                NaN   \n",
       "161                 27.000000                   NaN  ...                NaN   \n",
       "162                 24.700000                   NaN  ...                NaN   \n",
       "163                 21.700000                   NaN  ...                NaN   \n",
       "164                 26.100000                   NaN  ...                NaN   \n",
       "\n",
       "     Cognitive_Baseline  Z_Scored_Cognitive_Baseline  \\\n",
       "0                    28                     1.518764   \n",
       "1                    22                     0.465551   \n",
       "2                    19                    -0.061056   \n",
       "3                    17                    -0.412127   \n",
       "4                    19                    -0.061056   \n",
       "..                  ...                          ...   \n",
       "160                 NaN                          NaN   \n",
       "161                 NaN                          NaN   \n",
       "162                 NaN                          NaN   \n",
       "163                 NaN                          NaN   \n",
       "164                 NaN                          NaN   \n",
       "\n",
       "     Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                        -1.518764   \n",
       "1                                        -0.465551   \n",
       "2                                         0.061056   \n",
       "3                                         0.412127   \n",
       "4                                         0.061056   \n",
       "..                                             ...   \n",
       "160                                            NaN   \n",
       "161                                            NaN   \n",
       "162                                            NaN   \n",
       "163                                            NaN   \n",
       "164                                            NaN   \n",
       "\n",
       "     Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                           0.72                                 0.28   \n",
       "1                           0.48                                 0.52   \n",
       "2                           0.36                                 0.64   \n",
       "3                           0.28                                 0.72   \n",
       "4                           0.36                                 0.64   \n",
       "..                           ...                                  ...   \n",
       "160                          NaN                                  NaN   \n",
       "161                          NaN                                  NaN   \n",
       "162                          NaN                                  NaN   \n",
       "163                          NaN                                  NaN   \n",
       "164                          NaN                                  NaN   \n",
       "\n",
       "     ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0         12.222658      14.493929            -1.714513            -1.227368  \n",
       "1         14.020048      15.257338            -1.155843            -1.022243  \n",
       "2         15.118727      17.376384            -0.814348            -0.452865  \n",
       "3         13.112424      15.287916            -1.437954            -1.014027  \n",
       "4         15.086568      12.951426            -0.824344            -1.641831  \n",
       "..              ...            ...                  ...                  ...  \n",
       "160             NaN            NaN                  NaN                  NaN  \n",
       "161             NaN            NaN                  NaN                  NaN  \n",
       "162             NaN            NaN                  NaN                  NaN  \n",
       "163             NaN            NaN                  NaN                  NaN  \n",
       "164             NaN            NaN                  NaN                  NaN  \n",
       "\n",
       "[148 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Subiculum_T_By_Origin_Group_',\n",
       "       'Z_Scored_Subiculum_Connectivity_T', 'Subiculum_Connectivity_T',\n",
       "       'Amnesia_Lesion_T_Map', 'Memory_Network_T', 'Z_Scored_Memory_Network_R',\n",
       "       'Memory_Network_R', 'Subiculum_Grey_Matter', 'Subiculum_White_Matter',\n",
       "       'Subiculum_CSF', 'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Z_Score_Sign',\n",
       "       'Subiculum_Group_By_Inflection_Point', 'Cognitive_Outcome',\n",
       "       'Estimated_Outcome', 'Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline__Lower_is_Better_',\n",
       "       'Min_Max_Normalized_Baseline', 'MinMaxNormBaseline_Higher_is_Better',\n",
       "       'ROI_to_Alz_Max', 'ROI_to_PD_Max', 'Standardzied_AD_Max',\n",
       "       'Standardized_PD_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'City'  # The column you'd like to evaluate\n",
    "condition = 'not'  # The condition to check ('equal', 'above', 'below', 'not')\n",
    "value = 'Toronto' # The value to drop if found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>...</th>\n",
       "      <th>Estimated_Outcome</th>\n",
       "      <th>Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.705128</td>\n",
       "      <td>-0.028151</td>\n",
       "      <td>-0.028151</td>\n",
       "      <td>-38.461538</td>\n",
       "      <td>-0.489205</td>\n",
       "      <td>-0.489205</td>\n",
       "      <td>63.323903</td>\n",
       "      <td>0.513828</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>15.816634</td>\n",
       "      <td>17.617107</td>\n",
       "      <td>-0.597423</td>\n",
       "      <td>-0.388183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.282051</td>\n",
       "      <td>0.435498</td>\n",
       "      <td>0.435498</td>\n",
       "      <td>-15.384615</td>\n",
       "      <td>-1.718309</td>\n",
       "      <td>-1.718309</td>\n",
       "      <td>53.317851</td>\n",
       "      <td>0.249080</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>15.524025</td>\n",
       "      <td>13.452311</td>\n",
       "      <td>-0.688373</td>\n",
       "      <td>-1.507246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.534722</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>-29.166667</td>\n",
       "      <td>-1.145694</td>\n",
       "      <td>-1.145694</td>\n",
       "      <td>57.979468</td>\n",
       "      <td>0.247062</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>0.816622</td>\n",
       "      <td>-0.816622</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>16.546984</td>\n",
       "      <td>13.932696</td>\n",
       "      <td>-0.370413</td>\n",
       "      <td>-1.378169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.557971</td>\n",
       "      <td>0.133118</td>\n",
       "      <td>0.133118</td>\n",
       "      <td>-30.434783</td>\n",
       "      <td>-0.043697</td>\n",
       "      <td>-0.043697</td>\n",
       "      <td>66.950749</td>\n",
       "      <td>0.514661</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>0.641086</td>\n",
       "      <td>-0.641086</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>19.669539</td>\n",
       "      <td>21.341523</td>\n",
       "      <td>0.600149</td>\n",
       "      <td>0.612551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.551282</td>\n",
       "      <td>-0.955451</td>\n",
       "      <td>-0.955451</td>\n",
       "      <td>-84.615385</td>\n",
       "      <td>0.240855</td>\n",
       "      <td>0.240855</td>\n",
       "      <td>69.267271</td>\n",
       "      <td>0.530628</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>18.295718</td>\n",
       "      <td>19.263977</td>\n",
       "      <td>0.173133</td>\n",
       "      <td>0.054323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>111</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>1.581744</td>\n",
       "      <td>1.581744</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>2.004069</td>\n",
       "      <td>2.004069</td>\n",
       "      <td>83.621480</td>\n",
       "      <td>0.514182</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.289805</td>\n",
       "      <td>1.289805</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.92</td>\n",
       "      <td>17.022328</td>\n",
       "      <td>16.151031</td>\n",
       "      <td>-0.222665</td>\n",
       "      <td>-0.782112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>113</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>-0.460891</td>\n",
       "      <td>-0.460891</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.301223</td>\n",
       "      <td>0.301223</td>\n",
       "      <td>69.758721</td>\n",
       "      <td>0.497508</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.763198</td>\n",
       "      <td>0.763198</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>16.225059</td>\n",
       "      <td>17.818822</td>\n",
       "      <td>-0.470475</td>\n",
       "      <td>-0.333983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-0.295699</td>\n",
       "      <td>0.420542</td>\n",
       "      <td>0.420542</td>\n",
       "      <td>-16.129032</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>0.015433</td>\n",
       "      <td>67.432125</td>\n",
       "      <td>0.461019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>2.045371</td>\n",
       "      <td>-2.045371</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.16</td>\n",
       "      <td>15.295109</td>\n",
       "      <td>18.046377</td>\n",
       "      <td>-0.759525</td>\n",
       "      <td>-0.272840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>115</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.885057</td>\n",
       "      <td>-0.225336</td>\n",
       "      <td>-0.225336</td>\n",
       "      <td>-48.275862</td>\n",
       "      <td>0.472801</td>\n",
       "      <td>0.472801</td>\n",
       "      <td>71.155530</td>\n",
       "      <td>0.227943</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>1.694300</td>\n",
       "      <td>-1.694300</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.24</td>\n",
       "      <td>18.623736</td>\n",
       "      <td>16.113043</td>\n",
       "      <td>0.275089</td>\n",
       "      <td>-0.792319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>116</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-0.675439</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>-36.842105</td>\n",
       "      <td>-0.657659</td>\n",
       "      <td>-0.657659</td>\n",
       "      <td>61.952527</td>\n",
       "      <td>0.485376</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>16.462336</td>\n",
       "      <td>18.503053</td>\n",
       "      <td>-0.396724</td>\n",
       "      <td>-0.150134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>118</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-0.057292</td>\n",
       "      <td>0.681812</td>\n",
       "      <td>0.681812</td>\n",
       "      <td>-3.125000</td>\n",
       "      <td>0.643782</td>\n",
       "      <td>0.643782</td>\n",
       "      <td>72.547473</td>\n",
       "      <td>0.247475</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>2.220907</td>\n",
       "      <td>-2.220907</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.842273</td>\n",
       "      <td>19.964170</td>\n",
       "      <td>-0.589454</td>\n",
       "      <td>0.242462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>119</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-1.489583</td>\n",
       "      <td>-0.887835</td>\n",
       "      <td>-0.887835</td>\n",
       "      <td>-81.250000</td>\n",
       "      <td>-0.637257</td>\n",
       "      <td>-0.637257</td>\n",
       "      <td>62.118622</td>\n",
       "      <td>0.503218</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>17.332886</td>\n",
       "      <td>19.198042</td>\n",
       "      <td>-0.126137</td>\n",
       "      <td>0.036607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>120</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-0.509259</td>\n",
       "      <td>0.186501</td>\n",
       "      <td>0.186501</td>\n",
       "      <td>-27.777778</td>\n",
       "      <td>-0.781964</td>\n",
       "      <td>-0.781964</td>\n",
       "      <td>60.940570</td>\n",
       "      <td>0.284504</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.236591</td>\n",
       "      <td>0.236591</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>10.126563</td>\n",
       "      <td>9.630690</td>\n",
       "      <td>-2.366028</td>\n",
       "      <td>-2.534098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>121</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.079710</td>\n",
       "      <td>0.657244</td>\n",
       "      <td>0.657244</td>\n",
       "      <td>-4.347826</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>71.223601</td>\n",
       "      <td>0.483995</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>0.641086</td>\n",
       "      <td>-0.641086</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>22.476761</td>\n",
       "      <td>23.985143</td>\n",
       "      <td>1.472698</td>\n",
       "      <td>1.322880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>122</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-2.566667</td>\n",
       "      <td>-2.068210</td>\n",
       "      <td>-2.068210</td>\n",
       "      <td>-140.000000</td>\n",
       "      <td>0.190387</td>\n",
       "      <td>0.190387</td>\n",
       "      <td>68.856418</td>\n",
       "      <td>0.512125</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.763198</td>\n",
       "      <td>0.763198</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>21.466327</td>\n",
       "      <td>23.582758</td>\n",
       "      <td>1.158632</td>\n",
       "      <td>1.214761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>123</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.840498</td>\n",
       "      <td>1.840498</td>\n",
       "      <td>54.545455</td>\n",
       "      <td>-0.341607</td>\n",
       "      <td>-0.341607</td>\n",
       "      <td>64.525483</td>\n",
       "      <td>0.507649</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>19.453283</td>\n",
       "      <td>21.676897</td>\n",
       "      <td>0.532931</td>\n",
       "      <td>0.702664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>124</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-0.343750</td>\n",
       "      <td>0.367883</td>\n",
       "      <td>0.367883</td>\n",
       "      <td>-18.750000</td>\n",
       "      <td>-0.864253</td>\n",
       "      <td>-0.864253</td>\n",
       "      <td>60.270662</td>\n",
       "      <td>0.460080</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>13.457540</td>\n",
       "      <td>15.819523</td>\n",
       "      <td>-1.330684</td>\n",
       "      <td>-0.871186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>125</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-0.687500</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-37.500000</td>\n",
       "      <td>0.536747</td>\n",
       "      <td>0.536747</td>\n",
       "      <td>71.676110</td>\n",
       "      <td>0.479488</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>22.443084</td>\n",
       "      <td>24.738357</td>\n",
       "      <td>1.462230</td>\n",
       "      <td>1.525265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>126</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-1.833333</td>\n",
       "      <td>-1.264551</td>\n",
       "      <td>-1.264551</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.081582</td>\n",
       "      <td>1.081582</td>\n",
       "      <td>76.111573</td>\n",
       "      <td>0.477498</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>20.186102</td>\n",
       "      <td>22.065363</td>\n",
       "      <td>0.760708</td>\n",
       "      <td>0.807044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>127</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-1.401961</td>\n",
       "      <td>-0.791810</td>\n",
       "      <td>-0.791810</td>\n",
       "      <td>-76.470588</td>\n",
       "      <td>0.009987</td>\n",
       "      <td>0.009987</td>\n",
       "      <td>67.387791</td>\n",
       "      <td>0.501404</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>16.996513</td>\n",
       "      <td>18.394062</td>\n",
       "      <td>-0.230689</td>\n",
       "      <td>-0.179419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>128</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.381944</td>\n",
       "      <td>0.326026</td>\n",
       "      <td>0.326026</td>\n",
       "      <td>-20.833333</td>\n",
       "      <td>1.010239</td>\n",
       "      <td>1.010239</td>\n",
       "      <td>75.530777</td>\n",
       "      <td>0.511383</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>0.816622</td>\n",
       "      <td>-0.816622</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>17.204945</td>\n",
       "      <td>18.264558</td>\n",
       "      <td>-0.165904</td>\n",
       "      <td>-0.214216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>129</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-0.654762</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>-35.714286</td>\n",
       "      <td>1.271528</td>\n",
       "      <td>1.271528</td>\n",
       "      <td>77.657909</td>\n",
       "      <td>0.496203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>21.078380</td>\n",
       "      <td>22.489816</td>\n",
       "      <td>1.038049</td>\n",
       "      <td>0.921092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>130</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.130952</td>\n",
       "      <td>0.601088</td>\n",
       "      <td>0.601088</td>\n",
       "      <td>-7.142857</td>\n",
       "      <td>-0.161232</td>\n",
       "      <td>-0.161232</td>\n",
       "      <td>65.993904</td>\n",
       "      <td>0.510956</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.938734</td>\n",
       "      <td>0.938734</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.84</td>\n",
       "      <td>18.617648</td>\n",
       "      <td>20.415928</td>\n",
       "      <td>0.273197</td>\n",
       "      <td>0.363848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>131</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-0.733333</td>\n",
       "      <td>-0.059061</td>\n",
       "      <td>-0.059061</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>0.576257</td>\n",
       "      <td>0.576257</td>\n",
       "      <td>71.997755</td>\n",
       "      <td>0.458336</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>0.114480</td>\n",
       "      <td>-0.114480</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>18.410547</td>\n",
       "      <td>19.822564</td>\n",
       "      <td>0.208825</td>\n",
       "      <td>0.204413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>133</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.938667</td>\n",
       "      <td>0.938667</td>\n",
       "      <td>74.948112</td>\n",
       "      <td>0.503698</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>18.420784</td>\n",
       "      <td>19.612924</td>\n",
       "      <td>0.212007</td>\n",
       "      <td>0.148084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>134</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>1.335524</td>\n",
       "      <td>1.335524</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>0.609143</td>\n",
       "      <td>0.609143</td>\n",
       "      <td>72.265483</td>\n",
       "      <td>0.402215</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>23.613479</td>\n",
       "      <td>26.048153</td>\n",
       "      <td>1.826016</td>\n",
       "      <td>1.877202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>135</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-0.838095</td>\n",
       "      <td>-0.173870</td>\n",
       "      <td>-0.173870</td>\n",
       "      <td>-45.714286</td>\n",
       "      <td>-0.484585</td>\n",
       "      <td>-0.484585</td>\n",
       "      <td>63.361510</td>\n",
       "      <td>0.481574</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>2.747514</td>\n",
       "      <td>-2.747514</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.101986</td>\n",
       "      <td>17.647381</td>\n",
       "      <td>-0.508729</td>\n",
       "      <td>-0.380049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>137</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.416667</td>\n",
       "      <td>-0.807926</td>\n",
       "      <td>-0.807926</td>\n",
       "      <td>-77.272727</td>\n",
       "      <td>-0.210108</td>\n",
       "      <td>-0.210108</td>\n",
       "      <td>65.596015</td>\n",
       "      <td>0.486037</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>16.837360</td>\n",
       "      <td>18.171505</td>\n",
       "      <td>-0.280158</td>\n",
       "      <td>-0.239219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>138</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-0.174603</td>\n",
       "      <td>0.553251</td>\n",
       "      <td>0.553251</td>\n",
       "      <td>-9.523810</td>\n",
       "      <td>1.634297</td>\n",
       "      <td>1.634297</td>\n",
       "      <td>80.611188</td>\n",
       "      <td>0.471613</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>20.531868</td>\n",
       "      <td>22.371502</td>\n",
       "      <td>0.868180</td>\n",
       "      <td>0.889302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>139</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-1.941176</td>\n",
       "      <td>-1.382736</td>\n",
       "      <td>-1.382736</td>\n",
       "      <td>-105.882353</td>\n",
       "      <td>-0.768437</td>\n",
       "      <td>-0.768437</td>\n",
       "      <td>61.050691</td>\n",
       "      <td>0.520899</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>17.957341</td>\n",
       "      <td>19.952870</td>\n",
       "      <td>0.067958</td>\n",
       "      <td>0.239426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>140</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.109898</td>\n",
       "      <td>1.109898</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>0.660878</td>\n",
       "      <td>0.660878</td>\n",
       "      <td>72.686647</td>\n",
       "      <td>0.415110</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>24.418011</td>\n",
       "      <td>27.143129</td>\n",
       "      <td>2.076083</td>\n",
       "      <td>2.171417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>141</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744598</td>\n",
       "      <td>0.744598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.169749</td>\n",
       "      <td>1.169749</td>\n",
       "      <td>76.829337</td>\n",
       "      <td>0.503535</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>17.856530</td>\n",
       "      <td>19.670979</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.163683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>142</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.328286</td>\n",
       "      <td>0.328286</td>\n",
       "      <td>69.979040</td>\n",
       "      <td>0.521941</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.236591</td>\n",
       "      <td>0.236591</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>14.769545</td>\n",
       "      <td>16.131533</td>\n",
       "      <td>-0.922882</td>\n",
       "      <td>-0.787351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>143</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>0.870170</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>-0.118729</td>\n",
       "      <td>-0.118729</td>\n",
       "      <td>66.339918</td>\n",
       "      <td>0.569043</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.587663</td>\n",
       "      <td>0.587663</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>20.179952</td>\n",
       "      <td>19.975031</td>\n",
       "      <td>0.758797</td>\n",
       "      <td>0.245380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>144</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-1.128205</td>\n",
       "      <td>-0.491801</td>\n",
       "      <td>-0.491801</td>\n",
       "      <td>-61.538462</td>\n",
       "      <td>-1.032653</td>\n",
       "      <td>-1.032653</td>\n",
       "      <td>58.899731</td>\n",
       "      <td>0.470849</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>11.801552</td>\n",
       "      <td>14.042861</td>\n",
       "      <td>-1.845403</td>\n",
       "      <td>-1.348567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>145</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>2.183535</td>\n",
       "      <td>2.183535</td>\n",
       "      <td>85.082502</td>\n",
       "      <td>0.420971</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.236591</td>\n",
       "      <td>0.236591</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>21.327670</td>\n",
       "      <td>22.294652</td>\n",
       "      <td>1.115534</td>\n",
       "      <td>0.868652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>146</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.351301</td>\n",
       "      <td>-0.351301</td>\n",
       "      <td>-54.545455</td>\n",
       "      <td>-2.576474</td>\n",
       "      <td>-2.576474</td>\n",
       "      <td>46.331586</td>\n",
       "      <td>0.469571</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.465341</td>\n",
       "      <td>1.465341</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>16.512422</td>\n",
       "      <td>18.197520</td>\n",
       "      <td>-0.381156</td>\n",
       "      <td>-0.232229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>147</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-0.087302</td>\n",
       "      <td>0.648924</td>\n",
       "      <td>0.648924</td>\n",
       "      <td>-4.761905</td>\n",
       "      <td>0.262965</td>\n",
       "      <td>0.262965</td>\n",
       "      <td>69.447270</td>\n",
       "      <td>0.530166</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.290015</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>19.159295</td>\n",
       "      <td>19.553377</td>\n",
       "      <td>0.441553</td>\n",
       "      <td>0.132084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>148</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-3.807692</td>\n",
       "      <td>-3.428250</td>\n",
       "      <td>-3.428250</td>\n",
       "      <td>-207.692308</td>\n",
       "      <td>1.035924</td>\n",
       "      <td>1.035924</td>\n",
       "      <td>75.739873</td>\n",
       "      <td>0.489891</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.114269</td>\n",
       "      <td>1.114269</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>19.340269</td>\n",
       "      <td>21.114119</td>\n",
       "      <td>0.497804</td>\n",
       "      <td>0.551449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>149</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>-1.063636</td>\n",
       "      <td>-1.063636</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>-0.650900</td>\n",
       "      <td>-0.650900</td>\n",
       "      <td>62.007555</td>\n",
       "      <td>0.480527</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.640876</td>\n",
       "      <td>1.640876</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>19.062652</td>\n",
       "      <td>20.569076</td>\n",
       "      <td>0.411514</td>\n",
       "      <td>0.404998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>150</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.018573</td>\n",
       "      <td>1.018573</td>\n",
       "      <td>13.636364</td>\n",
       "      <td>0.759359</td>\n",
       "      <td>0.759359</td>\n",
       "      <td>73.488381</td>\n",
       "      <td>0.461720</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>23.954833</td>\n",
       "      <td>26.691505</td>\n",
       "      <td>1.932117</td>\n",
       "      <td>2.050067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0       101  62.0                                 -0.392857   \n",
       "1       102  77.0                                 -0.666667   \n",
       "2       103  76.0                                 -1.447368   \n",
       "3       104  65.0                                 -2.372549   \n",
       "4       105  50.0                                 -0.192982   \n",
       "5       106  66.0                                 -0.705128   \n",
       "6       107  64.0                                 -0.282051   \n",
       "7       108  60.0                                 -0.534722   \n",
       "8       109  72.0                                 -0.557971   \n",
       "9       110  72.0                                 -1.551282   \n",
       "10      111  62.0                                  0.763889   \n",
       "11      113  69.0                                 -1.100000   \n",
       "12      114  67.0                                 -0.295699   \n",
       "13      115  60.0                                 -0.885057   \n",
       "14      116  67.0                                 -0.675439   \n",
       "15      118  52.0                                 -0.057292   \n",
       "16      119  75.0                                 -1.489583   \n",
       "17      120  68.0                                 -0.509259   \n",
       "18      121  72.0                                 -0.079710   \n",
       "19      122  58.0                                 -2.566667   \n",
       "20      123  47.0                                  1.000000   \n",
       "21      124  61.0                                 -0.343750   \n",
       "22      125  73.0                                 -0.687500   \n",
       "23      126  69.0                                 -1.833333   \n",
       "24      127  74.0                                 -1.401961   \n",
       "25      128  72.0                                 -0.381944   \n",
       "26      129  69.0                                 -0.654762   \n",
       "27      130  66.0                                 -0.130952   \n",
       "28      131  68.0                                 -0.733333   \n",
       "29      133  74.0                                  0.114583   \n",
       "30      134  66.0                                  0.539216   \n",
       "31      135  57.0                                 -0.838095   \n",
       "32      137  57.0                                 -1.416667   \n",
       "33      138  72.0                                 -0.174603   \n",
       "34      139  58.0                                 -1.941176   \n",
       "35      140  73.0                                  0.333333   \n",
       "36      141  72.0                                  0.000000   \n",
       "37      142  77.0                                  0.101852   \n",
       "38      143  71.0                                  0.114583   \n",
       "39      144  79.0                                 -1.128205   \n",
       "40      145  74.0                                  0.101852   \n",
       "41      146  76.0                                 -1.000000   \n",
       "42      147  59.0                                 -0.087302   \n",
       "43      148  51.0                                 -3.807692   \n",
       "44      149  77.0                                 -1.650000   \n",
       "45      150  71.0                                  0.250000   \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                            0.314066        \n",
       "1                                            0.013999        \n",
       "2                                           -0.841572        \n",
       "3                                           -1.855477        \n",
       "4                                            0.533109        \n",
       "5                                           -0.028151        \n",
       "6                                            0.435498        \n",
       "7                                            0.158596        \n",
       "8                                            0.133118        \n",
       "9                                           -0.955451        \n",
       "10                                           1.581744        \n",
       "11                                          -0.460891        \n",
       "12                                           0.420542        \n",
       "13                                          -0.225336        \n",
       "14                                           0.004385        \n",
       "15                                           0.681812        \n",
       "16                                          -0.887835        \n",
       "17                                           0.186501        \n",
       "18                                           0.657244        \n",
       "19                                          -2.068210        \n",
       "20                                           1.840498        \n",
       "21                                           0.367883        \n",
       "22                                          -0.008833        \n",
       "23                                          -1.264551        \n",
       "24                                          -0.791810        \n",
       "25                                           0.326026        \n",
       "26                                           0.027045        \n",
       "27                                           0.601088        \n",
       "28                                          -0.059061        \n",
       "29                                           0.870170        \n",
       "30                                           1.335524        \n",
       "31                                          -0.173870        \n",
       "32                                          -0.807926        \n",
       "33                                           0.553251        \n",
       "34                                          -1.382736        \n",
       "35                                           1.109898        \n",
       "36                                           0.744598        \n",
       "37                                           0.856218        \n",
       "38                                           0.870170        \n",
       "39                                          -0.491801        \n",
       "40                                           0.856218        \n",
       "41                                          -0.351301        \n",
       "42                                           0.648924        \n",
       "43                                          -3.428250        \n",
       "44                                          -1.063636        \n",
       "45                                           1.018573        \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                 0.314066                     -21.428571   \n",
       "1                                 0.013999                     -36.363636   \n",
       "2                                -0.841572                     -78.947368   \n",
       "3                                -1.855477                    -129.411765   \n",
       "4                                 0.533109                     -10.526316   \n",
       "5                                -0.028151                     -38.461538   \n",
       "6                                 0.435498                     -15.384615   \n",
       "7                                 0.158596                     -29.166667   \n",
       "8                                 0.133118                     -30.434783   \n",
       "9                                -0.955451                     -84.615385   \n",
       "10                                1.581744                      41.666667   \n",
       "11                               -0.460891                     -60.000000   \n",
       "12                                0.420542                     -16.129032   \n",
       "13                               -0.225336                     -48.275862   \n",
       "14                                0.004385                     -36.842105   \n",
       "15                                0.681812                      -3.125000   \n",
       "16                               -0.887835                     -81.250000   \n",
       "17                                0.186501                     -27.777778   \n",
       "18                                0.657244                      -4.347826   \n",
       "19                               -2.068210                    -140.000000   \n",
       "20                                1.840498                      54.545455   \n",
       "21                                0.367883                     -18.750000   \n",
       "22                               -0.008833                     -37.500000   \n",
       "23                               -1.264551                    -100.000000   \n",
       "24                               -0.791810                     -76.470588   \n",
       "25                                0.326026                     -20.833333   \n",
       "26                                0.027045                     -35.714286   \n",
       "27                                0.601088                      -7.142857   \n",
       "28                               -0.059061                     -40.000000   \n",
       "29                                0.870170                       6.250000   \n",
       "30                                1.335524                      29.411765   \n",
       "31                               -0.173870                     -45.714286   \n",
       "32                               -0.807926                     -77.272727   \n",
       "33                                0.553251                      -9.523810   \n",
       "34                               -1.382736                    -105.882353   \n",
       "35                                1.109898                      18.181818   \n",
       "36                                0.744598                       0.000000   \n",
       "37                                0.856218                       5.555556   \n",
       "38                                0.870170                       6.250000   \n",
       "39                               -0.491801                     -61.538462   \n",
       "40                                0.856218                       5.555556   \n",
       "41                               -0.351301                     -54.545455   \n",
       "42                                0.648924                      -4.761905   \n",
       "43                               -3.428250                    -207.692308   \n",
       "44                               -1.063636                     -90.000000   \n",
       "45                                1.018573                      13.636364   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                               -1.282630                          -1.282630   \n",
       "1                               -1.760917                          -1.760917   \n",
       "2                               -0.595369                          -0.595369   \n",
       "3                               -0.945206                          -0.945206   \n",
       "4                               -1.151973                          -1.151973   \n",
       "5                               -0.489205                          -0.489205   \n",
       "6                               -1.718309                          -1.718309   \n",
       "7                               -1.145694                          -1.145694   \n",
       "8                               -0.043697                          -0.043697   \n",
       "9                                0.240855                           0.240855   \n",
       "10                               2.004069                           2.004069   \n",
       "11                               0.301223                           0.301223   \n",
       "12                               0.015433                           0.015433   \n",
       "13                               0.472801                           0.472801   \n",
       "14                              -0.657659                          -0.657659   \n",
       "15                               0.643782                           0.643782   \n",
       "16                              -0.637257                          -0.637257   \n",
       "17                              -0.781964                          -0.781964   \n",
       "18                               0.481163                           0.481163   \n",
       "19                               0.190387                           0.190387   \n",
       "20                              -0.341607                          -0.341607   \n",
       "21                              -0.864253                          -0.864253   \n",
       "22                               0.536747                           0.536747   \n",
       "23                               1.081582                           1.081582   \n",
       "24                               0.009987                           0.009987   \n",
       "25                               1.010239                           1.010239   \n",
       "26                               1.271528                           1.271528   \n",
       "27                              -0.161232                          -0.161232   \n",
       "28                               0.576257                           0.576257   \n",
       "29                               0.938667                           0.938667   \n",
       "30                               0.609143                           0.609143   \n",
       "31                              -0.484585                          -0.484585   \n",
       "32                              -0.210108                          -0.210108   \n",
       "33                               1.634297                           1.634297   \n",
       "34                              -0.768437                          -0.768437   \n",
       "35                               0.660878                           0.660878   \n",
       "36                               1.169749                           1.169749   \n",
       "37                               0.328286                           0.328286   \n",
       "38                              -0.118729                          -0.118729   \n",
       "39                              -1.032653                          -1.032653   \n",
       "40                               2.183535                           2.183535   \n",
       "41                              -2.576474                          -2.576474   \n",
       "42                               0.262965                           0.262965   \n",
       "43                               1.035924                           1.035924   \n",
       "44                              -0.650900                          -0.650900   \n",
       "45                               0.759359                           0.759359   \n",
       "\n",
       "    Subiculum_Connectivity_T  Amnesia_Lesion_T_Map  ...  Estimated_Outcome  \\\n",
       "0                  56.864683              0.447264  ...                NaN   \n",
       "1                  52.970984              0.436157  ...                NaN   \n",
       "2                  62.459631              0.497749  ...                NaN   \n",
       "3                  59.611631              0.432617  ...                NaN   \n",
       "4                  57.928350              0.193389  ...                NaN   \n",
       "5                  63.323903              0.513828  ...                NaN   \n",
       "6                  53.317851              0.249080  ...                NaN   \n",
       "7                  57.979468              0.247062  ...                NaN   \n",
       "8                  66.950749              0.514661  ...                NaN   \n",
       "9                  69.267271              0.530628  ...                NaN   \n",
       "10                 83.621480              0.514182  ...                NaN   \n",
       "11                 69.758721              0.497508  ...                NaN   \n",
       "12                 67.432125              0.461019  ...                NaN   \n",
       "13                 71.155530              0.227943  ...                NaN   \n",
       "14                 61.952527              0.485376  ...                NaN   \n",
       "15                 72.547473              0.247475  ...                NaN   \n",
       "16                 62.118622              0.503218  ...                NaN   \n",
       "17                 60.940570              0.284504  ...                NaN   \n",
       "18                 71.223601              0.483995  ...                NaN   \n",
       "19                 68.856418              0.512125  ...                NaN   \n",
       "20                 64.525483              0.507649  ...                NaN   \n",
       "21                 60.270662              0.460080  ...                NaN   \n",
       "22                 71.676110              0.479488  ...                NaN   \n",
       "23                 76.111573              0.477498  ...                NaN   \n",
       "24                 67.387791              0.501404  ...                NaN   \n",
       "25                 75.530777              0.511383  ...                NaN   \n",
       "26                 77.657909              0.496203  ...                NaN   \n",
       "27                 65.993904              0.510956  ...                NaN   \n",
       "28                 71.997755              0.458336  ...                NaN   \n",
       "29                 74.948112              0.503698  ...                NaN   \n",
       "30                 72.265483              0.402215  ...                NaN   \n",
       "31                 63.361510              0.481574  ...                NaN   \n",
       "32                 65.596015              0.486037  ...                NaN   \n",
       "33                 80.611188              0.471613  ...                NaN   \n",
       "34                 61.050691              0.520899  ...                NaN   \n",
       "35                 72.686647              0.415110  ...                NaN   \n",
       "36                 76.829337              0.503535  ...                NaN   \n",
       "37                 69.979040              0.521941  ...                NaN   \n",
       "38                 66.339918              0.569043  ...                NaN   \n",
       "39                 58.899731              0.470849  ...                NaN   \n",
       "40                 85.082502              0.420971  ...                NaN   \n",
       "41                 46.331586              0.469571  ...                NaN   \n",
       "42                 69.447270              0.530166  ...                NaN   \n",
       "43                 75.739873              0.489891  ...                NaN   \n",
       "44                 62.007555              0.480527  ...                NaN   \n",
       "45                 73.488381              0.461720  ...                NaN   \n",
       "\n",
       "    Cognitive_Baseline  Z_Scored_Cognitive_Baseline  \\\n",
       "0                   28                     1.518764   \n",
       "1                   22                     0.465551   \n",
       "2                   19                    -0.061056   \n",
       "3                   17                    -0.412127   \n",
       "4                   19                    -0.061056   \n",
       "5                   13                    -1.114269   \n",
       "6                   13                    -1.114269   \n",
       "7                   24                     0.816622   \n",
       "8                   23                     0.641086   \n",
       "9                   13                    -1.114269   \n",
       "10                  12                    -1.289805   \n",
       "11                  15                    -0.763198   \n",
       "12                  31                     2.045371   \n",
       "13                  29                     1.694300   \n",
       "14                  19                    -0.061056   \n",
       "15                  32                     2.220907   \n",
       "16                  16                    -0.587663   \n",
       "17                  18                    -0.236591   \n",
       "18                  23                     0.641086   \n",
       "19                  15                    -0.763198   \n",
       "20                  22                     0.465551   \n",
       "21                  16                    -0.587663   \n",
       "22                  16                    -0.587663   \n",
       "23                  21                     0.290015   \n",
       "24                  17                    -0.412127   \n",
       "25                  24                     0.816622   \n",
       "26                  28                     1.518764   \n",
       "27                  14                    -0.938734   \n",
       "28                  20                     0.114480   \n",
       "29                  16                    -0.587663   \n",
       "30                  17                    -0.412127   \n",
       "31                  35                     2.747514   \n",
       "32                  22                     0.465551   \n",
       "33                  21                     0.290015   \n",
       "34                  17                    -0.412127   \n",
       "35                  22                     0.465551   \n",
       "36                  19                    -0.061056   \n",
       "37                  18                    -0.236591   \n",
       "38                  16                    -0.587663   \n",
       "39                  13                    -1.114269   \n",
       "40                  18                    -0.236591   \n",
       "41                  11                    -1.465341   \n",
       "42                  21                     0.290015   \n",
       "43                  13                    -1.114269   \n",
       "44                  10                    -1.640876   \n",
       "45                  22                     0.465551   \n",
       "\n",
       "    Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                       -1.518764   \n",
       "1                                       -0.465551   \n",
       "2                                        0.061056   \n",
       "3                                        0.412127   \n",
       "4                                        0.061056   \n",
       "5                                        1.114269   \n",
       "6                                        1.114269   \n",
       "7                                       -0.816622   \n",
       "8                                       -0.641086   \n",
       "9                                        1.114269   \n",
       "10                                       1.289805   \n",
       "11                                       0.763198   \n",
       "12                                      -2.045371   \n",
       "13                                      -1.694300   \n",
       "14                                       0.061056   \n",
       "15                                      -2.220907   \n",
       "16                                       0.587663   \n",
       "17                                       0.236591   \n",
       "18                                      -0.641086   \n",
       "19                                       0.763198   \n",
       "20                                      -0.465551   \n",
       "21                                       0.587663   \n",
       "22                                       0.587663   \n",
       "23                                      -0.290015   \n",
       "24                                       0.412127   \n",
       "25                                      -0.816622   \n",
       "26                                      -1.518764   \n",
       "27                                       0.938734   \n",
       "28                                      -0.114480   \n",
       "29                                       0.587663   \n",
       "30                                       0.412127   \n",
       "31                                      -2.747514   \n",
       "32                                      -0.465551   \n",
       "33                                      -0.290015   \n",
       "34                                       0.412127   \n",
       "35                                      -0.465551   \n",
       "36                                       0.061056   \n",
       "37                                       0.236591   \n",
       "38                                       0.587663   \n",
       "39                                       1.114269   \n",
       "40                                       0.236591   \n",
       "41                                       1.465341   \n",
       "42                                      -0.290015   \n",
       "43                                       1.114269   \n",
       "44                                       1.640876   \n",
       "45                                      -0.465551   \n",
       "\n",
       "    Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                          0.72                                 0.28   \n",
       "1                          0.48                                 0.52   \n",
       "2                          0.36                                 0.64   \n",
       "3                          0.28                                 0.72   \n",
       "4                          0.36                                 0.64   \n",
       "5                          0.12                                 0.88   \n",
       "6                          0.12                                 0.88   \n",
       "7                          0.56                                 0.44   \n",
       "8                          0.52                                 0.48   \n",
       "9                          0.12                                 0.88   \n",
       "10                         0.08                                 0.92   \n",
       "11                         0.20                                 0.80   \n",
       "12                         0.84                                 0.16   \n",
       "13                         0.76                                 0.24   \n",
       "14                         0.36                                 0.64   \n",
       "15                         0.88                                 0.12   \n",
       "16                         0.24                                 0.76   \n",
       "17                         0.32                                 0.68   \n",
       "18                         0.52                                 0.48   \n",
       "19                         0.20                                 0.80   \n",
       "20                         0.48                                 0.52   \n",
       "21                         0.24                                 0.76   \n",
       "22                         0.24                                 0.76   \n",
       "23                         0.44                                 0.56   \n",
       "24                         0.28                                 0.72   \n",
       "25                         0.56                                 0.44   \n",
       "26                         0.72                                 0.28   \n",
       "27                         0.16                                 0.84   \n",
       "28                         0.40                                 0.60   \n",
       "29                         0.24                                 0.76   \n",
       "30                         0.28                                 0.72   \n",
       "31                         1.00                                 0.00   \n",
       "32                         0.48                                 0.52   \n",
       "33                         0.44                                 0.56   \n",
       "34                         0.28                                 0.72   \n",
       "35                         0.48                                 0.52   \n",
       "36                         0.36                                 0.64   \n",
       "37                         0.32                                 0.68   \n",
       "38                         0.24                                 0.76   \n",
       "39                         0.12                                 0.88   \n",
       "40                         0.32                                 0.68   \n",
       "41                         0.04                                 0.96   \n",
       "42                         0.44                                 0.56   \n",
       "43                         0.12                                 0.88   \n",
       "44                         0.00                                 1.00   \n",
       "45                         0.48                                 0.52   \n",
       "\n",
       "    ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0        12.222658      14.493929            -1.714513            -1.227368  \n",
       "1        14.020048      15.257338            -1.155843            -1.022243  \n",
       "2        15.118727      17.376384            -0.814348            -0.452865  \n",
       "3        13.112424      15.287916            -1.437954            -1.014027  \n",
       "4        15.086568      12.951426            -0.824344            -1.641831  \n",
       "5        15.816634      17.617107            -0.597423            -0.388183  \n",
       "6        15.524025      13.452311            -0.688373            -1.507246  \n",
       "7        16.546984      13.932696            -0.370413            -1.378169  \n",
       "8        19.669539      21.341523             0.600149             0.612551  \n",
       "9        18.295718      19.263977             0.173133             0.054323  \n",
       "10       17.022328      16.151031            -0.222665            -0.782112  \n",
       "11       16.225059      17.818822            -0.470475            -0.333983  \n",
       "12       15.295109      18.046377            -0.759525            -0.272840  \n",
       "13       18.623736      16.113043             0.275089            -0.792319  \n",
       "14       16.462336      18.503053            -0.396724            -0.150134  \n",
       "15       15.842273      19.964170            -0.589454             0.242462  \n",
       "16       17.332886      19.198042            -0.126137             0.036607  \n",
       "17       10.126563       9.630690            -2.366028            -2.534098  \n",
       "18       22.476761      23.985143             1.472698             1.322880  \n",
       "19       21.466327      23.582758             1.158632             1.214761  \n",
       "20       19.453283      21.676897             0.532931             0.702664  \n",
       "21       13.457540      15.819523            -1.330684            -0.871186  \n",
       "22       22.443084      24.738357             1.462230             1.525265  \n",
       "23       20.186102      22.065363             0.760708             0.807044  \n",
       "24       16.996513      18.394062            -0.230689            -0.179419  \n",
       "25       17.204945      18.264558            -0.165904            -0.214216  \n",
       "26       21.078380      22.489816             1.038049             0.921092  \n",
       "27       18.617648      20.415928             0.273197             0.363848  \n",
       "28       18.410547      19.822564             0.208825             0.204413  \n",
       "29       18.420784      19.612924             0.212007             0.148084  \n",
       "30       23.613479      26.048153             1.826016             1.877202  \n",
       "31       16.101986      17.647381            -0.508729            -0.380049  \n",
       "32       16.837360      18.171505            -0.280158            -0.239219  \n",
       "33       20.531868      22.371502             0.868180             0.889302  \n",
       "34       17.957341      19.952870             0.067958             0.239426  \n",
       "35       24.418011      27.143129             2.076083             2.171417  \n",
       "36       17.856530      19.670979             0.036624             0.163683  \n",
       "37       14.769545      16.131533            -0.922882            -0.787351  \n",
       "38       20.179952      19.975031             0.758797             0.245380  \n",
       "39       11.801552      14.042861            -1.845403            -1.348567  \n",
       "40       21.327670      22.294652             1.115534             0.868652  \n",
       "41       16.512422      18.197520            -0.381156            -0.232229  \n",
       "42       19.159295      19.553377             0.441553             0.132084  \n",
       "43       19.340269      21.114119             0.497804             0.551449  \n",
       "44       19.062652      20.569076             0.411514             0.404998  \n",
       "45       23.954833      26.691505             1.932117             2.050067  \n",
       "\n",
       "[46 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = None # ['Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group', 'Z_Scored_Subiculum_T_By_Origin_Group_'] #['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to standardize column Disease: Could not convert AlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimerAlzheimer to numeric\n",
      "Unable to standardize column City: Could not convert TorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoTorontoToronto to numeric\n",
      "Unable to standardize column Age_Group: Could not convert youngoldoldoldyoungoldyoungyoungoldoldyoungoldoldyoungoldyoungoldoldoldyoungyoungyoungoldoldoldoldoldoldoldoldoldyoungyoungoldyoungoldoldoldoldoldoldoldyoungyoungoldold to numeric\n",
      "Unable to standardize column Age_And_Disease: Could not convert Toronto_youngToronto_oldToronto_oldToronto_oldToronto_youngToronto_oldToronto_youngToronto_youngToronto_oldToronto_oldToronto_youngToronto_oldToronto_oldToronto_youngToronto_oldToronto_youngToronto_oldToronto_oldToronto_oldToronto_youngToronto_youngToronto_youngToronto_oldToronto_oldToronto_oldToronto_oldToronto_oldToronto_oldToronto_oldToronto_oldToronto_oldToronto_youngToronto_youngToronto_oldToronto_youngToronto_oldToronto_oldToronto_oldToronto_oldToronto_oldToronto_oldToronto_oldToronto_youngToronto_youngToronto_oldToronto_old to numeric\n",
      "Unable to standardize column Age_Disease_and_Cohort: Could not convert Toronto_young_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_young_AlzheimerToronto_old_AlzheimerToronto_young_AlzheimerToronto_young_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_young_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_young_AlzheimerToronto_old_AlzheimerToronto_young_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_young_AlzheimerToronto_young_AlzheimerToronto_young_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_young_AlzheimerToronto_young_AlzheimerToronto_old_AlzheimerToronto_young_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_old_AlzheimerToronto_young_AlzheimerToronto_young_AlzheimerToronto_old_AlzheimerToronto_old_Alzheimer to numeric\n",
      "Unable to standardize column Subiculum_Group_By_Z_Score_Sign: Could not convert lowlowlowlowlowlowlowlowlowhighhighhighhighhighlowhighlowlowhighhighlowlowhighhighhighhighhighlowhighhighhighlowlowhighlowhighhighhighlowlowhighlowhighhighlowhigh to numeric\n",
      "Unable to standardize column Subiculum_Group_By_Inflection_Point: Could not convert lowlowlowlowlowlowlowlowlowhighhighhighlowhighlowhighlowlowhighhighlowlowhighhighlowhighhighlowhighhighhighlowlowhighlowhighhighhighlowlowhighlowhighhighlowhigh to numeric\n",
      "Unable to standardize column Cognitive_Outcome: Could not convert improveimprovedeclinedeclineimprovedeclineimproveimproveimprovedeclineimprovedeclineimprovedeclineimproveimprovedeclineimproveimprovedeclineimproveimprovedeclinedeclinedeclineimproveimproveimprovedeclineimproveimprovedeclinedeclineimprovedeclineimproveimproveimproveimprovedeclineimprovedeclineimprovedeclinedeclineimprove to numeric\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>...</th>\n",
       "      <th>Estimated_Outcome</th>\n",
       "      <th>Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.668629</td>\n",
       "      <td>-0.618120</td>\n",
       "      <td>0.317537</td>\n",
       "      <td>0.317537</td>\n",
       "      <td>0.317537</td>\n",
       "      <td>0.317537</td>\n",
       "      <td>-1.296803</td>\n",
       "      <td>-1.296803</td>\n",
       "      <td>-1.296803</td>\n",
       "      <td>-0.075475</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.535547</td>\n",
       "      <td>1.535547</td>\n",
       "      <td>-1.535547</td>\n",
       "      <td>1.535547</td>\n",
       "      <td>-1.535547</td>\n",
       "      <td>-1.733459</td>\n",
       "      <td>-1.240930</td>\n",
       "      <td>-1.733459</td>\n",
       "      <td>-1.240930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.600823</td>\n",
       "      <td>1.277448</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>-1.780375</td>\n",
       "      <td>-1.780375</td>\n",
       "      <td>-1.780375</td>\n",
       "      <td>-0.200665</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>-1.168615</td>\n",
       "      <td>-1.033539</td>\n",
       "      <td>-1.168615</td>\n",
       "      <td>-1.033539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.533016</td>\n",
       "      <td>1.151077</td>\n",
       "      <td>-0.850871</td>\n",
       "      <td>-0.850871</td>\n",
       "      <td>-0.850871</td>\n",
       "      <td>-0.850871</td>\n",
       "      <td>-0.601947</td>\n",
       "      <td>-0.601947</td>\n",
       "      <td>-0.601947</td>\n",
       "      <td>0.493589</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>-0.823347</td>\n",
       "      <td>-0.457869</td>\n",
       "      <td>-0.823347</td>\n",
       "      <td>-0.457869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.465210</td>\n",
       "      <td>-0.239006</td>\n",
       "      <td>-1.875980</td>\n",
       "      <td>-1.875980</td>\n",
       "      <td>-1.875980</td>\n",
       "      <td>-1.875980</td>\n",
       "      <td>-0.955650</td>\n",
       "      <td>-0.955650</td>\n",
       "      <td>-0.955650</td>\n",
       "      <td>-0.240569</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>0.416681</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>0.416681</td>\n",
       "      <td>-1.453843</td>\n",
       "      <td>-1.025232</td>\n",
       "      <td>-1.453843</td>\n",
       "      <td>-1.025232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.397403</td>\n",
       "      <td>-2.134575</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>-1.164703</td>\n",
       "      <td>-1.164703</td>\n",
       "      <td>-1.164703</td>\n",
       "      <td>-2.937139</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>-0.833453</td>\n",
       "      <td>-1.659974</td>\n",
       "      <td>-0.833453</td>\n",
       "      <td>-1.659974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.329597</td>\n",
       "      <td>-0.112635</td>\n",
       "      <td>-0.028462</td>\n",
       "      <td>-0.028462</td>\n",
       "      <td>-0.028462</td>\n",
       "      <td>-0.028462</td>\n",
       "      <td>-0.494611</td>\n",
       "      <td>-0.494611</td>\n",
       "      <td>-0.494611</td>\n",
       "      <td>0.674839</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>-0.604024</td>\n",
       "      <td>-0.392473</td>\n",
       "      <td>-0.604024</td>\n",
       "      <td>-0.392473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.261790</td>\n",
       "      <td>-0.365378</td>\n",
       "      <td>0.440311</td>\n",
       "      <td>0.440311</td>\n",
       "      <td>0.440311</td>\n",
       "      <td>0.440311</td>\n",
       "      <td>-1.737296</td>\n",
       "      <td>-1.737296</td>\n",
       "      <td>-1.737296</td>\n",
       "      <td>-2.309392</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>-0.695979</td>\n",
       "      <td>-1.523901</td>\n",
       "      <td>-0.695979</td>\n",
       "      <td>-1.523901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.193984</td>\n",
       "      <td>-0.870863</td>\n",
       "      <td>0.160349</td>\n",
       "      <td>0.160349</td>\n",
       "      <td>0.160349</td>\n",
       "      <td>0.160349</td>\n",
       "      <td>-1.158354</td>\n",
       "      <td>-1.158354</td>\n",
       "      <td>-1.158354</td>\n",
       "      <td>-2.332131</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.825646</td>\n",
       "      <td>0.825646</td>\n",
       "      <td>-0.825646</td>\n",
       "      <td>0.825646</td>\n",
       "      <td>-0.825646</td>\n",
       "      <td>-0.374507</td>\n",
       "      <td>-1.393397</td>\n",
       "      <td>-0.374507</td>\n",
       "      <td>-1.393397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.126177</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.134589</td>\n",
       "      <td>0.134589</td>\n",
       "      <td>0.134589</td>\n",
       "      <td>0.134589</td>\n",
       "      <td>-0.044180</td>\n",
       "      <td>-0.044180</td>\n",
       "      <td>-0.044180</td>\n",
       "      <td>0.684226</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.648171</td>\n",
       "      <td>0.648171</td>\n",
       "      <td>-0.648171</td>\n",
       "      <td>0.648171</td>\n",
       "      <td>-0.648171</td>\n",
       "      <td>0.606780</td>\n",
       "      <td>0.619320</td>\n",
       "      <td>0.606780</td>\n",
       "      <td>0.619320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.058371</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>-0.966009</td>\n",
       "      <td>-0.966009</td>\n",
       "      <td>-0.966009</td>\n",
       "      <td>-0.966009</td>\n",
       "      <td>0.243516</td>\n",
       "      <td>0.243516</td>\n",
       "      <td>0.243516</td>\n",
       "      <td>0.864200</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.054924</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.054924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.990564</td>\n",
       "      <td>-0.618120</td>\n",
       "      <td>1.599222</td>\n",
       "      <td>1.599222</td>\n",
       "      <td>1.599222</td>\n",
       "      <td>1.599222</td>\n",
       "      <td>2.026214</td>\n",
       "      <td>2.026214</td>\n",
       "      <td>2.026214</td>\n",
       "      <td>0.678820</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.304057</td>\n",
       "      <td>-1.304057</td>\n",
       "      <td>1.304057</td>\n",
       "      <td>-1.304057</td>\n",
       "      <td>1.304057</td>\n",
       "      <td>-0.225126</td>\n",
       "      <td>-0.790754</td>\n",
       "      <td>-0.225126</td>\n",
       "      <td>-0.790754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.854951</td>\n",
       "      <td>0.266478</td>\n",
       "      <td>-0.465984</td>\n",
       "      <td>-0.465984</td>\n",
       "      <td>-0.465984</td>\n",
       "      <td>-0.465984</td>\n",
       "      <td>0.304551</td>\n",
       "      <td>0.304551</td>\n",
       "      <td>0.304551</td>\n",
       "      <td>0.490881</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.771632</td>\n",
       "      <td>-0.771632</td>\n",
       "      <td>0.771632</td>\n",
       "      <td>-0.771632</td>\n",
       "      <td>0.771632</td>\n",
       "      <td>-0.475674</td>\n",
       "      <td>-0.337674</td>\n",
       "      <td>-0.475674</td>\n",
       "      <td>-0.337674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.787145</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.425189</td>\n",
       "      <td>0.425189</td>\n",
       "      <td>0.425189</td>\n",
       "      <td>0.425189</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>0.079576</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.067973</td>\n",
       "      <td>2.067973</td>\n",
       "      <td>-2.067973</td>\n",
       "      <td>2.067973</td>\n",
       "      <td>-2.067973</td>\n",
       "      <td>-0.767918</td>\n",
       "      <td>-0.275855</td>\n",
       "      <td>-0.767918</td>\n",
       "      <td>-0.275855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.719338</td>\n",
       "      <td>-0.870863</td>\n",
       "      <td>-0.227826</td>\n",
       "      <td>-0.227826</td>\n",
       "      <td>-0.227826</td>\n",
       "      <td>-0.227826</td>\n",
       "      <td>0.478026</td>\n",
       "      <td>0.478026</td>\n",
       "      <td>0.478026</td>\n",
       "      <td>-2.547637</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.713022</td>\n",
       "      <td>1.713022</td>\n",
       "      <td>-1.713022</td>\n",
       "      <td>1.713022</td>\n",
       "      <td>-1.713022</td>\n",
       "      <td>0.278129</td>\n",
       "      <td>-0.801074</td>\n",
       "      <td>0.278129</td>\n",
       "      <td>-0.801074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.651532</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>-0.664926</td>\n",
       "      <td>-0.664926</td>\n",
       "      <td>-0.664926</td>\n",
       "      <td>0.354131</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>-0.401108</td>\n",
       "      <td>-0.151793</td>\n",
       "      <td>-0.401108</td>\n",
       "      <td>-0.151793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.515919</td>\n",
       "      <td>-1.881833</td>\n",
       "      <td>0.689346</td>\n",
       "      <td>0.689346</td>\n",
       "      <td>0.689346</td>\n",
       "      <td>0.689346</td>\n",
       "      <td>0.650896</td>\n",
       "      <td>0.650896</td>\n",
       "      <td>0.650896</td>\n",
       "      <td>-2.327480</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.245448</td>\n",
       "      <td>2.245448</td>\n",
       "      <td>-2.245448</td>\n",
       "      <td>2.245448</td>\n",
       "      <td>-2.245448</td>\n",
       "      <td>-0.595967</td>\n",
       "      <td>0.245141</td>\n",
       "      <td>-0.595967</td>\n",
       "      <td>0.245141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.448113</td>\n",
       "      <td>1.024706</td>\n",
       "      <td>-0.897646</td>\n",
       "      <td>-0.897646</td>\n",
       "      <td>-0.897646</td>\n",
       "      <td>-0.897646</td>\n",
       "      <td>-0.644299</td>\n",
       "      <td>-0.644299</td>\n",
       "      <td>-0.644299</td>\n",
       "      <td>0.555234</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>-0.127531</td>\n",
       "      <td>0.037012</td>\n",
       "      <td>-0.127531</td>\n",
       "      <td>0.037012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.380306</td>\n",
       "      <td>0.140107</td>\n",
       "      <td>0.188562</td>\n",
       "      <td>0.188562</td>\n",
       "      <td>0.188562</td>\n",
       "      <td>0.188562</td>\n",
       "      <td>-0.790605</td>\n",
       "      <td>-0.790605</td>\n",
       "      <td>-0.790605</td>\n",
       "      <td>-1.910090</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.239206</td>\n",
       "      <td>-0.239206</td>\n",
       "      <td>0.239206</td>\n",
       "      <td>-0.239206</td>\n",
       "      <td>0.239206</td>\n",
       "      <td>-2.392173</td>\n",
       "      <td>-2.562100</td>\n",
       "      <td>-2.392173</td>\n",
       "      <td>-2.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.312500</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.664506</td>\n",
       "      <td>0.664506</td>\n",
       "      <td>0.664506</td>\n",
       "      <td>0.664506</td>\n",
       "      <td>0.486480</td>\n",
       "      <td>0.486480</td>\n",
       "      <td>0.486480</td>\n",
       "      <td>0.338560</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.648171</td>\n",
       "      <td>0.648171</td>\n",
       "      <td>-0.648171</td>\n",
       "      <td>0.648171</td>\n",
       "      <td>-0.648171</td>\n",
       "      <td>1.488971</td>\n",
       "      <td>1.337498</td>\n",
       "      <td>1.488971</td>\n",
       "      <td>1.337498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.244693</td>\n",
       "      <td>-1.123605</td>\n",
       "      <td>-2.091064</td>\n",
       "      <td>-2.091064</td>\n",
       "      <td>-2.091064</td>\n",
       "      <td>-2.091064</td>\n",
       "      <td>0.192491</td>\n",
       "      <td>0.192491</td>\n",
       "      <td>0.192491</td>\n",
       "      <td>0.655642</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.771632</td>\n",
       "      <td>-0.771632</td>\n",
       "      <td>0.771632</td>\n",
       "      <td>-0.771632</td>\n",
       "      <td>0.771632</td>\n",
       "      <td>1.171435</td>\n",
       "      <td>1.228184</td>\n",
       "      <td>1.171435</td>\n",
       "      <td>1.228184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.176887</td>\n",
       "      <td>-2.513689</td>\n",
       "      <td>1.860835</td>\n",
       "      <td>1.860835</td>\n",
       "      <td>1.860835</td>\n",
       "      <td>1.860835</td>\n",
       "      <td>-0.345382</td>\n",
       "      <td>-0.345382</td>\n",
       "      <td>-0.345382</td>\n",
       "      <td>0.605188</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>0.538820</td>\n",
       "      <td>0.710429</td>\n",
       "      <td>0.538820</td>\n",
       "      <td>0.710429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.109080</td>\n",
       "      <td>-0.744491</td>\n",
       "      <td>0.371948</td>\n",
       "      <td>0.371948</td>\n",
       "      <td>0.371948</td>\n",
       "      <td>0.371948</td>\n",
       "      <td>-0.873803</td>\n",
       "      <td>-0.873803</td>\n",
       "      <td>-0.873803</td>\n",
       "      <td>0.068986</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>-1.345388</td>\n",
       "      <td>-0.880813</td>\n",
       "      <td>-1.345388</td>\n",
       "      <td>-0.880813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.041274</td>\n",
       "      <td>0.771963</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>0.542678</td>\n",
       "      <td>0.542678</td>\n",
       "      <td>0.542678</td>\n",
       "      <td>0.287759</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>1.478388</td>\n",
       "      <td>1.542120</td>\n",
       "      <td>1.478388</td>\n",
       "      <td>1.542120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.266478</td>\n",
       "      <td>-1.278524</td>\n",
       "      <td>-1.278524</td>\n",
       "      <td>-1.278524</td>\n",
       "      <td>-1.278524</td>\n",
       "      <td>1.093533</td>\n",
       "      <td>1.093533</td>\n",
       "      <td>1.093533</td>\n",
       "      <td>0.265323</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.29322</td>\n",
       "      <td>0.293220</td>\n",
       "      <td>-0.293220</td>\n",
       "      <td>0.293220</td>\n",
       "      <td>-0.293220</td>\n",
       "      <td>0.769114</td>\n",
       "      <td>0.815961</td>\n",
       "      <td>0.769114</td>\n",
       "      <td>0.815961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.094339</td>\n",
       "      <td>0.898335</td>\n",
       "      <td>-0.800559</td>\n",
       "      <td>-0.800559</td>\n",
       "      <td>-0.800559</td>\n",
       "      <td>-0.800559</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.534792</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>0.416681</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>0.416681</td>\n",
       "      <td>-0.233238</td>\n",
       "      <td>-0.181402</td>\n",
       "      <td>-0.233238</td>\n",
       "      <td>-0.181402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.162146</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.329628</td>\n",
       "      <td>0.329628</td>\n",
       "      <td>0.329628</td>\n",
       "      <td>0.329628</td>\n",
       "      <td>1.021402</td>\n",
       "      <td>1.021402</td>\n",
       "      <td>1.021402</td>\n",
       "      <td>0.647270</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.825646</td>\n",
       "      <td>0.825646</td>\n",
       "      <td>-0.825646</td>\n",
       "      <td>0.825646</td>\n",
       "      <td>-0.825646</td>\n",
       "      <td>-0.167737</td>\n",
       "      <td>-0.216583</td>\n",
       "      <td>-0.167737</td>\n",
       "      <td>-0.216583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.229952</td>\n",
       "      <td>0.266478</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>1.285578</td>\n",
       "      <td>1.285578</td>\n",
       "      <td>1.285578</td>\n",
       "      <td>0.476165</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.535547</td>\n",
       "      <td>1.535547</td>\n",
       "      <td>-1.535547</td>\n",
       "      <td>1.535547</td>\n",
       "      <td>-1.535547</td>\n",
       "      <td>1.049519</td>\n",
       "      <td>0.931270</td>\n",
       "      <td>1.049519</td>\n",
       "      <td>0.931270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.297759</td>\n",
       "      <td>-0.112635</td>\n",
       "      <td>0.607730</td>\n",
       "      <td>0.607730</td>\n",
       "      <td>0.607730</td>\n",
       "      <td>0.607730</td>\n",
       "      <td>-0.163014</td>\n",
       "      <td>-0.163014</td>\n",
       "      <td>-0.163014</td>\n",
       "      <td>0.642456</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.949107</td>\n",
       "      <td>-0.949107</td>\n",
       "      <td>0.949107</td>\n",
       "      <td>-0.949107</td>\n",
       "      <td>0.949107</td>\n",
       "      <td>0.276215</td>\n",
       "      <td>0.367868</td>\n",
       "      <td>0.276215</td>\n",
       "      <td>0.367868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.365565</td>\n",
       "      <td>0.140107</td>\n",
       "      <td>-0.059714</td>\n",
       "      <td>-0.059714</td>\n",
       "      <td>-0.059714</td>\n",
       "      <td>-0.059714</td>\n",
       "      <td>0.582625</td>\n",
       "      <td>0.582625</td>\n",
       "      <td>0.582625</td>\n",
       "      <td>0.049328</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.115745</td>\n",
       "      <td>0.115745</td>\n",
       "      <td>-0.115745</td>\n",
       "      <td>0.115745</td>\n",
       "      <td>-0.115745</td>\n",
       "      <td>0.211132</td>\n",
       "      <td>0.206672</td>\n",
       "      <td>0.211132</td>\n",
       "      <td>0.206672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.501178</td>\n",
       "      <td>0.898335</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.949039</td>\n",
       "      <td>0.949039</td>\n",
       "      <td>0.949039</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>0.214349</td>\n",
       "      <td>0.149720</td>\n",
       "      <td>0.214349</td>\n",
       "      <td>0.149720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.568985</td>\n",
       "      <td>-0.112635</td>\n",
       "      <td>1.350282</td>\n",
       "      <td>1.350282</td>\n",
       "      <td>1.350282</td>\n",
       "      <td>1.350282</td>\n",
       "      <td>0.615874</td>\n",
       "      <td>0.615874</td>\n",
       "      <td>0.615874</td>\n",
       "      <td>-0.583256</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>0.416681</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>0.416681</td>\n",
       "      <td>1.846193</td>\n",
       "      <td>1.897945</td>\n",
       "      <td>1.846193</td>\n",
       "      <td>1.897945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.636791</td>\n",
       "      <td>-1.249976</td>\n",
       "      <td>-0.175791</td>\n",
       "      <td>-0.175791</td>\n",
       "      <td>-0.175791</td>\n",
       "      <td>-0.175791</td>\n",
       "      <td>-0.489940</td>\n",
       "      <td>-0.489940</td>\n",
       "      <td>-0.489940</td>\n",
       "      <td>0.311270</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.777874</td>\n",
       "      <td>2.777874</td>\n",
       "      <td>-2.777874</td>\n",
       "      <td>2.777874</td>\n",
       "      <td>-2.777874</td>\n",
       "      <td>-0.514351</td>\n",
       "      <td>-0.384248</td>\n",
       "      <td>-0.514351</td>\n",
       "      <td>-0.384248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.772404</td>\n",
       "      <td>-1.249976</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>-0.212429</td>\n",
       "      <td>-0.212429</td>\n",
       "      <td>-0.212429</td>\n",
       "      <td>0.361575</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>-0.283253</td>\n",
       "      <td>-0.241862</td>\n",
       "      <td>-0.283253</td>\n",
       "      <td>-0.241862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.840211</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.559364</td>\n",
       "      <td>0.559364</td>\n",
       "      <td>0.559364</td>\n",
       "      <td>0.559364</td>\n",
       "      <td>1.652356</td>\n",
       "      <td>1.652356</td>\n",
       "      <td>1.652356</td>\n",
       "      <td>0.198994</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.29322</td>\n",
       "      <td>0.293220</td>\n",
       "      <td>-0.293220</td>\n",
       "      <td>0.293220</td>\n",
       "      <td>-0.293220</td>\n",
       "      <td>0.877774</td>\n",
       "      <td>0.899129</td>\n",
       "      <td>0.877774</td>\n",
       "      <td>0.899129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.908017</td>\n",
       "      <td>-1.123605</td>\n",
       "      <td>-1.398015</td>\n",
       "      <td>-1.398015</td>\n",
       "      <td>-1.398015</td>\n",
       "      <td>-1.398015</td>\n",
       "      <td>-0.776928</td>\n",
       "      <td>-0.776928</td>\n",
       "      <td>-0.776928</td>\n",
       "      <td>0.754540</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>0.416681</td>\n",
       "      <td>-0.416681</td>\n",
       "      <td>0.416681</td>\n",
       "      <td>0.068709</td>\n",
       "      <td>0.242072</td>\n",
       "      <td>0.068709</td>\n",
       "      <td>0.242072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.975824</td>\n",
       "      <td>0.771963</td>\n",
       "      <td>1.122162</td>\n",
       "      <td>1.122162</td>\n",
       "      <td>1.122162</td>\n",
       "      <td>1.122162</td>\n",
       "      <td>0.668180</td>\n",
       "      <td>0.668180</td>\n",
       "      <td>0.668180</td>\n",
       "      <td>-0.437912</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>2.099023</td>\n",
       "      <td>2.195411</td>\n",
       "      <td>2.099023</td>\n",
       "      <td>2.195411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.043630</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.752826</td>\n",
       "      <td>0.752826</td>\n",
       "      <td>0.752826</td>\n",
       "      <td>0.752826</td>\n",
       "      <td>1.182675</td>\n",
       "      <td>1.182675</td>\n",
       "      <td>1.182675</td>\n",
       "      <td>0.558811</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>-0.061731</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>0.037028</td>\n",
       "      <td>0.165492</td>\n",
       "      <td>0.037028</td>\n",
       "      <td>0.165492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.111437</td>\n",
       "      <td>1.277448</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.331913</td>\n",
       "      <td>0.331913</td>\n",
       "      <td>0.331913</td>\n",
       "      <td>0.766285</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.239206</td>\n",
       "      <td>-0.239206</td>\n",
       "      <td>0.239206</td>\n",
       "      <td>-0.239206</td>\n",
       "      <td>0.239206</td>\n",
       "      <td>-0.933080</td>\n",
       "      <td>-0.796051</td>\n",
       "      <td>-0.933080</td>\n",
       "      <td>-0.796051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.179243</td>\n",
       "      <td>0.519221</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>-0.120041</td>\n",
       "      <td>-0.120041</td>\n",
       "      <td>-0.120041</td>\n",
       "      <td>1.297212</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>-0.594156</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>0.767181</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>0.767181</td>\n",
       "      <td>0.248092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.247050</td>\n",
       "      <td>1.530191</td>\n",
       "      <td>-0.497236</td>\n",
       "      <td>-0.497236</td>\n",
       "      <td>-0.497236</td>\n",
       "      <td>-0.497236</td>\n",
       "      <td>-1.044063</td>\n",
       "      <td>-1.044063</td>\n",
       "      <td>-1.044063</td>\n",
       "      <td>0.190382</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>-1.865794</td>\n",
       "      <td>-1.363469</td>\n",
       "      <td>-1.865794</td>\n",
       "      <td>-1.363469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.314856</td>\n",
       "      <td>0.898335</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>2.207664</td>\n",
       "      <td>2.207664</td>\n",
       "      <td>2.207664</td>\n",
       "      <td>-0.371849</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.239206</td>\n",
       "      <td>-0.239206</td>\n",
       "      <td>0.239206</td>\n",
       "      <td>-0.239206</td>\n",
       "      <td>0.239206</td>\n",
       "      <td>1.127861</td>\n",
       "      <td>0.878251</td>\n",
       "      <td>1.127861</td>\n",
       "      <td>0.878251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.382663</td>\n",
       "      <td>1.151077</td>\n",
       "      <td>-0.355183</td>\n",
       "      <td>-0.355183</td>\n",
       "      <td>-0.355183</td>\n",
       "      <td>-0.355183</td>\n",
       "      <td>-2.604944</td>\n",
       "      <td>-2.604944</td>\n",
       "      <td>-2.604944</td>\n",
       "      <td>0.175968</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.481533</td>\n",
       "      <td>-1.481533</td>\n",
       "      <td>1.481533</td>\n",
       "      <td>-1.481533</td>\n",
       "      <td>1.481533</td>\n",
       "      <td>-0.385368</td>\n",
       "      <td>-0.234795</td>\n",
       "      <td>-0.385368</td>\n",
       "      <td>-0.234795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.450469</td>\n",
       "      <td>-0.997234</td>\n",
       "      <td>0.656095</td>\n",
       "      <td>0.656095</td>\n",
       "      <td>0.656095</td>\n",
       "      <td>0.656095</td>\n",
       "      <td>0.265871</td>\n",
       "      <td>0.265871</td>\n",
       "      <td>0.265871</td>\n",
       "      <td>0.858991</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.29322</td>\n",
       "      <td>0.293220</td>\n",
       "      <td>-0.293220</td>\n",
       "      <td>0.293220</td>\n",
       "      <td>-0.293220</td>\n",
       "      <td>0.446432</td>\n",
       "      <td>0.133543</td>\n",
       "      <td>0.446432</td>\n",
       "      <td>0.133543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.518276</td>\n",
       "      <td>-2.008204</td>\n",
       "      <td>-3.466132</td>\n",
       "      <td>-3.466132</td>\n",
       "      <td>-3.466132</td>\n",
       "      <td>-3.466132</td>\n",
       "      <td>1.047371</td>\n",
       "      <td>1.047371</td>\n",
       "      <td>1.047371</td>\n",
       "      <td>0.405024</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>-1.126582</td>\n",
       "      <td>1.126582</td>\n",
       "      <td>0.503304</td>\n",
       "      <td>0.557542</td>\n",
       "      <td>0.503304</td>\n",
       "      <td>0.557542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.586082</td>\n",
       "      <td>1.277448</td>\n",
       "      <td>-1.075389</td>\n",
       "      <td>-1.075389</td>\n",
       "      <td>-1.075389</td>\n",
       "      <td>-1.075389</td>\n",
       "      <td>-0.658092</td>\n",
       "      <td>-0.658092</td>\n",
       "      <td>-0.658092</td>\n",
       "      <td>0.299463</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.659008</td>\n",
       "      <td>-1.659008</td>\n",
       "      <td>1.659008</td>\n",
       "      <td>-1.659008</td>\n",
       "      <td>1.659008</td>\n",
       "      <td>0.416061</td>\n",
       "      <td>0.409473</td>\n",
       "      <td>0.416061</td>\n",
       "      <td>0.409473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.653889</td>\n",
       "      <td>0.519221</td>\n",
       "      <td>1.029828</td>\n",
       "      <td>1.029828</td>\n",
       "      <td>1.029828</td>\n",
       "      <td>1.029828</td>\n",
       "      <td>0.767750</td>\n",
       "      <td>0.767750</td>\n",
       "      <td>0.767750</td>\n",
       "      <td>0.087472</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>0.470695</td>\n",
       "      <td>-0.470695</td>\n",
       "      <td>1.953467</td>\n",
       "      <td>2.072721</td>\n",
       "      <td>1.953467</td>\n",
       "      <td>2.072721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject       Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0  -1.668629 -0.618120                                  0.317537   \n",
       "1  -1.600823  1.277448                                  0.014153   \n",
       "2  -1.533016  1.151077                                 -0.850871   \n",
       "3  -1.465210 -0.239006                                 -1.875980   \n",
       "4  -1.397403 -2.134575                                  0.539000   \n",
       "5  -1.329597 -0.112635                                 -0.028462   \n",
       "6  -1.261790 -0.365378                                  0.440311   \n",
       "7  -1.193984 -0.870863                                  0.160349   \n",
       "8  -1.126177  0.645592                                  0.134589   \n",
       "9  -1.058371  0.645592                                 -0.966009   \n",
       "10 -0.990564 -0.618120                                  1.599222   \n",
       "11 -0.854951  0.266478                                 -0.465984   \n",
       "12 -0.787145  0.013736                                  0.425189   \n",
       "13 -0.719338 -0.870863                                 -0.227826   \n",
       "14 -0.651532  0.013736                                  0.004434   \n",
       "15 -0.515919 -1.881833                                  0.689346   \n",
       "16 -0.448113  1.024706                                 -0.897646   \n",
       "17 -0.380306  0.140107                                  0.188562   \n",
       "18 -0.312500  0.645592                                  0.664506   \n",
       "19 -0.244693 -1.123605                                 -2.091064   \n",
       "20 -0.176887 -2.513689                                  1.860835   \n",
       "21 -0.109080 -0.744491                                  0.371948   \n",
       "22 -0.041274  0.771963                                 -0.008930   \n",
       "23  0.026533  0.266478                                 -1.278524   \n",
       "24  0.094339  0.898335                                 -0.800559   \n",
       "25  0.162146  0.645592                                  0.329628   \n",
       "26  0.229952  0.266478                                  0.027344   \n",
       "27  0.297759 -0.112635                                  0.607730   \n",
       "28  0.365565  0.140107                                 -0.059714   \n",
       "29  0.501178  0.898335                                  0.879785   \n",
       "30  0.568985 -0.112635                                  1.350282   \n",
       "31  0.636791 -1.249976                                 -0.175791   \n",
       "32  0.772404 -1.249976                                 -0.816854   \n",
       "33  0.840211  0.645592                                  0.559364   \n",
       "34  0.908017 -1.123605                                 -1.398015   \n",
       "35  0.975824  0.771963                                  1.122162   \n",
       "36  1.043630  0.645592                                  0.752826   \n",
       "37  1.111437  1.277448                                  0.865679   \n",
       "38  1.179243  0.519221                                  0.879785   \n",
       "39  1.247050  1.530191                                 -0.497236   \n",
       "40  1.314856  0.898335                                  0.865679   \n",
       "41  1.382663  1.151077                                 -0.355183   \n",
       "42  1.450469 -0.997234                                  0.656095   \n",
       "43  1.518276 -2.008204                                 -3.466132   \n",
       "44  1.586082  1.277448                                 -1.075389   \n",
       "45  1.653889  0.519221                                  1.029828   \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                            0.317537        \n",
       "1                                            0.014153        \n",
       "2                                           -0.850871        \n",
       "3                                           -1.875980        \n",
       "4                                            0.539000        \n",
       "5                                           -0.028462        \n",
       "6                                            0.440311        \n",
       "7                                            0.160349        \n",
       "8                                            0.134589        \n",
       "9                                           -0.966009        \n",
       "10                                           1.599222        \n",
       "11                                          -0.465984        \n",
       "12                                           0.425189        \n",
       "13                                          -0.227826        \n",
       "14                                           0.004434        \n",
       "15                                           0.689346        \n",
       "16                                          -0.897646        \n",
       "17                                           0.188562        \n",
       "18                                           0.664506        \n",
       "19                                          -2.091064        \n",
       "20                                           1.860835        \n",
       "21                                           0.371948        \n",
       "22                                          -0.008930        \n",
       "23                                          -1.278524        \n",
       "24                                          -0.800559        \n",
       "25                                           0.329628        \n",
       "26                                           0.027344        \n",
       "27                                           0.607730        \n",
       "28                                          -0.059714        \n",
       "29                                           0.879785        \n",
       "30                                           1.350282        \n",
       "31                                          -0.175791        \n",
       "32                                          -0.816854        \n",
       "33                                           0.559364        \n",
       "34                                          -1.398015        \n",
       "35                                           1.122162        \n",
       "36                                           0.752826        \n",
       "37                                           0.865679        \n",
       "38                                           0.879785        \n",
       "39                                          -0.497236        \n",
       "40                                           0.865679        \n",
       "41                                          -0.355183        \n",
       "42                                           0.656095        \n",
       "43                                          -3.466132        \n",
       "44                                          -1.075389        \n",
       "45                                           1.029828        \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                 0.317537                       0.317537   \n",
       "1                                 0.014153                       0.014153   \n",
       "2                                -0.850871                      -0.850871   \n",
       "3                                -1.875980                      -1.875980   \n",
       "4                                 0.539000                       0.539000   \n",
       "5                                -0.028462                      -0.028462   \n",
       "6                                 0.440311                       0.440311   \n",
       "7                                 0.160349                       0.160349   \n",
       "8                                 0.134589                       0.134589   \n",
       "9                                -0.966009                      -0.966009   \n",
       "10                                1.599222                       1.599222   \n",
       "11                               -0.465984                      -0.465984   \n",
       "12                                0.425189                       0.425189   \n",
       "13                               -0.227826                      -0.227826   \n",
       "14                                0.004434                       0.004434   \n",
       "15                                0.689346                       0.689346   \n",
       "16                               -0.897646                      -0.897646   \n",
       "17                                0.188562                       0.188562   \n",
       "18                                0.664506                       0.664506   \n",
       "19                               -2.091064                      -2.091064   \n",
       "20                                1.860835                       1.860835   \n",
       "21                                0.371948                       0.371948   \n",
       "22                               -0.008930                      -0.008930   \n",
       "23                               -1.278524                      -1.278524   \n",
       "24                               -0.800559                      -0.800559   \n",
       "25                                0.329628                       0.329628   \n",
       "26                                0.027344                       0.027344   \n",
       "27                                0.607730                       0.607730   \n",
       "28                               -0.059714                      -0.059714   \n",
       "29                                0.879785                       0.879785   \n",
       "30                                1.350282                       1.350282   \n",
       "31                               -0.175791                      -0.175791   \n",
       "32                               -0.816854                      -0.816854   \n",
       "33                                0.559364                       0.559364   \n",
       "34                               -1.398015                      -1.398015   \n",
       "35                                1.122162                       1.122162   \n",
       "36                                0.752826                       0.752826   \n",
       "37                                0.865679                       0.865679   \n",
       "38                                0.879785                       0.879785   \n",
       "39                               -0.497236                      -0.497236   \n",
       "40                                0.865679                       0.865679   \n",
       "41                               -0.355183                      -0.355183   \n",
       "42                                0.656095                       0.656095   \n",
       "43                               -3.466132                      -3.466132   \n",
       "44                               -1.075389                      -1.075389   \n",
       "45                                1.029828                       1.029828   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                               -1.296803                          -1.296803   \n",
       "1                               -1.780375                          -1.780375   \n",
       "2                               -0.601947                          -0.601947   \n",
       "3                               -0.955650                          -0.955650   \n",
       "4                               -1.164703                          -1.164703   \n",
       "5                               -0.494611                          -0.494611   \n",
       "6                               -1.737296                          -1.737296   \n",
       "7                               -1.158354                          -1.158354   \n",
       "8                               -0.044180                          -0.044180   \n",
       "9                                0.243516                           0.243516   \n",
       "10                               2.026214                           2.026214   \n",
       "11                               0.304551                           0.304551   \n",
       "12                               0.015603                           0.015603   \n",
       "13                               0.478026                           0.478026   \n",
       "14                              -0.664926                          -0.664926   \n",
       "15                               0.650896                           0.650896   \n",
       "16                              -0.644299                          -0.644299   \n",
       "17                              -0.790605                          -0.790605   \n",
       "18                               0.486480                           0.486480   \n",
       "19                               0.192491                           0.192491   \n",
       "20                              -0.345382                          -0.345382   \n",
       "21                              -0.873803                          -0.873803   \n",
       "22                               0.542678                           0.542678   \n",
       "23                               1.093533                           1.093533   \n",
       "24                               0.010097                           0.010097   \n",
       "25                               1.021402                           1.021402   \n",
       "26                               1.285578                           1.285578   \n",
       "27                              -0.163014                          -0.163014   \n",
       "28                               0.582625                           0.582625   \n",
       "29                               0.949039                           0.949039   \n",
       "30                               0.615874                           0.615874   \n",
       "31                              -0.489940                          -0.489940   \n",
       "32                              -0.212429                          -0.212429   \n",
       "33                               1.652356                           1.652356   \n",
       "34                              -0.776928                          -0.776928   \n",
       "35                               0.668180                           0.668180   \n",
       "36                               1.182675                           1.182675   \n",
       "37                               0.331913                           0.331913   \n",
       "38                              -0.120041                          -0.120041   \n",
       "39                              -1.044063                          -1.044063   \n",
       "40                               2.207664                           2.207664   \n",
       "41                              -2.604944                          -2.604944   \n",
       "42                               0.265871                           0.265871   \n",
       "43                               1.047371                           1.047371   \n",
       "44                              -0.658092                          -0.658092   \n",
       "45                               0.767750                           0.767750   \n",
       "\n",
       "    Subiculum_Connectivity_T  Amnesia_Lesion_T_Map  ...  Estimated_Outcome  \\\n",
       "0                  -1.296803             -0.075475  ...                NaN   \n",
       "1                  -1.780375             -0.200665  ...                NaN   \n",
       "2                  -0.601947              0.493589  ...                NaN   \n",
       "3                  -0.955650             -0.240569  ...                NaN   \n",
       "4                  -1.164703             -2.937139  ...                NaN   \n",
       "5                  -0.494611              0.674839  ...                NaN   \n",
       "6                  -1.737296             -2.309392  ...                NaN   \n",
       "7                  -1.158354             -2.332131  ...                NaN   \n",
       "8                  -0.044180              0.684226  ...                NaN   \n",
       "9                   0.243516              0.864200  ...                NaN   \n",
       "10                  2.026214              0.678820  ...                NaN   \n",
       "11                  0.304551              0.490881  ...                NaN   \n",
       "12                  0.015603              0.079576  ...                NaN   \n",
       "13                  0.478026             -2.547637  ...                NaN   \n",
       "14                 -0.664926              0.354131  ...                NaN   \n",
       "15                  0.650896             -2.327480  ...                NaN   \n",
       "16                 -0.644299              0.555234  ...                NaN   \n",
       "17                 -0.790605             -1.910090  ...                NaN   \n",
       "18                  0.486480              0.338560  ...                NaN   \n",
       "19                  0.192491              0.655642  ...                NaN   \n",
       "20                 -0.345382              0.605188  ...                NaN   \n",
       "21                 -0.873803              0.068986  ...                NaN   \n",
       "22                  0.542678              0.287759  ...                NaN   \n",
       "23                  1.093533              0.265323  ...                NaN   \n",
       "24                  0.010097              0.534792  ...                NaN   \n",
       "25                  1.021402              0.647270  ...                NaN   \n",
       "26                  1.285578              0.476165  ...                NaN   \n",
       "27                 -0.163014              0.642456  ...                NaN   \n",
       "28                  0.582625              0.049328  ...                NaN   \n",
       "29                  0.949039              0.560647  ...                NaN   \n",
       "30                  0.615874             -0.583256  ...                NaN   \n",
       "31                 -0.489940              0.311270  ...                NaN   \n",
       "32                 -0.212429              0.361575  ...                NaN   \n",
       "33                  1.652356              0.198994  ...                NaN   \n",
       "34                 -0.776928              0.754540  ...                NaN   \n",
       "35                  0.668180             -0.437912  ...                NaN   \n",
       "36                  1.182675              0.558811  ...                NaN   \n",
       "37                  0.331913              0.766285  ...                NaN   \n",
       "38                 -0.120041              1.297212  ...                NaN   \n",
       "39                 -1.044063              0.190382  ...                NaN   \n",
       "40                  2.207664             -0.371849  ...                NaN   \n",
       "41                 -2.604944              0.175968  ...                NaN   \n",
       "42                  0.265871              0.858991  ...                NaN   \n",
       "43                  1.047371              0.405024  ...                NaN   \n",
       "44                 -0.658092              0.299463  ...                NaN   \n",
       "45                  0.767750              0.087472  ...                NaN   \n",
       "\n",
       "    Cognitive_Baseline  Z_Scored_Cognitive_Baseline  \\\n",
       "0             1.535547                     1.535547   \n",
       "1             0.470695                     0.470695   \n",
       "2            -0.061731                    -0.061731   \n",
       "3            -0.416681                    -0.416681   \n",
       "4            -0.061731                    -0.061731   \n",
       "5            -1.126582                    -1.126582   \n",
       "6            -1.126582                    -1.126582   \n",
       "7             0.825646                     0.825646   \n",
       "8             0.648171                     0.648171   \n",
       "9            -1.126582                    -1.126582   \n",
       "10           -1.304057                    -1.304057   \n",
       "11           -0.771632                    -0.771632   \n",
       "12            2.067973                     2.067973   \n",
       "13            1.713022                     1.713022   \n",
       "14           -0.061731                    -0.061731   \n",
       "15            2.245448                     2.245448   \n",
       "16           -0.594156                    -0.594156   \n",
       "17           -0.239206                    -0.239206   \n",
       "18            0.648171                     0.648171   \n",
       "19           -0.771632                    -0.771632   \n",
       "20            0.470695                     0.470695   \n",
       "21           -0.594156                    -0.594156   \n",
       "22           -0.594156                    -0.594156   \n",
       "23             0.29322                     0.293220   \n",
       "24           -0.416681                    -0.416681   \n",
       "25            0.825646                     0.825646   \n",
       "26            1.535547                     1.535547   \n",
       "27           -0.949107                    -0.949107   \n",
       "28            0.115745                     0.115745   \n",
       "29           -0.594156                    -0.594156   \n",
       "30           -0.416681                    -0.416681   \n",
       "31            2.777874                     2.777874   \n",
       "32            0.470695                     0.470695   \n",
       "33             0.29322                     0.293220   \n",
       "34           -0.416681                    -0.416681   \n",
       "35            0.470695                     0.470695   \n",
       "36           -0.061731                    -0.061731   \n",
       "37           -0.239206                    -0.239206   \n",
       "38           -0.594156                    -0.594156   \n",
       "39           -1.126582                    -1.126582   \n",
       "40           -0.239206                    -0.239206   \n",
       "41           -1.481533                    -1.481533   \n",
       "42             0.29322                     0.293220   \n",
       "43           -1.126582                    -1.126582   \n",
       "44           -1.659008                    -1.659008   \n",
       "45            0.470695                     0.470695   \n",
       "\n",
       "    Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                       -1.535547   \n",
       "1                                       -0.470695   \n",
       "2                                        0.061731   \n",
       "3                                        0.416681   \n",
       "4                                        0.061731   \n",
       "5                                        1.126582   \n",
       "6                                        1.126582   \n",
       "7                                       -0.825646   \n",
       "8                                       -0.648171   \n",
       "9                                        1.126582   \n",
       "10                                       1.304057   \n",
       "11                                       0.771632   \n",
       "12                                      -2.067973   \n",
       "13                                      -1.713022   \n",
       "14                                       0.061731   \n",
       "15                                      -2.245448   \n",
       "16                                       0.594156   \n",
       "17                                       0.239206   \n",
       "18                                      -0.648171   \n",
       "19                                       0.771632   \n",
       "20                                      -0.470695   \n",
       "21                                       0.594156   \n",
       "22                                       0.594156   \n",
       "23                                      -0.293220   \n",
       "24                                       0.416681   \n",
       "25                                      -0.825646   \n",
       "26                                      -1.535547   \n",
       "27                                       0.949107   \n",
       "28                                      -0.115745   \n",
       "29                                       0.594156   \n",
       "30                                       0.416681   \n",
       "31                                      -2.777874   \n",
       "32                                      -0.470695   \n",
       "33                                      -0.293220   \n",
       "34                                       0.416681   \n",
       "35                                      -0.470695   \n",
       "36                                       0.061731   \n",
       "37                                       0.239206   \n",
       "38                                       0.594156   \n",
       "39                                       1.126582   \n",
       "40                                       0.239206   \n",
       "41                                       1.481533   \n",
       "42                                      -0.293220   \n",
       "43                                       1.126582   \n",
       "44                                       1.659008   \n",
       "45                                      -0.470695   \n",
       "\n",
       "    Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                      1.535547                            -1.535547   \n",
       "1                      0.470695                            -0.470695   \n",
       "2                     -0.061731                             0.061731   \n",
       "3                     -0.416681                             0.416681   \n",
       "4                     -0.061731                             0.061731   \n",
       "5                     -1.126582                             1.126582   \n",
       "6                     -1.126582                             1.126582   \n",
       "7                      0.825646                            -0.825646   \n",
       "8                      0.648171                            -0.648171   \n",
       "9                     -1.126582                             1.126582   \n",
       "10                    -1.304057                             1.304057   \n",
       "11                    -0.771632                             0.771632   \n",
       "12                     2.067973                            -2.067973   \n",
       "13                     1.713022                            -1.713022   \n",
       "14                    -0.061731                             0.061731   \n",
       "15                     2.245448                            -2.245448   \n",
       "16                    -0.594156                             0.594156   \n",
       "17                    -0.239206                             0.239206   \n",
       "18                     0.648171                            -0.648171   \n",
       "19                    -0.771632                             0.771632   \n",
       "20                     0.470695                            -0.470695   \n",
       "21                    -0.594156                             0.594156   \n",
       "22                    -0.594156                             0.594156   \n",
       "23                     0.293220                            -0.293220   \n",
       "24                    -0.416681                             0.416681   \n",
       "25                     0.825646                            -0.825646   \n",
       "26                     1.535547                            -1.535547   \n",
       "27                    -0.949107                             0.949107   \n",
       "28                     0.115745                            -0.115745   \n",
       "29                    -0.594156                             0.594156   \n",
       "30                    -0.416681                             0.416681   \n",
       "31                     2.777874                            -2.777874   \n",
       "32                     0.470695                            -0.470695   \n",
       "33                     0.293220                            -0.293220   \n",
       "34                    -0.416681                             0.416681   \n",
       "35                     0.470695                            -0.470695   \n",
       "36                    -0.061731                             0.061731   \n",
       "37                    -0.239206                             0.239206   \n",
       "38                    -0.594156                             0.594156   \n",
       "39                    -1.126582                             1.126582   \n",
       "40                    -0.239206                             0.239206   \n",
       "41                    -1.481533                             1.481533   \n",
       "42                     0.293220                            -0.293220   \n",
       "43                    -1.126582                             1.126582   \n",
       "44                    -1.659008                             1.659008   \n",
       "45                     0.470695                            -0.470695   \n",
       "\n",
       "    ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0        -1.733459      -1.240930            -1.733459            -1.240930  \n",
       "1        -1.168615      -1.033539            -1.168615            -1.033539  \n",
       "2        -0.823347      -0.457869            -0.823347            -0.457869  \n",
       "3        -1.453843      -1.025232            -1.453843            -1.025232  \n",
       "4        -0.833453      -1.659974            -0.833453            -1.659974  \n",
       "5        -0.604024      -0.392473            -0.604024            -0.392473  \n",
       "6        -0.695979      -1.523901            -0.695979            -1.523901  \n",
       "7        -0.374507      -1.393397            -0.374507            -1.393397  \n",
       "8         0.606780       0.619320             0.606780             0.619320  \n",
       "9         0.175047       0.054924             0.175047             0.054924  \n",
       "10       -0.225126      -0.790754            -0.225126            -0.790754  \n",
       "11       -0.475674      -0.337674            -0.475674            -0.337674  \n",
       "12       -0.767918      -0.275855            -0.767918            -0.275855  \n",
       "13        0.278129      -0.801074             0.278129            -0.801074  \n",
       "14       -0.401108      -0.151793            -0.401108            -0.151793  \n",
       "15       -0.595967       0.245141            -0.595967             0.245141  \n",
       "16       -0.127531       0.037012            -0.127531             0.037012  \n",
       "17       -2.392173      -2.562100            -2.392173            -2.562100  \n",
       "18        1.488971       1.337498             1.488971             1.337498  \n",
       "19        1.171435       1.228184             1.171435             1.228184  \n",
       "20        0.538820       0.710429             0.538820             0.710429  \n",
       "21       -1.345388      -0.880813            -1.345388            -0.880813  \n",
       "22        1.478388       1.542120             1.478388             1.542120  \n",
       "23        0.769114       0.815961             0.769114             0.815961  \n",
       "24       -0.233238      -0.181402            -0.233238            -0.181402  \n",
       "25       -0.167737      -0.216583            -0.167737            -0.216583  \n",
       "26        1.049519       0.931270             1.049519             0.931270  \n",
       "27        0.276215       0.367868             0.276215             0.367868  \n",
       "28        0.211132       0.206672             0.211132             0.206672  \n",
       "29        0.214349       0.149720             0.214349             0.149720  \n",
       "30        1.846193       1.897945             1.846193             1.897945  \n",
       "31       -0.514351      -0.384248            -0.514351            -0.384248  \n",
       "32       -0.283253      -0.241862            -0.283253            -0.241862  \n",
       "33        0.877774       0.899129             0.877774             0.899129  \n",
       "34        0.068709       0.242072             0.068709             0.242072  \n",
       "35        2.099023       2.195411             2.099023             2.195411  \n",
       "36        0.037028       0.165492             0.037028             0.165492  \n",
       "37       -0.933080      -0.796051            -0.933080            -0.796051  \n",
       "38        0.767181       0.248092             0.767181             0.248092  \n",
       "39       -1.865794      -1.363469            -1.865794            -1.363469  \n",
       "40        1.127861       0.878251             1.127861             0.878251  \n",
       "41       -0.385368      -0.234795            -0.385368            -0.234795  \n",
       "42        0.446432       0.133543             0.446432             0.133543  \n",
       "43        0.503304       0.557542             0.503304             0.557542  \n",
       "44        0.416061       0.409473             0.416061             0.409473  \n",
       "45        1.953467       2.072721             1.953467             2.072721  \n",
       "\n",
       "[46 rows x 44 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in data_df.columns:\n",
    "#     if 'CSF' and 'eh' not in col:\n",
    "#         data_df[col] = data_df[col] * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Define Your Formula\n",
    "\n",
    "This is the formula relating outcome to predictors, and takes the form:\n",
    "- y = B0 + B1 + B2 + B3 + . . . BN\n",
    "\n",
    "It is defined using the columns of your dataframe instead of the variables above:\n",
    "- 'Apples_Picked ~ hours_worked + owns_apple_picking_machine'\n",
    "\n",
    "____\n",
    "**ANOVA**\n",
    "- Tests differences in means for one categorical variable.\n",
    "- formula = 'Outcome ~ C(Group1)'\n",
    "\n",
    "**2-Way ANOVA**\n",
    "- Tests differences in means for two categorical variables without interaction.\n",
    "- formula = 'Outcome ~ C(Group1) + C(Group2)'\n",
    "\n",
    "**2-Way ANOVA with Interaction**\n",
    "- Tests for interaction effects between two categorical variables.\n",
    "- formula = 'Outcome ~ C(Group1) * C(Group2)'\n",
    "\n",
    "**ANCOVA**\n",
    "- Similar to ANOVA, but includes a covariate to control for its effect.\n",
    "- formula = 'Outcome ~ C(Group1) + Covariate'\n",
    "\n",
    "**2-Way ANCOVA**\n",
    "- Extends ANCOVA with two categorical variables and their interaction, controlling for a covariate.\n",
    "- formula = 'Outcome ~ C(Group1) * C(Group2) + Covariate'\n",
    "\n",
    "**Multiple Regression**\n",
    "- Assesses the impact of multiple predictors on an outcome.\n",
    "- formula = 'Outcome ~ Predictor1 + Predictor2'\n",
    "\n",
    "**Simple Linear Regression**\n",
    "- Assesses the impact of a single predictor on an outcome.\n",
    "- formula = 'Outcome ~ Predictor'\n",
    "\n",
    "**MANOVA**\n",
    "- Assesses multiple dependent variables across groups.\n",
    "- Note: Not typically set up with a formula in statsmodels. Requires specialized functions.\n",
    "\n",
    "____\n",
    "Use the printout below to design your formula. \n",
    "- Left of the \"~\" symbol is the thing to be predicted. \n",
    "- Right of the \"~\" symbol are the predictors. \n",
    "- \":\" indicates an interaction between two things. \n",
    "- \"*\" indicates and interactions AND it accounts for the simple effects too. \n",
    "- \"+\" indicates that you want to add another predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Subiculum_T_By_Origin_Group_',\n",
       "       'Z_Scored_Subiculum_Connectivity_T', 'Subiculum_Connectivity_T',\n",
       "       'Amnesia_Lesion_T_Map', 'Memory_Network_T', 'Z_Scored_Memory_Network_R',\n",
       "       'Memory_Network_R', 'Subiculum_Grey_Matter', 'Subiculum_White_Matter',\n",
       "       'Subiculum_CSF', 'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Age_Group', 'Age_And_Disease',\n",
       "       'Age_Disease_and_Cohort', 'Subiculum_Group_By_Z_Score_Sign',\n",
       "       'Subiculum_Group_By_Inflection_Point', 'Cognitive_Outcome',\n",
       "       'Estimated_Outcome', 'Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline__Lower_is_Better_',\n",
       "       'Min_Max_Normalized_Baseline', 'MinMaxNormBaseline_Higher_is_Better',\n",
       "       'ROI_to_Alz_Max', 'ROI_to_PD_Max', 'Standardzied_AD_Max',\n",
       "       'Standardized_PD_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"Percent_Cognitive_Improvement ~  Z_Scored_Subiculum_T_By_Origin_Group_*Cognitive_Baseline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Visualize Your Design Matrix\n",
    "\n",
    "This is the explanatory variable half of your regression formula\n",
    "_______________________________________________________\n",
    "Create Design Matrix: Use the create_design_matrix method. You can provide a list of formula variables which correspond to column names in your dataframe.\n",
    "\n",
    "- design_matrix = palm.create_design_matrix(formula_vars=[\"var1\", \"var2\", \"var1*var2\"])\n",
    "- To include interaction terms, use * between variables, like \"var1*var2\".\n",
    "- By default, an intercept will be added unless you set intercept=False\n",
    "- **don't explicitly add the 'intercept' column. I'll do it for you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Cognitive_Baseline[T.-1.5058812083195714]</th>\n",
       "      <th>Cognitive_Baseline[T.-1.0659608553273385]</th>\n",
       "      <th>Cognitive_Baseline[T.-0.6260405023351056]</th>\n",
       "      <th>Cognitive_Baseline[T.-0.1861201493428726]</th>\n",
       "      <th>Cognitive_Baseline[T.0.25380020364936035]</th>\n",
       "      <th>Cognitive_Baseline[T.0.6937205566415933]</th>\n",
       "      <th>Cognitive_Baseline[T.1.1336409096338262]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.5058812083195714]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.0659608553273385]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.6260405023351056]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.1861201493428726]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.25380020364936035]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.6937205566415933]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.1.1336409096338262]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.050889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.050889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.985072</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.985072</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.126581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.126581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.274873</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.274873</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.959062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.959062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455878</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.455878</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.149857</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.149857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.754732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.504571</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.504571</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.343476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.343476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.059392</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.059392</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.456257</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.456257</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.097288</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.097288</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.622962</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.622962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.328612</td>\n",
       "      <td>1.328612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.854046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.854046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.641116</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.641116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.121400</td>\n",
       "      <td>-1.121400</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.165184</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.165184</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.828023</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.828023</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.741815</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.741815</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Intercept  Cognitive_Baseline[T.-1.5058812083195714]  \\\n",
       "46        1.0                                        0.0   \n",
       "47        1.0                                        0.0   \n",
       "48        1.0                                        0.0   \n",
       "49        1.0                                        0.0   \n",
       "50        1.0                                        0.0   \n",
       "51        1.0                                        0.0   \n",
       "52        1.0                                        0.0   \n",
       "53        1.0                                        0.0   \n",
       "54        1.0                                        0.0   \n",
       "55        1.0                                        0.0   \n",
       "56        1.0                                        0.0   \n",
       "57        1.0                                        0.0   \n",
       "58        1.0                                        0.0   \n",
       "59        1.0                                        0.0   \n",
       "60        1.0                                        0.0   \n",
       "61        1.0                                        0.0   \n",
       "62        1.0                                        0.0   \n",
       "64        1.0                                        0.0   \n",
       "65        1.0                                        0.0   \n",
       "66        1.0                                        1.0   \n",
       "67        1.0                                        0.0   \n",
       "68        1.0                                        0.0   \n",
       "69        1.0                                        1.0   \n",
       "70        1.0                                        0.0   \n",
       "72        1.0                                        0.0   \n",
       "73        1.0                                        0.0   \n",
       "\n",
       "    Cognitive_Baseline[T.-1.0659608553273385]  \\\n",
       "46                                        0.0   \n",
       "47                                        1.0   \n",
       "48                                        0.0   \n",
       "49                                        0.0   \n",
       "50                                        0.0   \n",
       "51                                        0.0   \n",
       "52                                        0.0   \n",
       "53                                        0.0   \n",
       "54                                        0.0   \n",
       "55                                        0.0   \n",
       "56                                        0.0   \n",
       "57                                        0.0   \n",
       "58                                        1.0   \n",
       "59                                        0.0   \n",
       "60                                        0.0   \n",
       "61                                        0.0   \n",
       "62                                        0.0   \n",
       "64                                        0.0   \n",
       "65                                        0.0   \n",
       "66                                        0.0   \n",
       "67                                        0.0   \n",
       "68                                        0.0   \n",
       "69                                        0.0   \n",
       "70                                        0.0   \n",
       "72                                        0.0   \n",
       "73                                        1.0   \n",
       "\n",
       "    Cognitive_Baseline[T.-0.6260405023351056]  \\\n",
       "46                                        0.0   \n",
       "47                                        0.0   \n",
       "48                                        0.0   \n",
       "49                                        1.0   \n",
       "50                                        0.0   \n",
       "51                                        0.0   \n",
       "52                                        1.0   \n",
       "53                                        0.0   \n",
       "54                                        0.0   \n",
       "55                                        0.0   \n",
       "56                                        0.0   \n",
       "57                                        0.0   \n",
       "58                                        0.0   \n",
       "59                                        0.0   \n",
       "60                                        0.0   \n",
       "61                                        0.0   \n",
       "62                                        0.0   \n",
       "64                                        1.0   \n",
       "65                                        0.0   \n",
       "66                                        0.0   \n",
       "67                                        0.0   \n",
       "68                                        0.0   \n",
       "69                                        0.0   \n",
       "70                                        0.0   \n",
       "72                                        0.0   \n",
       "73                                        0.0   \n",
       "\n",
       "    Cognitive_Baseline[T.-0.1861201493428726]  \\\n",
       "46                                        1.0   \n",
       "47                                        0.0   \n",
       "48                                        0.0   \n",
       "49                                        0.0   \n",
       "50                                        0.0   \n",
       "51                                        1.0   \n",
       "52                                        0.0   \n",
       "53                                        0.0   \n",
       "54                                        0.0   \n",
       "55                                        0.0   \n",
       "56                                        0.0   \n",
       "57                                        0.0   \n",
       "58                                        0.0   \n",
       "59                                        0.0   \n",
       "60                                        0.0   \n",
       "61                                        0.0   \n",
       "62                                        1.0   \n",
       "64                                        0.0   \n",
       "65                                        0.0   \n",
       "66                                        0.0   \n",
       "67                                        1.0   \n",
       "68                                        0.0   \n",
       "69                                        0.0   \n",
       "70                                        0.0   \n",
       "72                                        0.0   \n",
       "73                                        0.0   \n",
       "\n",
       "    Cognitive_Baseline[T.0.25380020364936035]  \\\n",
       "46                                        0.0   \n",
       "47                                        0.0   \n",
       "48                                        0.0   \n",
       "49                                        0.0   \n",
       "50                                        1.0   \n",
       "51                                        0.0   \n",
       "52                                        0.0   \n",
       "53                                        0.0   \n",
       "54                                        0.0   \n",
       "55                                        0.0   \n",
       "56                                        0.0   \n",
       "57                                        0.0   \n",
       "58                                        0.0   \n",
       "59                                        0.0   \n",
       "60                                        1.0   \n",
       "61                                        0.0   \n",
       "62                                        0.0   \n",
       "64                                        0.0   \n",
       "65                                        0.0   \n",
       "66                                        0.0   \n",
       "67                                        0.0   \n",
       "68                                        0.0   \n",
       "69                                        0.0   \n",
       "70                                        0.0   \n",
       "72                                        0.0   \n",
       "73                                        0.0   \n",
       "\n",
       "    Cognitive_Baseline[T.0.6937205566415933]  \\\n",
       "46                                       0.0   \n",
       "47                                       0.0   \n",
       "48                                       0.0   \n",
       "49                                       0.0   \n",
       "50                                       0.0   \n",
       "51                                       0.0   \n",
       "52                                       0.0   \n",
       "53                                       0.0   \n",
       "54                                       0.0   \n",
       "55                                       1.0   \n",
       "56                                       0.0   \n",
       "57                                       0.0   \n",
       "58                                       0.0   \n",
       "59                                       0.0   \n",
       "60                                       0.0   \n",
       "61                                       1.0   \n",
       "62                                       0.0   \n",
       "64                                       0.0   \n",
       "65                                       0.0   \n",
       "66                                       0.0   \n",
       "67                                       0.0   \n",
       "68                                       0.0   \n",
       "69                                       0.0   \n",
       "70                                       1.0   \n",
       "72                                       1.0   \n",
       "73                                       0.0   \n",
       "\n",
       "    Cognitive_Baseline[T.1.1336409096338262]  \\\n",
       "46                                       0.0   \n",
       "47                                       0.0   \n",
       "48                                       1.0   \n",
       "49                                       0.0   \n",
       "50                                       0.0   \n",
       "51                                       0.0   \n",
       "52                                       0.0   \n",
       "53                                       1.0   \n",
       "54                                       0.0   \n",
       "55                                       0.0   \n",
       "56                                       1.0   \n",
       "57                                       1.0   \n",
       "58                                       0.0   \n",
       "59                                       1.0   \n",
       "60                                       0.0   \n",
       "61                                       0.0   \n",
       "62                                       0.0   \n",
       "64                                       0.0   \n",
       "65                                       1.0   \n",
       "66                                       0.0   \n",
       "67                                       0.0   \n",
       "68                                       1.0   \n",
       "69                                       0.0   \n",
       "70                                       0.0   \n",
       "72                                       0.0   \n",
       "73                                       0.0   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_  \\\n",
       "46                               1.050889   \n",
       "47                              -0.985072   \n",
       "48                               1.126581   \n",
       "49                              -0.274873   \n",
       "50                               0.067831   \n",
       "51                               1.959062   \n",
       "52                              -0.455878   \n",
       "53                              -1.149857   \n",
       "54                               0.754732   \n",
       "55                              -0.504571   \n",
       "56                               2.343476   \n",
       "57                               0.998571   \n",
       "58                              -1.059392   \n",
       "59                               0.002864   \n",
       "60                               0.420446   \n",
       "61                              -0.456257   \n",
       "62                              -1.097288   \n",
       "64                               0.196578   \n",
       "65                              -0.622962   \n",
       "66                               1.328612   \n",
       "67                               0.854046   \n",
       "68                              -0.641116   \n",
       "69                              -1.121400   \n",
       "70                              -1.165184   \n",
       "72                              -0.828023   \n",
       "73                              -0.741815   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.5058812083195714]  \\\n",
       "46                                           0.000000                                 \n",
       "47                                          -0.000000                                 \n",
       "48                                           0.000000                                 \n",
       "49                                          -0.000000                                 \n",
       "50                                           0.000000                                 \n",
       "51                                           0.000000                                 \n",
       "52                                          -0.000000                                 \n",
       "53                                          -0.000000                                 \n",
       "54                                           0.000000                                 \n",
       "55                                          -0.000000                                 \n",
       "56                                           0.000000                                 \n",
       "57                                           0.000000                                 \n",
       "58                                          -0.000000                                 \n",
       "59                                           0.000000                                 \n",
       "60                                           0.000000                                 \n",
       "61                                          -0.000000                                 \n",
       "62                                          -0.000000                                 \n",
       "64                                           0.000000                                 \n",
       "65                                          -0.000000                                 \n",
       "66                                           1.328612                                 \n",
       "67                                           0.000000                                 \n",
       "68                                          -0.000000                                 \n",
       "69                                          -1.121400                                 \n",
       "70                                          -0.000000                                 \n",
       "72                                          -0.000000                                 \n",
       "73                                          -0.000000                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.0659608553273385]  \\\n",
       "46                                           0.000000                                 \n",
       "47                                          -0.985072                                 \n",
       "48                                           0.000000                                 \n",
       "49                                          -0.000000                                 \n",
       "50                                           0.000000                                 \n",
       "51                                           0.000000                                 \n",
       "52                                          -0.000000                                 \n",
       "53                                          -0.000000                                 \n",
       "54                                           0.000000                                 \n",
       "55                                          -0.000000                                 \n",
       "56                                           0.000000                                 \n",
       "57                                           0.000000                                 \n",
       "58                                          -1.059392                                 \n",
       "59                                           0.000000                                 \n",
       "60                                           0.000000                                 \n",
       "61                                          -0.000000                                 \n",
       "62                                          -0.000000                                 \n",
       "64                                           0.000000                                 \n",
       "65                                          -0.000000                                 \n",
       "66                                           0.000000                                 \n",
       "67                                           0.000000                                 \n",
       "68                                          -0.000000                                 \n",
       "69                                          -0.000000                                 \n",
       "70                                          -0.000000                                 \n",
       "72                                          -0.000000                                 \n",
       "73                                          -0.741815                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.6260405023351056]  \\\n",
       "46                                           0.000000                                 \n",
       "47                                          -0.000000                                 \n",
       "48                                           0.000000                                 \n",
       "49                                          -0.274873                                 \n",
       "50                                           0.000000                                 \n",
       "51                                           0.000000                                 \n",
       "52                                          -0.455878                                 \n",
       "53                                          -0.000000                                 \n",
       "54                                           0.000000                                 \n",
       "55                                          -0.000000                                 \n",
       "56                                           0.000000                                 \n",
       "57                                           0.000000                                 \n",
       "58                                          -0.000000                                 \n",
       "59                                           0.000000                                 \n",
       "60                                           0.000000                                 \n",
       "61                                          -0.000000                                 \n",
       "62                                          -0.000000                                 \n",
       "64                                           0.196578                                 \n",
       "65                                          -0.000000                                 \n",
       "66                                           0.000000                                 \n",
       "67                                           0.000000                                 \n",
       "68                                          -0.000000                                 \n",
       "69                                          -0.000000                                 \n",
       "70                                          -0.000000                                 \n",
       "72                                          -0.000000                                 \n",
       "73                                          -0.000000                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.1861201493428726]  \\\n",
       "46                                           1.050889                                 \n",
       "47                                          -0.000000                                 \n",
       "48                                           0.000000                                 \n",
       "49                                          -0.000000                                 \n",
       "50                                           0.000000                                 \n",
       "51                                           1.959062                                 \n",
       "52                                          -0.000000                                 \n",
       "53                                          -0.000000                                 \n",
       "54                                           0.000000                                 \n",
       "55                                          -0.000000                                 \n",
       "56                                           0.000000                                 \n",
       "57                                           0.000000                                 \n",
       "58                                          -0.000000                                 \n",
       "59                                           0.000000                                 \n",
       "60                                           0.000000                                 \n",
       "61                                          -0.000000                                 \n",
       "62                                          -1.097288                                 \n",
       "64                                           0.000000                                 \n",
       "65                                          -0.000000                                 \n",
       "66                                           0.000000                                 \n",
       "67                                           0.854046                                 \n",
       "68                                          -0.000000                                 \n",
       "69                                          -0.000000                                 \n",
       "70                                          -0.000000                                 \n",
       "72                                          -0.000000                                 \n",
       "73                                          -0.000000                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.25380020364936035]  \\\n",
       "46                                           0.000000                                 \n",
       "47                                          -0.000000                                 \n",
       "48                                           0.000000                                 \n",
       "49                                          -0.000000                                 \n",
       "50                                           0.067831                                 \n",
       "51                                           0.000000                                 \n",
       "52                                          -0.000000                                 \n",
       "53                                          -0.000000                                 \n",
       "54                                           0.000000                                 \n",
       "55                                          -0.000000                                 \n",
       "56                                           0.000000                                 \n",
       "57                                           0.000000                                 \n",
       "58                                          -0.000000                                 \n",
       "59                                           0.000000                                 \n",
       "60                                           0.420446                                 \n",
       "61                                          -0.000000                                 \n",
       "62                                          -0.000000                                 \n",
       "64                                           0.000000                                 \n",
       "65                                          -0.000000                                 \n",
       "66                                           0.000000                                 \n",
       "67                                           0.000000                                 \n",
       "68                                          -0.000000                                 \n",
       "69                                          -0.000000                                 \n",
       "70                                          -0.000000                                 \n",
       "72                                          -0.000000                                 \n",
       "73                                          -0.000000                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.6937205566415933]  \\\n",
       "46                                           0.000000                                \n",
       "47                                          -0.000000                                \n",
       "48                                           0.000000                                \n",
       "49                                          -0.000000                                \n",
       "50                                           0.000000                                \n",
       "51                                           0.000000                                \n",
       "52                                          -0.000000                                \n",
       "53                                          -0.000000                                \n",
       "54                                           0.000000                                \n",
       "55                                          -0.504571                                \n",
       "56                                           0.000000                                \n",
       "57                                           0.000000                                \n",
       "58                                          -0.000000                                \n",
       "59                                           0.000000                                \n",
       "60                                           0.000000                                \n",
       "61                                          -0.456257                                \n",
       "62                                          -0.000000                                \n",
       "64                                           0.000000                                \n",
       "65                                          -0.000000                                \n",
       "66                                           0.000000                                \n",
       "67                                           0.000000                                \n",
       "68                                          -0.000000                                \n",
       "69                                          -0.000000                                \n",
       "70                                          -1.165184                                \n",
       "72                                          -0.828023                                \n",
       "73                                          -0.000000                                \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.1.1336409096338262]  \n",
       "46                                           0.000000                               \n",
       "47                                          -0.000000                               \n",
       "48                                           1.126581                               \n",
       "49                                          -0.000000                               \n",
       "50                                           0.000000                               \n",
       "51                                           0.000000                               \n",
       "52                                          -0.000000                               \n",
       "53                                          -1.149857                               \n",
       "54                                           0.000000                               \n",
       "55                                          -0.000000                               \n",
       "56                                           2.343476                               \n",
       "57                                           0.998571                               \n",
       "58                                          -0.000000                               \n",
       "59                                           0.002864                               \n",
       "60                                           0.000000                               \n",
       "61                                          -0.000000                               \n",
       "62                                          -0.000000                               \n",
       "64                                           0.000000                               \n",
       "65                                          -0.622962                               \n",
       "66                                           0.000000                               \n",
       "67                                           0.000000                               \n",
       "68                                          -0.641116                               \n",
       "69                                          -0.000000                               \n",
       "70                                          -0.000000                               \n",
       "72                                          -0.000000                               \n",
       "73                                          -0.000000                               "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the design matrix\n",
    "outcome_matrix, design_matrix = cal_palm.define_design_matrix(formula, data_df)\n",
    "design_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Visualize Your Dependent Variable\n",
    "\n",
    "I have generated this for you based on the formula you provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-1.607729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.319953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.006007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.287847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.249926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.248059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.287847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.253582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.513172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.774576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-2.342312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.513172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.782099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.006007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-3.172267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.513172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.017053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.513172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-0.006007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.513172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-0.006007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.054924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.009638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.532447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.293610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Percent_Cognitive_Improvement\n",
       "46                      -1.607729\n",
       "47                       1.319953\n",
       "48                      -0.006007\n",
       "49                      -0.287847\n",
       "50                       0.249926\n",
       "51                       0.248059\n",
       "52                      -0.287847\n",
       "53                       0.253582\n",
       "54                       0.513172\n",
       "55                       0.774576\n",
       "56                      -2.342312\n",
       "57                       0.513172\n",
       "58                       0.782099\n",
       "59                      -0.006007\n",
       "60                      -3.172267\n",
       "61                       0.513172\n",
       "62                      -0.017053\n",
       "64                       0.513172\n",
       "65                      -0.006007\n",
       "66                       1.325800\n",
       "67                       0.513172\n",
       "68                      -0.006007\n",
       "69                       1.054924\n",
       "70                      -0.009638\n",
       "72                      -0.532447\n",
       "73                      -0.293610"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Generate Contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a Contrast Matrix\n",
    "- This is different from the contrast matrices used in cell-means regressions such as in PALM, but it is much more powerful. \n",
    "\n",
    "\n",
    "\n",
    "For more information on contrast matrices, please refer to this: https://cran.r-project.org/web/packages/codingMatrices/vignettes/codingMatrices.pdf\n",
    "\n",
    "Generally, these drastically effect the results of ANOVA. However, they are mereley a nuisance for a regression.\n",
    "In essence, they assess the coefficients of a given\n",
    "\n",
    "________________________________________________________________\n",
    "A coding matrix (a contrast matrix if it sums to zero) is simply a way of defining what coefficients to evaluate and how to evaluate them. \n",
    "If a coefficient is set to 1 and everything else is set to zero, we are taking the mean of the coefficient's means and assessing if they significantly\n",
    "deviate from zero--IE we are checking if it had a significant impact on the ability to predict the depdendent variable.\n",
    "If a coefficient is set to 1, another is -1, and others are 0, we are assessing how the means of the two coefficients deviate from eachother. \n",
    "If several coefficients are 1 and several others are -1, we are assessing how the group-level means of the two coefficients deviate from eachother.\n",
    "If a group of coefficients are 1, a group is -1, and a group is 0, we are only assessing how the groups +1 and -1 have differing means. \n",
    "\n",
    "1: This value indicates that the corresponding variable's coefficient in the model is included in the contrast. It means you are interested in estimating the effect of that variable.\n",
    "\n",
    "0: This value indicates that the corresponding variable's coefficient in the model is not included in the contrast. It means you are not interested in estimating the effect of that variable.\n",
    "\n",
    "-1: This value indicates that the corresponding variable's coefficient in the model is included in the contrast, but with an opposite sign. It means you are interested in estimating the negative effect of that variable.\n",
    "\n",
    "----------------------------------------------------------------\n",
    "The contrast matrix is typically a matrix with dimensions (number of contrasts) x (number of regression coefficients). Each row of the contrast matrix represents a contrast or comparison you want to test.\n",
    "\n",
    "For example, let's say you have the following regression coefficients in your model:\n",
    "\n",
    "Intercept, Age, connectivity, Age_interaction_connectivity\n",
    "A contrast matric has dimensions of [n_predictors, n_experiments] where each experiment is a contrast\n",
    "\n",
    "If you want to test the hypothesis that the effect of Age is significant, you can set up a contrast matrix with a row that specifies this contrast (actually an averaging vector):\n",
    "```\n",
    "[0,1,0,0]. This is an averaging vector because it sums to 1\n",
    "```\n",
    "This contrast will test the coefficient corresponding to the Age variable against zero.\n",
    "\n",
    "\n",
    "If you want to test the hypothesis that the effect of Age is different from the effect of connectivity, you can set up a contrast matrix with two rows:\n",
    "```\n",
    "[0,1,−1,0]. This is a contrast because it sums to 0\n",
    "```\n",
    "\n",
    "Thus, if you want to see if any given effect is significant compared to the intercept (average), you can use the following contrast matrix:\n",
    "```\n",
    "[1,0,0,0]\n",
    "[-1,1,0,0]\n",
    "[-1,0,1,0]\n",
    "[-1,0,0,1] actually a coding matrix of averaging vectors\n",
    "```\n",
    "\n",
    "The first row tests the coefficient for Age against zero, and the second row tests the coefficient for connectivity against zero. The difference between the two coefficients can then be assessed.\n",
    "_____\n",
    "You can define any number of contrasts in the contrast matrix to test different hypotheses or comparisons of interest in your regression analysis.\n",
    "\n",
    "It's important to note that the specific contrasts you choose depend on your research questions and hypotheses. You should carefully consider the comparisons you want to make and design the contrast matrix accordingly.\n",
    "\n",
    "- Examples:\n",
    "    - [Two Sample T-Test](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM#Two-Group_Difference_.28Two-Sample_Unpaired_T-Test.29)\n",
    "    - [One Sample with Covariate](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM#Single-Group_Average_with_Additional_Covariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a basic contrast matrix set up to evaluate the significance of each variable.\n",
      "Here is an example of what your contrast matrix looks like as a dataframe: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Cognitive_Baseline[T.-1.5058812083195714]</th>\n",
       "      <th>Cognitive_Baseline[T.-1.0659608553273385]</th>\n",
       "      <th>Cognitive_Baseline[T.-0.6260405023351056]</th>\n",
       "      <th>Cognitive_Baseline[T.-0.1861201493428726]</th>\n",
       "      <th>Cognitive_Baseline[T.0.25380020364936035]</th>\n",
       "      <th>Cognitive_Baseline[T.0.6937205566415933]</th>\n",
       "      <th>Cognitive_Baseline[T.1.1336409096338262]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.5058812083195714]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.0659608553273385]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.6260405023351056]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.1861201493428726]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.25380020364936035]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.6937205566415933]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.1.1336409096338262]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Intercept  Cognitive_Baseline[T.-1.5058812083195714]  \\\n",
       "0           1                                          0   \n",
       "1           0                                          1   \n",
       "2           0                                          0   \n",
       "3           0                                          0   \n",
       "4           0                                          0   \n",
       "5           0                                          0   \n",
       "6           0                                          0   \n",
       "7           0                                          0   \n",
       "8           0                                          0   \n",
       "9           0                                          0   \n",
       "10          0                                          0   \n",
       "11          0                                          0   \n",
       "12          0                                          0   \n",
       "13          0                                          0   \n",
       "14          0                                          0   \n",
       "15          0                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.-1.0659608553273385]  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           1   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "5                                           0   \n",
       "6                                           0   \n",
       "7                                           0   \n",
       "8                                           0   \n",
       "9                                           0   \n",
       "10                                          0   \n",
       "11                                          0   \n",
       "12                                          0   \n",
       "13                                          0   \n",
       "14                                          0   \n",
       "15                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.-0.6260405023351056]  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           1   \n",
       "4                                           0   \n",
       "5                                           0   \n",
       "6                                           0   \n",
       "7                                           0   \n",
       "8                                           0   \n",
       "9                                           0   \n",
       "10                                          0   \n",
       "11                                          0   \n",
       "12                                          0   \n",
       "13                                          0   \n",
       "14                                          0   \n",
       "15                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.-0.1861201493428726]  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           1   \n",
       "5                                           0   \n",
       "6                                           0   \n",
       "7                                           0   \n",
       "8                                           0   \n",
       "9                                           0   \n",
       "10                                          0   \n",
       "11                                          0   \n",
       "12                                          0   \n",
       "13                                          0   \n",
       "14                                          0   \n",
       "15                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.0.25380020364936035]  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "5                                           1   \n",
       "6                                           0   \n",
       "7                                           0   \n",
       "8                                           0   \n",
       "9                                           0   \n",
       "10                                          0   \n",
       "11                                          0   \n",
       "12                                          0   \n",
       "13                                          0   \n",
       "14                                          0   \n",
       "15                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.0.6937205566415933]  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "5                                          0   \n",
       "6                                          1   \n",
       "7                                          0   \n",
       "8                                          0   \n",
       "9                                          0   \n",
       "10                                         0   \n",
       "11                                         0   \n",
       "12                                         0   \n",
       "13                                         0   \n",
       "14                                         0   \n",
       "15                                         0   \n",
       "\n",
       "    Cognitive_Baseline[T.1.1336409096338262]  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "5                                          0   \n",
       "6                                          0   \n",
       "7                                          1   \n",
       "8                                          0   \n",
       "9                                          0   \n",
       "10                                         0   \n",
       "11                                         0   \n",
       "12                                         0   \n",
       "13                                         0   \n",
       "14                                         0   \n",
       "15                                         0   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_  \\\n",
       "0                                       0   \n",
       "1                                       0   \n",
       "2                                       0   \n",
       "3                                       0   \n",
       "4                                       0   \n",
       "5                                       0   \n",
       "6                                       0   \n",
       "7                                       0   \n",
       "8                                       1   \n",
       "9                                       0   \n",
       "10                                      0   \n",
       "11                                      0   \n",
       "12                                      0   \n",
       "13                                      0   \n",
       "14                                      0   \n",
       "15                                      0   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.5058812083195714]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   1                                 \n",
       "10                                                  0                                 \n",
       "11                                                  0                                 \n",
       "12                                                  0                                 \n",
       "13                                                  0                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.0659608553273385]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   0                                 \n",
       "10                                                  1                                 \n",
       "11                                                  0                                 \n",
       "12                                                  0                                 \n",
       "13                                                  0                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.6260405023351056]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   0                                 \n",
       "10                                                  0                                 \n",
       "11                                                  1                                 \n",
       "12                                                  0                                 \n",
       "13                                                  0                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.1861201493428726]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   0                                 \n",
       "10                                                  0                                 \n",
       "11                                                  0                                 \n",
       "12                                                  1                                 \n",
       "13                                                  0                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.25380020364936035]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   0                                 \n",
       "10                                                  0                                 \n",
       "11                                                  0                                 \n",
       "12                                                  0                                 \n",
       "13                                                  1                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.6937205566415933]  \\\n",
       "0                                                   0                                \n",
       "1                                                   0                                \n",
       "2                                                   0                                \n",
       "3                                                   0                                \n",
       "4                                                   0                                \n",
       "5                                                   0                                \n",
       "6                                                   0                                \n",
       "7                                                   0                                \n",
       "8                                                   0                                \n",
       "9                                                   0                                \n",
       "10                                                  0                                \n",
       "11                                                  0                                \n",
       "12                                                  0                                \n",
       "13                                                  0                                \n",
       "14                                                  1                                \n",
       "15                                                  0                                \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.1.1336409096338262]  \n",
       "0                                                   0                               \n",
       "1                                                   0                               \n",
       "2                                                   0                               \n",
       "3                                                   0                               \n",
       "4                                                   0                               \n",
       "5                                                   0                               \n",
       "6                                                   0                               \n",
       "7                                                   0                               \n",
       "8                                                   0                               \n",
       "9                                                   0                               \n",
       "10                                                  0                               \n",
       "11                                                  0                               \n",
       "12                                                  0                               \n",
       "13                                                  0                               \n",
       "14                                                  0                               \n",
       "15                                                  1                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the same contrast matrix, but as an array.\n",
      "Copy it into a cell below and edit it for more control over your analysis.\n",
      "[\n",
      "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "contrast_matrix = cal_palm.generate_basic_contrast_matrix(design_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit Contrast Matrix Here\n",
    "- The generic contrast matrix will simply check if your Betas are significantly different from the intercept (average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrast_matrix = [\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, -1, 1, -1, 1, -1, 1, -1, 1],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalize Contrast Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Cognitive_Baseline[T.-1.5058812083195714]</th>\n",
       "      <th>Cognitive_Baseline[T.-1.0659608553273385]</th>\n",
       "      <th>Cognitive_Baseline[T.-0.6260405023351056]</th>\n",
       "      <th>Cognitive_Baseline[T.-0.1861201493428726]</th>\n",
       "      <th>Cognitive_Baseline[T.0.25380020364936035]</th>\n",
       "      <th>Cognitive_Baseline[T.0.6937205566415933]</th>\n",
       "      <th>Cognitive_Baseline[T.1.1336409096338262]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.5058812083195714]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.0659608553273385]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.6260405023351056]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.1861201493428726]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.25380020364936035]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.6937205566415933]</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.1.1336409096338262]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Intercept  Cognitive_Baseline[T.-1.5058812083195714]  \\\n",
       "0           1                                          0   \n",
       "1           0                                          1   \n",
       "2           0                                          0   \n",
       "3           0                                          0   \n",
       "4           0                                          0   \n",
       "5           0                                          0   \n",
       "6           0                                          0   \n",
       "7           0                                          0   \n",
       "8           0                                          0   \n",
       "9           0                                          0   \n",
       "10          0                                          0   \n",
       "11          0                                          0   \n",
       "12          0                                          0   \n",
       "13          0                                          0   \n",
       "14          0                                          0   \n",
       "15          0                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.-1.0659608553273385]  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           1   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "5                                           0   \n",
       "6                                           0   \n",
       "7                                           0   \n",
       "8                                           0   \n",
       "9                                           0   \n",
       "10                                          0   \n",
       "11                                          0   \n",
       "12                                          0   \n",
       "13                                          0   \n",
       "14                                          0   \n",
       "15                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.-0.6260405023351056]  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           1   \n",
       "4                                           0   \n",
       "5                                           0   \n",
       "6                                           0   \n",
       "7                                           0   \n",
       "8                                           0   \n",
       "9                                           0   \n",
       "10                                          0   \n",
       "11                                          0   \n",
       "12                                          0   \n",
       "13                                          0   \n",
       "14                                          0   \n",
       "15                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.-0.1861201493428726]  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           1   \n",
       "5                                           0   \n",
       "6                                           0   \n",
       "7                                           0   \n",
       "8                                           0   \n",
       "9                                           0   \n",
       "10                                          0   \n",
       "11                                          0   \n",
       "12                                          0   \n",
       "13                                          0   \n",
       "14                                          0   \n",
       "15                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.0.25380020364936035]  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "5                                           1   \n",
       "6                                           0   \n",
       "7                                           0   \n",
       "8                                           0   \n",
       "9                                           0   \n",
       "10                                          0   \n",
       "11                                          0   \n",
       "12                                          0   \n",
       "13                                          0   \n",
       "14                                          0   \n",
       "15                                          0   \n",
       "\n",
       "    Cognitive_Baseline[T.0.6937205566415933]  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "5                                          0   \n",
       "6                                          1   \n",
       "7                                          0   \n",
       "8                                          0   \n",
       "9                                          0   \n",
       "10                                         0   \n",
       "11                                         0   \n",
       "12                                         0   \n",
       "13                                         0   \n",
       "14                                         0   \n",
       "15                                         0   \n",
       "\n",
       "    Cognitive_Baseline[T.1.1336409096338262]  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "5                                          0   \n",
       "6                                          0   \n",
       "7                                          1   \n",
       "8                                          0   \n",
       "9                                          0   \n",
       "10                                         0   \n",
       "11                                         0   \n",
       "12                                         0   \n",
       "13                                         0   \n",
       "14                                         0   \n",
       "15                                         0   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_  \\\n",
       "0                                       0   \n",
       "1                                       0   \n",
       "2                                       0   \n",
       "3                                       0   \n",
       "4                                       0   \n",
       "5                                       0   \n",
       "6                                       0   \n",
       "7                                       0   \n",
       "8                                       1   \n",
       "9                                       0   \n",
       "10                                      0   \n",
       "11                                      0   \n",
       "12                                      0   \n",
       "13                                      0   \n",
       "14                                      0   \n",
       "15                                      0   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.5058812083195714]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   1                                 \n",
       "10                                                  0                                 \n",
       "11                                                  0                                 \n",
       "12                                                  0                                 \n",
       "13                                                  0                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.0659608553273385]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   0                                 \n",
       "10                                                  1                                 \n",
       "11                                                  0                                 \n",
       "12                                                  0                                 \n",
       "13                                                  0                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.6260405023351056]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   0                                 \n",
       "10                                                  0                                 \n",
       "11                                                  1                                 \n",
       "12                                                  0                                 \n",
       "13                                                  0                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.1861201493428726]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   0                                 \n",
       "10                                                  0                                 \n",
       "11                                                  0                                 \n",
       "12                                                  1                                 \n",
       "13                                                  0                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.25380020364936035]  \\\n",
       "0                                                   0                                 \n",
       "1                                                   0                                 \n",
       "2                                                   0                                 \n",
       "3                                                   0                                 \n",
       "4                                                   0                                 \n",
       "5                                                   0                                 \n",
       "6                                                   0                                 \n",
       "7                                                   0                                 \n",
       "8                                                   0                                 \n",
       "9                                                   0                                 \n",
       "10                                                  0                                 \n",
       "11                                                  0                                 \n",
       "12                                                  0                                 \n",
       "13                                                  1                                 \n",
       "14                                                  0                                 \n",
       "15                                                  0                                 \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.6937205566415933]  \\\n",
       "0                                                   0                                \n",
       "1                                                   0                                \n",
       "2                                                   0                                \n",
       "3                                                   0                                \n",
       "4                                                   0                                \n",
       "5                                                   0                                \n",
       "6                                                   0                                \n",
       "7                                                   0                                \n",
       "8                                                   0                                \n",
       "9                                                   0                                \n",
       "10                                                  0                                \n",
       "11                                                  0                                \n",
       "12                                                  0                                \n",
       "13                                                  0                                \n",
       "14                                                  1                                \n",
       "15                                                  0                                \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.1.1336409096338262]  \n",
       "0                                                   0                               \n",
       "1                                                   0                               \n",
       "2                                                   0                               \n",
       "3                                                   0                               \n",
       "4                                                   0                               \n",
       "5                                                   0                               \n",
       "6                                                   0                               \n",
       "7                                                   0                               \n",
       "8                                                   0                               \n",
       "9                                                   0                               \n",
       "10                                                  0                               \n",
       "11                                                  0                               \n",
       "12                                                  0                               \n",
       "13                                                  0                               \n",
       "14                                                  0                               \n",
       "15                                                  1                               "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_matrix_df = cal_palm.finalize_contrast_matrix(design_matrix=design_matrix, \n",
    "                                                    contrast_matrix=contrast_matrix) \n",
    "contrast_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Define Exchangeability Blocks (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional - Exchangability Blocks\n",
    "- This is optional and for when you are doing a meta-analysis\n",
    "- Not yet implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is just an example, you will have to edit to adapt to your data, \n",
    "### but it should be integers, starting with 1,2,3....\n",
    "\n",
    "# coding_key = {\"Prosopagnosia_w_Yeo1000\": 1,\n",
    "#              \"Corbetta_Lesions\": 1,\n",
    "#              \"DBS_dataset\": 2\n",
    "#              }\n",
    "\n",
    "# eb_matrix = pd.DataFrame()\n",
    "# eb_matrix = clean_df['dataset'].replace(coding_key)\n",
    "# display(eb_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Run the Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Results Are Displayed Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Results: Ordinary least squares\n",
      "================================================================================================================================\n",
      "Model:                               OLS                                            Adj. R-squared:                      0.417  \n",
      "Dependent Variable:                  Percent_Cognitive_Improvement                  AIC:                                 68.3928\n",
      "Date:                                2024-02-23 15:17                               BIC:                                 87.2643\n",
      "No. Observations:                    26                                             Log-Likelihood:                      -19.196\n",
      "Df Model:                            14                                             F-statistic:                         2.279  \n",
      "Df Residuals:                        11                                             Prob (F-statistic):                  0.0879 \n",
      "R-squared:                           0.744                                          Scale:                               0.60591\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                                 Coef.  Std.Err.    t    P>|t|   [0.025   0.975]\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                                                        1.0293   0.5590  1.8413 0.0927  -0.2011  2.2597\n",
      "Cognitive_Baseline[T.-1.5058812083195714]                                        0.1496   0.7664  0.1952 0.8488  -1.5373  1.8365\n",
      "Cognitive_Baseline[T.-1.0659608553273385]                                       -4.3208   3.2146 -1.3441 0.2060 -11.3961  2.7546\n",
      "Cognitive_Baseline[T.-0.6260405023351056]                                       -0.8147   0.7939 -1.0261 0.3269  -2.5620  0.9327\n",
      "Cognitive_Baseline[T.-0.1861201493428726]                                       -1.2244   0.7023 -1.7434 0.1091  -2.7701  0.3213\n",
      "Cognitive_Baseline[T.0.25380020364936035]                                       -0.1211   0.9111 -0.1329 0.8967  -2.1265  1.8843\n",
      "Cognitive_Baseline[T.0.6937205566415933]                                         0.0171   1.2418  0.0138 0.9893  -2.7162  2.7503\n",
      "Cognitive_Baseline[T.1.1336409096338262]                                        -1.1058   0.6277 -1.7616 0.1059  -2.4874  0.2758\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_                                           -0.6839   0.5069 -1.3491 0.2044  -1.7996  0.4319\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.5058812083195714]  0.7944   0.6513  1.2198 0.2481  -0.6391  2.2279\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-1.0659608553273385] -3.5091   3.2732 -1.0721 0.3066 -10.7133  3.6951\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.6260405023351056]  2.0064   1.5997  1.2542 0.2358  -1.5146  5.5273\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.-0.1861201493428726]  0.6538   0.5912  1.1059 0.2924  -0.6474  1.9549\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.25380020364936035] -9.0213   2.8574 -3.1572 0.0091 -15.3104 -2.7322\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.0.6937205566415933]   1.8483   1.4117  1.3093 0.2171  -1.2587  4.9554\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline[T.1.1336409096338262]   0.1666   0.5564  0.2995 0.7702  -1.0580  1.3913\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "Omnibus:                                4.503                         Durbin-Watson:                           2.510            \n",
      "Prob(Omnibus):                          0.105                         Jarque-Bera (JB):                        2.774            \n",
      "Skew:                                   -0.538                        Prob(JB):                                0.250            \n",
      "Kurtosis:                               4.185                         Condition No.:                           14997747968599240\n",
      "================================================================================================================================\n",
      "* The condition number is large (1e+16). This might indicate             strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "# Fit the regression model\n",
    "model = sm.OLS(outcome_matrix, design_matrix)\n",
    "results = model.fit()\n",
    "print(results.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the Regression as a Forest Plot\n",
    "- This will probably look poor if you ran a regression without standardizing your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/supplements/alzheimer_regressions as regression_forest_plot.svg and .png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import ForestPlot\n",
    "forest = ForestPlot(model=results, sig_digits=2, out_dir=out_dir, table=False)\n",
    "forest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize The Model's Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import model_diagnostics\n",
    "model_diagnostics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the Partial Regression Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import PartialRegressionPlot\n",
    "partial_plot = PartialRegressionPlot(model=results, design_matrix=design_matrix, out_dir=out_dir, palette='Reds')\n",
    "partial_plot = partial_plot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Run the Contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast Results Are Displayed Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_results = results.t_test(contrast_matrix_df)\n",
    "print(contrast_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Compare the Coefficient Between 2 Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_column = 'Age_Group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import RegressionAnalysis\n",
    "regression_test = RegressionAnalysis(outcome_df=outcome_matrix, design_df=design_matrix, groups_df=data_df[[groups_column]], N=10000, metric='similarity', two_tail=False, out_dir=out_dir)\n",
    "regression_test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - Compare Distribution of T Values Between Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_together=False\n",
    "groups_column = 'Age_Group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import BootstrappedRegressionAnalysis\n",
    "\n",
    "# Create an instance of BootstrappedRegressionAnalysis\n",
    "bootstrapped_regression_test = BootstrappedRegressionAnalysis(outcome_df=outcome_matrix, design_df=design_matrix, groups_df=data_df[[groups_column]], N=10000, out_dir=out_dir, plot_together=plot_together)\n",
    "\n",
    "# Run the bootstrapped regression analysis\n",
    "df1 = bootstrapped_regression_test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Predict Another Dataframe\n",
    "- Can use this to predict data from a second group, such as the 'other_df' defined in \"Step 01, Drop Rows Based on Value of a Column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "def calculate_ssr(observations, predictions):\n",
    "    \"\"\"\n",
    "    Calculate the regression sum of squares.\n",
    "    This is the sum of squared deviations, with deviation being Y_hat_i - Y_bar\n",
    "    \n",
    "    SSR is a measure used to quantify the variance in the observed data that is not explained by the model. \n",
    "    It is calculated as the sum of the squares of the differences between the observed values and the model's predictions. \n",
    "    The more the 'mean' predicts \n",
    "    A lower SSR indicates a better model fit, meaning the model's predictions are closer to the actual observations.\n",
    "    \n",
    "    SSR = Σ(y_hat - y_bar)^2 \n",
    "    \n",
    "    Parameters:\n",
    "    - observations (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated SSR.\n",
    "    \"\"\"\n",
    "    y_hat = observations\n",
    "    y_bar = np.mean(predictions)\n",
    "    ssr = np.sum((y_hat - y_bar) ** 2)\n",
    "    return ssr\n",
    "\n",
    "def calculate_sse(observations, predictions):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squares due to error (SSE).\n",
    "    \n",
    "    SSE is a measure of the total deviation of the response values from the fit to the response values. \n",
    "    It is calculated as the sum of the squares of the differences between the predicted values and the observed values. \n",
    "    A lower SSE indicates a model that more accurately fits the data.\n",
    "    \n",
    "    SSE = Σ(y - y_hat)^2\n",
    "    \n",
    "    Parameters:\n",
    "    - observations (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated SSE.\n",
    "    \"\"\"\n",
    "    y = observations\n",
    "    y_hat = predictions\n",
    "    sse = np.sum((y - y_hat) ** 2)\n",
    "    return sse\n",
    "\n",
    "def calculate_ssto(observations):\n",
    "    \"\"\"\n",
    "    Calculate the total sum of squares (SSTO).\n",
    "    \n",
    "    SSTO is a measure of the total variance in the observed data and is used as a comparative tool for model evaluation. \n",
    "    It is calculated as the sum of the squares of the differences between the observed values and their overall mean. \n",
    "    SSTO is used in the denominator of the coefficient of determination, R^2, which assesses the fit of the model.\n",
    "    \n",
    "    SSTO = Σ(y - y_bar)^2\n",
    "    \n",
    "    Parameters:\n",
    "    - observations (array-like): The actual observed outcomes.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated SSTO.\n",
    "    \"\"\"\n",
    "    y = observations\n",
    "    y_bar = np.mean(y)\n",
    "    ssto = np.sum((y - y_bar) ** 2)\n",
    "    return ssto\n",
    "\n",
    "\n",
    "def calculate_msr(ssr, num_regressors):\n",
    "    \"\"\"\n",
    "    Calculate the mean square due to regression (MSR).\n",
    "    \n",
    "    MSR is a measure of the variation explained by the independent variables in the model. It is calculated as the \n",
    "    sum of squared residuals (SSR) divided by the degrees of freedom, which is the number of independent variables (regressors) minus one.\n",
    "    A higher MSR indicates that the model explains a greater amount of variation in the outcome variable.\n",
    "    \n",
    "    MSR = SSR / (number of regressors - 1)\n",
    "    \n",
    "    Parameters:\n",
    "    - ssr (float): The sum of squared residuals from the regression model.\n",
    "    - num_regressors (int): The number of independent variables in the model.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated MSR.\n",
    "    \"\"\"\n",
    "    return ssr / (num_regressors - 1)\n",
    "\n",
    "def calculate_mse(sse, num_regressors, num_observations):\n",
    "    \"\"\"\n",
    "    Calculate the mean square error (MSE).\n",
    "    \n",
    "    MSE is a measure of the average of the squares of the errors, that is, the average squared difference between the observed actual outcomes and the outcomes predicted by the model. It is calculated as the sum of squared errors (SSE) divided by the degrees of freedom, which is the number of observations minus the number of regressors.\n",
    "    A lower MSE indicates a better fit of the model to the data.\n",
    "    \n",
    "    MSE = SSE / (number of observations - number of regressors)\n",
    "    \n",
    "    Parameters:\n",
    "    - sse (float): The sum of squared errors from the regression model.\n",
    "    - num_regressors (int): The number of independent variables in the model.\n",
    "    - num_observations (int): The number of observations in the data set.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated MSE.\n",
    "    \"\"\"\n",
    "    return sse / (num_observations - num_regressors)\n",
    "\n",
    "def calculate_f_stat(msr, mse, num_regressors, num_observations):\n",
    "    \"\"\"\n",
    "    Calculate the F-statistic.\n",
    "    \n",
    "    The F-statistic is used to compare statistical models that have been fitted to a data set in order to identify the model that best fits the population from which the data were sampled. It is the ratio of the mean square due to regression (MSR) to the mean square error (MSE).\n",
    "    \n",
    "    The F-statistic follows the F-distribution under the null hypothesis that the model with no independent variables fits the data as well as your model. A higher F-statistic implies that the null hypothesis is false, and your model adds value in explaining the variation in the data.\n",
    "    \n",
    "    F = MSR / MSE\n",
    "    \n",
    "    Parameters:\n",
    "    - msr (float): The mean square due to regression.\n",
    "    - mse (float): The mean square error.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated F-statistic.\n",
    "    - float: The p-value from the F-distribution.\n",
    "    \"\"\"\n",
    "    f_stat = msr / mse\n",
    "    # The degrees of freedom for the numerator (dfn) is the number of independent variables (regressors).\n",
    "    # The degrees of freedom for the denominator (dfd) is the total number of observations minus the number of independent variables minus 1.\n",
    "    # These values need to be defined or calculated outside of this function.\n",
    "    dfn = num_regressors - 1\n",
    "    dfd = num_observations - num_regressors \n",
    "    p_value = f.sf(f_stat, dfn, dfd)\n",
    "    return f_stat, p_value\n",
    "\n",
    "def run_goodness_of_fit(target_outcome_matrix, predictions, target_design_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the F-statistic and p-value for a linear regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - target_outcome_matrix (array-like): The actual observed outcomes (Y_actual).\n",
    "    - predictions (array-like): The outcomes predicted by the model (Y_hat).\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated F-statistic.\n",
    "    - float: The p-value from the F-distribution.\n",
    "    \"\"\"\n",
    "    # Calculate the regression sum of squares (SSR).\n",
    "    ssr = calculate_ssr(target_outcome_matrix, predictions)\n",
    "\n",
    "    # Calculate the sum of squares due to error (SSE).\n",
    "    sse = calculate_sse(target_outcome_matrix, predictions)\n",
    "\n",
    "    # Calculate the number of regressors and observations.\n",
    "    num_regressors = target_design_matrix.shape[1]\n",
    "    num_observations = len(target_outcome_matrix)\n",
    "\n",
    "    # Calculate the mean square due to regression (MSR).\n",
    "    msr = calculate_msr(ssr, num_regressors)\n",
    "\n",
    "    # Calculate the mean square error (MSE).\n",
    "    mse = calculate_mse(sse, num_regressors, num_observations)\n",
    "\n",
    "    # Calculate the F-statistic and p-value.\n",
    "    f_stat, p_value = calculate_f_stat(msr, mse, num_regressors, num_observations)\n",
    "\n",
    "    return f_stat, p_value\n",
    "\n",
    "def calculate_r_squared(observations, predictions):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared (coefficient of determination) value.\n",
    "\n",
    "    R-squared measures the proportion of the variance in the observed outcomes that is explained by the predictions.\n",
    "\n",
    "    R-squared = 1 - (SSE / SSTO)\n",
    "\n",
    "    Parameters:\n",
    "    - observations (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated R-squared value.\n",
    "    \"\"\"\n",
    "    # Calculate SSE (Sum of Squares of Errors)\n",
    "    sse = np.sum((observations - predictions) ** 2)\n",
    "\n",
    "    # Calculate SSTO (Total Sum of Squares)\n",
    "    y_mean = np.mean(observations)\n",
    "    ssto = np.sum((observations - y_mean) ** 2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r_squared = 1 - (sse / ssto)\n",
    "    \n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the design matrix\n",
    "target_outcome_matrix, target_design_matrix = cal_palm.define_design_matrix(formula, data_df)\n",
    "predictions = results.predict(target_design_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract F-Test for Goodness of Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_stat, p_value = run_goodness_of_fit(target_outcome_matrix=target_outcome_matrix.to_numpy().flatten(), \n",
    "                    predictions=predictions.to_numpy().flatten(), \n",
    "                    target_design_matrix=target_design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract R-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.1882\n"
     ]
    }
   ],
   "source": [
    "r_squared = calculate_r_squared(target_outcome_matrix.to_numpy().flatten(), predictions.to_numpy().flatten())\n",
    "print(f\"R-squared: {r_squared:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 5.809972048853231\n",
      "Root Mean Squared Error (RMSE): 2.4103883605869885\n",
      "Mean Absolute Error (MAE): 1.6551566070694164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(target_outcome_matrix, predictions)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(target_outcome_matrix, predictions)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot The Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_scatter_with_f_stat_in_title(target_outcome_matrix, predictions, f_stat, p_value, out_dir=None):\n",
    "    \"\"\"\n",
    "    Create a scatterplot of predicted vs. observed values with F-statistic and p-value in the title.\n",
    "\n",
    "    Parameters:\n",
    "    - target_outcome_matrix (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "    - f_stat (float): The F-statistic value.\n",
    "    - p_value (float): The p-value.\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "    # Create a scatterplot using Seaborn\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=predictions,\n",
    "                    y=target_outcome_matrix,\n",
    "                    label='Predicted vs. Observed')\n",
    "\n",
    "    # Add a diagonal line\n",
    "    xlim = plt.xlim()  # Get current X-axis limits\n",
    "    ylim = plt.ylim()  # Get current Y-axis limits\n",
    "    min_limit = min(xlim[0], ylim[0])\n",
    "    max_limit = max(xlim[1], ylim[1])\n",
    "    plt.plot([min_limit, max_limit], [min_limit, max_limit], linestyle='--', color='gray', label='Perfect Fit')\n",
    "\n",
    "    # Add labels and title with F-statistic and p-value\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Observed Values')\n",
    "    plt.title(f'Scatterplot of Predicted vs. Observed Values\\nF-statistic: {f_stat:.2f}, p-value: {p_value:.4f}')\n",
    "\n",
    "    # Set axis limits\n",
    "    plt.xlim(min_limit, max_limit)\n",
    "    plt.ylim(min_limit, max_limit)\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "    \n",
    "    if out_dir:\n",
    "        # Save the figure\n",
    "        plt.savefig(f\"{out_dir}/predicted_plot.png\", bbox_inches='tight')\n",
    "        plt.savefig(f\"{out_dir}/predicted_plot.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/predicted_plot.svg')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals(target_outcome_matrix, predictions, f_stat, p_value, out_dir=None):\n",
    "    \"\"\"\n",
    "    Create a scatterplot of residuals with F-statistic and p-value in the title and save it.\n",
    "\n",
    "    Parameters:\n",
    "    - target_outcome_matrix (array-like): The actual observed outcomes.\n",
    "    - predictions (array-like): The outcomes predicted by the model.\n",
    "    - f_stat (float): The F-statistic value.\n",
    "    - p_value (float): The p-value.\n",
    "    - out_dir (str, optional): The directory to save the plot. If None, the plot won't be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "    # Calculate residuals\n",
    "    residuals = target_outcome_matrix - predictions\n",
    "\n",
    "    # Create a scatterplot of residuals\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=predictions,\n",
    "                    y=residuals,\n",
    "                    label='Residuals vs. Predicted')\n",
    "\n",
    "    # Calculate y-axis limits\n",
    "    y_lim_min = min(-3, np.min(residuals) - 0.5)\n",
    "    y_lim_max = max(3, np.max(residuals) + 0.5)\n",
    "\n",
    "    # Set y-axis limits\n",
    "    plt.ylim(y_lim_min, y_lim_max)\n",
    "\n",
    "    # Add a horizontal line at y=0\n",
    "    plt.axhline(0, color='gray', linestyle='--', label='Zero Residual')\n",
    "\n",
    "    # Add labels and title with F-statistic and p-value\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'Scatterplot of Residuals vs. Predicted Values\\nF-statistic: {f_stat:.2f}, p-value: {p_value:.4f}')\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "\n",
    "    if out_dir:\n",
    "        # Save the figure\n",
    "        plt.savefig(f\"{out_dir}/residuals_plot.png\", bbox_inches='tight')\n",
    "        plt.savefig(f\"{out_dir}/residuals_plot.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/residuals_plot.svg')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with saving:\n",
    "# plot_residuals(target_outcome_matrix.to_numpy().flatten(), predictions.to_numpy().flatten(), f_stat, p_value, out_dir=\"your_output_directory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(target_outcome_matrix.to_numpy().flatten(), predictions.to_numpy().flatten(), f_stat, p_value, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_with_f_stat_in_title(target_outcome_matrix.to_numpy().flatten(), predictions.to_numpy().flatten(), f_stat, p_value, out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Visualize an ANCOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_outcomes(model, data_df=design_matrix):\n",
    "    \"\"\"\n",
    "    Predicts the outcomes for each row in the data DataFrame based on the given model and formula.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The fitted OLS model.\n",
    "    - data_df: DataFrame containing the data for prediction.\n",
    "\n",
    "    Returns:\n",
    "    - predictions: A DataFrame with predicted values for each row in the data_df.\n",
    "    \"\"\"\n",
    "    # Add a column for predictions to the data_df\n",
    "    \n",
    "    return model.predict(data_df)\n",
    "\n",
    "def calculate_average_predictions(data_df, predictions, group1_column, group2_column):\n",
    "    \"\"\"\n",
    "    Calculates average predictions for each combination of unique values in Group 1 and Group 2.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the 'Group 1' and 'Group 2' columns.\n",
    "    - predictions: Series containing the predicted values.\n",
    "    - group1_column: Name of the Group 1 column in data_df.\n",
    "    - group2_column: Name of the Group 2 column in data_df.\n",
    "\n",
    "    Returns:\n",
    "    - average_predictions_df: DataFrame with 'Group 1', 'Group 2', and 'Average Prediction' columns.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame to store the results\n",
    "    average_predictions_df = pd.DataFrame(columns=['Group 1', 'Group 2', 'Average Prediction'])\n",
    "\n",
    "    # Get unique values in Group 1 and Group 2\n",
    "    unique_group1_values = data_df[group1_column].unique()\n",
    "    unique_group2_values = data_df[group2_column].unique()\n",
    "\n",
    "    # Loop through unique values of Group 1 and Group 2\n",
    "    for group1_value in unique_group1_values:\n",
    "        for group2_value in unique_group2_values:\n",
    "            # Filter predictions based on the indices where Group 1 and Group 2 match\n",
    "            filtered_predictions = predictions[(data_df[group1_column] == group1_value) & (data_df[group2_column] == group2_value)]\n",
    "\n",
    "            # Calculate the average prediction for this combination\n",
    "            average_prediction = filtered_predictions.mean()\n",
    "\n",
    "            # Add the result to the DataFrame\n",
    "            average_predictions_df = average_predictions_df.append({'Group 1': group1_value, 'Group 2': group2_value, 'Average Prediction': average_prediction}, ignore_index=True)\n",
    "\n",
    "    return average_predictions_df\n",
    "\n",
    "def plot_grouped_barplot(data_df, out_dir=out_dir):\n",
    "    \"\"\"\n",
    "    Plots a grouped barplot with 'Group 2' on the x-axis and 'Group 1' bars side-by-side.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the data for plotting.\n",
    "    \"\"\"\n",
    "    # Set the style for the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Create the barplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Group 2', y='Average Prediction', hue='Group 1', data=data_df)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Group 2')\n",
    "    plt.ylabel('Average Prediction')\n",
    "    plt.title('Grouped Barplot of Average Predictions')\n",
    "\n",
    "    # Show the legend\n",
    "    plt.legend(title='Group 1', loc='upper right')\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"{out_dir}/estimated_marginal_mean.png\", bbox_inches='tight')\n",
    "    plt.savefig(f\"{out_dir}/estimated_marginal_mean.svg\", bbox_inches='tight')\n",
    "    print(f'Saved to {out_dir}/estimated_marginal_mean.svg')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "def calculate_average_actual(data_df, group1_column, group2_column, outcome_column):\n",
    "    \"\"\"\n",
    "    Calculates average actual outcomes for each combination of unique values in Group 1 and Group 2.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the data.\n",
    "    - group1_column: Name of the Group 1 column in data_df.\n",
    "    - group2_column: Name of the Group 2 column in data_df.\n",
    "    - outcome_column: Name of the column containing the actual outcomes.\n",
    "\n",
    "    Returns:\n",
    "    - average_actual_df: DataFrame with 'Group 1', 'Group 2', and 'Average Actual' columns.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame to store the results\n",
    "    average_actual_df = pd.DataFrame(columns=['Group 1', 'Group 2', 'Average Actual'])\n",
    "\n",
    "    # Get unique values in Group 1 and Group 2\n",
    "    unique_group1_values = data_df[group1_column].unique()\n",
    "    unique_group2_values = data_df[group2_column].unique()\n",
    "\n",
    "    # Loop through unique values of Group 1 and Group 2\n",
    "    for group1_value in unique_group1_values:\n",
    "        for group2_value in unique_group2_values:\n",
    "            # Filter data based on the indices where Group 1 and Group 2 match\n",
    "            filtered_data = data_df[(data_df[group1_column] == group1_value) & (data_df[group2_column] == group2_value)]\n",
    "\n",
    "            # Calculate the average actual outcome for this combination\n",
    "            average_actual = filtered_data[outcome_column].mean()\n",
    "\n",
    "            # Add the result to the DataFrame\n",
    "            average_actual_df = average_actual_df.append({'Group 1': group1_value, 'Group 2': group2_value, 'Average Actual': average_actual}, ignore_index=True)\n",
    "\n",
    "    return average_actual_df\n",
    "\n",
    "\n",
    "def plot_grouped_barplot_actual(data_df, out_dir=out_dir):\n",
    "    \"\"\"\n",
    "    Plots a grouped barplot with 'Group 2' on the x-axis and 'Group 1' bars side-by-side for average actual outcomes.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the data for plotting.\n",
    "    \"\"\"\n",
    "    # Set the style for the plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Create the barplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Group 2', y='Average Actual', hue='Group 1', data=data_df)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Group 2')\n",
    "    plt.ylabel('Average Actual Outcome')\n",
    "    plt.title('Grouped Barplot of Average Actual Outcomes')\n",
    "\n",
    "    # Show the legend\n",
    "    plt.legend(title='Group 1', loc='upper right')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    out_dir = out_dir # Replace with your output directory\n",
    "    plt.savefig(f\"{out_dir}/actual_marginal_mean.png\", bbox_inches='tight')\n",
    "    plt.savefig(f\"{out_dir}/actual_marginal_mean.svg\", bbox_inches='tight')\n",
    "    print(f'Saved to {out_dir}/actual_marginal_mean.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = 'Subiculum_Group_By_Inflection_Point'\n",
    "group_2  ='Age_Disease_and_Cohort'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_outcomes(model=results, data_df=design_matrix)\n",
    "average_predictions_df = calculate_average_predictions(data_df, predictions, group_1, group_2)\n",
    "plot_grouped_barplot(average_predictions_df, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped_barplot_actual(calculate_average_actual(data_df, group_1, group_2, outcome_matrix.columns[-1]),out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalizable EMM Plot**\n",
    "- Set Labels below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_label = 'X-axis Label'\n",
    "y_label = 'Y-axis Label'\n",
    "plus_2_stdev=False\n",
    "minus_2_stdev=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import EMMPlot\n",
    "\n",
    "emm_plot = EMMPlot(formula, data_df, model=results)\n",
    "emm_plot.extract_unique_variables()\n",
    "emm_plot.create_emm_df(plus_2_stdev=plus_2_stdev, minus_2_stdev=minus_2_stdev)\n",
    "emm_plot.define_design_matrix()\n",
    "emm_plot.predict_emm_design_df()\n",
    "emm_plot.plot_predictions()#x_label, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import FactorialPlot\n",
    "factor_plot = FactorialPlot(formula, data_df, model=results)\n",
    "factor_plot.extract_unique_variables()\n",
    "factor_plot.create_emm_df()\n",
    "factor_plot.define_design_matrix()\n",
    "factor_plot.predict_emm_design_df()\n",
    "factor_plot.create_interaction_plot(factor_plot.emm_df.columns.to_list()[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the column names used to generate the original formula. The order of the list determines how variables are categorized in the following interaction plot. \n",
    "\n",
    "- list_of_categories = ['Subiculum_Group_By_Inflection', 'City', 'Age_Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_categories = ['Subiculum_Group_By_Inflection_Point', 'City', 'Age_Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import FlexibleInteractionPlot\n",
    "\n",
    "interaction_plot = FlexibleInteractionPlot(formula, data_df, model=results)\n",
    "interaction_plot.extract_unique_variables()\n",
    "interaction_plot.create_emm_df()\n",
    "interaction_plot.define_design_matrix()\n",
    "interaction_plot.predict_emm_design_df()\n",
    "interaction_plot.create_interaction_plot_dynamic(interaction_plot.emm_df.dropna(), list_of_categories, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit DataFrame Contents for Plotting Purposes\n",
    "- After changing the contents of a category, you can create the above plot again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_df['City'] = np.where(data_df['City'] == 'Toronto', 'Alzheimer Toronto (N=46)', data_df['City'])\n",
    "data_df['City'] = np.where(data_df['City'] == 'Wurzburg', 'Parkinson Wurzburg (N=26)', data_df['City'])\n",
    "data_df['City'] = np.where(data_df['City'] == 'Parkinson Wurzburg (N=17)', 'Parkinson Boston (N=17)', data_df['City'])\n",
    "data_df['City'] = np.where(data_df['City'] == 'Queensland', 'Parkinson Queensland (N=59)', data_df['City'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Get Zero Points of Each Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero point is the point where the coefficient cross zero.\n",
    "When the response topology has a saddle point, the zero point indicates where the saddle is. \n",
    "Technically, the linear regression's formula does not have a saddle point. \n",
    "However, if you rotate the response topology which exhibits a saddle point by 45 degrees, you will observe a saddle point in orientation with the Cartesian plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.calculus_utils import find_zero_point_of_coefficients\n",
    "find_zero_point_of_coefficients(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - Compare 2 Different Sets of Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed__0', 'PTID', 'SITEID', 'VISCODE', 'USERDATE', 'EXAMDATE', 'Q1',\n",
       "       'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12',\n",
       "       'Q14', 'TOTAL11', 'TOTALMOD', 'subject', 'CerebellumSBM',\n",
       "       'CerebellumCSF', 'CerebellumGM', 'CerebellumWM', 'FrontalSurface',\n",
       "       'FrontalCSF', 'FrontalGM', 'FrontalWM', 'InsularSurface', 'InsularCSF',\n",
       "       'InsularGM', 'InsularWM', 'MTLSurface', 'MTLCSF', 'MTLGM', 'MTLWM',\n",
       "       'OccipitalSurface', 'OccipitalCSF', 'OccipitalGM', 'OccipitalWM',\n",
       "       'ParietalSurface', 'ParietalCSF', 'ParietalGM', 'ParietalWM',\n",
       "       'TemporalSurface', 'TemporalCSF', 'TemporalGM', 'TemporalWM',\n",
       "       'SubcortexSurface', 'SubcortexSurfaceVentricle', 'SubcortexCSF',\n",
       "       'SubcortexGM', 'SubcortexWM', 'WholeBrainSurface', 'WholeBrainCSF',\n",
       "       'WholeBrainGM', 'WholeBrainWM', 'temp_ins_csf', 'temp_ins_gm',\n",
       "       'temp_ins_wm', 'temp_ins_surface', 'frontal', 'temporal', 'parietal',\n",
       "       'occipital', 'cerebellum', 'Mesial_Temporal', 'ventricle',\n",
       "       'cerebellar_vermis', 'cerebellar_fissures', 'frontal_eh', 'temporal_eh',\n",
       "       'parietal_eh', 'occipital_eh', 'cerebellum_eh', 'mesial_temporal_eh',\n",
       "       'ventricle_eh', 'CerebellumComp', 'SubcortexComp', 'TemporalComp',\n",
       "       'MTLComp', 'OccipitalComp', 'FrontalComp', 'ParietalComp',\n",
       "       'InsularComp', 'ins_temp_comp', 'Occipital_CSF_CT', 'Frontal_CSFCT',\n",
       "       'Parietal_CSFCT', 'Temp_Ins_CSFCT', 'Occipital_CSFWM', 'Frontal_CSFWM',\n",
       "       'Parietal_CSFWM', 'temp_ins_CSFWM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TOTALMOD ~  FrontalCSF + OccipitalCSF + ParietalCSF + temp_ins_csf'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Nested Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   df_resid        ssr  df_diff   ss_diff    F  Pr(>F)\n",
      "0      42.0  31.831684      0.0       NaN  NaN     NaN\n",
      "1      42.0  31.097587     -0.0  0.734097 -inf     NaN\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.api import anova_lm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "smaller_formula = 'TOTALMOD ~ Frontal_CSFCT + Parietal_CSFCT + Temp_Ins_CSFCT + Occipital_CSF_CT'\n",
    "\n",
    "larger_formula = 'TOTALMOD ~  FrontalCSF + OccipitalCSF + ParietalCSF + temp_ins_csf'\n",
    "\n",
    "#----------------------------------------------------------------DO NOT TOUCH!----------------------------------------------------------------\n",
    "table1 = anova_lm(smf.ols(smaller_formula, data=data_df).fit(), smf.ols(larger_formula, data=data_df).fit())\n",
    "print(table1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Un-nested Models\n",
    "- Need to employ permutation because there is no existing method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula1 = 'TOTALMOD ~  OccipitalComp + FrontalComp + ParietalComp + ins_temp_comp'\n",
    "\n",
    "formula2 = 'TOTALMOD ~  Frontal_CSFWM + Occipital_CSFWM + Parietal_CSFWM + temp_ins_CSFWM'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 121.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original R^2 Difference: 0.13706773474983291\n",
      "P-value from permutation test: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Original R^2 values from the models\n",
    "r2_model1 = smf.ols(formula1, data=data_df).fit().rsquared\n",
    "r2_model2 = smf.ols(formula2, data=data_df).fit().rsquared\n",
    "\n",
    "# Difference in R^2 of original models\n",
    "original_diff = r2_model1 - r2_model2\n",
    "\n",
    "# Number of permutations\n",
    "n_permutations = 1000\n",
    "\n",
    "# Store differences from permutations\n",
    "perm_diffs = []\n",
    "\n",
    "for i in tqdm(range(n_permutations)):\n",
    "    # Permute the outcome variable\n",
    "    data_df_permuted = data_df.copy()\n",
    "    data_df_permuted['TOTALMOD'] = np.random.permutation(data_df_permuted['TOTALMOD'].values)\n",
    "    \n",
    "    # Fit the models to the permuted dataset and calculate R^2\n",
    "    perm_model1 = smf.ols(formula1, data=data_df_permuted).fit()\n",
    "    perm_model2 = smf.ols(formula2, data=data_df_permuted).fit()\n",
    "    \n",
    "    # Difference in R^2 for the permuted models\n",
    "    perm_diff = perm_model1.rsquared - perm_model2.rsquared\n",
    "    \n",
    "    # Store the difference\n",
    "    perm_diffs.append(perm_diff)\n",
    "\n",
    "# Calculate the p-value as the proportion of permuted differences\n",
    "# that are greater than or equal to the observed difference\n",
    "p_value = np.mean([abs(diff) >= abs(original_diff) for diff in perm_diffs])\n",
    "\n",
    "print(f\"Original R^2 Difference: {original_diff}\")\n",
    "print(f\"P-value from permutation test: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generate Comparison of Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame with the data\n",
    "data = {\n",
    "    'Imaging Method': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '14'],\n",
    "    'R-squared': [0.187, 0.535, 0.097, 0.163, 0.058, .039, .475, .627, 0.108, 0.146,  0.391, 0.156, 0.148]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_palette(\"tab10\")\n",
    "sns.barplot(x='R-squared', y='Imaging Method', data=df)\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xlim(0, 1)\n",
    "plt.title('R-squared Values of Questio by Surface Method')\n",
    "plt.xlabel('R-squared')\n",
    "plt.ylabel('ADAS-Cog Question')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Barplot Of Methods Within Groups\n",
    "- The groups to compare within are the top level keys of the dictionary. \n",
    "- The groups to compare across are the second-level keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Barplot Of Methods Within Groups\n",
    "- The groups to compare within are the top level keys of the dictionary. \n",
    "- The groups to compare across are the second-level keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Question 1': {'CSF': 0.126, 'Surface': 0.187, 'GM': 0.161},\n",
    "    'Question 2': {'CSF': 0.547, 'Surface': 0.535, 'GM': 0.075},\n",
    "    'Question 3': {'CSF': 0.297, 'Surface': 0.097, 'GM': 0.451},\n",
    "    'Question 4': {'CSF': 0.268, 'Surface': 0.163, 'GM': 0.260},\n",
    "    'Question 5': {'CSF': 0.120, 'Surface': 0.058, 'GM': 0.351},\n",
    "    'Question 6': {'CSF': 0.433, 'Surface': 0.039, 'GM': 0.140},\n",
    "    'Question 7': {'CSF': 0.840, 'Surface': 0.475, 'GM': 0.383},\n",
    "    'Question 8': {'CSF': 0.540, 'Surface': 0.627, 'GM': 0.349},\n",
    "    'Question 9': {'CSF': 0.190, 'Surface': 0.108, 'GM': 0.377},\n",
    "    'Question 10': {'CSF': 0.151, 'Surface': 0.146, 'GM': 0.475},\n",
    "    'Question 11': {'CSF': 0.596, 'Surface': 0.391, 'GM': 0.407},\n",
    "    'Question 12': {'CSF': 0.151, 'Surface': 0.156, 'GM': 0.475},\n",
    "    'Question 14': {'CSF': 0.479, 'Surface': 0.148, 'GM': 0.191},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Flattening the nested dictionary and creating a DataFrame\n",
    "# flat_data = [(question, method, score) for question, methods in data.items() for method, score in methods.items()]\n",
    "# df = pd.DataFrame(flat_data, columns=['Question', 'Imaging Method', 'R-squared'])\n",
    "\n",
    "# # Create the bar plot\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.set_palette(\"tab10\")\n",
    "# sns.barplot(x='Question', y='R-squared', hue='Imaging Method', data=df)\n",
    "\n",
    "# # Customizing the plot\n",
    "# plt.ylim(0, 1)\n",
    "# plt.title('R-squared Values by Question and Imaging Method')\n",
    "# plt.xlabel('Question')\n",
    "# plt.ylabel('R-squared')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n",
    "\n",
    "# Flattening the nested dictionary and creating a DataFrame\n",
    "flat_data = [(question, method, score) for question, methods in data.items() for method, score in methods.items()]\n",
    "df = pd.DataFrame(flat_data, columns=['Question', 'Imaging Method', 'R-squared'])\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_palette(\"tab10\")\n",
    "sns.barplot(x='R-squared', y='Question', hue='Imaging Method', data=df)\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xlim(0, 1)\n",
    "plt.title('R-squared Values by Question and Imaging Method')\n",
    "plt.ylabel('Question')\n",
    "plt.xlabel('R-squared')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2=pd.DataFrame(data)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.average(df_2.iloc[0, :]), \n",
    "np.std(df_2.iloc[0, :]),\n",
    "\n",
    "\n",
    "np.average(df_2.iloc[1, :]),\n",
    "np.std(df_2.iloc[1, :]),\n",
    "\n",
    "np.average(df_2.iloc[2, :]),\n",
    "np.std(df_2.iloc[2, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula\n",
    "data_df['Cognitive_Baseline'] = data_df['Cognitive_Baseline'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Results: Ordinary least squares\n",
      "=======================================================================================================\n",
      "Model:                       OLS                                   Adj. R-squared:             0.019   \n",
      "Dependent Variable:          Percent_Cognitive_Improvement         AIC:                        134.4710\n",
      "Date:                        2024-02-23 15:22                      BIC:                        141.7855\n",
      "No. Observations:            46                                    Log-Likelihood:             -63.235 \n",
      "Df Model:                    3                                     F-statistic:                1.296   \n",
      "Df Residuals:                42                                    Prob (F-statistic):         0.288   \n",
      "R-squared:                   0.085                                 Scale:                      1.0025  \n",
      "-------------------------------------------------------------------------------------------------------\n",
      "                                                          Coef.  Std.Err.    t    P>|t|   [0.025 0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Intercept                                                -0.0012   0.1491 -0.0080 0.9937 -0.3020 0.2996\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_                     0.1660   0.1554  1.0677 0.2917 -0.1477 0.4796\n",
      "Cognitive_Baseline                                        0.2205   0.1495  1.4752 0.1476 -0.0812 0.5222\n",
      "Z_Scored_Subiculum_T_By_Origin_Group_:Cognitive_Baseline  0.0089   0.1544  0.0574 0.9545 -0.3028 0.3205\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Omnibus:                          12.631                    Durbin-Watson:                       2.128 \n",
      "Prob(Omnibus):                    0.002                     Jarque-Bera (JB):                    14.199\n",
      "Skew:                             -0.990                    Prob(JB):                            0.001 \n",
      "Kurtosis:                         4.867                     Condition No.:                       1     \n",
      "=======================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Assuming 'formula' is a string like 'y ~ x1 + x2' and data_df is your DataFrame\n",
    "result = smf.ols(formula=formula, data=data_df).fit()\n",
    "\n",
    "# To view the regression results summary\n",
    "print(result.summary2())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy.\n",
    "\n",
    "-- Calvin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
