{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will take your regression or classification problem, the y variables (dependent) and x variables (independent) and determine which have what level of contribution to your dataset.\n",
    "## This will allow you to visualize the information context of Xn to Y, and decide what to keep in future analyses, such as development of lienar regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import glob as glob\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "#Calculate Correlation\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = 'ols_linear_regression'\n",
    "spreadsheet_dir = r'/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list_proper_subjects.csv'\n",
    "out_dir = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/analyses'\n",
    "save = True\n",
    "if os.path.exists(out_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pre-prepared Regression Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name = 'study_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.dataframe_utilities import preprocess_colnames_for_regression\n",
    "if os.path.basename(spreadsheet_dir).split('.')[1] == 'csv':\n",
    "    data_df = pd.read_csv(spreadsheet_dir)\n",
    "else:\n",
    "    data_df = pd.read_excel(spreadsheet_dir, sheet_name=sheet_name)\n",
    "data_df = preprocess_colnames_for_regression(data_df.reset_index(drop=True))\n",
    "display(data_df)\n",
    "# display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_index=[11, 47, 48, 49]\n",
    "# data_df = data_df.drop(index=outlier_index)\n",
    "# data_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the outcome variable\n",
    "# outcome_variable =  data_df.pop('%_Change_from_baseline_(ADAS_Cog11)')\n",
    "# data_df['outcome'] = outcome_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encode Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df['Cognitive Status'] = np.where(data_df['Cognitive Status'] == 'MCI', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle NANs\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- set the string of column_to_drop_from if you would only like to remove rows with NaNs in a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_nans = True\n",
    "column_to_drop_from = 'Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_nans:\n",
    "    if column_to_drop_from == None:\n",
    "        data_df.dropna(inplace=True)\n",
    "    else:\n",
    "        data_df.dropna(subset=[column_to_drop_from], inplace=True)\n",
    "# data_df = data_df.fillna(method='pad')\n",
    "data_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate DF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_split_by = 'Cohort'\n",
    "value_to_split_by = 1\n",
    "concatenate_split=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index for alignment\n",
    "data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split the Dataframes by the Row Value in the Column to Split By\n",
    "filtered_df = data_df[data_df[column_to_split_by] == value_to_split_by]\n",
    "dropped_df = data_df[data_df[column_to_split_by] != value_to_split_by]\n",
    "\n",
    "if concatenate_split:\n",
    "    # Reset the index again for the new dataframes\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    dropped_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Rename columns for the dropped dataframe\n",
    "    dropped_df.columns = [str(col) + '_2' for col in dropped_df.columns]\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    result_df = pd.concat([filtered_df, dropped_df], axis=1)\n",
    "\n",
    "    # Finalize\n",
    "    data_df = result_df\n",
    "else:\n",
    "    data_df=filtered_df\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Rows by Value in a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column = 'Age'\n",
    "# value = 65\n",
    "# drop_greater_or_equal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if drop_greater_or_equal:\n",
    "#     data_df = data_df[data_df[column] <= value]\n",
    "# else:\n",
    "#     data_df = data_df[data_df[column] >= value]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tailor the Dataframe\n",
    "# data_df = data_df.loc[:, ['Age','Mesial_Temporal_Grade','Subiculum_Connectivity', 'outcome']]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter Columns you Don't want to standardize into a lsit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Cohort']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "import numpy as np\n",
    "\n",
    "preserved_df = data_df.copy()\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# Select the columns to be standardized\n",
    "for col in data_df.columns:\n",
    "    if col not in cols_not_to_standardize:\n",
    "        try:\n",
    "            data_df[col] = (data_df[col] - np.mean(data_df[col])) / np.std(data_df[col])\n",
    "            # scaler.fit_transform(data_df[col])\n",
    "        # cols_to_standardize = [col for col in data_df.columns if col not in cols_not_to_standardize]\n",
    "        except:\n",
    "            print('Unable to standardize column.')\n",
    "    \n",
    "# Standardize\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over = data_df['basline'] >= np.mean(data_df['basline'])\n",
    "# under = data_df['basline'] < np.mean(data_df['basline'])\n",
    "# data_df['basline'][over] = 1\n",
    "# data_df['basline'][under] = 0\n",
    "\n",
    "# display(data_df)\n",
    "# print(np.max(data_df.age))\n",
    "mc_test = data_df.copy()\n",
    "# mc_test.pop('outcome')\n",
    "# mc_test = mc_test.loc[:, ['Ventral_Attention', 'Limbic']]\n",
    "# mc_test['interaction'] = mc_test['Limbic']*mc_test['Ventral_Attention']\n",
    "from calvin_utils.statistical_utils.statistical_measurements import calculate_vif\n",
    "calculate_vif(mc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET INFORMATION ABOUT THE DATA\n",
    "# sns.pairplot(data_df)\n",
    "data_df.describe().transpose()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Statsmodel Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "formula = '__Change_from_baseline__ADAS_Cog11_~Age * Memory_Network_R'\n",
    "#----------------------------------------------------------------DO NOT TOUCH\n",
    "results = smf.ols(formula, data=data_df).fit()\n",
    "print(results.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.predict(data_df)\n",
    "data_df['__Change_from_baseline__ADAS_Cog11_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Diagnostics on the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import model_diagnostics\n",
    "model_diagnostics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation - LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def leave_one_out_cv(data_df: pd.DataFrame, formula: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs Leave-One-Out Cross Validation (LOOCV) on the provided data using the specified formula.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_df: A pandas DataFrame containing the data.\n",
    "    - formula: A string in the format 'response ~ predictors' specifying the regression formula.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing:\n",
    "        * Average Root Mean Squared Error from LOOCV\n",
    "        * Pearson Correlation Coefficient between actual and predicted values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure formula is correctly formatted\n",
    "    formula_parts = formula.split(\"~\")\n",
    "    if len(formula_parts) != 2:\n",
    "        raise ValueError(\"Formula should be in the format 'response ~ predictors'.\")\n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(data_df)\n",
    "\n",
    "    models = []\n",
    "    squared_error_list = []\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "\n",
    "    for train_index, test_index in loo.split(data_df):\n",
    "        train_data, test_data = data_df.iloc[train_index], data_df.iloc[test_index]\n",
    "\n",
    "        model = smf.ols(f'{formula_parts[0]} ~ {formula_parts[1]}', data=train_data).fit()\n",
    "        models.append(model)\n",
    "\n",
    "        test_x = test_data.drop(columns=[formula_parts[0]])\n",
    "        test_y = test_data[formula_parts[0]]\n",
    "        pred_y = model.predict(test_x)\n",
    "\n",
    "        squared_error = np.square(pred_y - test_y)\n",
    "        squared_error_list.extend(squared_error)\n",
    "\n",
    "        predictions.extend(pred_y)\n",
    "        actual_values.extend(test_y)\n",
    "\n",
    "    average_squared_error = np.mean(squared_error_list)\n",
    "    average_rmse = np.sqrt(average_squared_error)\n",
    "    correlation, _ = pearsonr(predictions, actual_values)\n",
    "\n",
    "    return average_rmse, correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse, pear_corr = leave_one_out_cv(data_df, results.model.formula)\n",
    "print('LOOCV Metrics')\n",
    "print('RMSE is : ', rmse)\n",
    "print('Pearson R is : ', pear_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation - K Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "class KFoldsCrossValidation:\n",
    "    \"\"\"\n",
    "    This class performs k-folds cross-validation on a given DataFrame and regression formula.\n",
    "    It calculates both in-sample and out-of-sample R-squared values as well as the RMSE (Root Mean Squared Error).\n",
    "    \n",
    "    Parameters:\n",
    "    - data_df (pd.DataFrame): The data as a Pandas DataFrame.\n",
    "    - formula (str): The regression formula in the format 'response ~ predictors'.\n",
    "    - n_splits (int): The number of folds for k-folds cross-validation. Default is 5.\n",
    "    \n",
    "    Methods:\n",
    "    - run: Performs the k-folds cross-validation and returns metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_df, formula, n_splits=5):\n",
    "        \"\"\"\n",
    "        Initialize the KFoldsCrossValidation class with data, formula, and number of splits.\n",
    "        \"\"\"\n",
    "        self.data_df = data_df\n",
    "        self.formula = formula\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run k-folds cross-validation and calculate metrics.\n",
    "        \n",
    "        Returns:\n",
    "        - A tuple containing:\n",
    "            * List of out-of-sample R-squared values for each fold\n",
    "            * List of in-sample R-squared values for each fold\n",
    "            * List of RMSE values for each fold\n",
    "            * Average out-of-sample R-squared value\n",
    "            * Average in-sample R-squared value\n",
    "            * Average RMSE value\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=1)\n",
    "        \n",
    "        out_of_sample_r_squared_values = []\n",
    "        in_sample_r_squared_values = []\n",
    "        rmse_values = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(self.data_df):\n",
    "            train, test = self.data_df.iloc[train_index], self.data_df.iloc[test_index]\n",
    "            \n",
    "            # Fit the model\n",
    "            results = smf.ols(self.formula, data=train).fit()\n",
    "            \n",
    "            # Calculate in-sample R-squared\n",
    "            in_sample_r_squared = results.rsquared\n",
    "            in_sample_r_squared_values.append(in_sample_r_squared)\n",
    "            \n",
    "            # Calculate predictions for the test set\n",
    "            predictions = results.predict(test)\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(mean_squared_error(test['outcome'], predictions))\n",
    "            rmse_values.append(rmse)\n",
    "            \n",
    "            # Calculate out-of-sample R-squared for the test set\n",
    "            sse = np.sum((test['outcome'] - predictions) ** 2)\n",
    "            tss = np.sum((test['outcome'] - np.mean(test['outcome'])) ** 2)\n",
    "            out_of_sample_r_squared = 1 - (sse / tss)\n",
    "            out_of_sample_r_squared_values.append(out_of_sample_r_squared)\n",
    "            \n",
    "        # Calculate the average out-of-sample and in-sample R-squared and RMSE\n",
    "        avg_out_of_sample_r_squared = np.mean(out_of_sample_r_squared_values)\n",
    "        avg_in_sample_r_squared = np.mean(in_sample_r_squared_values)\n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "        \n",
    "        return out_of_sample_r_squared_values, in_sample_r_squared_values, rmse_values, avg_out_of_sample_r_squared, avg_in_sample_r_squared, avg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KFoldsCrossValidation class and run the k-folds cross-validation\n",
    "kf_cv = KFoldsCrossValidation(data_df, formula, n_splits=35)\n",
    "out_of_sample_r_squared_values, in_sample_r_squared_values, rmse_values, avg_out_of_sample_r_squared, avg_in_sample_r_squared, avg_rmse = kf_cv.run()\n",
    "\n",
    "print('Folded cross-validation out-of-sample r-squared: ', avg_out_of_sample_r_squared)\n",
    "print('Folded cross-validation in-sample r-squared: ', avg_in_sample_r_squared)\n",
    "print('Folded cross-validation RMSE: ', avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Elimination of DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RegressionFormulaGeneratorUpdated:\n",
    "    def __init__(self, base_formula):\n",
    "        self.base_formula = base_formula\n",
    "        self.base_predictors = self.extract_base_predictors()\n",
    "        self.all_formulas = []\n",
    "\n",
    "    def extract_base_predictors(self):\n",
    "        return self.base_formula.split(\"~\")[1].split(\"*\")\n",
    "\n",
    "    def generate_lower_level_interactions(self, predictors):\n",
    "        lower_level_interactions = []\n",
    "        for r in range(2, len(predictors) + 1):\n",
    "            for subset in combinations(predictors, r):\n",
    "                lower_level_interactions.append(\":\".join(subset))\n",
    "        return lower_level_interactions\n",
    "\n",
    "    def is_valid_formula(self, terms):\n",
    "        interactions = [term for term in terms if \":\" in term]\n",
    "        individual_components = [term for term in terms if \":\" not in term]\n",
    "        \n",
    "        # Ensure all individual components of each interaction are present\n",
    "        for interaction in interactions:\n",
    "            components = interaction.split(\":\")\n",
    "            if not all(comp in individual_components for comp in components):\n",
    "                return False\n",
    "\n",
    "        # Ensure all lower-level interactions of each higher-level interaction are present\n",
    "        for interaction in interactions:\n",
    "            components = interaction.split(\":\")\n",
    "            if len(components) > 2:\n",
    "                lower_level_combinations = self.generate_lower_level_interactions(components)\n",
    "                if not all(lower in interactions for lower in lower_level_combinations):\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def generate_all_formulas(self):\n",
    "        predictors = self.base_predictors\n",
    "        lower_level_interactions = self.generate_lower_level_interactions(predictors)\n",
    "\n",
    "        # Generate power set of all possible terms (base predictors + interactions)\n",
    "        for r in range(1, len(predictors) + len(lower_level_interactions) + 1):\n",
    "            for subset in combinations(predictors + lower_level_interactions, r):\n",
    "                if self.is_valid_formula(subset):\n",
    "                    formula = f\"outcome~{'+'.join(subset)}\"\n",
    "                    self.all_formulas.append(formula)\n",
    "\n",
    "    def get_all_formulas(self):\n",
    "        if not self.all_formulas:\n",
    "            self.generate_all_formulas()\n",
    "        return self.all_formulas\n",
    "\n",
    "class RegressionEvaluator:\n",
    "    def __init__(self, data_df: pd.DataFrame, formulas_df: pd.DataFrame):\n",
    "        self.data_df = data_df\n",
    "        self.formulas_df = formulas_df\n",
    "        self.results_df = pd.DataFrame()\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        r_squared_list = []\n",
    "        loocv_rmse_list = []\n",
    "        loocv_r_list = []\n",
    "\n",
    "        for index, row in tqdm(self.formulas_df.iterrows()):\n",
    "            formula = row['formula']\n",
    "\n",
    "            # Fit the model and get R-squared\n",
    "            results = smf.ols(f\"{formula}\", data=self.data_df).fit()\n",
    "            r_squared = results.rsquared\n",
    "\n",
    "            # Perform LOOCV\n",
    "            loocv_rmse, loocv_r = leave_one_out_cv(self.data_df, formula)\n",
    "\n",
    "            r_squared_list.append(r_squared)\n",
    "            loocv_rmse_list.append(loocv_rmse)\n",
    "            loocv_r_list.append(loocv_r)\n",
    "\n",
    "        self.results_df['formula'] = self.formulas_df['formula']\n",
    "        self.results_df['r_squared'] = r_squared_list\n",
    "        self.results_df['loocv_rmse'] = loocv_rmse_list\n",
    "        self.results_df['loocv_r'] = loocv_r_list\n",
    "        \n",
    "    def return_results(self):\n",
    "        sorted_results_df = self.results_df.sort_values(by='loocv_rmse', ascending=True)\n",
    "        return sorted_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify All Possible Backward Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all valid regression formulas using the updated class\n",
    "generator_updated = RegressionFormulaGeneratorUpdated(formula)\n",
    "all_formulas_updated = generator_updated.get_all_formulas()\n",
    "\n",
    "# Create a new Pandas DataFrame to store the updated formulas\n",
    "df_formulas_updated = pd.DataFrame(all_formulas_updated, columns=['formula'])\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "df_formulas_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Backward Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminator = RegressionEvaluator(data_df, df_formulas_updated)\n",
    "eliminator.evaluate_models()\n",
    "results_df = eliminator.return_results()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structural Equation Modelling - Structural Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "def structural_coefficients(model: statsmodels.regression.linear_model.RegressionResultsWrapper, \n",
    "                            data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function calculates the structural coefficients of a linear regression model, along with their \n",
    "    associated model coefficients (beta weights). It also computes a 'suppressor index' which indicates \n",
    "    the likelihood of a variable being a suppressor variable (high beta weight, near-zero structural coefficient).\n",
    "    A suppressor index over 10 is a good heuristic for identifying a suppressor variable\n",
    "    \n",
    "    Parameters:\n",
    "    model (statsmodels.regression.linear_model.RegressionResultsWrapper): The fitted linear regression model.\n",
    "    data (pd.DataFrame): The dataset used in the model.\n",
    "    \n",
    "    Returns:\n",
    "    structural_coefs_df (pd.DataFrame): A dataframe containing the predictors, their structural coefficients,\n",
    "                                        model coefficients (beta weights), and suppressor index.\n",
    "                                        \n",
    "                                        If the sum of the structure coefficients is higher than 1, they are correlated (multicollinear)\n",
    "    \"\"\"\n",
    "    # Calculating the predicted values\n",
    "    y_predicted = model.predict(data)\n",
    "\n",
    "    # Creating a temporary dataframe to store interaction terms\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    # Calculating the structural coefficients\n",
    "    structural_coefs = {}\n",
    "    for pred in model.params.index:\n",
    "        if pred == 'Intercept':\n",
    "            continue\n",
    "\n",
    "        # Check if the predictor is an interaction term\n",
    "        if ':' in pred:\n",
    "            # Split the interaction term into its components\n",
    "            components = pred.split(':')\n",
    "            # Multiply the components and store the result in the temporary dataframe\n",
    "            temp_product = data[components[0]]\n",
    "            for component in components[1:]:\n",
    "                temp_product *= data[component]\n",
    "            temp_df[pred] = temp_product\n",
    "        else:\n",
    "            temp_df[pred] = data[pred]\n",
    "        \n",
    "        coef, _ = pearsonr(temp_df[pred], y_predicted)\n",
    "        structural_coefs[pred] = np.square(coef)\n",
    "\n",
    "    # Calculating the model coefficients\n",
    "    model_coefs = model.params.drop('Intercept')\n",
    "\n",
    "    # Creating a dataframe to store the results\n",
    "    structural_coefs_df = pd.DataFrame(list(zip(structural_coefs.keys(), structural_coefs.values(), model_coefs.values)), \n",
    "                                       columns=['predictor', 'structural_coefficient', 'model_coefficient'])\n",
    "\n",
    "    # Adding suppressor index column\n",
    "    structural_coefs_df['suppressor_index'] = structural_coefs_df['model_coefficient'].abs() / structural_coefs_df['structural_coefficient']\n",
    "\n",
    "    # Sorting the dataframe by the structural coefficients in descending order\n",
    "    structural_coefs_df.sort_values(by='structural_coefficient', ascending=False, inplace=True)\n",
    "    structural_coefs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return structural_coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_structural_coefs_df = structural_coefficients(results, data_df.copy())\n",
    "squared_structural_coefs_df.to_csv(os.path.join(out_dir, 'structural_coefficient_analysis.csv'))\n",
    "print('saved to: ', os.path.join(out_dir, 'structural_coefficient_analysis.csv'))\n",
    "display(squared_structural_coefs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sqrt, mean\n",
    "\n",
    "# Calculate the squared errors\n",
    "squared_errors = (data_df['percent_change_adascog11'] - results.fittedvalues) ** 2\n",
    "\n",
    "# Calculate the mean of the squared errors\n",
    "mse = mean(squared_errors)\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "print(\"Root Mean Squared Error:\", rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Two Lienar Regressions Using F-Test (ANOVA_LM In Statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.api import anova_lm\n",
    "smaller_formula = 'Percent_Cognitive_Improvement ~ Age + Subiculum_Connectivity'\n",
    "\n",
    "larger_formula = 'Percent_Cognitive_Improvement ~ Age * Subiculum_Connectivity'\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------DO NOT TOUCH!----------------------------------------------------------------\n",
    "table1 = anova_lm(smf.ols(smaller_formula, data=data_df).fit(), smf.ols(larger_formula, data=data_df).fit())\n",
    "print(table1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use 2D Interaction Plot to Visualize Interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Interaciton Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Variable Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def save_fig(fig, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    fig.savefig(os.path.join(out_dir, 'create_2D_interaction_plot.png'))\n",
    "    fig.savefig(os.path.join(out_dir, 'create_2D_interaction_plot.svg'))\n",
    "    print('Saved to: ', os.path.join(out_dir, 'create_2D_interaction_plot.svg'))\n",
    "\n",
    "def permutation_test_for_interaction(data_df: pd.DataFrame, \n",
    "                                     x_one_col: str, \n",
    "                                     x_two_col: str, \n",
    "                                     outcome_col: str, \n",
    "                                     results: GLMResults, \n",
    "                                     n_permutations: int = 10000) -> float:\n",
    "    \"\"\"\n",
    "    Performs a permutation test to assess the significance of the interaction effect.\n",
    "    \n",
    "    Returns the p-value.\n",
    "    \"\"\"\n",
    "    # Generate predictions for observed data\n",
    "    x_two_mean = data_df[x_two_col].mean()\n",
    "    x_two_std = data_df[x_two_col].std()\n",
    "    x_two_minus_2sd = x_two_mean - 2 * x_two_std\n",
    "    x_two_plus_2sd = x_two_mean + 2 * x_two_std\n",
    "    \n",
    "    X_pred_minus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * x_two_minus_2sd\n",
    "    })\n",
    "    X_pred_plus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * x_two_plus_2sd\n",
    "    })\n",
    "    \n",
    "    y_pred_minus_2sd = results.predict(X_pred_minus_2sd)\n",
    "    y_pred_plus_2sd = results.predict(X_pred_plus_2sd)\n",
    "    \n",
    "    observed_area_between_lines = np.abs(y_pred_minus_2sd - y_pred_plus_2sd).sum()\n",
    "    \n",
    "    # Initialize array to store areas from permutations\n",
    "    permuted_areas = np.zeros(n_permutations)\n",
    "    \n",
    "    # Perform permutations\n",
    "    for i in range(n_permutations):\n",
    "        # Permute the predictor variables\n",
    "        permuted_x_one = np.random.permutation(data_df[x_one_col])\n",
    "        permuted_x_two = np.random.permutation(data_df[x_two_col])\n",
    "        \n",
    "        # Recalculate mean and standard deviation for permuted x_two\n",
    "        x_two_mean_permuted = permuted_x_two.mean()\n",
    "        x_two_std_permuted = permuted_x_two.std()\n",
    "        x_two_minus_2sd_permuted = x_two_mean_permuted - 2 * x_two_std_permuted\n",
    "        x_two_plus_2sd_permuted = x_two_mean_permuted + 2 * x_two_std_permuted\n",
    "        \n",
    "        # Generate predictions for permuted data\n",
    "        X_pred_minus_2sd_permuted = pd.DataFrame({\n",
    "            x_one_col: permuted_x_one,\n",
    "            x_two_col: np.ones_like(permuted_x_one) * x_two_minus_2sd_permuted\n",
    "        })\n",
    "        X_pred_plus_2sd_permuted = pd.DataFrame({\n",
    "            x_one_col: permuted_x_one,\n",
    "            x_two_col: np.ones_like(permuted_x_one) * x_two_plus_2sd_permuted\n",
    "        })\n",
    "        \n",
    "        y_pred_minus_2sd_permuted = results.predict(X_pred_minus_2sd_permuted)\n",
    "        y_pred_plus_2sd_permuted = results.predict(X_pred_plus_2sd_permuted)\n",
    "        \n",
    "        permuted_area = np.abs(y_pred_minus_2sd_permuted - y_pred_plus_2sd_permuted).sum()\n",
    "        \n",
    "        # Store the permuted area\n",
    "        permuted_areas[i] = permuted_area\n",
    "    \n",
    "    # Calculate p-value\n",
    "    p_value = np.mean(permuted_areas >= observed_area_between_lines)\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "# Modifying the function to include the visual updates\n",
    "from typing import List, Optional\n",
    "\n",
    "def create_2D_interaction_plot(data_df: pd.DataFrame, \n",
    "                               x_one: Dict[str, str], \n",
    "                               x_two: Dict[str, str], \n",
    "                               outcome: Dict[str, str], \n",
    "                               results: GLMResults, \n",
    "                               legend_labels: Optional[List[str]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Creates a 2D interaction plot visualizing the effect of x_one on the outcome at two levels of x_two.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pd.DataFrame\n",
    "        Dataframe containing the predictor variables and outcome.\n",
    "    x_one : Dict[str, str]\n",
    "        Dictionary with label as key and column name of the first predictor variable in the dataframe as value.\n",
    "    x_two : Dict[str, str]\n",
    "        Dictionary with label as key and column name of the second predictor variable in the dataframe as value.\n",
    "    outcome : Dict[str, str]\n",
    "        Dictionary with label as key and column name of the outcome variable in the dataframe as value.\n",
    "    results : GLMResults\n",
    "        Fitted model used for making predictions.\n",
    "    legend_labels : List[str], optional\n",
    "        Labels to be used in the legend. Default is None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract column names from the dictionaries\n",
    "    x_one_label, x_one_col = list(x_one.keys())[0], list(x_one.values())[0]\n",
    "    x_two_label, x_two_col = list(x_two.keys())[0], list(x_two.values())[0]\n",
    "    outcome_label, outcome_col = list(outcome.keys())[0], list(outcome.values())[0]\n",
    "    \n",
    "    # Calculate p-value using permutation test\n",
    "    p_value = permutation_test_for_interaction(data_df, x_one_col, x_two_col, outcome_col, results)\n",
    "     \n",
    "    # Calculate mean and standard deviation for x_two\n",
    "    x_two_mean = data_df[x_two_col].mean()\n",
    "    x_two_std = data_df[x_two_col].std()\n",
    "    \n",
    "    # Create arrays for x_two at -2 and +2 standard deviations from the mean\n",
    "    x_two_minus_2sd = x_two_mean - 2 * x_two_std\n",
    "    x_two_plus_2sd = x_two_mean + 2 * x_two_std\n",
    "    \n",
    "    # Create DataFrames for prediction\n",
    "    X_pred_minus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * x_two_minus_2sd\n",
    "    })\n",
    "    \n",
    "    X_pred_plus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * x_two_plus_2sd\n",
    "    })\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred_minus_2sd = results.predict(X_pred_minus_2sd)\n",
    "    y_pred_plus_2sd = results.predict(X_pred_plus_2sd)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Generate predictions (this part could be refactored)\n",
    "    X_pred_minus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * (data_df[x_two_col].mean() - 2 * data_df[x_two_col].std())\n",
    "    })\n",
    "    X_pred_plus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * (data_df[x_two_col].mean() + 2 * data_df[x_two_col].std())\n",
    "    })\n",
    "    y_pred_minus_2sd = results.predict(X_pred_minus_2sd)\n",
    "    y_pred_plus_2sd = results.predict(X_pred_plus_2sd)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.plot(data_df[x_one_col], y_pred_minus_2sd, label=legend_labels[0] if legend_labels else f\"{x_two_label} at -2 SD\", color='blue')\n",
    "    plt.plot(data_df[x_one_col], y_pred_plus_2sd, label=legend_labels[1] if legend_labels else f\"{x_two_label} at +2 SD\", color='red')\n",
    "    \n",
    "    plt.xlabel(x_one_label)\n",
    "    plt.ylabel(outcome_label)\n",
    "    legend = plt.legend(frameon=False)\n",
    "    \n",
    "    # Add p-value to the title\n",
    "    plt.title(f\"p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Despine the plot\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "# The function can be used like this:\n",
    "# p_value = permutation_test_for_interaction(data_df, {'X1 Label': 'x1_col'}, {'X2 Label': 'x2_col'}, {'Outcome Label': 'outcome_col'}, results)\n",
    "# Then, the p-value can be set as the title in the plot function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_2D_interaction_plot(data_df.copy(), \n",
    "                                 x_one={'Subiculum Connectivity': 'Subiculum_Connectivity'}, \n",
    "                                 x_two={'Age': 'Age'}, \n",
    "                                 outcome={'Percent Improvement (MDRS)': 'outcome'}, \n",
    "                                 results=results,\n",
    "                                 legend_labels=['Young', 'Old'])\n",
    "save_fig(fig, out_dir)\n",
    "print('Saved to: ', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-Driven Split Interaction Plot\n",
    "\n",
    "\n",
    "This code is designed to create an interaction plot to visualize the effects of two factors and their interaction on the outcome variable.\n",
    "\n",
    "The interaction_plot function takes as input a dataframe, two factors (x_one and x_two), two corresponding labels for the conditions when the values of these factors are under the mean (x_one_under_mean and x_two_under_mean) and over the mean (x_one_over_mean and x_two_over_mean), and the response variable (outcome). If binarize is set to True, it converts the two factors into binary variables based on whether their values are above or below the mean. The function then creates a mapping for the x_two variable to numerical values for the purpose of plotting.\n",
    "\n",
    "It uses the interaction_plot function from the statsmodels package to create the plot. In the plot, x_two is represented on the x-axis, x_one is used to color the lines, and the outcome variable is plotted on the y-axis. The function also sets the labels for the x and y axes and the tick labels on the x-axis according to the inputs provided.\n",
    "\n",
    "The function also allows for saving the plot to an output directory specified by the user. If save is set to True, it saves the plot in both PNG and SVG formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the function to incorporate the changes\n",
    "from statsmodels.graphics.api import interaction_plot\n",
    "\n",
    "def two_dimensional_interaction_plot_v4(data_df, \n",
    "                    x_one, x_one_under_mean, x_one_over_mean, x_one_split_point,\n",
    "                    x_two, x_two_under_mean, x_two_over_mean, x_two_split_point,\n",
    "                    response, \n",
    "                    binarize=True, plot_error_bars=True,\n",
    "                    x_label='Subiculum Connectivity', y_label='Percent Improvement (MDRS)',\n",
    "                    save=False, out_dir=None):\n",
    "    \"\"\"\n",
    "    Function to create an interaction plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pandas.DataFrame\n",
    "        The dataframe containing the data.\n",
    "    x_one, x_two : str\n",
    "        Column names of the two factors.\n",
    "    x_one_under_mean, x_two_under_mean : str\n",
    "        Labels to be used when the values of x_one and x_two are under the mean, respectively.\n",
    "    x_one_over_mean, x_two_over_mean : str\n",
    "        Labels to be used when the values of x_one and x_two are over the mean, respectively.\n",
    "    x_one_split_point, x_two_split_point: int | None\n",
    "        Value to split the data of x by. If None, then x will be split by mean\n",
    "    response : str\n",
    "        Column name of the outcome variable.\n",
    "    binarize : bool, optional\n",
    "        Whether to convert x_one and x_two into binary variables.\n",
    "    plot_error_bars : bool, optional\n",
    "        Whether to plot error bars representing SEM.\n",
    "    x_label, y_label : str, optional\n",
    "        Labels for the x-axis and y-axis.\n",
    "    save : bool, optional\n",
    "        Whether to save the plot.\n",
    "    out_dir : str, optional\n",
    "        Directory where the plot will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The original dataframe with modified x_one and x_two if binarize is True.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Binarize x_two variable\n",
    "    if binarize:\n",
    "        if x_two_split_point is not None:\n",
    "            data_df[x_two] = np.where(data_df[x_two] <= x_two_split_point, f'{x_two_under_mean}', f'{x_two_over_mean}')\n",
    "        else:\n",
    "            data_df[x_two] = np.where(data_df[x_two] <= data_df[x_two].mean(), f'{x_two_under_mean}', f'{x_two_over_mean}')\n",
    "    \n",
    "    # Binarize x_one variable\n",
    "    if binarize:\n",
    "        if x_one_split_point is not None:\n",
    "            data_df[x_one] = np.where(data_df[x_one] <= x_one_split_point, f'{x_one_under_mean}', f'{x_one_over_mean}')\n",
    "        else:\n",
    "            data_df[x_one] = np.where(data_df[x_one] <= data_df[x_one].mean(), f'{x_one_under_mean}', f'{x_one_over_mean}')\n",
    "    \n",
    "    # Map the x_two categories to numbers for plotting\n",
    "    mapping = {x_two_under_mean: 0, x_two_over_mean: 1}\n",
    "    data_df[x_two + '_mapped'] = data_df[x_two].map(mapping)\n",
    "    \n",
    "    # Extracting means and SEM for the binarized groups\n",
    "    means = data_df.groupby([x_one, x_two + '_mapped'])[response].mean()\n",
    "    sem = data_df.groupby([x_one, x_two + '_mapped'])[response].sem()\n",
    "\n",
    "    # Plotting the interaction plot\n",
    "    fig, ax = plt.subplots()\n",
    "    colors_dict = {f'{x_one_under_mean}': 'Blue', f'{x_one_over_mean}': 'Red'}\n",
    "\n",
    "    for group, color in colors_dict.items():\n",
    "        group_data = [(0 if combo[1] == 0 else 1, means[combo], sem[combo]) \n",
    "                      for combo in means.index if combo[0] == group]\n",
    "        group_data.sort(key=lambda x: x[0])  # Sort by x-value for plotting\n",
    "        \n",
    "        x_vals = [item[0] for item in group_data]\n",
    "        y_vals = [item[1] for item in group_data]\n",
    "        y_errs = [item[2] for item in group_data] if plot_error_bars else None\n",
    "        \n",
    "        ax.errorbar(x_vals, y_vals, yerr=y_errs, color=color, fmt='-o', capsize=5, capthick=2, elinewidth=2, label=group, linestyle='-')\n",
    "\n",
    "    # Setting labels and other plot properties\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([x_two_under_mean, x_two_over_mean])\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "\n",
    "    # Displaying the plot\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function to display the interaction plot\n",
    "save = False\n",
    "#----------------------------------------------------------------\n",
    "interaction_figure = two_dimensional_interaction_plot_v4(data_df.copy(), \n",
    "                 x_one='Age', x_one_under_mean='Young', x_one_over_mean='Old', x_one_split_point=65.7,\n",
    "                 x_two='Subiculum_Connectivity', x_two_under_mean='Low Connectivity', x_two_over_mean='High Connectivity', x_two_split_point=61,\n",
    "                 response='outcome', \n",
    "                 x_label='Memory ROI Connectivity', \n",
    "                 y_label='Percent Improvement (ADAS-Cog11)',\n",
    "                 plot_error_bars=False)\n",
    "\n",
    "interaction_figure\n",
    "if save:\n",
    "    interaction_figure.savefig(os.path.join(out_dir, '2D_interaction_figure_pd.png'))\n",
    "    interaction_figure.savefig(os.path.join(out_dir, '2D_interaction_figure_pd.svg'))\n",
    "    print(f'saved to: {os.path.join(out_dir, \"2D_interaction_figure.png\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def permute_outcome_and_get_proportions_fixed(data_df, \n",
    "                                              x_one, x_one_under_mean, x_one_over_mean, x_one_split_point,\n",
    "                                              x_two, x_two_under_mean, x_two_over_mean, x_two_split_point,\n",
    "                                              response, binarize=True):\n",
    "    \"\"\"\n",
    "    Function to perform permutation testing on the outcome and compute the proportion of permuted means \n",
    "    greater than observed means for both x_one and x_two.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pandas.DataFrame\n",
    "        The dataframe containing the data.\n",
    "    x_one, x_two : str\n",
    "        Column names of the two factors.\n",
    "    x_one_under_mean, x_two_under_mean : str\n",
    "        Labels to be used when the values of x_one and x_two are under the mean, respectively.\n",
    "    x_one_over_mean, x_two_over_mean : str\n",
    "        Labels to be used when the values of x_one and x_two are over the mean, respectively.\n",
    "    x_one_split_point, x_two_split_point: int\n",
    "        Value to split the data of x by.\n",
    "    response : str\n",
    "        Column name of the outcome variable.\n",
    "    binarize : bool\n",
    "        Whether to convert x_one and x_two into binary variables.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Proportion of permuted means greater than observed mean for x_one and x_two.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy the data\n",
    "    df_copy = data_df.copy()\n",
    "    \n",
    "    # Binarize the variables to get observed values\n",
    "    if binarize:\n",
    "        if x_two_split_point is not None:\n",
    "            df_copy[x_two] = np.where(df_copy[x_two] <= x_two_split_point, 0, 1)\n",
    "        else:\n",
    "            df_copy[x_two] = np.where(df_copy[x_two] <= df_copy[x_two].mean(), 0, 1)\n",
    "        \n",
    "        if x_one_split_point is not None:\n",
    "            df_copy[x_one] = np.where(df_copy[x_one] <= x_one_split_point, 0, 1)\n",
    "        else:\n",
    "            df_copy[x_one] = np.where(df_copy[x_one] <= df_copy[x_one].mean(), 0, 1)\n",
    "    \n",
    "    #Observed Values between within a level of x_one across levels of x_two\n",
    "    delta_x_one_low = df_copy[response][(df_copy[x_one]==0) & (df_copy[x_two]==0)].mean() - df_copy[response][(df_copy[x_one]==0) & (df_copy[x_two]==1)].mean()\n",
    "    delta_x_one_high = df_copy[response][(df_copy[x_one]==1) & (df_copy[x_two]==0)].mean() - df_copy[response][(df_copy[x_one]==1) & (df_copy[x_two]==1)].mean()    \n",
    "    #Empiric Values\n",
    "    delta_x_one_low_list = []\n",
    "    delta_x_one_high_list = []\n",
    "    \n",
    "    #Observed Values between within a level of x_two across levels of x_one\n",
    "    delta_x_two_low = df_copy[response][(df_copy[x_two]==0) & (df_copy[x_one]==0)].mean() - df_copy[response][(df_copy[x_two]==0) & (df_copy[x_one]==1)].mean()\n",
    "    delta_x_two_high = df_copy[response][(df_copy[x_two]==1) & (df_copy[x_one]==0)].mean() - df_copy[response][(df_copy[x_two]==1) & (df_copy[x_one]==1)].mean()    \n",
    "    #Empiric Values\n",
    "    delta_x_two_low_list = []\n",
    "    delta_x_two_high_list = []\n",
    "    \n",
    "    \n",
    "    # Permute outcome and calculate means\n",
    "    for _ in tqdm(range(10000)):\n",
    "        permuted_data = data_df.copy()\n",
    "        permuted_data[response] = np.random.permutation(permuted_data[response].values)\n",
    "        \n",
    "            # Binarize the variables to get observed values\n",
    "        if binarize:\n",
    "            if x_two_split_point is not None:\n",
    "                permuted_data[x_two] = np.where(permuted_data[x_two] <= x_two_split_point, 0, 1)\n",
    "            else:\n",
    "                permuted_data[x_two] = np.where(permuted_data[x_two] <= permuted_data[x_two].mean(), 0, 1)\n",
    "            \n",
    "            if x_one_split_point is not None:\n",
    "                permuted_data[x_one] = np.where(permuted_data[x_one] <= x_one_split_point, 0, 1)\n",
    "            else:\n",
    "                permuted_data[x_one] = np.where(permuted_data[x_one] <= permuted_data[x_one].mean(), 0, 1)\n",
    "        \n",
    "        #Observed Values\n",
    "        delta_x_one_low_list.append(permuted_data[response][(permuted_data[x_one]==0) & (permuted_data[x_two]==0)].mean() - permuted_data[response][(permuted_data[x_one]==0) & (permuted_data[x_two]==1)].mean())\n",
    "        delta_x_one_high_list.append(permuted_data[response][(permuted_data[x_one]==1) & (permuted_data[x_two]==0)].mean() - permuted_data[response][(permuted_data[x_one]==1) & (permuted_data[x_two]==1)].mean())\n",
    "        delta_x_two_low_list.append(permuted_data[response][(permuted_data[x_two]==0) & (permuted_data[x_one]==0)].mean() - permuted_data[response][(permuted_data[x_two]==0) & (permuted_data[x_one]==1)].mean())\n",
    "        delta_x_two_high_list.append(permuted_data[response][(permuted_data[x_two]==1) & (permuted_data[x_one]==0)].mean() - permuted_data[response][(permuted_data[x_two]==1) & (permuted_data[x_one]==1)].mean())\n",
    "        \n",
    "    \n",
    "    x_one_proportion_level1 = np.mean(np.array(delta_x_one_low_list) > delta_x_one_low)\n",
    "    x_one_proportion_level2 = np.mean(np.array(delta_x_one_high_list) > delta_x_one_high)\n",
    "    x_two_proportion_level1 = np.mean(np.array(delta_x_two_low_list) > delta_x_two_low)\n",
    "    x_two_proportion_level2 = np.mean(np.array(delta_x_two_high_list) > delta_x_two_high)\n",
    "    \n",
    "    return x_one_proportion_level1, x_one_proportion_level2, x_two_proportion_level1, x_two_proportion_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the fixed function\n",
    "x_one='Age'; x_one_low_label='Young'; x_one_high_label='Old'\n",
    "x_one_split_point=63\n",
    "\n",
    "x_two='Subiculum_Connectivity'; x_two_low_label='Low Connectivity'; x_two_high_label='High Connectivity'\n",
    "x_two_split_point=66\n",
    "\n",
    "response='outcome'\n",
    "#----------------------------------------------------------------\n",
    "x_one_proportion_level1, x_one_proportion_level2, x_two_proportion_level1, x_two_proportion_level2 = permute_outcome_and_get_proportions_fixed(data_df.copy(), \n",
    "                 x_one=x_one, x_one_under_mean=x_one_low_label, x_one_over_mean=x_one_high_label, x_one_split_point=x_one_split_point,\n",
    "                 x_two=x_two, x_two_under_mean=x_two_low_label, x_two_over_mean=x_two_high_label, x_two_split_point=x_two_split_point,\n",
    "                 response=response)\n",
    "\n",
    "print(f'Significance of {x_one} between levels of {x_two} is: {x_one_proportion_level1} at the {x_one_low_label} {x_one} and {x_one_proportion_level2} at the {x_one_high_label} {x_one}')\n",
    "print(f'Significance of {x_two} between levels of {x_one} is: {x_two_proportion_level1} at the {x_two_low_label} {x_two} and {x_two_proportion_level2} at the {x_two_high_label} {x_two}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use response plane to visualize interaction effect\n",
    "- This models the marginal distribution of variables\n",
    "- If the model has 2 predictors and 1 response, then choose option A\n",
    "- If the model has more than 2 predictors, then choose option B. You will need to manually vary across the additional predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 variable method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "def create_interaction_plot(data_df: pd.DataFrame,\n",
    "                            x_one: str,\n",
    "                            x_two: str,\n",
    "                            outcome: str,\n",
    "                            results: GLMResults,\n",
    "                            num_slices: int = 100,\n",
    "                            out_dir: str = './',\n",
    "                            labels: dict = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a 3D plot visualizing the interaction of two predictor variables on the outcome.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pd.DataFrame\n",
    "        Dataframe containing the predictor variables and outcome.\n",
    "    x_one : str\n",
    "        Column name of the first predictor variable in the dataframe.\n",
    "    x_two : str\n",
    "        Column name of the second predictor variable in the dataframe.\n",
    "    outcome : str\n",
    "        Column name of the outcome variable in the dataframe.\n",
    "    results : GLMResults\n",
    "        Fitted model used for making predictions.\n",
    "    num_slices : int, optional\n",
    "        Number of slices to create in the 3D grid, default is 100.\n",
    "    out_dir : str, optional\n",
    "        Directory to save the output PNG and SVG files, default is the current directory.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        The created figure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create grid of predictor variable values\n",
    "    x1 = np.linspace(min(data_df[x_one]), max(data_df[x_one]), num_slices)\n",
    "    x2 = np.linspace(min(data_df[x_two]), max(data_df[x_two]), num_slices)\n",
    "    x1v, x2v = np.meshgrid(x1, x2)\n",
    "\n",
    "    # Set plot style\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette('Greys', 1, desat=1)\n",
    "\n",
    "    # Flattening the matrices to create a DataFrame for prediction\n",
    "    X_grid = pd.DataFrame({\n",
    "        x_one: x1v.ravel(),\n",
    "        x_two: x2v.ravel(),\n",
    "    })\n",
    "\n",
    "    # Generate response values for grid\n",
    "    y_pred = results.predict(X_grid).values.reshape(num_slices, num_slices)\n",
    "\n",
    "    # Create a new figure for plotting\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot the surface\n",
    "    ax.plot_surface(x1v, x2v, y_pred, cmap='Greys', alpha=1.0)\n",
    "\n",
    "    # Set the axes labels\n",
    "    format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "    # Set the axes labels\n",
    "    if labels is not None:\n",
    "        ax.set_xlabel(labels.get('x', format_label(x_one)))\n",
    "        ax.set_ylabel(labels.get('y', format_label(x_two)))\n",
    "        ax.set_zlabel(labels.get('z', format_label(outcome)))\n",
    "    else:\n",
    "        ax.set_xlabel(format_label(x_one))\n",
    "        ax.set_ylabel(format_label(x_two))\n",
    "        ax.set_zlabel(format_label(outcome))\n",
    "\n",
    "\n",
    "    # Save the plot as PNG and SVG files\n",
    "    fig.savefig(os.path.join(out_dir, '2_variable.png'), dpi=300)\n",
    "    fig.savefig(os.path.join(out_dir, '2_variable.svg'), format='svg')\n",
    "    print('Saved to file', out_dir)\n",
    "\n",
    "    return fig, y_pred, np.meshgrid(x1, x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = 'Age'\n",
    "x_two = 'Subiculum_Connectivity'\n",
    "outcome = 'outcome'\n",
    "#----------------------------------------------------------------DO NOT TOUCH THIS!----------------------------------------------------------------\n",
    "fig, y_pred, X_grid = create_interaction_plot(data_df, \n",
    "                        x_one=x_one, \n",
    "                        x_two=x_two, \n",
    "                        outcome=outcome, \n",
    "                        results=results, \n",
    "                        out_dir='/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/misc', \n",
    "                        labels={'x': 'Age', 'y': 'Subiculum Connectivity', 'z': 'Percent Improvement (MDRS)'});"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Variable Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import imageio\n",
    "import os\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "from typing import List\n",
    "from matplotlib import colors\n",
    "\n",
    "def create_interaction_gifs(data_df: pd.DataFrame, \n",
    "                            x_one: str, \n",
    "                            x_two: str, \n",
    "                            x_three: str, \n",
    "                            outcome: str, \n",
    "                            results: GLMResults, \n",
    "                            num_slices: int = 100,\n",
    "                            gif_duration: float = 0.3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates gifs visualizing the interaction of predictor variables on the outcome.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pd.DataFrame\n",
    "        Dataframe containing the predictor variables and outcome.\n",
    "    x_one : str\n",
    "        Column name of the first predictor variable in the dataframe.\n",
    "    x_two : str\n",
    "        Column name of the second predictor variable in the dataframe.\n",
    "    x_three : str\n",
    "        Column name of the third predictor variable in the dataframe.\n",
    "    outcome : str\n",
    "        Column name of the outcome variable in the dataframe.\n",
    "    results : GLMResults\n",
    "        Fitted model used for making predictions.\n",
    "    num_slices : int, optional\n",
    "        Number of slices to create in the gifs, default is 100.\n",
    "    gif_duration : float, optional\n",
    "        Duration of each frame in the gif in seconds, default is 0.3.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of file paths to the created gif files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if x_three is binary\n",
    "    unique_values = data_df[x_three].unique()\n",
    "    is_binary = len(unique_values) == 2\n",
    "\n",
    "    # Create grid of predictor variable values\n",
    "    x1 = np.linspace(min(data_df[x_one]), max(data_df[x_one]), num_slices)\n",
    "    x2 = np.linspace(min(data_df[x_two]), max(data_df[x_two]), num_slices)\n",
    "    \n",
    "    # Set plot style\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette('Greys', 1, desat=1)\n",
    "\n",
    "    gif_paths = []\n",
    "\n",
    "    if is_binary:\n",
    "        for x_three_value in unique_values:\n",
    "            # Flattening the matrices to create a DataFrame for prediction\n",
    "            X_grid = pd.DataFrame({\n",
    "                x_one: np.tile(x1, len(x2)),\n",
    "                x_two: np.repeat(x2, len(x1)),\n",
    "                x_three: np.ones_like(np.tile(x1, len(x2))) * x_three_value\n",
    "            })\n",
    "\n",
    "            # Generate response values for grid\n",
    "            y_pred = results.predict(X_grid).values.reshape(len(x2), len(x1))\n",
    "\n",
    "            # Create a new figure for plotting\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Plot the surface\n",
    "            ax.plot_surface(np.tile(x1, (len(x2), 1)), np.repeat(x2[:, np.newaxis], len(x1), axis=1), y_pred, cmap='bwr', alpha=0.8)\n",
    "\n",
    "            # Set the axes labels\n",
    "            format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "            ax.set_xlabel(format_label(x_one))\n",
    "            ax.set_ylabel(format_label(x_two))\n",
    "            ax.set_zlabel(format_label(outcome))\n",
    "\n",
    "            # Set the title with the actual value of x_three\n",
    "            title = f'{x_three} = {x_three_value}'\n",
    "            ax.set_title(title)\n",
    "\n",
    "            # Save the plot as an image file\n",
    "            image_path = f'plot_{x_three}_{x_three_value}.png'\n",
    "            plt.savefig(image_path, dpi=300)\n",
    "            gif_paths.append(image_path)\n",
    "            plt.close()\n",
    "    else:\n",
    "        # Flattening the matrices to create a DataFrame for prediction\n",
    "        x3 = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)\n",
    "        x1v, x2v, x3v = np.meshgrid(x1, x2, x3)\n",
    "\n",
    "        X_grid = pd.DataFrame({\n",
    "            x_one: x1v.ravel(),\n",
    "            x_two: x2v.ravel(),\n",
    "            x_three: x3v.ravel(),\n",
    "        })\n",
    "\n",
    "        # Generate response values for grid\n",
    "        y_pred = results.predict(X_grid).values.reshape(num_slices, num_slices, num_slices)\n",
    "\n",
    "        # Collect the file paths of images\n",
    "        image_paths = []\n",
    "\n",
    "        # Iterating over x3 (third predictor variable) and saving each plot as an image\n",
    "        for i in range(num_slices):\n",
    "            x3_slice = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)[i]\n",
    "\n",
    "            # Create a new figure for plotting\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Plot the surface\n",
    "            norm = colors.TwoSlopeNorm(vcenter=0)\n",
    "            ax.plot_surface(x1v[:, :, i], x2v[:, :, i], y_pred[:, :, i], cmap='Greys', alpha=0.8, norm=norm)\n",
    "\n",
    "            # Set the axes labels\n",
    "            format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "            ax.set_xlabel(format_label(x_one))\n",
    "            ax.set_ylabel(format_label(x_two))\n",
    "            ax.set_zlabel(format_label(outcome))\n",
    "\n",
    "            # Set the title with the actual value of x_three\n",
    "            title = f'{x_three} = {x3_slice:.2f}'\n",
    "            ax.set_title(title)\n",
    "\n",
    "            # Save the plot as an image file\n",
    "            image_path = f'plot_slice_{i+1}.png'\n",
    "            plt.savefig(image_path, dpi=300)\n",
    "            image_paths.append(image_path)\n",
    "            plt.close()\n",
    "\n",
    "        # Create a gif from the image files\n",
    "        gif_path = f'plots.gif'\n",
    "        images = [imageio.imread(image_path) for image_path in image_paths]\n",
    "        imageio.mimsave(gif_path, images, duration=gif_duration)\n",
    "        gif_paths.append(gif_path)\n",
    "\n",
    "        # Remove the individual image files\n",
    "        for image_path in image_paths:\n",
    "            os.remove(image_path)\n",
    "\n",
    "    return gif_paths\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# gifs = create_interaction_gifs(data_df, 'x_one', 'x_two', 'x_three', 'outcome', results)\n",
    "# print(\"GIFs created:\", gifs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = 'Subiculum_Connectivity'\n",
    "x_two = 'Age' \n",
    "x_three = 'Anticorrelated_Memory_Atrophy'\n",
    "outcome = 'cognitive_improvement'\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "gifs = create_interaction_gifs(data_df, \n",
    "                               x_one, \n",
    "                               x_two, \n",
    "                               x_three, \n",
    "                               outcome, \n",
    "                               results)\n",
    "print(\"GIFs created:\", gifs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Variable Method\n",
    "- 4th variable is expected to be 1-hot encoded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import imageio\n",
    "import os\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def create_interaction_gifs(data_df: pd.DataFrame, \n",
    "                            x_one: str, \n",
    "                            x_two: str, \n",
    "                            x_three: str, \n",
    "                            x_four: str, \n",
    "                            x_four_dict: dict,\n",
    "                            outcome: str, \n",
    "                            results: GLMResults, \n",
    "                            num_slices: int = 100,\n",
    "                            gif_duration: float = 0.3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates gifs visualizing the interaction of predictor variables on the outcome,\n",
    "    and the effect of a one-hot encoded variable (x_four).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pd.DataFrame\n",
    "        Dataframe containing the predictor variables and outcome.\n",
    "    x_one : str\n",
    "        Column name of the first predictor variable in the dataframe.\n",
    "    x_two : str\n",
    "        Column name of the second predictor variable in the dataframe.\n",
    "    x_three : str\n",
    "        Column name of the third predictor variable in the dataframe.\n",
    "    x_four : str\n",
    "        Column name of the one-hot encoded predictor variable (values should be 0 or 1) in the dataframe.\n",
    "    x_four_list : list\n",
    "        List of the x_four variables corresponding to their one hot encoded outcomes.\n",
    "    outcome : str\n",
    "        Column name of the outcome variable in the dataframe.\n",
    "    results : GLMResults\n",
    "        Fitted model used for making predictions.\n",
    "    num_slices : int, optional\n",
    "        Number of slices to create in the gifs, default is 100.\n",
    "    gif_duration : float, optional\n",
    "        Duration of each frame in the gif in seconds, default is 0.3.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of file paths to the created gif files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set plot style\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette('Greys', 1, desat=1)\n",
    "\n",
    "    gif_paths = []\n",
    "\n",
    "    # Iterate over x_four values 0 and 1\n",
    "    for x_four_value in [0, 1]:\n",
    "        distribution_df = data_df.copy()\n",
    "        distribution_df = distribution_df[distribution_df['Disease'] == x_four_value]\n",
    "        # Create grid of predictor variable values\n",
    "        x1 = np.linspace(min(distribution_df[x_one]), max(distribution_df[x_one]), num_slices)\n",
    "        x2 = np.linspace(min(distribution_df[x_two]), max(distribution_df[x_two]), num_slices)\n",
    "        x3 = np.linspace(min(distribution_df[x_three]), max(distribution_df[x_three]), num_slices)\n",
    "        x1v, x2v, x3v = np.meshgrid(x1, x2, x3)\n",
    "        \n",
    "        # Flattening the matrices to create a DataFrame for prediction\n",
    "        X_grid = pd.DataFrame({\n",
    "            x_one: x1v.ravel(),\n",
    "            x_two: x2v.ravel(),\n",
    "            x_three: x3v.ravel(),\n",
    "            x_four: np.ones_like(x1v).ravel() * x_four_value\n",
    "        })\n",
    "\n",
    "        # Generate response values for grid\n",
    "        y_pred = results.predict(X_grid).values.reshape(num_slices, num_slices, num_slices)\n",
    "\n",
    "        # Collect the file paths of images\n",
    "        image_paths = []\n",
    "\n",
    "        # Iterating over x3 (third predictor variable) and saving each plot as an image\n",
    "        for i in range(num_slices):\n",
    "            x3_slice = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)[i]\n",
    "            \n",
    "            # Create a new figure for plotting\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Plot the surface\n",
    "            ax.plot_surface(x1v[:, :, i], x2v[:, :, i], y_pred[:, :, i], cmap='bwr', alpha=0.8)\n",
    "\n",
    "            # Set the axes labels\n",
    "            format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "            ax.set_xlabel(format_label(x_one))\n",
    "            ax.set_ylabel(format_label(x_two))\n",
    "            ax.set_zlabel(format_label(outcome))\n",
    "            \n",
    "            # Set the title with the actual value of x_three and x_four\n",
    "            title = f'{x_four_dict[x_four_value]}, {x_three} = {x3_slice:.2f}'\n",
    "            ax.set_title(title)\n",
    "\n",
    "            # Save the plot as an image file\n",
    "            image_path = f'plot_{x_four}_{x_four_dict[x_four_value]}_slice_{i+1}.png'\n",
    "            plt.savefig(image_path, dpi=300)\n",
    "            image_paths.append(image_path)\n",
    "            plt.close()\n",
    "\n",
    "        # Create a gif from the image files\n",
    "        gif_path = f'{x_four}_{x_four_dict[x_four_value]}_plots.gif'\n",
    "        images = [imageio.imread(image_path) for image_path in image_paths]\n",
    "        imageio.mimsave(gif_path, images, duration=gif_duration)\n",
    "        gif_paths.append(gif_path)\n",
    "\n",
    "        # Remove the individual image files\n",
    "        for image_path in image_paths:\n",
    "            os.remove(image_path)\n",
    "\n",
    "    return gif_paths\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# gifs = create_interaction_gifs(data_df, 'Subiculum_Connectivity', 'Subiculum_Grey_Matter', 'Age', 'Disease', 'outcome', results)\n",
    "# print(\"GIFs created:\", gifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = 'Subiculum_Connectivity'\n",
    "x_two = 'Subiculum_Grey_Matter' \n",
    "x_three = 'Age'\n",
    "x_four = 'Disease'\n",
    "x_four_dict = {1: \"Alzheimer's\", 0: \"Parkinson's\"}\n",
    "outcome = 'cognitive_improvement'\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "gifs = create_interaction_gifs(data_df, \n",
    "                               x_one, \n",
    "                               x_two, \n",
    "                               x_three, \n",
    "                               x_four,\n",
    "                               x_four_dict,\n",
    "                               outcome, \n",
    "                               results)\n",
    "print(\"GIFs created:\", gifs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Variable Method\n",
    "- This is identical to the 4 variable method, but takes the 5th variable \n",
    "and calculates is at -2 and +2 standard deviations, then plots a response plane for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import imageio\n",
    "import os\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def create_interaction_gifs_with_5_variables(data_df: pd.DataFrame, \n",
    "                                             x_one: str, \n",
    "                                             x_two: str, \n",
    "                                             x_three: str, \n",
    "                                             x_four: str, \n",
    "                                             x_four_dict: dict,\n",
    "                                             x_five: str,\n",
    "                                             x_five_factor_levels: int,\n",
    "                                             outcome: str, \n",
    "                                             results: GLMResults, \n",
    "                                             num_slices: int = 100,\n",
    "                                             gif_duration: float = 0.3) -> List[str]:\n",
    "    \n",
    "    # Create grid of predictor variable values\n",
    "    x1 = np.linspace(min(data_df[x_one]), max(data_df[x_one]), num_slices)\n",
    "    x2 = np.linspace(min(data_df[x_two]), max(data_df[x_two]), num_slices)\n",
    "    x3 = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)\n",
    "    x1v, x2v, x3v = np.meshgrid(x1, x2, x3)\n",
    "\n",
    "    # Set plot style\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette('Greys', 1, desat=1)\n",
    "\n",
    "    gif_paths = []\n",
    "\n",
    "    # Iterate over x_four values 0 and 1\n",
    "    for x_four_value in [0, 1]:\n",
    "        \n",
    "        # Collect the file paths of images\n",
    "        image_paths = []\n",
    "\n",
    "        # Iterating over x3 (third predictor variable) and saving each plot as an image\n",
    "        for i in range(num_slices):\n",
    "            x3_slice = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)[i]\n",
    "            \n",
    "            # Create a new figure for plotting\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Choose to use x_five mean\n",
    "            x_five_mean = data_df[x_five].mean()\n",
    "            x_five_std = data_df[x_five].std()\n",
    "            if x_five_factor_levels == 1:\n",
    "                x_five_level = [x_five_mean]\n",
    "            # Choose to use x_five +/- 2 standard deviations\n",
    "            else:\n",
    "                x_five_level = [x_five_mean-(2*x_five_std), x_five_mean+(2*x_five_std)]\n",
    "            for level in range(0, len(x_five_level)):\n",
    "                \n",
    "                # Flattening the matrices to create a DataFrame for prediction\n",
    "                X_grid = pd.DataFrame({\n",
    "                    x_one: x1v.ravel(),\n",
    "                    x_two: x2v.ravel(),\n",
    "                    x_three: x3v.ravel(),\n",
    "                    x_four: np.ones_like(x1v).ravel() * x_four_value,\n",
    "                    x_five: np.ones_like(x1v).ravel() * x_five_level[level]\n",
    "                })\n",
    "\n",
    "                # Generate response values for grid\n",
    "                y_pred = results.predict(X_grid).values.reshape(num_slices, num_slices, num_slices)\n",
    "                \n",
    "                # Plot the surface\n",
    "                cmap = 'viridis' if x_five_level[level] == data_df[x_five].mean() else ('cool' if x_five_level[level] < 0 else 'magma')\n",
    "                ax.plot_surface(x1v[:, :, i], x2v[:, :, i], y_pred[:, :, i], cmap=cmap, alpha=0.5)\n",
    "\n",
    "            # Set the axes labels\n",
    "            format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "            ax.set_xlabel(format_label(x_one))\n",
    "            ax.set_ylabel(format_label(x_two))\n",
    "            ax.set_zlabel(format_label(outcome))\n",
    "            \n",
    "            # Set the title with the actual value of x_three and x_four\n",
    "            title = f'{x_four_dict[x_four_value]}, {x_three} = {x3_slice:.2f}'\n",
    "            ax.set_title(title)\n",
    "\n",
    "            # Save the plot as an image file\n",
    "            image_path = f'plot_{x_four}_{x_four_dict[x_four_value]}_slice_{i+1}.png'\n",
    "            plt.savefig(image_path, dpi=300)\n",
    "            image_paths.append(image_path)\n",
    "            plt.close()\n",
    "\n",
    "        # Create a gif from the image files\n",
    "        gif_path = f'{x_four}_{x_four_dict[x_four_value]}_plots.gif'\n",
    "        images = [imageio.imread(image_path) for image_path in image_paths]\n",
    "        imageio.mimsave(gif_path, images, duration=gif_duration)\n",
    "        gif_paths.append(gif_path)\n",
    "\n",
    "        # Remove the individual image files\n",
    "        for image_path in image_paths:\n",
    "            os.remove(image_path)\n",
    "\n",
    "    return gif_paths\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = 'Subiculum_Connectivity'\n",
    "x_two = 'Subiculum_Grey_Matter' \n",
    "x_three = 'Age'\n",
    "x_four = 'Disease'\n",
    "x_four_dict = {1: \"Alzheimer's\", 0: \"Parkinson's\"}\n",
    "x_five = 'Subiculum_CSF'\n",
    "x_five_factor_levels = 1\n",
    "outcome = 'cognitive_improvement'\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "gifs = create_interaction_gifs_with_5_variables(data_df, \n",
    "                               x_one, \n",
    "                               x_two, \n",
    "                               x_three, \n",
    "                               x_four,\n",
    "                               x_four_dict,\n",
    "                               x_five,\n",
    "                               x_five_factor_levels,\n",
    "                               outcome, \n",
    "                               results)\n",
    "print(\"GIFs created:\", gifs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Inflection Point of Joint Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Local Maxima/Minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "class SaddlePointFinderMaxMin:\n",
    "    \"\"\"\n",
    "    A class to identify saddle points in a 2D array using local maxima and minima method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the class with the 2D array to be analyzed.\n",
    "\n",
    "        Parameters:\n",
    "        data (np.ndarray): 2D array for which the saddle points are to be identified.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        \n",
    "    def find_local_minima_maxima(self, array):\n",
    "        \"\"\"\n",
    "        Find local minima and maxima in a 1D array.\n",
    "        \n",
    "        Parameters:\n",
    "        array (np.ndarray): 1D data array\n",
    "        \n",
    "        Returns:\n",
    "        tuple: Indices of local minima and maxima in the array.\n",
    "        \"\"\"\n",
    "        local_minima = argrelextrema(array, np.less)[0]\n",
    "        local_maxima = argrelextrema(array, np.greater)[0]\n",
    "        return local_minima, local_maxima\n",
    "    \n",
    "    def find_critical_points(self):\n",
    "        \"\"\"\n",
    "        Identify the saddle points in the 2D array using local maxima and minima method.\n",
    "\n",
    "        Returns:\n",
    "        list: List of coordinates (as tuples) of the identified saddle points.\n",
    "        \"\"\"\n",
    "        row_minima_coords = []\n",
    "        row_maxima_coords = []\n",
    "        for i, row in enumerate(self.data):\n",
    "            local_minima, local_maxima = self.find_local_minima_maxima(row)\n",
    "            for coord in local_minima:\n",
    "                row_minima_coords.append((i, coord))\n",
    "            for coord in local_maxima:\n",
    "                row_maxima_coords.append((i, coord))\n",
    "                \n",
    "        col_minima_coords = []\n",
    "        col_maxima_coords = []\n",
    "        for i, col in enumerate(self.data.T):\n",
    "            local_minima, local_maxima = self.find_local_minima_maxima(col)\n",
    "            for coord in local_minima:\n",
    "                col_minima_coords.append((coord, i))\n",
    "            for coord in local_maxima:\n",
    "                col_maxima_coords.append((coord, i))\n",
    "        \n",
    "        saddle_points = list(set(row_minima_coords).intersection(col_maxima_coords) | set(row_maxima_coords).intersection(col_minima_coords))\n",
    "        \n",
    "        return saddle_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the class with the given data\n",
    "finder = SaddlePointFinderMaxMin(y_pred)\n",
    "finder.find_critical_points()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema\n",
    "from matplotlib import colors \n",
    "\n",
    "class GradientCriticalPointFinder:\n",
    "    \"\"\"\n",
    "    A class to identify and visualize the approximate critical points in a 2D array using gradient-based methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_array):\n",
    "        \"\"\"\n",
    "        Initialize the GradientCriticalPointFinder with the 2D array to be analyzed.\n",
    "\n",
    "        Parameters:\n",
    "        data_array (np.ndarray): 2D array for which the critical points are to be identified.\n",
    "        \"\"\"\n",
    "        self.data_array = data_array\n",
    "        self.gradient_x = None\n",
    "        self.gradient_y = None\n",
    "        self.combined_gradient_abs = None\n",
    "        self.critical_points = None\n",
    "\n",
    "    def _compute_gradients(self):\n",
    "        \"\"\"\n",
    "        Computes the gradients for each dimension of the data array.\n",
    "        \"\"\"\n",
    "        self.gradient_x = np.gradient(self.data_array, axis=0)\n",
    "        self.gradient_y = np.gradient(self.data_array, axis=1)\n",
    "        combined_gradient = self.gradient_x * self.gradient_y\n",
    "        self.combined_gradient_abs = np.abs(combined_gradient)\n",
    "\n",
    "    def set_indices_to_one(self, zero_array, row_indices, col_indices):\n",
    "        for r, c in zip(row_indices, col_indices):\n",
    "            zero_array[r, c] = 1\n",
    "        return zero_array\n",
    "    \n",
    "    def _identify_critical_points(self, use_absolute_minima=False):\n",
    "        \"\"\"\n",
    "        Identify the approximate critical points by finding intersections between the minima of gradient_x and gradient_y.\n",
    "        \"\"\"\n",
    "        gradient_x_min = np.zeros_like(self.gradient_x)\n",
    "        gradient_y_min = np.zeros_like(self.gradient_y)\n",
    "\n",
    "        if use_absolute_minima:\n",
    "            # Find the index of the global minimum in each column for gradient_x\n",
    "            for i in range(0, self.gradient_x.shape[0]):\n",
    "                gradient_x_min[i, :] = np.where(self.gradient_x[i,:] == np.min(self.gradient_x[i,:]), 1, 0)\n",
    "                \n",
    "            # Find the index of the global minimum in each row for gradient_y\n",
    "            for i in range(0, self.gradient_y.shape[1]):\n",
    "                gradient_y_min[:, i] = np.where(self.gradient_y[:,i] == np.min(self.gradient_y[:,i]), 1, 0)\n",
    "\n",
    "        # Find coincident hits\n",
    "        critical_points_array = gradient_x_min * gradient_y_min\n",
    "        \n",
    "        # At the end of the method:\n",
    "        rows, cols = np.where(critical_points_array == 1)\n",
    "        self.critical_points = (rows, cols)\n",
    "        print('Critical points found at rows:', self.critical_points[0], 'cols:', self.critical_points[1])\n",
    "\n",
    "\n",
    "    def plot(self, use_absolute_minima=False):\n",
    "        \"\"\"\n",
    "        Generate the required plots.\n",
    "        \"\"\"\n",
    "        # Ensure gradients are computed\n",
    "        if self.gradient_x is None or self.gradient_y is None:\n",
    "            self._compute_gradients()\n",
    "\n",
    "        # Identify critical points\n",
    "        self._identify_critical_points(use_absolute_minima)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "        norm = colors.TwoSlopeNorm(vcenter=0)\n",
    "    \n",
    "        # Plot gradients with the norm applied\n",
    "        cax1 = axes[0].imshow(self.gradient_x, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[0].set_title(\"Gradient X\")\n",
    "        plt.colorbar(cax1, ax=axes[0], orientation='vertical')\n",
    "\n",
    "        cax2 = axes[1].imshow(self.gradient_y, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[1].set_title(\"Gradient Y\")\n",
    "        plt.colorbar(cax2, ax=axes[1], orientation='vertical')\n",
    "\n",
    "        # Plot absolute value of the multiplied gradients\n",
    "        cax3 = axes[2].imshow(self.combined_gradient_abs, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[2].set_title(\"Abs(Gradient X * Gradient Y)\")\n",
    "        plt.colorbar(cax3, ax=axes[2], orientation='vertical')\n",
    "        \n",
    "        # Plot data_array with critical points and the norm applied\n",
    "        cax4 = axes[3].imshow(self.data_array, cmap='bwr', origin='lower', norm=norm)\n",
    "        if len(self.critical_points[0]) > 0:\n",
    "            axes[3].scatter(self.critical_points[1], self.critical_points[0], color='black', marker='X', label='Critical Point')\n",
    "        axes[3].set_title(\"Function with Critical Points\")\n",
    "        plt.colorbar(cax4, ax=axes[3], orientation='vertical')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_critical_points(self):\n",
    "        \"\"\"\n",
    "        Return the coordinates of the identified critical points.\n",
    "        \"\"\"\n",
    "        return self.critical_points\n",
    "finder = GradientCriticalPointFinder(y_pred)\n",
    "finder.plot(use_absolute_minima=True)\n",
    "finder.get_critical_points()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema\n",
    "\n",
    "class GradientCriticalPointFinder:\n",
    "    \"\"\"\n",
    "    A class to identify and visualize the approximate critical points in a 2D array using gradient-based methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_array):\n",
    "        \"\"\"\n",
    "        Initialize the GradientCriticalPointFinder with the 2D array to be analyzed.\n",
    "\n",
    "        Parameters:\n",
    "        data_array (np.ndarray): 2D array for which the critical points are to be identified.\n",
    "        \"\"\"\n",
    "        self.data_array = data_array\n",
    "        self.gradient_x = None\n",
    "        self.gradient_y = None\n",
    "        self.combined_gradient_abs = None\n",
    "        self.critical_points = None\n",
    "\n",
    "    def _compute_gradients(self):\n",
    "        \"\"\"\n",
    "        Computes the gradients for each dimension of the data array.\n",
    "        \"\"\"\n",
    "        self.gradient_x = np.gradient(self.data_array, axis=0)\n",
    "        self.gradient_y = np.gradient(self.data_array, axis=1)\n",
    "\n",
    "    def _identify_critical_points(self):\n",
    "        \"\"\"\n",
    "        Identify the approximate critical points using element-wise multiplication and rounding.\n",
    "        \"\"\"\n",
    "        combined_gradient = self.gradient_x * self.gradient_y\n",
    "        self.combined_gradient_abs = np.abs(combined_gradient)\n",
    "        \n",
    "        # Find local minima\n",
    "        local_minima = argrelextrema(self.combined_gradient_abs, np.less)\n",
    "        self.critical_points = local_minima\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Generate the required plots.\n",
    "        \"\"\"\n",
    "        # Ensure gradients are computed\n",
    "        if self.gradient_x is None or self.gradient_y is None:\n",
    "            self._compute_gradients()\n",
    "\n",
    "        # Identify critical points\n",
    "        if self.critical_points is None:\n",
    "            self._identify_critical_points()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "        norm = colors.TwoSlopeNorm(vcenter=0)\n",
    "    \n",
    "        # Plot gradients with the norm applied\n",
    "        cax1 = axes[0].imshow(self.gradient_x, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[0].set_title(\"Gradient X\")\n",
    "        plt.colorbar(cax1, ax=axes[0], orientation='vertical')\n",
    "\n",
    "        cax2 = axes[1].imshow(self.gradient_y, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[1].set_title(\"Gradient Y\")\n",
    "        plt.colorbar(cax2, ax=axes[1], orientation='vertical')\n",
    "\n",
    "        # Plot absolute value of the multiplied gradients\n",
    "        cax3 = axes[2].imshow(self.combined_gradient_abs, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[2].set_title(\"Abs(Gradient X * Gradient Y)\")\n",
    "        plt.colorbar(cax3, ax=axes[2], orientation='vertical')\n",
    "        \n",
    "        # Plot data_array with critical points and the norm applied\n",
    "        cax4 = axes[3].imshow(self.data_array, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[3].scatter(self.critical_points[1], self.critical_points[0], color='black', marker='X', label='Critical Point')\n",
    "        axes[3].set_title(\"Function with Critical Points\")\n",
    "        plt.colorbar(cax4, ax=axes[3], orientation='vertical')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_critical_points(self):\n",
    "        \"\"\"\n",
    "        Return the coordinates of the identified critical points.\n",
    "        \"\"\"\n",
    "        if self.critical_points is None:\n",
    "            self._identify_critical_points()\n",
    "        \n",
    "        return self.critical_points\n",
    "finder = GradientCriticalPointFinder(y_pred)\n",
    "finder.plot()\n",
    "finder.get_critical_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_Grid[0] corresponds to the x-axis, and this derives the value of it at the gradient min.\n",
    "#In your experiements, this is generally age. \n",
    "print(X_grid[0][42, 58]) #<---row is the 0 value in gradient y, col is the 0 value in gradient x\n",
    "#X_Grid[1] corresponds to the z-axis, and this derives the value of it at the gradient min\n",
    "print(X_grid[1][42, 58])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Hessian discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianSaddlePointFinder:\n",
    "    \"\"\"\n",
    "    A class to identify the saddle points in a 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y_pred):\n",
    "        \"\"\"\n",
    "        Initialize the SaddlePointFinder with the 2D array to be analyzed.\n",
    "\n",
    "        Parameters:\n",
    "        y_pred (np.ndarray): 2D array for which the saddle points are to be identified.\n",
    "        \"\"\"\n",
    "        self.y_pred = y_pred\n",
    "\n",
    "    def _compute_derivatives(self):\n",
    "        \"\"\"\n",
    "        Computes the first order partial derivatives with respect to both dimensions.\n",
    "\n",
    "        Returns:\n",
    "        tuple: First order partial derivatives with respect to x and y dimensions.\n",
    "        \"\"\"\n",
    "        dy_d1 = np.gradient(self.y_pred, axis=0)\n",
    "        dy_d2 = np.gradient(self.y_pred, axis=1)\n",
    "        \n",
    "        return dy_d1, dy_d2\n",
    "\n",
    "    def _compute_second_mixed_derivatives(self):\n",
    "        \"\"\"\n",
    "        Computes the second order partial derivatives and both mixed partial derivatives.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Second order partial derivatives with respect to x and y dimensions, and both mixed partial derivatives.\n",
    "        \"\"\"\n",
    "        dy_d1, dy_d2 = self._compute_derivatives()\n",
    "        \n",
    "        # Second order partial derivative with respect to x dimension (rows)\n",
    "        d2y_d1_2 = np.gradient(dy_d1, axis=0)\n",
    "        \n",
    "        # Second order partial derivative with respect to y dimension (columns)\n",
    "        d2y_d2_2 = np.gradient(dy_d2, axis=1)\n",
    "        \n",
    "        # Mixed partial derivatives\n",
    "        d2y_d1d2 = np.gradient(dy_d1, axis=1)\n",
    "        d2y_d2d1 = np.gradient(dy_d2, axis=0)\n",
    "        \n",
    "        return d2y_d1_2, d2y_d2_2, d2y_d1d2, d2y_d2d1\n",
    "\n",
    "    def find_saddle_points(self):\n",
    "        \"\"\"\n",
    "        Identify the saddle points in the 2D array.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Coordinates of the saddle point or (None, None) if no saddle point exists.\n",
    "        \"\"\"\n",
    "        second_deriv_1, second_deriv_2, mixed_deriv_1, mixed_deriv_2 = self._compute_second_mixed_derivatives()\n",
    "        \n",
    "        # Check equivalence of mixed partial derivatives\n",
    "        if not np.allclose(mixed_deriv_1, mixed_deriv_2):\n",
    "            return \"Error: Mixed partial derivatives are not equivalent.\"\n",
    "        \n",
    "        # Calculate D\n",
    "        D = second_deriv_1 * second_deriv_2 - mixed_deriv_1**2\n",
    "        \n",
    "        # Identify saddle points\n",
    "        saddle_points = np.where(D < 0)\n",
    "        \n",
    "        print('Saddle points found at coordinates:')\n",
    "        print(np.min(D))\n",
    "        if saddle_points[0].size == 0:\n",
    "            return None, None\n",
    "        else:\n",
    "            return saddle_points[0][0], saddle_points[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test proof of saddle point idenfitication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the final class with the sample_array\n",
    "z = np.linspace(0,10,100)\n",
    "x = z \n",
    "\n",
    "# Create the grid of coordinates\n",
    "xv, zv = np.meshgrid(x,z)\n",
    "\n",
    "# Calculate y at every coordinate set\n",
    "sample_array = xv**2 - zv**2\n",
    "\n",
    "# Identify the single saddle point, which should be at 0,0\n",
    "saddle_finder_final = SaddlePointFinderFinal(sample_array)\n",
    "saddle_coordinates_final = saddle_finder_final.find_saddle_points()\n",
    "saddle_coordinates_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Saddle Point of Your Array of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saddle_finder_final = SaddlePointFinderFinal(y_pred)\n",
    "saddle_coordinates_final = saddle_finder_final.find_saddle_points()\n",
    "saddle_coordinates_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[saddle_coordinates_final[0], saddle_coordinates_final[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape\n",
    "from numpy import savetxt\n",
    "import os\n",
    "savetxt(os.path.join(out_dir,'y_pred.csv'), y_pred, delimiter=',')\n",
    "print('saved to: ', os.path.join(out_dir,'y_pred.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid.to_numpy().shape\n",
    "\n",
    "print(X_grid.to_numpy()[saddle_coordinates_final[0], 0])\n",
    "print(X_grid.to_numpy()[saddle_coordinates_final[1], 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Within Joint Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_statistics_and_create_dataframe(resampled_metrics, observed_metric, test_type, test_tails='two_tailed'):\n",
    "    \"\"\"\n",
    "    Compute p-value and confidence interval and organize them in a DataFrame.\n",
    "    \n",
    "    :param resampled_metrics: List of metrics computed from the resampled or permuted data\n",
    "    :param observed_metric: Observed metric computed from the original data\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :param test_type: Type of test performed ('resampling' or 'permutation')\n",
    "    :param test_tails: Type of tail for p-value testing ('two_tailed', 'right_tailed', or 'left_tailed')\n",
    "    :return: DataFrame with p-value and confidence interval\n",
    "    \"\"\"\n",
    "    # Calculate p-value and confidence interval for low and high values\n",
    "    if test_type =='permutation':\n",
    "        if test_tails == 'two_tailed':\n",
    "            print('Two-tailed p-value testing')\n",
    "            p_value_low = np.mean(np.abs([x[0] for x in resampled_metrics]) >= np.abs(observed_metric[0]))\n",
    "            p_value_high = np.mean(np.abs([x[1] for x in resampled_metrics]) >= np.abs(observed_metric[1]))\n",
    "        elif test_tails == 'right_tailed':\n",
    "            print('Right-tailed p-value testing')\n",
    "            p_value_low = np.mean([x[0] for x in resampled_metrics] >= observed_metric[0])\n",
    "            p_value_high = np.mean([x[1] for x in resampled_metrics] >= observed_metric[1])\n",
    "        elif test_tails == 'left_tailed':\n",
    "            print('Left-tailed p-value testing.')\n",
    "            p_value_low = np.mean([x[0] for x in resampled_metrics] <= observed_metric[0])\n",
    "            p_value_high = np.mean([x[1] for x in resampled_metrics] <= observed_metric[1])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for test_tails. Must be one of 'two_tailed', 'right_tailed', or 'left_tailed'.\")\n",
    "    elif test_type == 'resampling':\n",
    "        p_value_low = np.mean(np.sign(np.mean([x[0] for x in resampled_metrics])) != np.sign(observed_metric[0]))\n",
    "        p_value_high = np.mean(np.sign(np.mean([x[1] for x in resampled_metrics])) != np.sign(observed_metric[1]))\n",
    "    else:\n",
    "        raise ValueError(f\"Test type is not supported: {test_type}\")\n",
    "    \n",
    "    conf_int_low = np.percentile([x[0] for x in resampled_metrics], [2.5, 97.5])\n",
    "    conf_int_high = np.percentile([x[1] for x in resampled_metrics], [2.5, 97.5])\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        'Observed Metric': [observed_metric[0], observed_metric[1]],\n",
    "        'P-value': [p_value_low, p_value_high],\n",
    "        '95% CI Lower': [conf_int_low[0], conf_int_high[0]],\n",
    "        '95% CI Upper': [conf_int_low[1], conf_int_high[1]]\n",
    "    }\n",
    "    index = ['Delta Y pred indep_var_2 +/- stdev holding indep_var_1 at -2stdev', \n",
    "                'Delta Y pred indep_var_2 +/- stdev holding indep_var_1 at +2stdev']\n",
    "\n",
    "    return pd.DataFrame(data, index=[index])\n",
    "\n",
    "\n",
    "def compare_responses(df, dependent_var, independent_var1, independent_var2, interaction=True):\n",
    "    \"\"\"\n",
    "    Compare responses of a dataset using point testing method.\n",
    "    \n",
    "    :param df: DataFrame containing the dataset\n",
    "    :param dependent_var: Name of the dependent variable\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param interaction: Boolean indicating if interaction between independent variables should be considered\n",
    "    :param test_tails: Type of tail for p-value testing ('two_tailed', 'right_tailed', or 'left_tailed')\n",
    "    :return: Comparison metric (absolute differences between low and high points)\n",
    "    \"\"\"\n",
    "    if interaction:\n",
    "        formula = f'{dependent_var} ~ {independent_var1} * {independent_var2}'\n",
    "    else:\n",
    "        formula = f'{dependent_var} ~ {independent_var1} + {independent_var2}'\n",
    "\n",
    "    # Fit the model\n",
    "    model = smf.ols(formula=formula, data=df).fit()\n",
    "\n",
    "    # Evaluate at specific points\n",
    "    diff_low, diff_high = evaluate_at_specific_points(df, independent_var1, independent_var2, model)\n",
    "\n",
    "    # Return the absolute differences\n",
    "    return abs(diff_low), abs(diff_high)\n",
    "\n",
    "def evaluate_at_specific_points(df, independent_var1, independent_var2, model):\n",
    "    \"\"\"\n",
    "    Evaluate the model at specific points.\n",
    "    \n",
    "    :param df: DataFrame containing the dataset\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param model: Fitted model\n",
    "    :return: Differences between low and high points\n",
    "    \"\"\"\n",
    "    # Determine the low and high values of the independent variables\n",
    "    iv1_sd = data_df[independent_var1].std()\n",
    "    iv2_sd = data_df[independent_var2].std()\n",
    "    iv1_mean = data_df[independent_var1].mean()\n",
    "    iv2_mean = data_df[independent_var2].mean()\n",
    "\n",
    "    # Create the dataframes for low_1 and high_1 predictions\n",
    "    low_1_df = pd.DataFrame({\n",
    "        independent_var1: iv1_mean - 2 * iv1_sd,\n",
    "        independent_var2: [iv2_mean - 2 * iv2_sd, iv2_mean + 2 * iv2_sd]\n",
    "    })\n",
    "\n",
    "    high_1_df = pd.DataFrame({\n",
    "        independent_var1: iv1_mean + 2 * iv1_sd,\n",
    "        independent_var2: [iv2_mean - 2 * iv2_sd, iv2_mean + 2 * iv2_sd]\n",
    "    })\n",
    "\n",
    "    # Perform the predictions on the dataframes\n",
    "    low_1_predictions = model.predict(low_1_df)\n",
    "    high_1_predictions = model.predict(high_1_df)\n",
    "\n",
    "    # Calculate the differences between the predictions\n",
    "    difference_low = low_1_predictions[1] - low_1_predictions[0]\n",
    "    difference_high = high_1_predictions[1] - high_1_predictions[0]\n",
    "\n",
    "    return difference_low, difference_high\n",
    "\n",
    "def resample_or_permutation_test(df, dependent_var, independent_var1, independent_var2, interaction=True, num_resamples=1000, test_type='permutation', test_tails='two_tails'):\n",
    "    \"\"\"\n",
    "    Perform resampling or permutation test to compare responses of two datasets and return results in a DataFrame.\n",
    "    \n",
    "    :param df: DataFrame containing the dataset\n",
    "    :param dependent_var: Name of the dependent variable\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param interaction: Boolean indicating if interaction between independent variables should be considered\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :param num_slices: Number of values to take for each variable to form the grid (used in correlation)\n",
    "    :param num_resamples: Number of resamples to perform\n",
    "    :param test_type: Type of test to perform ('resampling' or 'permutation')\n",
    "    :return: p-value and confidence interval\n",
    "    \"\"\"\n",
    "    # Calculate observed metric\n",
    "    observed_metric = compare_responses(df, dependent_var, independent_var1, independent_var2, interaction)\n",
    "    \n",
    "    # Initialize results\n",
    "    resampled_metrics = []\n",
    "\n",
    "    for i in tqdm(range(num_resamples)):\n",
    "        # Perform resampling or permutation\n",
    "        if test_type == 'resampling':\n",
    "            sample_df = df.sample(frac=1, replace=True)\n",
    "        elif test_type == 'permutation':\n",
    "            sample_df = df.copy()\n",
    "            sample_df[dependent_var] = np.random.permutation(sample_df[dependent_var].values)\n",
    "        else:\n",
    "            raise ValueError(\"test_type must be either 'resampling' or 'permutation'.\")\n",
    "        \n",
    "        # Calculate resampled metric\n",
    "        resampled_metric = compare_responses(sample_df, dependent_var, independent_var1, independent_var2, interaction)\n",
    "        resampled_metrics.append(resampled_metric)\n",
    "    \n",
    "    # Calculate statistics and create DataFrame\n",
    "    results_dataframe = compute_statistics_and_create_dataframe(resampled_metrics, observed_metric, test_type, test_tails=test_tails)\n",
    "    \n",
    "    return results_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dataframe 1 and dataframe 2 for comaprison by permutation test\n",
    "df_1 = data_df[data_df['Disease'] == 1]\n",
    "df_2 = data_df[data_df['Disease'] == 0]\n",
    "dependent_var = 'outcome'\n",
    "independent_var1 = 'Age'\n",
    "independent_var2 = 'Subiculum_Connectivity'\n",
    "enable_interaction = True\n",
    "num_iterations = 10000\n",
    "test_tails = 'two_tailed'\n",
    "#:param test_tails: Type of tail for p-value testing ('two_tailed', 'right_tailed', or 'left_tailed')\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "permutation_results = resample_or_permutation_test(df = df_1,\n",
    "                                                    dependent_var = dependent_var,\n",
    "                                                    independent_var1 = independent_var1,\n",
    "                                                    independent_var2 = independent_var2,\n",
    "                                                    interaction = enable_interaction,\n",
    "                                                    num_resamples=num_iterations, \n",
    "                                                    test_type='permutation',\n",
    "                                                    test_tails=test_tails)\n",
    "permutation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "bootstrap_results = resample_or_permutation_test(df = df_2,\n",
    "                                                    dependent_var = dependent_var,\n",
    "                                                    independent_var1 = independent_var1,\n",
    "                                                    independent_var2 = independent_var2,\n",
    "                                                    interaction = enable_interaction,\n",
    "                                                    num_resamples=num_iterations, \n",
    "                                                    test_type='resampling')\n",
    "bootstrap_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Between Joint Distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of 2 Joint Distributions**\n",
    "1) Joint Distributions and Multiple Regression:\n",
    "\n",
    "The concept of \"joint distributions\" refers to the combined distribution of multiple variables, taking into account their mutual relationships. In the context of multiple regression analysis, we utilize a statistical model to examine the joint distribution of the independent variables and their relationship with the dependent variable. By fitting a regression model, such as the one used in this code, we can investigate how the independent variables collectively influence the dependent variable.\n",
    "\n",
    "In the provided code, we compare the responses of two datasets based on their joint distributions. This means that we analyze the relationship between the independent variables (independent_var1 and independent_var2) as a pair and their collective influence on the dependent variable. We fit separate regression models for each dataset, considering both independent variables as predictors and the dependent variable as the response.\n",
    "\n",
    "Analyzing the joint distributions allows us to gain insights into how the independent variables interact with each other and jointly impact the dependent variable. By considering the joint relationship between the variables, we can understand how changes in one independent variable may affect the response when the other independent variable is also taken into account.\n",
    "\n",
    "2) Permutation vs Bootstrap:\n",
    "\n",
    "The code employs two different approaches for hypothesis testing: permutation testing and bootstrap resampling. Both methods aim to assess the significance of the observed metrics by generating alternative datasets.\n",
    "\n",
    "Permutation testing involves randomly permuting the values of the dependent variable while keeping the independent variables unchanged. This process breaks the relationship between the dependent and independent variables, creating new datasets under the assumption of no association. By comparing the observed metric with metrics obtained from the permuted datasets, we can estimate p-values and confidence intervals, providing a measure of the statistical significance of our results. A permuted dataset is one wherein the independent variables are unrelated to the dependent variable. They are the null distribution. \n",
    "\n",
    "Bootstrap resampling, on the other hand, involves randomly sampling data points from the original datasets with replacement. This resampling process allows us to create multiple datasets of the same size as the original datasets. By generating new datasets through resampling, we can examine the distribution of the metrics of interest and estimate their variability. This information can be used to estimate confidence intervals and assess the statistical significance of our findings. A bootstrap resampling process creates alternative datasets wherein the independent variables are still related to the dependent variable. These define significance by demonstrating the confidence intervals of the dataset do not cross a threshold, which is typically 0.\n",
    "\n",
    "3) Correlation vs Point-test:\n",
    "\n",
    "The code provides two different methods for comparing responses based on the joint distributions: correlation and point testing.\n",
    "\n",
    "The correlation analysis aims to assess the spatial correlation between the joint distributions of the two datasets. This analysis examines the overall patterns and similarities in the responses across the range of the joint distributions. By calculating the correlation coefficient between the predicted responses, we can determine if there is a consistent relationship between the joint distributions of the independent variables and the dependent variable across the datasets. Then, we compare this to bootstrapped or permuted datasets to derive significance estimates.\n",
    "\n",
    "In contrast, point testing focuses on evaluating the differences in the predicted responses at specific points of the joint distributions. Specifically, we examine the differences at +/- 2 standard deviations of the independent variables. This analysis allows us to determine if the differences between the responses at these specific points significantly vary between the two datasets. By comparing the differences in response values at these critical points, we can identify regions of the joint distributions where the relationship between the independent variables and the dependent variable differs significantly. The purpose of this is to condense a 3-dimensional joint distribution into 2-dimensions at set values which is good for generating figures. Then, we compare this to bootstrapped or permuted datasets to derive significance estimates.\n",
    "\n",
    "In summary, comparing the joint distributions enables us to analyze the combined effects of the independent variables on the dependent variable. The correlation and point testing methods offer different perspectives on the relationship between the variables, while permutation testing and bootstrap resampling provide means to assess the statistical significance of our findings.\n",
    "\n",
    "**Instructions**\n",
    "Testing: 'correlation' or 'point_testing'\n",
    "Significance Testing: 'permutation' or 'resampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_statistics_and_create_dataframe(resampled_metrics, observed_metric, comparison_method, test_type, test_tails='right_tailed'):\n",
    "    \"\"\"\n",
    "    Compute p-value and confidence interval and organize them in a DataFrame.\n",
    "    \n",
    "    :param resampled_metrics: List of metrics computed from the resampled or permuted data\n",
    "    :param observed_metric: Observed metric computed from the original data\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :return: DataFrame with p-value and confidence interval\n",
    "    \"\"\"\n",
    "    if comparison_method == 'correlation':\n",
    "        # Calculate p-value and confidence interval\n",
    "        p_value = np.mean(np.abs(resampled_metrics) >= np.abs(observed_metric))\n",
    "        conf_int = np.percentile(resampled_metrics, [2.5, 97.5])\n",
    "\n",
    "        # Create DataFrame\n",
    "        data = {\n",
    "            'Observed Metric': [observed_metric],\n",
    "            'P-value': [p_value],\n",
    "            '95% CI Lower': [conf_int[0]],\n",
    "            '95% CI Upper': [conf_int[1]]\n",
    "        }\n",
    "        index = 'Spatial correlation of the joint distributions'\n",
    "    \n",
    "    elif comparison_method == 'point_testing':\n",
    "        # Calculate p-value and confidence interval for low and high values\n",
    "        if test_type =='permutation':\n",
    "            if test_tails == 'two_tailed':\n",
    "                print('Two-tailed p-value testing')\n",
    "                p_value_low = np.mean(np.abs([x[0] for x in resampled_metrics]) >= np.abs(observed_metric[0]))\n",
    "                p_value_high = np.mean(np.abs([x[1] for x in resampled_metrics]) >= np.abs(observed_metric[1]))\n",
    "            elif test_tails == 'right_tailed':\n",
    "                print('Right-tailed p-value testing')\n",
    "                p_value_low = np.mean([x[0] for x in resampled_metrics] >= observed_metric[0])\n",
    "                p_value_high = np.mean([x[1] for x in resampled_metrics] >= observed_metric[1])\n",
    "            elif test_tails == 'left_tailed':\n",
    "                print('Left-tailed p-value testing.')\n",
    "                p_value_low = np.mean([x[0] for x in resampled_metrics] <= observed_metric[0])\n",
    "                p_value_high = np.mean([x[1] for x in resampled_metrics] <= observed_metric[1])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid value for test_tails. Must be one of 'two_tailed', 'right_tailed', or 'left_tailed'.\")\n",
    "        elif test_type == 'resampling':\n",
    "            p_value_low = np.mean(np.sign(np.mean([x[0] for x in resampled_metrics])) != (np.sign(np.mean([x[0] for x in resampled_metrics]))))\n",
    "            p_value_high = np.mean(np.sign(np.mean([x[1] for x in resampled_metrics])) != (np.sign(np.mean([x[1] for x in resampled_metrics]))))\n",
    "        else:\n",
    "            raise ValueError (f'Test type is not supported {test_type}')\n",
    "        conf_int_low = np.percentile(resampled_metrics[0], [2.5, 97.5])\n",
    "        conf_int_high = np.percentile(resampled_metrics[1], [2.5, 97.5])\n",
    "\n",
    "        # Create DataFrame\n",
    "        data = {\n",
    "            'Observed Metric': [observed_metric[0], observed_metric[1]],\n",
    "            'P-value': [p_value_low, p_value_high],\n",
    "            '95% CI Lower': [conf_int_low[0], conf_int_high[0]],\n",
    "            '95% CI Upper': [conf_int_low[1], conf_int_high[1]]\n",
    "        }\n",
    "        index = ['Delta Y pred indep_var_2 +/- stdev holding indep_var_1 at -2stdev', \n",
    "                 'Delta Y pred indep_var_2 +/- stdev holding indep_var_1 at +2stdev']\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"comparison_method must be either 'correlation' or 'point_testing'.\")\n",
    "\n",
    "    return pd.DataFrame(data, index=[index])\n",
    "\n",
    "\n",
    "def calculate_predicted_response(data_df, independent_var1, independent_var2, model, num_slices=100):\n",
    "    \"\"\"\n",
    "    Calculate predicted response values for a grid of values of two independent variables.\n",
    "    \n",
    "    :param data_df: DataFrame containing data\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param model: Fitted model\n",
    "    :param num_slices: Number of values to take for each variable to form the grid\n",
    "    :return: Flattened array of predicted responses\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(min(data_df[independent_var1]), max(data_df[independent_var1]), num_slices)\n",
    "    x2 = np.linspace(min(data_df[independent_var2]), max(data_df[independent_var2]), num_slices)\n",
    "    x1v, x2v = np.meshgrid(x1, x2)\n",
    "\n",
    "    X_grid = pd.DataFrame({\n",
    "        independent_var1: x1v.ravel(),\n",
    "        independent_var2: x2v.ravel(),\n",
    "    })\n",
    "\n",
    "    y_pred = model.predict(X_grid).values.reshape(num_slices, num_slices)\n",
    "\n",
    "    return y_pred.ravel()\n",
    "\n",
    "def evaluate_at_specific_points(data_df, independent_var1, independent_var2, model):\n",
    "    \"\"\"\n",
    "    Evaluate the model at specific points: +/- 2 standard deviations of the independent variables.\n",
    "\n",
    "    :param data_df: DataFrame containing data\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param model: Fitted model\n",
    "    :return: Differences between the predictions at specific points\n",
    "    \"\"\"\n",
    "    iv1_sd = data_df[independent_var1].std()\n",
    "    iv2_sd = data_df[independent_var2].std()\n",
    "    iv1_mean = data_df[independent_var1].mean()\n",
    "    iv2_mean = data_df[independent_var2].mean()\n",
    "\n",
    "    # Create the dataframes for low_1 and high_1 predictions\n",
    "    low_1_df = pd.DataFrame({\n",
    "        independent_var1: iv1_mean - 2 * iv1_sd,\n",
    "        independent_var2: [iv2_mean - 2 * iv2_sd, iv2_mean + 2 * iv2_sd]\n",
    "    })\n",
    "\n",
    "    high_1_df = pd.DataFrame({\n",
    "        independent_var1: iv1_mean + 2 * iv1_sd,\n",
    "        independent_var2: [iv2_mean - 2 * iv2_sd, iv2_mean + 2 * iv2_sd]\n",
    "    })\n",
    "\n",
    "    # Perform the predictions on the dataframes\n",
    "    low_1_predictions = model.predict(low_1_df)\n",
    "    high_1_predictions = model.predict(high_1_df)\n",
    "\n",
    "    # Calculate the differences between the predictions\n",
    "    difference_low = low_1_predictions[1] - low_1_predictions[0]\n",
    "    difference_high = high_1_predictions[1] - high_1_predictions[0]\n",
    "\n",
    "    return difference_low, difference_high\n",
    "\n",
    "def compare_responses(df1, df2, dependent_var, independent_var1, independent_var2, interaction=True, comparison_method='correlation', num_slices=100):\n",
    "    \"\"\"\n",
    "    Compare responses of two datasets using correlation or point testing method.\n",
    "    \n",
    "    :param df1: DataFrame containing first dataset\n",
    "    :param df2: DataFrame containing second dataset\n",
    "    :param dependent_var: Name of the dependent variable\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param interaction: Boolean indicating if interaction between independent variables should be considered\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :param num_slices: Number of values to take for each variable to form the grid (used in correlation)\n",
    "    :return: Comparison metric\n",
    "    \"\"\"\n",
    "    if interaction:\n",
    "        formula = f'{dependent_var} ~ {independent_var1} * {independent_var2}'\n",
    "    else:\n",
    "        formula = f'{dependent_var} ~ {independent_var1} + {independent_var2}'\n",
    "\n",
    "    # Fit the models\n",
    "    model1 = smf.ols(formula=formula, data=df1).fit()\n",
    "    model2 = smf.ols(formula=formula, data=df2).fit()\n",
    "\n",
    "    if comparison_method == 'correlation':\n",
    "        # Calculate predicted responses\n",
    "        response_1 = calculate_predicted_response(df1, independent_var1, independent_var2, model1, num_slices)\n",
    "        response_2 = calculate_predicted_response(df2, independent_var1, independent_var2, model2, num_slices)\n",
    "\n",
    "        # Calculate correlation between the predicted responses\n",
    "        return stats.pearsonr(response_1, response_2)[0]\n",
    "\n",
    "    elif comparison_method == 'point_testing':\n",
    "        # Evaluate at specific points\n",
    "        diff_low_1, diff_high_1 = evaluate_at_specific_points(df1, independent_var1, independent_var2, model1)\n",
    "        diff_low_2, diff_high_2 = evaluate_at_specific_points(df2, independent_var1, independent_var2, model2)\n",
    "        \n",
    "        # Return the absolute differences\n",
    "        return abs(diff_low_1 - diff_low_2), abs(diff_high_1 - diff_high_2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"comparison_method must be either 'correlation' or 'point_testing'.\")\n",
    "\n",
    "# Update the resample_or_permutation_test function\n",
    "def resample_or_permutation_test(df1, df2, dependent_var, independent_var1, independent_var2, interaction=True, comparison_method='correlation', num_slices=100, num_resamples=1000, test_type='permutation'):\n",
    "    \"\"\"\n",
    "    Perform resampling or permutation test to compare responses of two datasets and return results in a DataFrame.\n",
    "    \n",
    "    :param df1: DataFrame containing first dataset\n",
    "    :param df2: DataFrame containing second dataset\n",
    "    :param dependent_var: Name of the dependent variable\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param interaction: Boolean indicating if interaction between independent variables should be considered\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :param num_slices: Number of values to take for each variable to form the grid (used in correlation)\n",
    "    :param num_resamples: Number of resamples to perform\n",
    "    :param test_type: Type of test to perform ('resampling' or 'permutation')\n",
    "    :return: p-value and confidence interval\n",
    "    \"\"\"\n",
    "    # Calculate observed metric\n",
    "    observed_metric = compare_responses(df1, df2, dependent_var, independent_var1, independent_var2, interaction, comparison_method, num_slices)\n",
    "    \n",
    "    # Initialize results\n",
    "    resampled_metrics = []\n",
    "\n",
    "    for i in tqdm(range(num_resamples)):\n",
    "        # Perform resampling or permutation\n",
    "        if test_type == 'resampling':\n",
    "            sample_df1 = df1.sample(frac=1, replace=True)\n",
    "            sample_df2 = df2.sample(frac=1, replace=True)\n",
    "        elif test_type == 'permutation':\n",
    "            sample_df1 = df1.copy()\n",
    "            sample_df2 = df2.copy()\n",
    "            sample_df1[dependent_var] = np.random.permutation(sample_df1[dependent_var].values)\n",
    "            sample_df2[dependent_var] = np.random.permutation(sample_df2[dependent_var].values)\n",
    "        else:\n",
    "            raise ValueError(\"test_type must be either 'resampling' or 'permutation'.\")\n",
    "        \n",
    "        # Calculate resampled metric\n",
    "        resampled_metric = compare_responses(sample_df1, sample_df2, dependent_var, independent_var1, independent_var2, interaction, comparison_method, num_slices)\n",
    "        resampled_metrics.append(resampled_metric)\n",
    "    \n",
    "    # Calculate statistics and create DataFrame\n",
    "    results_dataframe = compute_statistics_and_create_dataframe(resampled_metrics, observed_metric, comparison_method, test_type)\n",
    "    \n",
    "    return results_dataframe, resampled_metrics, observed_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dataframe 1 and dataframe 2 for comaprison by permutation test\n",
    "df_1 = data_df[data_df['Cohort'] == 1]\n",
    "df_2 = data_df[data_df['Cohort'] == 0]\n",
    "dependent_var = 'Percent_Cognitive_Improvement'\n",
    "independent_var1 = 'Age'\n",
    "independent_var2 = 'Subiculum_Connectivity'\n",
    "enable_interaction = True\n",
    "num_iterations = 10000\n",
    "comparison_method = 'correlation' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation Test It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_results.loc[:,'P-value'] > 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (permutation_results.loc[:,'P-value'] > 0.05).any():\n",
    "    permutation_results,  empiric_distribution, observed_metric = resample_or_permutation_test(df1 = df_1,\n",
    "                                                                df2 = df_2,\n",
    "                                                                dependent_var = dependent_var,\n",
    "                                                                independent_var1 = independent_var1,\n",
    "                                                                independent_var2 = independent_var2,\n",
    "                                                                interaction = enable_interaction,\n",
    "                                                                test_type = 'permutation',\n",
    "                                                                comparison_method = comparison_method,\n",
    "                                                                num_resamples = num_iterations)\n",
    "permutation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap Test It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "bootstrap_results, empiric_distribution, observed_metric = resample_or_permutation_test(df1 = df_1,\n",
    "                                                            df2 = df_2,\n",
    "                                                            dependent_var = dependent_var,\n",
    "                                                            independent_var1 = independent_var1,\n",
    "                                                            independent_var2 = independent_var2,\n",
    "                                                            interaction = enable_interaction,\n",
    "                                                            test_type = 'resampling',\n",
    "                                                            comparison_method = comparison_method,\n",
    "                                                            num_resamples = num_iterations)\n",
    "bootstrap_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Histogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class DeltaCorrelation():\n",
    "    def __init__(self, data_df):\n",
    "        self.empiric_dist = None\n",
    "        self.observed_val = None\n",
    "\n",
    "    def set_empiric_and_observed(self, empiric_dist, observed_val):\n",
    "        self.empiric_dist = empiric_dist\n",
    "        self.observed_val = observed_val\n",
    "\n",
    "    def plot_kde_histogram(self, bins=50, one_tail=False, color_palette='dark'):\n",
    "        if self.empiric_dist is None or self.observed_val is None:\n",
    "            print(\"Empiric distribution and observed value must be set before plotting.\")\n",
    "            return\n",
    "\n",
    "        if one_tail:\n",
    "            empiric_dist = np.abs(self.empiric_dist)\n",
    "            observed_val = np.abs(self.observed_val)\n",
    "        else:\n",
    "            empiric_dist = self.empiric_dist\n",
    "            observed_val = self.observed_val\n",
    "\n",
    "        p_value = np.mean(empiric_dist >= observed_val)\n",
    "\n",
    "        sns.set_palette(color_palette)\n",
    "        current_palette = sns.color_palette(color_palette)\n",
    "        chosen_color = current_palette[3]\n",
    "\n",
    "        g = sns.displot(empiric_dist, kde=True, bins=bins, label=\"Empirical Distribution\", element=\"step\", color='blue', alpha=.25)\n",
    "        g.fig.set_size_inches(6.5, 4.5)\n",
    "        plt.axvline(x=observed_val, color='red', linestyle='-', linewidth=1.5, label=f\"Observed Value\", alpha=1)\n",
    "        plt.title(f\"Spatial Correlation = {observed_val:.2f}, p = {p_value:.4f}\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "\n",
    "        fig = g.fig\n",
    "\n",
    "        fig.savefig(f\"{out_dir}/kde_histogram.png\", bbox_inches='tight')\n",
    "        fig.savefig(f\"{out_dir}/kde_histogram.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/kde_histogram.svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = DeltaCorrelation(data_df)\n",
    "hist.set_empiric_and_observed(empiric_dist=empiric_distribution, observed_val=observed_metric)\n",
    "hist.plot_kde_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the Inflection in Arbitrary Number of Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from calvin_utils.statistical_utils.distribution_statistics import plot_with_annotation\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "class SaddlePointFinder:\n",
    "    \"\"\"\n",
    "    This class finds the saddle points in a linear regression model with interaction terms.\n",
    "    It uses the coefficients of the model as the first partial derivatives of the function with respect to each variable.\n",
    "    The class identifies points in the data where these partial derivatives are minimized, which are treated as saddle points.\n",
    "    \n",
    "    Note: \n",
    "    The approach is heuristic and not rigorous. It assumes that the coefficients can serve as good approximations\n",
    "    for the first partial derivatives, which may not always be the case. Therefore, the identified saddle points \n",
    "    are approximate and should be interpreted with caution.\n",
    "    \"\"\"\n",
    "    def __init__(self, dependent_var, independent_var1, independent_var2, num_resamples=1000):\n",
    "        \"\"\"\n",
    "        Initialize the SaddlePointFinder with variable names and resampling details.\n",
    "        \n",
    "        Parameters:\n",
    "        - dependent_var (str): The dependent variable in the formula.\n",
    "        - independent_var1, independent_var2 (str): The independent variables in the formula.\n",
    "        - num_resamples (int): The number of bootstrapping samples.\n",
    "        \"\"\"\n",
    "        self.dependent_var = dependent_var\n",
    "        self.independent_var1 = independent_var1\n",
    "        self.independent_var2 = independent_var2\n",
    "        self.num_resamples = num_resamples\n",
    "\n",
    "    def find_saddle_point(self, model, sample_df):\n",
    "        \"\"\"\n",
    "        Find the values of the regressors at the saddle point in a single model.\n",
    "        \n",
    "        Parameters:\n",
    "        - model: A fitted OLS model.\n",
    "        - sample_df: The DataFrame containing the sampled data.\n",
    "        \n",
    "        Returns:\n",
    "        - A tuple containing the saddle point values for the independent variables.\n",
    "        \"\"\"\n",
    "        coef_x1 = model.params.get(self.independent_var1, 0)\n",
    "        coef_x2 = model.params.get(self.independent_var2, 0)\n",
    "        coef_interaction = model.params.get(f\"{self.independent_var1}:{self.independent_var2}\", 0)\n",
    "\n",
    "        sample_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        y2 = coef_x1 + coef_interaction * sample_df[self.independent_var2]\n",
    "        index_min_y2 = np.argmin(np.abs(y2))\n",
    "        x2 = sample_df.loc[index_min_y2, self.independent_var2]\n",
    "\n",
    "        y1 = coef_x2 + coef_interaction * sample_df[self.independent_var1]\n",
    "        index_min_y1 = np.argmin(np.abs(y1))\n",
    "        x1 = sample_df.loc[index_min_y1, self.independent_var1]\n",
    "\n",
    "        return (x1, x2)\n",
    "\n",
    "    def resample_and_find_saddle_points(self, df):\n",
    "        \"\"\"\n",
    "        Perform bootstrapping to find approximate saddle points in the model.\n",
    "        \n",
    "        Parameters:\n",
    "        - df (DataFrame): The DataFrame containing the variables.\n",
    "        \n",
    "        Returns:\n",
    "        - DataFrame containing the saddle points for the independent variables.\n",
    "        \"\"\"\n",
    "        saddle_points = []\n",
    "        for _ in tqdm(range(self.num_resamples)):\n",
    "            sample_df = df.sample(frac=1, replace=True)\n",
    "            formula = f\"{self.dependent_var} ~ {self.independent_var1} * {self.independent_var2}\"\n",
    "            model = ols(formula=formula, data=sample_df).fit()\n",
    "            saddle_point = self.find_saddle_point(model, sample_df)\n",
    "            saddle_points.append(saddle_point)\n",
    "        return pd.DataFrame(saddle_points, columns=[self.independent_var1, self.independent_var2])\n",
    "\n",
    "    def run_multiple_dfs(self, df_list):\n",
    "        \"\"\"\n",
    "        Run the saddle point finder on multiple DataFrames.\n",
    "        \n",
    "        Parameters:\n",
    "        - df_list (list): List of DataFrames.\n",
    "        \n",
    "        Returns:\n",
    "        - List of DataFrames, each containing the saddle points for the corresponding input DataFrame.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for df in df_list:\n",
    "            result = self.resample_and_find_saddle_points(df)\n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def perform_t_tests_and_plots(self, df_dict, x_label='x', y_label='y', test_type='t-test_ind',  out_dir=None, colours=['red', 'blue']):\n",
    "        \"\"\"\n",
    "        Perform t-tests and generate plots for pairs of DataFrames.\n",
    "\n",
    "        Parameters:\n",
    "        - df_dict (dict): Dictionary containing DataFrame as key and its new column name as value.\n",
    "        - x_label (str): X-axis label for the plot.\n",
    "        - y_label (str): Y-axis label for the plot.\n",
    "        - test_type (str): Type of statistical test to perform.\n",
    "        - out_dir (str): Directory to save the output plots.\n",
    "        - colours (list): List of colours for the plot.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "        df_list = list(df_dict.values())\n",
    "        col_name_list = list(df_dict.keys())\n",
    "\n",
    "        for i, df1 in enumerate(df_list[:-1]):\n",
    "            for j, df2 in enumerate(df_list[i+1:]):\n",
    "                \n",
    "                col1_new = col_name_list[i]\n",
    "                col2_new = col_name_list[i+j+1]\n",
    "                \n",
    "                df1_renamed = df1.rename(columns={df1.columns[0]: col1_new})\n",
    "                df2_renamed = df2.rename(columns={df2.columns[0]: col2_new})\n",
    "\n",
    "                merged_df = pd.concat([df1_renamed[[col1_new]], df2_renamed[[col2_new]]], keys=[col1_new, col2_new], names=['Group', 'None']).reset_index(level=0).rename(columns={'Group': 'Group', 0: 'Value'})\n",
    "                \n",
    "                plt = plot_with_annotation(dataframe=merged_df, \n",
    "                                        col1=col1_new, \n",
    "                                        col2=col2_new,\n",
    "                                        xlabel=x_label, \n",
    "                                        ylabel=y_label, \n",
    "                                        test_type=test_type,\n",
    "                                        colours=colours)\n",
    "                if out_dir is not None:\n",
    "                    os.makedirs(os.path.join(out_dir, 'distribution_figures'), exist_ok=True)\n",
    "                    plt.savefig(os.path.join(out_dir, f'distribution_figures/{x_label}_{test_type}_{y_label}.png'))\n",
    "                    plt.savefig(os.path.join(out_dir, f'distribution_figures/{x_label}_{test_type}_{y_label}.svg'))\n",
    "                    print(f'Figure saved to: {os.path.join(out_dir, f\"distribution_figures/{x_label}_{test_type}_{y_label}\")}')\n",
    "                plt.show()\n",
    "\n",
    "def plot_95CI(df_dict, x_label='x', y_label='y', out_dir=None, colours=['red', 'blue']):\n",
    "    \"\"\"\n",
    "    Plot the 95% confidence intervals for each DataFrame using seaborn boxplot.\n",
    "\n",
    "    Parameters:\n",
    "    - df_dict (dict): Dictionary containing DataFrame as key and its new column name as value.\n",
    "    - x_label (str): X-axis label for the plot.\n",
    "    - y_label (str): Y-axis label for the plot.\n",
    "    - out_dir (str): Directory to save the output plots.\n",
    "    - colours (list): List of colours for the plot.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Prepare data for seaborn\n",
    "    aggregated_data = []\n",
    "    for label, df in df_dict.items():\n",
    "        df['label'] = label\n",
    "        aggregated_data.append(df)\n",
    "\n",
    "    concatenated_df = pd.concat(aggregated_data)\n",
    "    concatenated_df.columns = [y_label, x_label]\n",
    "\n",
    "    # Create the boxplot\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(x=x_label, y=y_label, data=concatenated_df, ax=ax, palette=colours, whis=[2.5, 97.5])\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(f'Distribution of {y_label} At Inflection Point')\n",
    "\n",
    "\n",
    "    # Save plot\n",
    "    if out_dir is not None:\n",
    "        os.makedirs(os.path.join(out_dir, '95CI_figures'), exist_ok=True)\n",
    "        plt.savefig(os.path.join(out_dir, f'95CI_figures/{x_label}_{y_label}.png'))\n",
    "        plt.savefig(os.path.join(out_dir, f'95CI_figures/{x_label}_{y_label}.svg'))\n",
    "        print(f'Figure saved to: {os.path.join(out_dir, f\"95CI_figures/{x_label}_{y_label}\")}')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def run_ks_test(df_dict):\n",
    "    \"\"\"\n",
    "    Run a Kolmogorov-Smirnov test on two datasets to determine if their distributions are significantly different.\n",
    "\n",
    "    Parameters:\n",
    "    - df_dict (dict): Dictionary containing two DataFrames as key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "    - p_value (float): p-value from the KS test.\n",
    "    \"\"\"\n",
    "    if len(df_dict) != 2:\n",
    "        return \"The dictionary should contain exactly two datasets.\"\n",
    "    \n",
    "    data_1, data_2 = list(df_dict.values())\n",
    "    \n",
    "    # Extracting the single column from each DataFrame\n",
    "    data_1_column = data_1.iloc[:, 0]\n",
    "    data_2_column = data_2.iloc[:, 0]\n",
    "    \n",
    "    # Running the KS test\n",
    "    _, p_value = ks_2samp(data_1_column, data_2_column)\n",
    "    \n",
    "    return p_value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_var = 'Percent_Cognitive_Improvement'\n",
    "independent_var1 = 'Age'\n",
    "independent_var2 = 'Subiculum_Connectivity'\n",
    "num_resamples=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "saddle_finder = SaddlePointFinder(dependent_var=dependent_var, independent_var1=independent_var1, independent_var2=independent_var2, num_resamples=num_resamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a Single Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = saddle_finder.resample_and_find_saddle_points(data_df)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Multiple Dataframes\n",
    "- Enter as many as you want into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = data_df[data_df['Cohort'] == 1]\n",
    "df_2 = data_df[data_df['Cohort'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:35<00:00, 278.49it/s]\n",
      "100%|██████████| 10000/10000 [00:35<00:00, 283.88it/s]\n"
     ]
    }
   ],
   "source": [
    "df_list = [df_1, df_2]\n",
    "results = saddle_finder.run_multiple_dfs(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Subiculum_Connectivity'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results[1].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_label = 'Alzheimer'\n",
    "df_2_label = 'Parkinson'\n",
    "value_of_interest = 'Age'\n",
    "y_label = 'Age'\n",
    "x_label = 'Disease'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age    79.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].loc[:, [value_of_interest]].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot 95% CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved to: /Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/analyses/saddle_point/95CI_figures/Disease_Age\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4.446140841067589e-112"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = {df_1_label: results[0].loc[:, [value_of_interest]], df_2_label: results[1].loc[:, [value_of_interest]]}\n",
    "plot_95CI(df_dict=df_dict, x_label=x_label, y_label=y_label, out_dir='/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/analyses/saddle_point', colours=['red', 'blue'])\n",
    "run_ks_test(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Distribution and Test Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].loc[:, [value_of_interest]].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Mann-Whitney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value annotation legend:\n",
      "      ns: p <= 1.00e+00\n",
      "       *: 1.00e-02 < p <= 5.00e-02\n",
      "      **: 1.00e-03 < p <= 1.00e-02\n",
      "     ***: 1.00e-04 < p <= 1.00e-03\n",
      "    ****: p <= 1.00e-04\n",
      "\n",
      "Alzheimer vs. Parkinson: t-test independent samples with Bonferroni correction, P_val:2.138e-28 t=-1.107e+01\n",
      "Figure saved to: /Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/analyses/saddle_point/distribution_figures/Disease_t-test_ind_Age\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 650x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_dict = {df_1_label: results[0].loc[:, [value_of_interest]], df_2_label: results[1].loc[:, [value_of_interest]]}\n",
    "saddle_finder.perform_t_tests_and_plots(df_dict=df_dict, x_label=x_label, y_label=y_label, out_dir='/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/analyses/saddle_point', colours=['red', 'blue'], test_type='t-test_ind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See If Two Regressions Are Significantly Different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (if you haven't already)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RegressionComparison:\n",
    "    def __init__(self, df, dependent_var, independent_vars, split_var, split_value, constant_value_for_second_predictor=np.nan):\n",
    "        \"\"\"\n",
    "        Initialize the RegressionComparison object with data and variable names.\n",
    "        \"\"\"\n",
    "        self.df1 = df[df[split_var] <= split_value]\n",
    "        self.df2 = df[df[split_var] > split_value]\n",
    "        self.dependent_var = dependent_var\n",
    "        self.independent_vars = independent_vars\n",
    "        self.formula = f\"{dependent_var} ~ {' + '.join(independent_vars)}\"\n",
    "        self.constant_value_for_second_predictor=constant_value_for_second_predictor\n",
    "        if self.df1.empty or self.df2.empty:\n",
    "            raise ValueError(\"One of the datasets is empty after the split. Check your split variable and value.\")\n",
    "    \n",
    "    def fit_models(self):\n",
    "        \"\"\"\n",
    "        Fit OLS regression models for each dataset.\n",
    "        \"\"\"\n",
    "        self.model1 = smf.ols(formula=self.formula, data=self.df1).fit()\n",
    "        self.model2 = smf.ols(formula=self.formula, data=self.df2).fit()\n",
    "        \n",
    "    def create_evaluation_dataframe(self, n_points=100):\n",
    "        \"\"\"\n",
    "        Create a DataFrame for evaluation with different ranges for the first predictor\n",
    "        and a constant value for the second predictor.\n",
    "        \n",
    "        :param constant_value_for_second_predictor: The constant value to set for the second predictor.\n",
    "        :param n_points: Number of points where the model will be evaluated.\n",
    "        :return: Two DataFrames for evaluation, one for each dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # For the first dataset\n",
    "        x_values_df1 = np.linspace(min(self.df1[self.independent_vars[0]]), \n",
    "                                max(self.df1[self.independent_vars[0]]), n_points)\n",
    "        eval_df1 = pd.DataFrame({self.independent_vars[0]: x_values_df1})\n",
    "        eval_df1[self.independent_vars[1]] = self.constant_value_for_second_predictor\n",
    "        \n",
    "        # For the second dataset\n",
    "        x_values_df2 = np.linspace(min(self.df2[self.independent_vars[0]]), \n",
    "                                max(self.df2[self.independent_vars[0]]), n_points)\n",
    "        eval_df2 = pd.DataFrame({self.independent_vars[0]: x_values_df2})\n",
    "        eval_df2[self.independent_vars[1]] = self.constant_value_for_second_predictor\n",
    "        \n",
    "        return eval_df1, eval_df2\n",
    "\n",
    "\n",
    "    def calculate_observed_delta(self, n_points=100):\n",
    "        \"\"\"\n",
    "        Calculate the observed delta between the two regression models for 100 datapoints.\n",
    "        \"\"\"\n",
    "        self.x_df1, self.x_df2 = self.create_evaluation_dataframe()\n",
    "\n",
    "        pred1 = self.model1.predict(self.x_df1)\n",
    "        pred2 = self.model2.predict(self.x_df2)\n",
    "\n",
    "        self.observed_delta = np.sum(pred1 - pred2)\n",
    "        \n",
    "\n",
    "    def permute_outcomes(self):\n",
    "        \"\"\"Permute the dependent variable in df1 and df2.\"\"\"\n",
    "        permuted_y1 = np.random.permutation(self.df1[self.dependent_var].values)\n",
    "        permuted_y2 = np.random.permutation(self.df2[self.dependent_var].values)\n",
    "\n",
    "        perm_df1 = self.df1.copy()\n",
    "        perm_df2 = self.df2.copy()\n",
    "\n",
    "        perm_df1[self.dependent_var] = permuted_y1\n",
    "        perm_df2[self.dependent_var] = permuted_y2\n",
    "\n",
    "        return perm_df1, perm_df2\n",
    "\n",
    "\n",
    "    def calculate_empirical_delta(self, n_permutations=1000, n_points=100):\n",
    "        \"\"\"Calculate the empirical delta using permuted data.\"\"\"\n",
    "        self.empirical_deltas = []\n",
    "\n",
    "        for _ in tqdm(range(n_permutations)):\n",
    "            # Step 1: Permute the outcomes\n",
    "            perm_df1, perm_df2 = self.permute_outcomes()\n",
    "\n",
    "            # Step 2: Fit models on permuted data\n",
    "            model1 = smf.ols(formula=self.formula, data=perm_df1).fit()\n",
    "            model2 = smf.ols(formula=self.formula, data=perm_df2).fit()\n",
    "\n",
    "            # Step 3: Create evaluation DataFrames for permuted data\n",
    "            x_df1, x_df2 = self.create_evaluation_dataframe(n_points)\n",
    "\n",
    "            # Step 4: Make predictions on evaluation DataFrames\n",
    "            pred1 = model1.predict(x_df1)\n",
    "            pred2 = model2.predict(x_df2)\n",
    "\n",
    "            # Step 5: Calculate and store empirical delta\n",
    "            empirical_delta = np.sum(pred1 - pred2)\n",
    "            self.empirical_deltas.append(empirical_delta)\n",
    "\n",
    "    def calculate_significance(self):\n",
    "        \"\"\"\n",
    "        Calculate the p-value based on empirical deltas.\n",
    "        \"\"\"\n",
    "        mask = self.empirical_deltas >= self.observed_delta\n",
    "        self.p_value = np.mean(mask)\n",
    "        \n",
    "        return self.p_value\n",
    "\n",
    "    def run(self, n_permutations=1000, n_points=100):\n",
    "        \"\"\"\n",
    "        Orchestrates the complete pipeline.\n",
    "        \n",
    "        :param n_permutations: Number of permutations for empirical delta calculation.\n",
    "        :param n_points: Number of points to use in observed delta calculation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Fit the models\n",
    "        self.fit_models()\n",
    "        \n",
    "        # Step 2: Create evaluation DataFrames\n",
    "        self.x_df1, self.x_df2 = self.create_evaluation_dataframe(n_points)\n",
    "        \n",
    "        # Step 3: Calculate observed delta\n",
    "        self.calculate_observed_delta(n_points)\n",
    "        \n",
    "        # Step 4: Calculate empirical delta\n",
    "        self.calculate_empirical_delta(n_permutations)\n",
    "        \n",
    "        # Step 5: Calculate significance\n",
    "        p_value = self.calculate_significance()\n",
    "        \n",
    "        return p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punch In Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables based on your specific case\n",
    "dependent_var = 'Z_Scored_Cognitive_Improvement_By_Group'\n",
    "independent_vars = ['Subiculum_Connectivity', 'Age']\n",
    "split_var = 'Cohort'\n",
    "split_value = 0.5\n",
    "value_to_set_to = 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class with the variables\n",
    "reg_comp = RegressionComparison(df=data_df, \n",
    "                                dependent_var=dependent_var, \n",
    "                                independent_vars=independent_vars, \n",
    "                                split_var=split_var, \n",
    "                                split_value=split_value, \n",
    "                                constant_value_for_second_predictor=value_to_set_to)\n",
    "\n",
    "# Fit the models\n",
    "p = reg_comp.run()\n",
    "print(\"Significance Count:\", p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Fancy Response Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.import_matrices import import_matrices_from_folder\n",
    "from calvin_utils.generate_nifti import nifti_from_matrix\n",
    "from nimlab import datasets as nimds\n",
    "import numpy as np\n",
    "from nilearn import image, plotting, maskers\n",
    "\n",
    "#Name variables to plot\n",
    "var_one = 'Limbic'\n",
    "var_two = 'Ventral_Attention'\n",
    "val_var_two = 65.4\n",
    "\n",
    "#get conectivity values of interest\n",
    "matrix_path = '/Users/cu135/Dropbox (Partners HealthCare)/memory/functional_networks/seed_generated_networks/yeo_networks_from_thick_yeo_seeds/all_05_limbic_T.nii'\n",
    "matrix = import_matrices_from_folder(matrix_path, file_pattern='')\n",
    "\n",
    "#get conectivity values of interest\n",
    "matrix_path_2 = '/Users/cu135/Dropbox (Partners HealthCare)/memory/functional_networks/seed_generated_networks/yeo_networks_from_thick_yeo_seeds/all_04_ventral_attention_T.nii'\n",
    "matrix_2 = import_matrices_from_folder(matrix_path, file_pattern='')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------DO NOT MODIFY!------------------------------------------------------------------------------------------------\n",
    "#Set reression input values into a dataframe\n",
    "input_df = pd.DataFrame()\n",
    "input_df[var_one] = matrix.iloc[:,0]\n",
    "input_df[var_two] = matrix_2.iloc[:,0]\n",
    "\n",
    "from nimlab import datasets as nimds\n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_data = mni_mask.get_fdata().flatten()\n",
    "brain_indices = np.where(mask_data > 0)[0]\n",
    "input_df = input_df.iloc[brain_indices, :]\n",
    "\n",
    "#Standardize matrix via z score\n",
    "from calvin_utils.z_score_matrix import z_score_matrix\n",
    "for col in input_df.columns:\n",
    "    input_df.loc[:, col] = z_score_matrix(input_df.loc[:, col])\n",
    "\n",
    "#Display results\n",
    "display(input_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "#Generate a new matrix from the input dataframe\n",
    "responses = []\n",
    "\n",
    "#Work on a voxel-wise basis by iterating over index\n",
    "for i in tqdm.tqdm(range(0, len(input_df))):\n",
    "    #Do not calculate on zero values as intercept will be applied in the regressoin\n",
    "    if np.sum(input_df.iloc[i,0]) != 0:\n",
    "        #Assign a temporary dataframe with values that the statsmodels model is expecting\n",
    "        temp_df = pd.DataFrame({var_one: input_df.iloc[i,0], var_two: input_df.iloc[i,1]}, index=['temp_vals'])\n",
    "        #Calculate the voxelwise predicted outcome at a given voxel\n",
    "        responses.append(results.predict(temp_df)[0])\n",
    "    else:\n",
    "        #If voxel is zero-connectivity, assign zero so as to avoid application of intercept\n",
    "        responses.append(0)\n",
    "        \n",
    "#Store responses in a dataframe\n",
    "response_df = pd.DataFrame()\n",
    "response_df['response_topology'] = responses\n",
    "display(response_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Place back in brain mask\n",
    "mask_data[brain_indices] = response_df.loc[:, 'response_topology']\n",
    "response_toplogy = mask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.generate_nifti import view_and_save_nifti\n",
    "matrix_img = view_and_save_nifti(response_toplogy, out_dir)\n",
    "matrix_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Driven Search for Most Relevant Regressors\n",
    "Combines:\n",
    "- All possible subsets regression combined with leave-one-out cross validation\n",
    "- Identifies formula with lowest RMSE and highest Pearson Correlation of predictor to observed outcome values\n",
    "- Presents the optimal formulas and allows selection of which formula to interpret with structural analysis and coefficient analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_interaction_terms(df, predictors, max_interaction_level=3):\n",
    "    interaction_df = df[predictors].copy()\n",
    "\n",
    "    def add_interaction_terms(df, columns, interaction_level):\n",
    "        if interaction_level == 1:\n",
    "            return df\n",
    "\n",
    "        for col_combination in itertools.combinations(columns, interaction_level):\n",
    "            interaction_term = ':'.join(col_combination)\n",
    "            interaction_values = np.prod(df[list(col_combination)], axis=1)\n",
    "            df[interaction_term] = interaction_values\n",
    "\n",
    "        return add_interaction_terms(df, columns, interaction_level - 1)\n",
    "\n",
    "    return add_interaction_terms(interaction_df, predictors, max_interaction_level)\n",
    "\n",
    "def loocv_regression(df, interaction_df, formula, outcome_var):\n",
    "    full_df = interaction_df.copy()\n",
    "    full_df[outcome_var] = df[outcome_var]\n",
    "    model = sm.formula.ols(formula, data=full_df).fit()\n",
    "    y_actual, y_predicted = [], []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_test = interaction_df.iloc[[i]]\n",
    "        x_train = interaction_df.drop(i)\n",
    "        y_train = df[outcome_var].drop(i)\n",
    "        \n",
    "        model_train_df = x_train.join(y_train)\n",
    "        model_train = sm.formula.ols(formula, data=model_train_df).fit()\n",
    "        y_actual.append(df[outcome_var].iloc[i])\n",
    "        y_predicted.append(model_train.predict(x_test).values[0])\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    corr, _ = pearsonr(y_actual, y_predicted)\n",
    "    return rmse, corr\n",
    "\n",
    "def all_subsets_regression(df, outcome_var):\n",
    "    predictor_columns = [col for col in df.columns if col != outcome_var]\n",
    "    interaction_df = generate_interaction_terms(df, predictor_columns, max_interaction_level=3)\n",
    "\n",
    "    results = []\n",
    "    for n_predictors in tqdm(range(1, len(predictor_columns) + 1)):\n",
    "        for predictor_combination in itertools.combinations(predictor_columns, n_predictors):\n",
    "            formula = f\"{outcome_var} ~ {' + '.join(predictor_combination)}\"\n",
    "            loocv_rmse, loocv_corr = loocv_regression(df, interaction_df, formula, outcome_var)\n",
    "\n",
    "            results.append({\n",
    "                'formula': formula,\n",
    "                'rmse': loocv_rmse,\n",
    "                'correlation': loocv_corr\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def all_subsets_regression_no_loocv(df, outcome_var):\n",
    "    predictor_columns = [col for col in df.columns if col != outcome_var]\n",
    "\n",
    "    results = []\n",
    "    for n_predictors in tqdm(range(1, len(predictor_columns) + 1)):\n",
    "        for predictor_combination in itertools.combinations(predictor_columns, n_predictors):\n",
    "            formula = f\"{outcome_var} ~ {' + '.join(predictor_combination)}\"\n",
    "            model = sm.formula.ols(formula, data=df).fit()\n",
    "\n",
    "            y_actual = df[outcome_var].values\n",
    "            y_predicted = model.predict(df[predictor_columns])\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "            corr, _ = pearsonr(y_actual, y_predicted)\n",
    "\n",
    "            results.append({\n",
    "                'formula': formula,\n",
    "                'rmse': rmse,\n",
    "                'correlation': corr\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def all_subsets_regression(df, outcome_var, limit_to_two_way_interactions=False, max_predictors=None):\n",
    "    predictor_columns = [col for col in df.columns if col != outcome_var]\n",
    "\n",
    "    results = []\n",
    "    for n_predictors in tqdm(range(1, min(max_predictors + 1, len(predictor_columns) + 1))):\n",
    "        for predictor_combination in itertools.combinations(predictor_columns, n_predictors):\n",
    "            # Check whether to limit interactions to two-way\n",
    "            if limit_to_two_way_interactions and n_predictors > 1:\n",
    "                # Generate combinations for two-way interactions\n",
    "                interaction_combinations = itertools.combinations(predictor_combination, 2)\n",
    "                interaction_terms = [' : '.join(interaction) for interaction in interaction_combinations]\n",
    "                # Include both individual predictors and interaction terms\n",
    "                formula = f\"{outcome_var} ~ {' + '.join(predictor_combination)} + {' + '.join(interaction_terms)}\"\n",
    "            else:\n",
    "                # Original formula generation with all possible interactions\n",
    "                formula = f\"{outcome_var} ~ {' * '.join(predictor_combination)}\"\n",
    "            \n",
    "            print(formula)\n",
    "            loocv_rmse, loocv_corr = loocv_regression(df, formula, outcome_var)\n",
    "\n",
    "            results.append({\n",
    "                'formula': formula,\n",
    "                'rmse': loocv_rmse,\n",
    "                'correlation': loocv_corr\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def loocv_regression(df, formula, outcome_var):\n",
    "    model = sm.formula.ols(formula, data=df).fit()\n",
    "    y_actual, y_predicted = [], []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        index_label = df.index[i]\n",
    "        x_test = df.iloc[[i]].drop(outcome_var, axis=1)\n",
    "        x_train = df.drop(index_label).drop(outcome_var, axis=1)\n",
    "        y_train = df[outcome_var].drop(index_label)\n",
    "\n",
    "        model_train_df = x_train.join(y_train)\n",
    "        model_train = sm.formula.ols(formula, data=model_train_df).fit()\n",
    "        y_actual.append(df[outcome_var].iloc[i])\n",
    "        y_predicted.append(model_train.predict(x_test).values[0])\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    corr, _ = pearsonr(y_actual, y_predicted)\n",
    "    return rmse, corr\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# outcome_var = 'your_outcome_variable'\n",
    "# results_df = all_subsets_regression(df, outcome_var)\n",
    "# print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_var = 'outcome'\n",
    "limit_to_two_way_interactions = False\n",
    "regressor_limit = 3\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "df = data_df.copy()\n",
    "results_df = all_subsets_regression(df, outcome_var, limit_to_two_way_interactions=limit_to_two_way_interactions, max_predictors=regressor_limit)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 20 formulas by Correlation of y-hat to y')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 20 formulas by RMSE of y-hat to y')\n",
    "# Sort DataFrame by 'rmse' in ascending order\n",
    "rmse_df = results_df.copy().sort_values(by='rmse', ascending=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "rmse_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing top 5 formulas\n",
    "\n",
    "top_five_formulas = results_df.head(5)['formula']\n",
    "print(\"Top 5 Formulas:\")\n",
    "print(top_five_formulas)\n",
    "\n",
    "\n",
    "print('---Lowest RMSE Information---')\n",
    "index_of_min_rmse = results_df['rmse'].idxmin()\n",
    "print(results_df.loc[index_of_min_rmse, 'formula'])\n",
    "print('RMSE: ',results_df.loc[index_of_min_rmse, 'rmse'])\n",
    "print('Pearson R: ', results_df.loc[index_of_min_rmse, 'correlation'])\n",
    "print('Formula index: ', index_of_min_rmse)\n",
    "\n",
    "print('\\n---Highest Correlation Information---')\n",
    "index_of_max_corr = results_df['correlation'].idxmax()\n",
    "print(results_df.loc[index_of_max_corr, 'formula'])\n",
    "print('RMSE: ',results_df.loc[index_of_max_corr, 'rmse'])\n",
    "print('Pearson R: ', results_df.loc[index_of_max_corr, 'correlation'])\n",
    "print('Formula index: ', index_of_max_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "formula_to_test = 6\n",
    "#----------------------------------------------------------------\n",
    "results = smf.ols(results_df.loc[formula_to_test, 'formula'], data=df).fit()\n",
    "# results = smf.ols('percent_change_adascog11 ~ Subiculum*Age*total_hippocampal_csf*frontal', data=data_df).fit()\n",
    "\n",
    "\n",
    "print(results.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import model_diagnostics\n",
    "model_diagnostics(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Optimal Interaction Subset of Optimal Model by Backward Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import itertools\n",
    "\n",
    "\n",
    "def backward_subsets_loocv_regression(df, formula):\n",
    "    # Extract outcome_var from the formula\n",
    "    outcome_var = formula.split(' ~ ')[0].strip()\n",
    "    \n",
    "    y_actual, y_predicted = [], []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        index_label = df.index[i]\n",
    "        x_test = df.iloc[[i]].drop(outcome_var, axis=1)\n",
    "        x_train = df.drop(index_label).drop(outcome_var, axis=1)\n",
    "        y_train = df[outcome_var].drop(index_label)\n",
    "\n",
    "        model_train_df = x_train.join(y_train)\n",
    "        model_train = smf.ols(formula, data=model_train_df).fit()\n",
    "        y_actual.append(df[outcome_var].iloc[i])\n",
    "        y_predicted.append(model_train.predict(x_test).values[0])\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    corr, _ = pearsonr(y_actual, y_predicted)\n",
    "    return rmse, corr\n",
    "\n",
    "\n",
    "def serial_interaction_removal_loocv(df, formula):\n",
    "    # Split the formula into the left and right side\n",
    "    left, right = formula.split(\"~\")\n",
    "    \n",
    "    # Get all the terms from the right side of the formula\n",
    "    terms = right.strip().split(\"*\")\n",
    "    terms = [term.strip() for term in terms]\n",
    "    \n",
    "    # Generate all combinations of + and *\n",
    "    operators_combinations = itertools.product([\"+\", \"*\"], repeat=len(terms)-1)\n",
    "    \n",
    "    # Generate the formulas\n",
    "    formulas = []\n",
    "    for operators in operators_combinations:\n",
    "        combined_formula = left + \" ~ \" + terms[0]\n",
    "        for op, term in zip(operators, terms[1:]):\n",
    "            combined_formula += \" \" + op + \" \" + term\n",
    "        formulas.append(combined_formula.strip())\n",
    "\n",
    "    # Run LOOCV on each formula\n",
    "    loocv_results = []\n",
    "    for sub_formula in formulas:\n",
    "        loocv_rmse, loocv_corr = backward_subsets_loocv_regression(df, sub_formula)\n",
    "        loocv_results.append({\n",
    "            'formula': sub_formula,\n",
    "            'rmse': loocv_rmse,\n",
    "            'correlation': loocv_corr\n",
    "        })\n",
    "    \n",
    "    # Convert results to DataFrame, sort by correlation and reset index\n",
    "    loocv_results_df = pd.DataFrame(loocv_results)\n",
    "    loocv_results_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "    loocv_results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return loocv_results_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `df` is your DataFrame, and `formula` is a string like \"y ~ a * b * c\"\n",
    "# best_formula = \"y ~ a * b * c\"\n",
    "# serial_loocv_df = serial_interaction_removal_loocv(df, best_formula)\n",
    "# print(serial_loocv_df)\n",
    "\n",
    "best_formula = results_df.loc[formula_to_test, 'formula']\n",
    "optimal_interaction_loocv_df = serial_interaction_removal_loocv(df, best_formula)\n",
    "optimal_interaction_loocv_df.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Coefficient Analysis of Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "def structural_coefficients(model, data):\n",
    "    # Extracting the predictor names\n",
    "    predictor_names = model.model.exog_names\n",
    "    if 'Intercept' in predictor_names:\n",
    "        predictor_names.remove('Intercept')\n",
    "\n",
    "    # Calculating the predicted values\n",
    "    y_predicted = model.predict(data)\n",
    "\n",
    "    # Creating a temporary dataframe to store interaction terms\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    for pred in predictor_names:\n",
    "        # Check if the predictor is an interaction term\n",
    "        if ':' in pred:\n",
    "            # Split the interaction term into its components\n",
    "            components = pred.split(':')\n",
    "            # Multiply the components and store the result in the temporary dataframe\n",
    "            temp_product = data[components[0]]\n",
    "            for component in components[1:]:\n",
    "                temp_product *= data[component]\n",
    "            temp_df[pred] = temp_product\n",
    "        else:\n",
    "            temp_df[pred] = data[pred]\n",
    "\n",
    "    # Calculating the structural coefficients\n",
    "    structural_coefs = {}\n",
    "    for pred in predictor_names:\n",
    "        coef, _ = pearsonr(temp_df[pred], y_predicted)\n",
    "        structural_coefs[pred] = np.square(coef)\n",
    "\n",
    "    # Creating a dataframe to store the results\n",
    "    structural_coefs_df = pd.DataFrame(structural_coefs.items(), columns=['predictor', 'structural_coefficient'])\n",
    "\n",
    "    # Sorting the dataframe by the structural coefficients in descending order\n",
    "    structural_coefs_df.sort_values(by='structural_coefficient', ascending=False, inplace=True)\n",
    "    structural_coefs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return structural_coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "def structural_coefficients(model: statsmodels.regression.linear_model.RegressionResultsWrapper, \n",
    "                            data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function calculates the structural coefficients of a linear regression model, along with their \n",
    "    associated model coefficients (beta weights). It also computes a 'suppressor index' which indicates \n",
    "    the likelihood of a variable being a suppressor variable (high beta weight, near-zero structural coefficient).\n",
    "    A suppressor index over 10 is a good heuristic for identifying a suppressor variable\n",
    "    \n",
    "    Parameters:\n",
    "    model (statsmodels.regression.linear_model.RegressionResultsWrapper): The fitted linear regression model.\n",
    "    data (pd.DataFrame): The dataset used in the model.\n",
    "    \n",
    "    Returns:\n",
    "    structural_coefs_df (pd.DataFrame): A dataframe containing the predictors, their structural coefficients,\n",
    "                                        model coefficients (beta weights), and suppressor index.\n",
    "                                        \n",
    "                                        If the sum of the structure coefficients is higher than 1, they are correlated (multicollinear)\n",
    "    \"\"\"\n",
    "    # Calculating the predicted values\n",
    "    y_predicted = model.predict(data)\n",
    "\n",
    "    # Creating a temporary dataframe to store interaction terms\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    # Calculating the structural coefficients\n",
    "    structural_coefs = {}\n",
    "    for pred in model.params.index:\n",
    "        if pred == 'Intercept':\n",
    "            continue\n",
    "\n",
    "        # Check if the predictor is an interaction term\n",
    "        if ':' in pred:\n",
    "            # Split the interaction term into its components\n",
    "            components = pred.split(':')\n",
    "            # Multiply the components and store the result in the temporary dataframe\n",
    "            temp_product = data[components[0]]\n",
    "            for component in components[1:]:\n",
    "                temp_product *= data[component]\n",
    "            temp_df[pred] = temp_product\n",
    "        else:\n",
    "            temp_df[pred] = data[pred]\n",
    "        \n",
    "        coef, _ = pearsonr(temp_df[pred], y_predicted)\n",
    "        structural_coefs[pred] = np.square(coef)\n",
    "\n",
    "    # Calculating the model coefficients\n",
    "    model_coefs = model.params.drop('Intercept')\n",
    "\n",
    "    # Creating a dataframe to store the results\n",
    "    structural_coefs_df = pd.DataFrame(list(zip(structural_coefs.keys(), structural_coefs.values(), model_coefs.values)), \n",
    "                                       columns=['predictor', 'structural_coefficient', 'model_coefficient'])\n",
    "\n",
    "    # Adding suppressor index column\n",
    "    structural_coefs_df['suppressor_index'] = structural_coefs_df['model_coefficient'].abs() / structural_coefs_df['structural_coefficient']\n",
    "\n",
    "    # Sorting the dataframe by the structural coefficients in descending order\n",
    "    structural_coefs_df.sort_values(by='structural_coefficient', ascending=False, inplace=True)\n",
    "    structural_coefs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return structural_coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_structural_coefs_df = structural_coefficients(results, df.copy())\n",
    "squared_structural_coefs_df.to_csv(os.path.join(out_dir, 'structural_coefficient_analysis.csv'))\n",
    "print('saved to: ', os.path.join(out_dir, 'structural_coefficient_analysis.csv'))\n",
    "display(squared_structural_coefs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the sum of the structure coefficients is higher than 1, they are correlated\n",
    "print(squared_structural_coefs_df.loc[:, 'structural_coefficient'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary2())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check optimal model without LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "no_loocv_results_df = all_subsets_regression_no_loocv(df, outcome_var)\n",
    "no_loocv_results_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing top 5 formulas\n",
    "no_loocv_top_five_formulas = no_loocv_results_df.head(10)['formula']\n",
    "print(\"Top 5 Formulas:\")\n",
    "print(no_loocv_top_five_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "formula_to_test = 0\n",
    "#----------------------------------------------------------------\n",
    "results = smf.ols(no_loocv_results_df.loc[formula_to_test, 'formula'], data=df).fit()\n",
    "print(results.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Out Consistently High-Performing Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class ModelPerformanceClusterRefined:\n",
    "    def __init__(self, data, rmse_threshold, pearsonr_threshold, normative_thresholding=False):\n",
    "        self.data = data\n",
    "        self.metric_df = None\n",
    "        self.rmse_threshold = rmse_threshold\n",
    "        self.pearsonr_threshold = pearsonr_threshold\n",
    "        self.normative_thresholding=normative_thresholding\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Remove rows with RMSE values more than 2 standard deviations from the mean.\n",
    "        \"\"\"\n",
    "        if self.normative_thresholding:\n",
    "            rmse_mean = self.data['rmse'].mean()\n",
    "            rmse_std = self.data['rmse'].std()\n",
    "            corr_mean = self.data['correlation'].mean()\n",
    "            corr_std = self.data['correlation'].std()\n",
    "            self.data = self.data[~(self.data['rmse'] > rmse_mean + 2*rmse_std)]\n",
    "            self.data = self.data[~(self.data['correlation'] < corr_mean - 2*corr_std)]\n",
    "        else:\n",
    "            self.data = self.data[~(self.data['rmse'] > self.rmse_threshold)]\n",
    "            self.data = self.data[~(self.data['correlation'] < self.pearsonr_threshold)]\n",
    "\n",
    "    def extract_predictors_metrics(self):\n",
    "        \"\"\"\n",
    "        Extract predictors from regression formula strings and relate them to RMSE and Pearson R.\n",
    "        \"\"\"\n",
    "        # Placeholder for storing extracted data\n",
    "        extracted_data = []\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            # Splitting to get the predictors\n",
    "            terms = row['formula'].split(\"~\")[1].strip().split(\" + \")\n",
    "            \n",
    "            predictors = []\n",
    "            for term in terms:\n",
    "                # Check for interaction terms\n",
    "                if '*' in term:\n",
    "                    predictors.extend(term.split('*'))\n",
    "                else:\n",
    "                    predictors.append(term)\n",
    "\n",
    "            for predictor in predictors:\n",
    "                extracted_data.append({\n",
    "                    'Predictor': predictor.strip(),\n",
    "                    'RMSE': row['rmse'],\n",
    "                    'PearsonR': row['correlation']\n",
    "                })\n",
    "\n",
    "        self.metric_df = pd.DataFrame(extracted_data)\n",
    "\n",
    "    def apply_hierarchical_clustering(self, n_clusters=3):\n",
    "        \"\"\"\n",
    "        Perform Agglomerative Hierarchical Clustering on RMSE and Pearson R.\n",
    "        \"\"\"\n",
    "        # Extract the metric values for clustering\n",
    "        X = self.metric_df[[\"RMSE\", \"PearsonR\"]].values\n",
    "\n",
    "        # Apply Hierarchical Clustering\n",
    "        cluster = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n",
    "        self.metric_df['Cluster_Label'] = cluster.fit_predict(X)\n",
    "\n",
    "        return self.metric_df['Cluster_Label']\n",
    "\n",
    "    def visualize_scatter(self):\n",
    "        \"\"\"\n",
    "        Visualize a scatter plot of Pearson R vs. RMSE, colored by the cluster label.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.scatterplot(data=self.metric_df, x='PearsonR', y='RMSE', hue='Cluster_Label', palette='tab10', s=100, alpha=0.7)\n",
    "        plt.title(\"Scatter Plot of Pearson R vs. RMSE\")\n",
    "        plt.legend(title='Cluster Label', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def predictors_in_cluster(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Identify and return the unique predictors associated with a specific cluster.\n",
    "        \"\"\"\n",
    "        # Filter the metric_df by the given cluster label\n",
    "        cluster_data = self.metric_df[self.metric_df['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        # Return unique predictors\n",
    "        return cluster_data['Predictor'].unique()\n",
    "\n",
    "\n",
    "# The class has been updated with the preprocess_data method. You can use this refined class in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = 6\n",
    "RMSE_threshold = 1.2\n",
    "R_threshold = 0.2\n",
    "normative_thresholding=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the use of the Hierarchical clustering with the sample dataframe\n",
    "cluster_obj_hierarchical = ModelPerformanceClusterRefined(results_df, RMSE_threshold, R_threshold,normative_thresholding)\n",
    "cluster_obj_hierarchical.preprocess_data()\n",
    "cluster_obj_hierarchical.extract_predictors_metrics()\n",
    "cluster_obj_hierarchical.apply_hierarchical_clustering(n_clusters=number_clusters)\n",
    "cluster_obj_hierarchical.visualize_scatter()\n",
    "cluster_obj_hierarchical.predictors_in_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_obj_hierarchical.predictors_in_cluster(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class ModelPerformanceClusterFinal(ModelPerformanceClusterRefined):\n",
    "    \n",
    "    def compute_r_squared(self, data_df):\n",
    "        \"\"\"\n",
    "        Compute R-squared for each regression formula on data_df.\n",
    "        \"\"\"\n",
    "        adj_r_squared_values = []\n",
    "        r_squared_values = []\n",
    "        bic_values = []\n",
    "        aic_values = []\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            formula = row['formula']\n",
    "            model = smf.ols(formula=formula, data=data_df).fit()\n",
    "            r_squared_values.append(model.rsquared)\n",
    "            adj_r_squared_values.append(model.rsquared_adj)\n",
    "            bic_values.append(model.aic)\n",
    "            aic_values.append(model.bic)\n",
    "\n",
    "        self.data['r_squared'] = r_squared_values\n",
    "        self.data['adj_r_squared'] = adj_r_squared_values\n",
    "        self.data['bic'] = bic_values\n",
    "        self.data['aic'] = aic_values\n",
    "\n",
    "    def apply_agglomerative_clustering(self, n_clusters=5):\n",
    "        \"\"\"\n",
    "        Perform Agglomerative Clustering considering R-squared, LOOCV Pearson R, and LOOCV RMSE.\n",
    "        \"\"\"\n",
    "        # Extract the metric values for clustering\n",
    "        X = self.data[[\"adj_r_squared\", \"correlation\", \"rmse\"]].values\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        # Apply Agglomerative Clustering\n",
    "        cluster = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward').fit(X)\n",
    "        self.data['Cluster_Label'] = cluster.labels_\n",
    "        \n",
    "    def visualize_3D_scatter(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize a 3D scatter plot of R-squared vs. Pearson R vs. RMSE, colored by Cluster label and shape by number of predictors.\n",
    "        \"\"\"\n",
    "        \n",
    "        def count_predictors(formula):\n",
    "            right_side = formula.split(\"~\")[1].strip()\n",
    "\n",
    "            # Check for both '*' and '+' in the formula\n",
    "            if '*' in right_side and '+' in right_side:\n",
    "                terms = []\n",
    "                for term in right_side.split('+'):\n",
    "                    terms.extend(term.split('*'))\n",
    "                return len(terms)\n",
    "\n",
    "            # Check for '*' in the formula\n",
    "            elif '*' in right_side:\n",
    "                return len(right_side.split('*'))\n",
    "\n",
    "            # Check for '+' in the formula\n",
    "            elif '+' in right_side:\n",
    "                return len(right_side.split('+'))\n",
    "\n",
    "            # If neither exists, it means there's only one predictor\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "        # Calculate the number of predictors for each formula\n",
    "        self.data['num_predictors'] = self.data['formula'].apply(count_predictors)\n",
    "        \n",
    "        # Map the number of predictors to a specific symbol\n",
    "        self.data['symbol'] = self.data['num_predictors'].map({\n",
    "            1: 'circle',\n",
    "            2: 'square',\n",
    "            3: 'diamond',\n",
    "            4: 'cross'\n",
    "        })\n",
    "        # Assign a default symbol for formulas with more than 4 predictors\n",
    "        self.data['symbol'].fillna('cross', inplace=True)\n",
    "        \n",
    "        fig = px.scatter_3d(self.data, x='r_squared', y='correlation', z='rmse',\n",
    "                            color='Cluster_Label', opacity=0.7, hover_name='formula',\n",
    "                            hover_data=['adj_r_squared', 'bic', 'aic'], symbol='symbol',\n",
    "                            labels={'adj_r_squared': 'Adj. R-Squared', 'r_squared': 'R-Squared', 'correlation': 'LOOCV Pearson R', 'rmse': 'LOOCV RMSE'},\n",
    "                            color_continuous_scale=px.colors.sequential.Viridis)\n",
    "        fig.update_layout(title=\"3D Scatter Plot of R-Squared, Pearson R, and RMSE\", \n",
    "                        autosize=True, width=1200, height=800)\n",
    "        # fig.show()\n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            save_path = os.path.join(save_path, '3d_scatterplot.html')\n",
    "            fig.write_html(save_path)\n",
    "            fig.show()\n",
    "        else:\n",
    "            fig.show()\n",
    "        \n",
    "    def get_essential_data(self):\n",
    "        \"\"\"\n",
    "        Return a dataframe with formulae, R^2, RMSE, PearsonR, and cluster label.\n",
    "        \"\"\"\n",
    "        return self.data[['r_squared', 'rmse', 'correlation', 'formula', 'Cluster_Label']]\n",
    "\n",
    "# This final class integrates R-squared computation, preprocessing based on your refined approach, Agglomerative Clustering, and 3D scatter plot visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine Number of 3D Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the object\n",
    "num_clusters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------DO NOT TOUCH--\n",
    "cluster_obj_final = ModelPerformanceClusterFinal(results_df, rmse_threshold=RMSE_threshold, pearsonr_threshold=R_threshold, normative_thresholding=True)\n",
    "# Preprocess the data\n",
    "cluster_obj_final.preprocess_data()\n",
    "\n",
    "# Compute R-squared for all regression formulas in results_df using data_df\n",
    "cluster_obj_final.compute_r_squared(data_df)\n",
    "\n",
    "# Apply Agglomerative Clustering\n",
    "cluster_obj_final.apply_agglomerative_clustering(n_clusters=num_clusters)\n",
    "\n",
    "# Visualize the 3D scatter plot\n",
    "cluster_obj_final.visualize_3D_scatter(save_path=out_dir)\n",
    "cluster_df = cluster_obj_final.get_essential_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the Optimal Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import re \n",
    "\n",
    "class ModelPerformanceAnalysis:\n",
    "\n",
    "    def __init__(self, cluster_df):\n",
    "        self.data = cluster_df\n",
    "\n",
    "    def filter_by_cluster(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Filter the data by the specified cluster label.\n",
    "        \"\"\"\n",
    "        return self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "\n",
    "    def apply_pca(self):\n",
    "        \"\"\"\n",
    "        Apply PCA on performance metrics across all clusters after standardizing the data.\n",
    "        \"\"\"\n",
    "        pca_data = self.data[['r_squared', 'correlation']].copy()\n",
    "        pca_data['Inv_RMSE'] = 1 / self.data['rmse']\n",
    "        # pca_data['Inv_aic'] = 1 / self.data['aic']\n",
    "        # pca_data['Inv_bic'] = 1 / self.data['bic']\n",
    "\n",
    "        # Standardizing the data\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(pca_data)\n",
    "        \n",
    "        pca = PCA(n_components=1)\n",
    "        pca_result = pca.fit_transform(standardized_data)\n",
    "        \n",
    "        self.data['PCA_1'] = pca_result\n",
    "\n",
    "    def get_pca_dataframe(self):\n",
    "        \"\"\"\n",
    "        Return the dataframe with PCA values.\n",
    "        \"\"\"\n",
    "        return self.data\n",
    "\n",
    "    def visualize_pca_performance(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Visualize the first principal component against each predictor in the specified cluster.\n",
    "        \"\"\"\n",
    "        pca_cluster_data = self.filter_by_cluster(cluster_label)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='PCA_1', y='formula', data=pca_cluster_data.sort_values(by='PCA_1', ascending=False))\n",
    "        \n",
    "        plt.title(f\"Principal Component Analysis for Cluster {cluster_label}\")\n",
    "        plt.xlabel(\"First Principal Component\")\n",
    "        plt.ylabel(\"Formula\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def extract_predictors(self, data):\n",
    "        \"\"\"\n",
    "        Extract predictors from the 'formula' column of the given data.\n",
    "        \"\"\"\n",
    "        predictors_list = []\n",
    "        for formula in data['formula']:\n",
    "            terms = re.split(r' ~ | \\+ |\\*', formula)\n",
    "            predictors_list.extend(terms[1:])\n",
    "        # Strip white spaces and ensure uniqueness\n",
    "        return [predictor.strip() for predictor in predictors_list]\n",
    "\n",
    "\n",
    "    def visualize_predictor_incidence(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Visualize the incidence of each predictor within the specified cluster.\n",
    "        \"\"\"\n",
    "        cluster_data = self.filter_by_cluster(cluster_label)\n",
    "        \n",
    "        # Extract predictors from regression formula strings\n",
    "        predictors_list = self.extract_predictors(cluster_data)\n",
    "        predictors_series = pd.Series(predictors_list)\n",
    "        \n",
    "        # Count the incidence of each predictor\n",
    "        predictor_counts = predictors_series.value_counts()\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=predictor_counts.values, y=predictor_counts.index, color='skyblue')\n",
    "        \n",
    "        plt.title(f\"Predictor Incidence in Cluster {cluster_label}\")\n",
    "        plt.xlabel(\"Count\")\n",
    "        plt.ylabel(\"Predictor\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_predictor_average_pca(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Compute the average PCA score and standard error for each predictor within the specified cluster.\n",
    "        \"\"\"\n",
    "        cluster_data = self.filter_by_cluster(cluster_label)\n",
    "        \n",
    "        # A set to store unique predictors\n",
    "        unique_predictors = set(self.extract_predictors(cluster_data))\n",
    "        \n",
    "        # Dictionaries to store average PCA scores and standard errors for each predictor\n",
    "        predictor_pca_avg = defaultdict(float)\n",
    "        predictor_pca_std_err = defaultdict(float)\n",
    "        \n",
    "        # Go through each predictor and get rows where it appears, then compute average PCA and standard error\n",
    "        for predictor in unique_predictors:\n",
    "            predictor_rows = cluster_data[cluster_data['formula'].str.contains(f\"\\\\b{predictor}\\\\b\", regex=True)]\n",
    "            unique_pca_scores = predictor_rows['PCA_1'].nunique()\n",
    "            print(f\"Number of unique PCA scores for predictor '{predictor}': {unique_pca_scores}\")\n",
    "            avg_pca = predictor_rows['PCA_1'].mean()\n",
    "            std_err = predictor_rows['PCA_1'].std() / (len(predictor_rows) ** 0.5)\n",
    "            predictor_pca_avg[predictor] = avg_pca\n",
    "            predictor_pca_std_err[predictor] = std_err\n",
    "        for predictor, std_err in predictor_pca_std_err.items():\n",
    "            print(f\"Standard error for predictor '{predictor}': {std_err}\")\n",
    "\n",
    "            \n",
    "        return predictor_pca_avg, predictor_pca_std_err\n",
    "\n",
    "    def visualize_predictor_average_pca_performance(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Visualize the average PCA score with standard errors for each predictor within the specified cluster using Seaborn.\n",
    "        \"\"\"\n",
    "        predictor_pca_avg, predictor_pca_std_err = self.get_predictor_average_pca(cluster_label)\n",
    "        predictor_data = pd.DataFrame({\n",
    "            'Predictor': list(predictor_pca_avg.keys()),\n",
    "            'Avg_PCA': list(predictor_pca_avg.values()),\n",
    "            'Std_Err': list(predictor_pca_std_err.values())\n",
    "        }).sort_values(by='Avg_PCA', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        sns.barplot(x='Avg_PCA', y='Predictor', data=predictor_data, palette=\"viridis\")\n",
    "\n",
    "        for index, value in enumerate(predictor_data['Avg_PCA']):\n",
    "            plt.errorbar(value, index, xerr=predictor_data['Std_Err'].iloc[index], color='black', capsize=5, fmt='none')\n",
    "        \n",
    "        plt.title(f\"Average PCA Scores for Predictors in Cluster {cluster_label}\")\n",
    "        plt.xlabel(\"Average PCA Score\")\n",
    "        plt.ylabel(\"Predictor\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What cluster would you like to evaluate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_of_interest=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_obj = ModelPerformanceAnalysis(cluster_obj_final.get_essential_data())\n",
    "analysis_obj.apply_pca()\n",
    "analysis_obj.visualize_pca_performance(cluster_of_interest)\n",
    "analysis_obj.get_pca_dataframe().to_csv(os.path.join(out_dir, 'raw_data/cluster_df_pc1.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_obj.visualize_predictor_average_pca_performance(cluster_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_obj.visualize_predictor_incidence(cluster_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Relationships within Clusters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relate Each Predictor (Dataframe column) to a Class of Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you Want to Map Specific Predictors to a 'Class of Information' set class_equal_predictor to false. \n",
    "\n",
    "ie) age, gender, education are all demographic factors and could be labeled 'demographic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_predictors(data):\n",
    "        \"\"\"\n",
    "        Extract predictors from the 'formula' column of the given data.\n",
    "        \"\"\"\n",
    "        predictors_list = []\n",
    "        for formula in data['formula']:\n",
    "            terms = re.split(r' ~ | \\+ |\\*', formula)\n",
    "            predictors_list.extend(terms[1:])\n",
    "        # Strip white spaces and ensure uniqueness\n",
    "        return pd.DataFrame({'Predictors': [predictor.strip() for predictor in predictors_list]})\n",
    "\n",
    "def prep_class_dict(data_df, class_equal_predictor=False):\n",
    "    if class_equal_predictor:\n",
    "        print('Copy-paste this example dictionary to map each predictor to a given class of information')\n",
    "        print('{')\n",
    "        for predictor in data_df['Predictors'].unique():\n",
    "            print(f'\"{predictor}\": \"{predictor}\",')\n",
    "        print('}')\n",
    "    else:\n",
    "        print('Copy-paste this example dictionary to map each predictor to a given class of information')\n",
    "        print('{')\n",
    "        for predictor in data_df['Predictors'].unique():\n",
    "            print(f'\"{predictor}\": \" \",')\n",
    "        print('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_equal_predictor = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_class_dict(extract_predictors(data_df), class_equal_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_to_class_dict = {\n",
    "    # Demographic\n",
    "    'Age': 'Demographic',\n",
    "    'subject_id': 'Demographic',\n",
    "    'Baseline': 'Demographic',# This seems like an identifier, but I'm mapping it to Demographic for now.\n",
    "\n",
    "    # Subiculum Stim\n",
    "    'Subiculum_Connectivity': 'Subiculum Stim',\n",
    "\n",
    "    # Network Stim\n",
    "    'Visual_Connectivity': 'Network Stim',\n",
    "    'Somatomotor_Connectivity': 'Network Stim',\n",
    "    'Dorsal_Attention_Connectivity': 'Network Stim',\n",
    "    'Ventral_Attention_Connectivity': 'Network Stim',\n",
    "    'Limbic_Connectivity': 'Network Stim',\n",
    "    'Frontoparietal_Connectivity': 'Network Stim',\n",
    "    'Default_Connectivity': 'Network Stim',\n",
    "\n",
    "    # Lobe Atrophy\n",
    "    'Frontal_Atrophy': 'Lobe Atrophy',\n",
    "    'Insula_Atrophy': 'Lobe Atrophy',\n",
    "    'Temporal_Atrophy': 'Lobe Atrophy',\n",
    "    'Occipital_Atrophy': 'Lobe Atrophy',\n",
    "    'Parietal_Atrophy': 'Lobe Atrophy',\n",
    "\n",
    "    # Network Atrophy\n",
    "    'Default_Atrophy': 'Network Atrophy',\n",
    "    'Visual_Atrophy': 'Network Atrophy',\n",
    "    'Limbic_Atrophy': 'Network Atrophy',\n",
    "    'Somatomotor_Atrophy': 'Network Atrophy',\n",
    "    'Dorsal_Attention_Atrophy': 'Network Atrophy',\n",
    "    'Ventral_Attention_Atrophy': 'Network Atrophy',\n",
    "    'Frontoparietal_Atrophy': 'Network Atrophy',\n",
    "\n",
    "    # Subiculum Atrophy\n",
    "    'Subiculum_Atrophy': 'Subiculum Atrophy',\n",
    "\n",
    "    # Unclassified (will need your inputs on these):\n",
    "    'Abs_Stim_Composite_Atophy_SpCorrel': 'Stim Atrophy Match',\n",
    "    'Raw_Stim_Composite_Atrophy_SpCorrel': 'Stim Atrophy Match',\n",
    "    'Cerebellar_Atrophy': 'Lobe Atrophy',\n",
    "    'Total_Atrophy_Voxels': 'Total Atrophy',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "# size = count * 15\n",
    "\n",
    "class InteractionVisualizer:\n",
    "    \n",
    "    def __init__(self, essential_data_df):\n",
    "        \"\"\"\n",
    "        Initializes the InteractionVisualizer class.\n",
    "        \n",
    "        :param essential_data_df: Dataframe containing formula, r_squared, rmse, correlation, and Cluster_Label columns.\n",
    "        \"\"\"\n",
    "        self.data = essential_data_df\n",
    "        \n",
    "    def compute_overall_interaction_counts(self, predictor_to_class_dict):\n",
    "        \"\"\"\n",
    "        Computes the overall interaction counts between every pair of classes across the entire dataset.\n",
    "        \"\"\"\n",
    "        interactions = defaultdict(int)\n",
    "        for formula in self.data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            for i in range(len(classes)):\n",
    "                for j in range(i+1, len(classes)):\n",
    "                    interactions[tuple(sorted([classes[i], classes[j]]))] += 1\n",
    "        return interactions\n",
    "\n",
    "    def extract_interactions_for_cluster(self, cluster_label, predictor_to_class_dict):\n",
    "        \"\"\"\n",
    "        Extract interactions from the formula column of the dataframe for a specific cluster.\n",
    "        Returns a dictionary with classes as keys and their normalized occurrence counts as values.\n",
    "        \"\"\"\n",
    "        overall_counts = self.compute_overall_class_counts(predictor_to_class_dict)\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        cluster_counts = defaultdict(int)\n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            for pred in predictors:\n",
    "                pred_class = predictor_to_class_dict.get(pred, \"Unknown\")\n",
    "                cluster_counts[pred_class] += 1\n",
    "\n",
    "        # Normalize the counts by dividing by the overall counts\n",
    "        normalized_counts = {k: v / overall_counts[k] for k, v in cluster_counts.items()}\n",
    "        return normalized_counts\n",
    "\n",
    "    def extract_normalized_interactions_for_cluster(self, cluster_label, predictor_to_class_dict):\n",
    "        \"\"\"\n",
    "        Extract and normalize interaction counts for a specific cluster.\n",
    "        \"\"\"\n",
    "        overall_interactions = self.compute_overall_interaction_counts(predictor_to_class_dict)\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        cluster_interactions = defaultdict(int)\n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            for i in range(len(classes)):\n",
    "                for j in range(i+1, len(classes)):\n",
    "                    cluster_interactions[tuple(sorted([classes[i], classes[j]]))] += 1\n",
    "\n",
    "        # Normalize the interaction counts\n",
    "        normalized_interactions = {k: v / overall_interactions[k] for k, v in cluster_interactions.items()}\n",
    "        return normalized_interactions\n",
    "\n",
    "    def map_interactions_to_classes(self, interaction_counts, predictor_to_class_dict):\n",
    "        \"\"\"\n",
    "        Maps the interactions to their respective classes.\n",
    "        \n",
    "        :param interaction_counts: A dictionary containing predictors and their occurrence counts.\n",
    "        :param predictor_to_class_dict: A dictionary mapping predictors to their respective classes.\n",
    "        \n",
    "        Returns a dictionary with classes as keys and the summed occurrence counts as values.\n",
    "        \"\"\"\n",
    "        class_counts = defaultdict(int)\n",
    "        for predictor, count in interaction_counts.items():\n",
    "            predictor_class = predictor_to_class_dict.get(predictor, \"Unknown\")\n",
    "            class_counts[predictor_class] += count\n",
    "        return class_counts\n",
    "    \n",
    "    def visualize_class_interactions_for_cluster(self, cluster_label, predictor_to_class_dict, class_scaling_factor=15, interaction_scaling_factor=1):\n",
    "        \"\"\"\n",
    "        Visualizes the interaction between classes for a specific cluster using a graph representation.\n",
    "        \n",
    "        :param cluster_label: The specific cluster label for which interactions are to be visualized.\n",
    "        :param predictor_to_class_dict: A dictionary mapping predictors to their respective classes.\n",
    "        :param class_scaling_factor: A scaling factor to adjust the node size.\n",
    "        \"\"\"\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Dictionary to store class-pair interactions\n",
    "        class_pair_counts = defaultdict(int)\n",
    "        \n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            \n",
    "            # Create pairs of predictors for each interaction in the formula\n",
    "            for i in range(len(predictors)):\n",
    "                for j in range(i+1, len(predictors)):\n",
    "                    class1 = predictor_to_class_dict.get(predictors[i], \"Unknown\")\n",
    "                    class2 = predictor_to_class_dict.get(predictors[j], \"Unknown\")\n",
    "                    \n",
    "                    # Sort class pair to ensure (class1, class2) and (class2, class1) are treated the same\n",
    "                    class_pair = tuple(sorted([class1, class2]))\n",
    "                    class_pair_counts[class_pair] += 1\n",
    "                    \n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Create nodes for each class\n",
    "        all_classes = set(predictor_to_class_dict.values())\n",
    "        for class_name in all_classes:\n",
    "            size = sum([count for pair, count in class_pair_counts.items() if class_name in pair])\n",
    "            size *= class_scaling_factor\n",
    "            G.add_node(class_name, size=size)\n",
    "            \n",
    "        # Create edges between class nodes with weights proportional to their interaction count\n",
    "        for (class1, class2), weight in class_pair_counts.items():\n",
    "            G.add_edge(class1, class2, weight=weight)\n",
    "            \n",
    "        # Plot the graph\n",
    "        # pos = nx.spring_layout(G)\n",
    "        pos = nx.spring_layout(G, k=.95, iterations=100)\n",
    "        sizes = [G.nodes[node]['size'] for node in G]\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=\"skyblue\")\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "        \n",
    "        # Draw edges with weights\n",
    "        for (u, v, d) in G.edges(data=True):\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=d['weight'] * 0.05)  # Adjust the factor (0.05) as needed\n",
    "        \n",
    "        plt.title(f\"Class Interaction Graph for Cluster {cluster_label}\")\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_all_clusters(self, predictor_to_class_dict, class_scaling_factor=15, interaction_scaling_factor=1):\n",
    "        \"\"\"\n",
    "        Visualizes the class interactions for all unique clusters in the data using subplots.\n",
    "        \n",
    "        :param predictor_to_class_dict: A dictionary mapping predictors to their respective classes.\n",
    "        \"\"\"\n",
    "        unique_clusters = self.data['Cluster_Label'].unique()\n",
    "\n",
    "        for cluster_label in unique_clusters:\n",
    "            plt.figure()\n",
    "            self.visualize_class_interactions_for_cluster(cluster_label, predictor_to_class_dict, class_scaling_factor, interaction_scaling_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionVisualizerFinal:\n",
    "\n",
    "    def __init__(self, essential_data_df):\n",
    "        self.data = essential_data_df\n",
    "\n",
    "    def compute_overall_interaction_counts(self, predictor_to_class_dict):\n",
    "        interactions = defaultdict(int)\n",
    "        for formula in self.data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            for i in range(len(classes)):\n",
    "                for j in range(i+1, len(classes)):\n",
    "                    interactions[tuple(sorted([classes[i], classes[j]]))] += 1\n",
    "        return interactions\n",
    "\n",
    "    def compute_overall_class_counts(self, predictor_to_class_dict):\n",
    "        class_counts = defaultdict(int)\n",
    "        for formula in self.data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            for pred in predictors:\n",
    "                pred_class = predictor_to_class_dict.get(pred, \"Unknown\")\n",
    "                class_counts[pred_class] += 1\n",
    "                if pred not in predictor_to_class_dict:\n",
    "                    print(f\"Unknown predictor: {pred}\")\n",
    "        return class_counts\n",
    "    \n",
    "    def extract_normalized_classes_for_cluster(self, cluster_label, predictor_to_class_dict, normalize=True):\n",
    "        overall_class_counts = self.compute_overall_class_counts(predictor_to_class_dict)\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        cluster_class_counts = defaultdict(int)\n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            for pred in predictors:\n",
    "                pred_class = predictor_to_class_dict.get(pred, \"Unknown\")\n",
    "                cluster_class_counts[pred_class] += 1\n",
    "\n",
    "        # Normalize the class counts if required\n",
    "        if normalize:\n",
    "            normalized_class_counts = {k: v / overall_class_counts[k] for k, v in cluster_class_counts.items()}\n",
    "        else:\n",
    "            normalized_class_counts = cluster_class_counts\n",
    "        return normalized_class_counts\n",
    "\n",
    "    def extract_normalized_interactions_for_cluster(self, cluster_label, predictor_to_class_dict, normalize=True):\n",
    "        overall_interactions = self.compute_overall_interaction_counts(predictor_to_class_dict)\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        cluster_interactions = defaultdict(int)\n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            for i in range(len(classes)):\n",
    "                for j in range(i+1, len(classes)):\n",
    "                    cluster_interactions[tuple(sorted([classes[i], classes[j]]))] += 1\n",
    "\n",
    "        # Normalize the interaction counts if required\n",
    "        if normalize:\n",
    "            normalized_interactions = {k: v / overall_interactions[k] for k, v in cluster_interactions.items()}\n",
    "        else:\n",
    "            normalized_interactions = cluster_interactions\n",
    "        return normalized_interactions\n",
    "\n",
    "\n",
    "    def compute_average_pca1_for_classes_within_cluster(self, cluster_label, predictor_to_class_dict):\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        class_pca1 = defaultdict(list)\n",
    "\n",
    "        for index, row in cluster_data.iterrows():\n",
    "            predictors = row['formula'].split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            for pred in predictors:\n",
    "                pred_class = predictor_to_class_dict.get(pred, \"Unknown\")\n",
    "                class_pca1[pred_class].append(row['PCA_1'])\n",
    "\n",
    "        avg_pca1 = {k: sum(v) / len(v) for k, v in class_pca1.items()}\n",
    "        return avg_pca1\n",
    "\n",
    "    def compute_average_pca1_for_interactions_within_cluster(self, cluster_label, predictor_to_class_dict):\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        interaction_pca1 = defaultdict(list)\n",
    "\n",
    "        for index, row in cluster_data.iterrows():\n",
    "            predictors = row['formula'].split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            interaction = tuple(sorted(classes))\n",
    "            interaction_pca1[interaction].append(row['PCA_1'])\n",
    "\n",
    "        avg_pca1 = {k: sum(v) / len(v) for k, v in interaction_pca1.items()}\n",
    "        return avg_pca1\n",
    "\n",
    "    def visualize_class_interactions_for_cluster(self, cluster_label, predictor_to_class_dict, class_scaling_factor, edge_scaling_factor, clustering_constant, normalize, color_by_pca1, out_dir):\n",
    "        normalized_class_counts = self.extract_normalized_classes_for_cluster(cluster_label, predictor_to_class_dict, normalize)\n",
    "        normalized_interactions = self.extract_normalized_interactions_for_cluster(cluster_label, predictor_to_class_dict, normalize)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        class_avg_pca1 = None\n",
    "        interaction_avg_pca1 = None\n",
    "        if color_by_pca1:\n",
    "            class_avg_pca1 = self.compute_average_pca1_for_classes_within_cluster(cluster_label, predictor_to_class_dict)\n",
    "            interaction_avg_pca1 = self.compute_average_pca1_for_interactions_within_cluster(cluster_label, predictor_to_class_dict)\n",
    "            overall_min_pca1 = self.data['PCA_1'].min()\n",
    "            overall_max_pca1 = self.data['PCA_1'].max()\n",
    "            color_map = plt.get_cmap(\"viridis\")\n",
    "        \n",
    "        for class_name, normalized_count in normalized_class_counts.items():\n",
    "            size = np.power(normalized_count * class_scaling_factor, 1.8)\n",
    "            if color_by_pca1:\n",
    "                normalized_pca1_value = (class_avg_pca1[class_name] - overall_min_pca1) / (overall_max_pca1 - overall_min_pca1)\n",
    "                color = color_map(normalized_pca1_value)\n",
    "            else:\n",
    "                color = \"skyblue\"\n",
    "            G.add_node(class_name, size=size, color=color)\n",
    "\n",
    "        for (class1, class2), normalized_weight in normalized_interactions.items():\n",
    "            interaction = tuple(sorted([class1, class2]))\n",
    "            if color_by_pca1:\n",
    "                normalized_pca1_value = (interaction_avg_pca1.get(interaction, overall_min_pca1) - overall_min_pca1) / (overall_max_pca1 - overall_min_pca1)\n",
    "                color = color_map(normalized_pca1_value)\n",
    "            else:\n",
    "                color = \"skyblue\"\n",
    "            G.add_edge(class1, class2, weight=np.power(normalized_weight * edge_scaling_factor, 1), color=color)\n",
    "\n",
    "        pos = nx.spring_layout(G, k=clustering_constant, iterations=100)\n",
    "        sizes = [G.nodes[node]['size'] for node in G]\n",
    "        \n",
    "        node_colors = [G.nodes[node]['color'] for node in G]\n",
    "        edge_colors = [d['color'] for u, v, d in G.edges(data=True)]\n",
    "        edge_widths = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=node_colors)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "        nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths)\n",
    "        \n",
    "        if color_by_pca1:\n",
    "            sm = plt.cm.ScalarMappable(cmap=color_map, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "            sm.set_array([])\n",
    "            plt.colorbar(sm, label=\"Normalized Component 1\")\n",
    "        \n",
    "        plt.title(f\"Class Interaction Graph for Cluster {cluster_label}\")\n",
    "    \n",
    "        if out_dir is not None:\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "            plt.savefig(os.path.join(out_dir, f\"Cluster_{cluster_label}.png\"), bbox_inches='tight')\n",
    "            plt.savefig(os.path.join(out_dir, f\"Cluster_{cluster_label}.svg\"), bbox_inches='tight')\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def visualize_all_clusters(self, predictor_to_class_dict, class_scaling_factor=250, edge_scaling_factor=15, clustering_constant=0.05, normalize=True, color_by_pca1=True, out_dir=None):\n",
    "        \"\"\"\n",
    "        Visualizes the class interactions for all unique clusters in the data.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            class_scaling_factor = class_scaling_factor*1000\n",
    "            edge_scaling_factor = edge_scaling_factor*100\n",
    "        unique_clusters = self.data['Cluster_Label'].unique()\n",
    "        for cluster_label in unique_clusters:\n",
    "            self.visualize_class_interactions_for_cluster(cluster_label, predictor_to_class_dict, class_scaling_factor, edge_scaling_factor, clustering_constant, normalize, color_by_pca1, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer_final = InteractionVisualizerFinal(analysis_obj.get_pca_dataframe())\n",
    "# out_dir=None\n",
    "visualizer_final.visualize_all_clusters(predictor_to_class_dict,\n",
    "                                        class_scaling_factor=.5, \n",
    "                                        edge_scaling_factor=.2, \n",
    "                                        clustering_constant=295, \n",
    "                                        normalize=True, \n",
    "                                        color_by_pca1=True, \n",
    "                                        out_dir=os.path.join(out_dir, 'raw_graphs'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation of the Data-Driven Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the PermutationPValueCalculator from your module\n",
    "from calvin_utils.permutation_analysis_utils.statistical_utils.p_value_statistics import PermutationPValueCalculator\n",
    "\n",
    "class PermutationTest(PermutationPValueCalculator):\n",
    "    \"\"\"\n",
    "    This class is designed to perform a permutation test for all-subsets regression outcomes.\n",
    "    It assesses the stability of the PCA outcomes against permutations of the data.\n",
    "    Inherits functionalities from the PermutationPValueCalculator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_df, observed_outcomes_df):\n",
    "        super().__init__(observed_outcomes_df, []) \n",
    "        self.data_df = data_df\n",
    "        self.observed_outcomes_df = observed_outcomes_df\n",
    "\n",
    "    def loocv_regression(self, permuted_data, formula):\n",
    "        \"\"\"\n",
    "        Performs Leave-One-Out Cross-Validation (LOOCV) regression on the permuted data using the given formula.\n",
    "\n",
    "        Parameters:\n",
    "        - permuted_data: DataFrame with permuted outcomes.\n",
    "        - formula: String specifying the regression formula.\n",
    "\n",
    "        Returns:\n",
    "        - rmse: Root Mean Squared Error from the regression.\n",
    "        - r_value: Pearson correlation coefficient.\n",
    "        - r_squared: R-squared value of the model.\n",
    "        \"\"\"\n",
    "        outcome_var = formula.split(\"~\")[0].strip()\n",
    "        model = smf.ols(formula=formula, data=permuted_data).fit()\n",
    "        \n",
    "        # Get y_actual and y_predicted for RMSE and correlation\n",
    "        y_actual = permuted_data[outcome_var]\n",
    "        y_predicted = model.predict(permuted_data)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "        r_value, _ = pearsonr(y_actual, y_predicted)\n",
    "        r_squared = model.rsquared\n",
    "        aic = model.aic\n",
    "        bic = model.bic\n",
    "        adj_r_squared = model.rsquared_adj\n",
    "        \n",
    "        return 1/rmse, r_value, r_squared, 1/aic, 1/bic, adj_r_squared\n",
    "\n",
    "    def perform_permutation(self):\n",
    "        \"\"\"\n",
    "        Performs a single permutation test by permuting every single column to destroy all structure.\n",
    "\n",
    "        Returns:\n",
    "        - pca_results: List of PCA values for each regression formula.\n",
    "        - regression_metrics: List of regression metrics (rmse, r_value, r_squared) for each regression formula.\n",
    "        \"\"\"\n",
    "        # Permute only outcomes\n",
    "        permuted_data = self.data_df.copy()\n",
    "        permuted_data['outcome'] = np.random.permutation(permuted_data['outcome'])\n",
    "        # Permute every column\n",
    "        # permuted_data = self.data_df.apply(np.random.permutation)\n",
    "        \n",
    "        # Store metrics for all formulas\n",
    "        metrics = []\n",
    "\n",
    "        # Go through each formula in observed outcomes\n",
    "        for _, row in self.observed_outcomes_df.iterrows():\n",
    "            formula = row['formula']\n",
    "            inv_rmse, r_value, r_squared, inv_aic, inv_bic, adj_r_squared = self.loocv_regression(permuted_data, formula)\n",
    "            metrics.append([inv_rmse, r_value, r_squared])\n",
    "\n",
    "        # Standardize using observed metrics mean and standard deviation\n",
    "        observed_metrics = self.observed_outcomes_df[['r_squared', 'rmse', 'correlation']].values\n",
    "\n",
    "        # Standardize using observed metrics mean and standard deviation\n",
    "        scaler = StandardScaler().fit(observed_metrics)  # Fit the scaler to observed metrics\n",
    "        standardized_metrics = scaler.transform(metrics)\n",
    "\n",
    "        # Apply PCA on standardized metrics\n",
    "        pca = PCA(n_components=1)\n",
    "        pca_results = pca.fit_transform(standardized_metrics)\n",
    "        \n",
    "        return pca_results\n",
    "\n",
    "    def run_permutations(self, n_permutations=10):\n",
    "        \"\"\"\n",
    "        Runs the specified number of permutation tests.\n",
    "\n",
    "        Parameters:\n",
    "        - n_permutations: Number of permutations to perform.\n",
    "\n",
    "        Returns:\n",
    "        - results_df: DataFrame containing PCA values for each formula and permutation.\n",
    "        - regression_metrics_list: List of regression metrics (rmse, r_value, r_squared) for each permutation.\n",
    "        \"\"\"\n",
    "        all_pca_results = []\n",
    "\n",
    "        for _ in tqdm(range(n_permutations)):\n",
    "            pca_results = self.perform_permutation()\n",
    "            all_pca_results.append(pca_results)\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        all_pca_results_array = np.array(all_pca_results)\n",
    "        print('Shape ', all_pca_results_array.shape)\n",
    "        # Convert the 2D array into a DataFrame\n",
    "        reshaped_array = all_pca_results_array.squeeze().T\n",
    "        results_df = pd.DataFrame(reshaped_array, columns=[f\"Permutation_{i}\" for i in range(n_permutations)])\n",
    "        results_df['formula'] = self.observed_outcomes_df['formula']\n",
    "        return results_df\n",
    "\n",
    "    def calculate_p_values_for_formulas(self, results_df):\n",
    "        \"\"\"\n",
    "        Calculate the uncorrected and FWER-corrected p-values for each formula.\n",
    "\n",
    "        Parameters:\n",
    "        - results_df: DataFrame containing PCA values for each formula and permutation.\n",
    "\n",
    "        Returns:\n",
    "        - p_values_df: DataFrame containing formulae, uncorrected p-values, and FWER-corrected p-values.\n",
    "        \"\"\"\n",
    "        # For uncorrected p-values\n",
    "        uncorrected_p_values = []\n",
    "        for _, row in self.observed_outcomes_df.iterrows():\n",
    "            observed_value = row['PCA_1']\n",
    "            permuted_values = results_df[results_df['formula'] == row['formula']].drop(columns='formula').values.flatten()\n",
    "            uncorrected_p = (permuted_values > observed_value).mean()\n",
    "            uncorrected_p_values.append(uncorrected_p)\n",
    "\n",
    "        # For FWER-corrected p-values\n",
    "        maxima_array = results_df.drop(columns='formula').max(axis=0).values\n",
    "\n",
    "        # Calculate percentiles for observed PCA values in maxima array\n",
    "        fwer_corrected_p_values = [1 - (np.sum(observed_value > maxima_array) / len(maxima_array)) for observed_value in self.observed_outcomes_df['PCA_1']]\n",
    "\n",
    "        # Construct the p-values DataFrame\n",
    "        p_values_df = pd.DataFrame({\n",
    "            'formula': self.observed_outcomes_df['formula'],\n",
    "            'uncorrected_p_value': uncorrected_p_values,\n",
    "            'fwer_corrected_p_value': fwer_corrected_p_values\n",
    "        })\n",
    "\n",
    "        return p_values_df\n",
    "    \n",
    "    \n",
    "    def bonferroni_adjustment(self, df_series):\n",
    "        \"\"\"\n",
    "        Adjusts the uncorrected p-values using Bonferroni correction.\n",
    "        \n",
    "        Parameters:\n",
    "        - p_values_df: DataFrame containing uncorrected p-values.\n",
    "        \n",
    "        Returns:\n",
    "        - p_values_df: DataFrame with an additional column for Bonferroni-adjusted p-values.\n",
    "        \"\"\"\n",
    "        return np.minimum(df_series * df_series.shape[0], 1)\n",
    "    \n",
    "    def variance_smoothed_fwe(self, results_df):\n",
    "        \"\"\"\n",
    "        Applies variance smoothing to the results DataFrame and calculates the FWER-corrected p-values.\n",
    "\n",
    "        Parameters:\n",
    "        - results_df: DataFrame containing PCA values for each formula and permutation.\n",
    "\n",
    "        Returns:\n",
    "        - results_df: Smoothed DataFrame.\n",
    "        - fwer_p_values: List of FWER-corrected p-values.\n",
    "        \"\"\"\n",
    "        # Step 1: Variance Smoothing\n",
    "        smoothed_results = self.apply_smoothing(results_df.drop(columns='formula'))\n",
    "        results_df[smoothed_results.columns] = smoothed_results\n",
    "\n",
    "        # Step 2: Calculate FWER-corrected p-values\n",
    "        fwer_p_values = self.calculate_fwe(results_df)\n",
    "        \n",
    "        return results_df, fwer_p_values\n",
    "\n",
    "    def apply_smoothing(self, df):\n",
    "        \"\"\"\n",
    "        Applies the desired smoothing technique to a DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame to be smoothed.\n",
    "\n",
    "        Returns:\n",
    "        - df: Smoothed DataFrame.\n",
    "        \"\"\"\n",
    "        # Assuming moving average smoothing for simplicity, with a window of 3\n",
    "        # Replace this with the desired smoothing method.\n",
    "        return df.rolling(window=3).mean()\n",
    "\n",
    "    def calculate_fwe(self, results_df):\n",
    "        \"\"\"\n",
    "        Calculates FWER-corrected p-values using the smoothed results DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - results_df: Smoothed DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        - fwer_corrected_p_values: List of FWER-corrected p-values.\n",
    "        \"\"\"\n",
    "        maxima_array = results_df.drop(columns='formula').max(axis=0).values\n",
    "        fwer_corrected_p_values = [1 - (np.sum(observed_value > maxima_array) / len(maxima_array)) for observed_value in self.observed_outcomes_df['PCA_1']]\n",
    "        return fwer_corrected_p_values\n",
    "\n",
    "\n",
    "    def run(self, n_permutations=10):\n",
    "        \"\"\"\n",
    "        Orchestrates the entire permutation test process.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_permutations: Number of permutations to perform.\n",
    "        \n",
    "        Returns:\n",
    "        - p_values_df: DataFrame containing formulae, uncorrected p-values, Bonferroni-adjusted p-values, and FWER-corrected p-values.\n",
    "        \"\"\"\n",
    "        results_df = self.run_permutations(n_permutations=n_permutations)\n",
    "        p_values_df = self.calculate_p_values_for_formulas(results_df)\n",
    "        p_values_df['bonferonni_adj'] = self.bonferroni_adjustment(p_values_df['uncorrected_p_value'])\n",
    "        _, fwer_p_values = self.variance_smoothed_fwe(results_df)\n",
    "        p_values_df['variance_smoothed_fwe'] = fwer_p_values\n",
    "        return p_values_df, results_df\n",
    "\n",
    "    def save_to_csv(self, p_values_df, out_dir):\n",
    "        \"\"\"\n",
    "        Saves the p_values_df to a specified directory.\n",
    "\n",
    "        Parameters:\n",
    "        - p_values_df: DataFrame containing p-values.\n",
    "        - out_dir: String specifying the directory to save the file.\n",
    "\n",
    "        Returns:\n",
    "        None.\n",
    "        \"\"\"\n",
    "        filename = \"p_values_results.csv\"\n",
    "        full_path = os.path.join(out_dir, filename)\n",
    "        \n",
    "        p_values_df.to_csv(full_path, index=False)\n",
    "        print(f\"Results saved to: {full_path}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# perm_test = PermutationTest(data_df, observed_outcomes_df)\n",
    "# results = perm_test.run_permutations(n_permutations=10)\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_test = PermutationTest(data_df, analysis_obj.get_pca_dataframe())\n",
    "p_values_df, permutation_results = perm_test.run(n_permutations=1000);\n",
    "perm_test.save_to_csv(p_values_df, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Loading In P-Values, Use This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_df = pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/clinical_analyses/ses-01/sub-all/all_data/all_metadata_spreadsheet/ols_linear_regression/p_values_results.csv')\n",
    "p_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Revised Clusters With P-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPerformanceClusterWithOpacity(ModelPerformanceClusterFinal):\n",
    "    \n",
    "    def visualize_3D_scatter(self, opacity_column='correlation', save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize a 3D scatter plot of R-squared vs. Pearson R vs. RMSE, colored by Cluster label and shape by number of predictors.\n",
    "        Opacity of the dots is coded by 1 minus the value in the specified column.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the original visualize_3D_scatter method to set 'num_predictors' and 'symbol' columns\n",
    "        super().visualize_3D_scatter()\n",
    "        \n",
    "        # Ensure the opacity_column exists in the dataframe\n",
    "        if opacity_column not in self.data.columns:\n",
    "            raise ValueError(f\"The specified opacity_column '{opacity_column}' does not exist in the dataframe.\")\n",
    "        \n",
    "        # Compute opacity values\n",
    "        self.data['custom_opacity'] = 1 - self.data[opacity_column]\n",
    "        \n",
    "        fig = px.scatter_3d(self.data, x='r_squared', y='correlation', z='rmse',\n",
    "                    color='Cluster_Label', hover_name='formula', opacity=0.8,\n",
    "                    hover_data=['adj_r_squared', 'bic', 'aic'], symbol='symbol',\n",
    "                    labels={'adj_r_squared': 'Adj. R-Squared', 'r_squared': 'R-Squared', 'correlation': 'LOOCV Pearson R', 'rmse': 'LOOCV RMSE'},\n",
    "                    color_continuous_scale=px.colors.sequential.Viridis)\n",
    "        \n",
    "        fig.update_layout(title=\"3D Scatter Plot of R-Squared, Pearson R, and RMSE with Custom Opacity\", \n",
    "                        autosize=True, width=1200, height=800)\n",
    "        # fig.show()\n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            save_path = os.path.join(save_path, '3d_scatterplot.html')\n",
    "            fig.write_html(save_path)\n",
    "        else:\n",
    "            fig.show()\n",
    "\n",
    "    def plot_3D_with_custom_opacities(self, x_column, y_column, z_column, opacity_column):\n",
    "        \"\"\"\n",
    "        Plot a 3D scatter plot using given columns for X, Y, and Z axes with custom opacities.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure the columns exist in the dataframe\n",
    "        for col in [x_column, y_column, z_column, opacity_column]:\n",
    "            if col not in self.data.columns:\n",
    "                raise ValueError(f\"The specified column '{col}' does not exist in the dataframe.\")\n",
    "        \n",
    "        # Extract data\n",
    "        X = self.data[x_column].values\n",
    "        Y = self.data[y_column].values\n",
    "        Z = self.data[z_column].values\n",
    "        \n",
    "        # Compute custom opacity\n",
    "        custom_opacity = 1 - self.data[opacity_column].values\n",
    "        max_opacity = np.max(custom_opacity)\n",
    "        custom_opacity = custom_opacity / max_opacity\n",
    "        \n",
    "        # 3D scatter plot\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        # Plot each point with its respective opacity\n",
    "        for x, y, z, opacity in zip(X, Y, Z, custom_opacity):\n",
    "            ax.scatter(x, y, z, color='b', alpha=opacity)\n",
    "\n",
    "        ax.set_xlabel(x_column)\n",
    "        ax.set_ylabel(y_column)\n",
    "        ax.set_zlabel(z_column)\n",
    "        ax.set_title(f\"3D Scatter plot of {x_column}, {y_column}, and {z_column} with varying opacity\")\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df_to_merge_1, df_to_merge_2):\n",
    "    results_df = df_to_merge_1.merge(df_to_merge_2, on='formula', how='left')\n",
    "    results_df.fillna(1, inplace=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = preprocess_inputs(pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/clinical_analyses/ses-01/sub-all/all_data/all_metadata_spreadsheet/ols_linear_regression/p_values_results.csv'), \n",
    "                               pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/clinical_analyses/ses-01/sub-all/all_data/all_metadata_spreadsheet/ols_linear_regression/raw_data/cluster_df_pc1.csv'))\n",
    "# results_df.to_csv(os.path.join(out_dir, 'loocv_results_and_p_vals.csv'))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a P-Value To Threshold By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "p_value_method = 'bonferonni_adj'\n",
    "\n",
    "RMSE_threshold  = 1.2\n",
    "R_threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the new class with the merged dataframe\n",
    "cluster_instance = ModelPerformanceClusterWithOpacity(results_df[results_df[p_value_method]<0.05], rmse_threshold=RMSE_threshold, pearsonr_threshold=R_threshold, normative_thresholding=True)\n",
    "cluster_instance.preprocess_data()\n",
    "cluster_instance.compute_r_squared(data_df)\n",
    "# cluster_instance.apply_agglomerative_clustering(n_clusters=num_clusters)\n",
    "cluster_instance.visualize_3D_scatter(save_path=os.path.join(out_dir, f'p_value_{p_value_method}_adjusted'))\n",
    "\n",
    "# cluster_instance.plot_3D_with_custom_opacities('r_squared', 'correlation', 'rmse', p_value_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Information Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Information Categories First**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = preprocess_inputs(cluster_instance.get_essential_data(), results_df[['PCA_1', 'formula']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_class_dict(extract_predictors(graph_data), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_predictor_to_class_dict = {\n",
    "    # Demographic\n",
    "    'Age': 'Demographic',\n",
    "    'Baseline': 'Demographic',\n",
    "    \n",
    "    # Network Stim\n",
    "    'Subiculum_Connectivity': 'Subiculum Stim',\n",
    "    'Visual_Connectivity': 'Network Stim',\n",
    "    'Dorsal_Attention_Connectivity': 'Network Stim',\n",
    "    'Ventral_Attention_Connectivity': 'Network Stim',\n",
    "    'Limbic_Connectivity': 'Network Stim',\n",
    "    'Frontoparietal_Connectivity': 'Network Stim',\n",
    "    'Default_Connectivity': 'Network Stim',\n",
    "    'Somatomotor_Connectivity': 'Network Stim',\n",
    "    \n",
    "    # Lobe Atrophy - Note: You haven't specified which ones are \"Lobe Atrophy\" in the provided dict, so I'm not categorizing any predictors under this for now.\n",
    "    \n",
    "    # Network Atrophy\n",
    "    'Cerebellar_Atrophy': 'Lobe Atrophy',\n",
    "    'Frontal_Atrophy': 'Lobe Atrophy',\n",
    "    'Dorsal_Attention_Atrophy': 'Network Atrophy',\n",
    "    'Visual_Atrophy': 'Network Atrophy',\n",
    "    'Frontoparietal_Atrophy': 'Network Atrophy',\n",
    "    'Abs_Stim_Composite_Atophy_SpCorrel': 'Network Stim-Atrophy Match',\n",
    "    'Parietal_Atrophy': 'Lobe Atrophy',\n",
    "    'Ventral_Attention_Atrophy': 'Network Atrophy',\n",
    "    'Limbic_Atrophy': 'Network Atrophy',\n",
    "    'Occipital_Atrophy': 'Lobe Atrophy',\n",
    "    'Default_Atrophy': 'Network Atrophy',\n",
    "    'Somatomotor_Atrophy': 'Network Atrophy',\n",
    "    'Raw_Stim_Composite_Atrophy_SpCorrel': 'Network Stim-Atrophy Match',\n",
    "    'Subiculum_Atrophy': 'Subiculum Atrophy',\n",
    "    'Temporal_Atrophy': 'Lobe Atrophy',\n",
    "    'Total_Atrophy_Voxels': 'Total Atrophy',\n",
    "    \n",
    "    # Unclassified (will need your inputs on these):\n",
    "    # None for now as all items from the provided dict have been classified.\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer_final = InteractionVisualizerFinal(graph_data)\n",
    "# out_dir=None\n",
    "visualizer_final.visualize_all_clusters(significant_predictor_to_class_dict,\n",
    "                                        class_scaling_factor=.95,\n",
    "                                        edge_scaling_factor=.2,\n",
    "                                        clustering_constant=100,\n",
    "                                        normalize=False,\n",
    "                                        color_by_pca1=True,\n",
    "                                        out_dir=os.path.join(out_dir, f'p_value_{p_value_method}_adjusted/class_graphs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Specific Information Within Each Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_class_dict(extract_predictors(graph_data), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_predictor_dict = {\n",
    "\"Age\": \"Age\",\n",
    "\"Cerebellar_Atrophy\": \"Cerebellar_Atrophy\",\n",
    "\"Dorsal_Attention_Atrophy\": \"Dorsal_Attention_Atrophy\",\n",
    "\"Subiculum_Connectivity\": \"Subiculum_Connectivity\",\n",
    "\"Visual_Atrophy\": \"Visual_Atrophy\",\n",
    "\"Frontoparietal_Atrophy\": \"Frontoparietal_Atrophy\",\n",
    "\"Abs_Stim_Composite_Atophy_SpCorrel\": \"Abs_Stim_Composite_Atophy_SpCorrel\",\n",
    "\"Parietal_Atrophy\": \"Parietal_Atrophy\",\n",
    "\"Ventral_Attention_Connectivity\": \"Ventral_Attention_Connectivity\",\n",
    "\"Baseline\": \"Baseline\",\n",
    "\"Ventral_Attention_Atrophy\": \"Ventral_Attention_Atrophy\",\n",
    "\"Limbic_Atrophy\": \"Limbic_Atrophy\",\n",
    "\"Occipital_Atrophy\": \"Occipital_Atrophy\",\n",
    "\"Visual_Connectivity\": \"Visual_Connectivity\",\n",
    "\"Default_Connectivity\": \"Default_Connectivity\",\n",
    "\"Limbic_Connectivity\": \"Limbic_Connectivity\",\n",
    "\"Default_Atrophy\": \"Default_Atrophy\",\n",
    "\"Somatomotor_Atrophy\": \"Somatomotor_Atrophy\",\n",
    "\"Dorsal_Attention_Connectivity\": \"Dorsal_Attention_Connectivity\",\n",
    "\"Frontal_Atrophy\": \"Frontal_Atrophy\",\n",
    "\"Frontoparietal_Connectivity\": \"Frontoparietal_Connectivity\",\n",
    "\"Somatomotor_Connectivity\": \"Somatomotor_Connectivity\",\n",
    "\"Raw_Stim_Composite_Atrophy_SpCorrel\": \"Raw_Stim_Composite_Atrophy_SpCorrel\",\n",
    "\"Subiculum_Atrophy\": \"Subiculum_Atrophy\",\n",
    "\"Temporal_Atrophy\": \"Temporal_Atrophy\",\n",
    "\"Total_Atrophy_Voxels\": \"Total_Atrophy_Voxels\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer_final = InteractionVisualizerFinal(graph_data)\n",
    "# out_dir=None\n",
    "visualizer_final.visualize_all_clusters(predictor_predictor_dict, \n",
    "                                        class_scaling_factor=.5, \n",
    "                                        edge_scaling_factor=.2, \n",
    "                                        clustering_constant=100, \n",
    "                                        normalize=False, \n",
    "                                        color_by_pca1=True, \n",
    "                                        out_dir=os.path.join(out_dir, f'p_value_{p_value_method}_adjusted/predictor_graphs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
