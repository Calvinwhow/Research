{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will take your regression or classification problem, the y variables (dependent) and x variables (independent) and determine which have what level of contribution to your dataset.\n",
    "## This will allow you to visualize the information context of Xn to Y, and decide what to keep in future analyses, such as development of lienar regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import glob as glob\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "#Calculate Correlation\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = 'ols_linear_regression'\n",
    "conn_path = r'/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list.csv'\n",
    "sheet_name = 'study_results'\n",
    "#----------------------------------------------------------------DONT TOUCH----------------------------------------------------------------\n",
    "out_dir = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/analyses'\n",
    "save = True\n",
    "if os.path.exists(out_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pre-prepared Regression Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Subiculum_Connectivity</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>Memory_Network_T</th>\n",
       "      <th>Memory_Network_R</th>\n",
       "      <th>Subiculum_Grey_Matter</th>\n",
       "      <th>Subiculum_White_Matter</th>\n",
       "      <th>Subiculum_CSF</th>\n",
       "      <th>Subiculum_Total</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Standardized_Age</th>\n",
       "      <th>Standardized_Percent_Improvement</th>\n",
       "      <th>Standardized_Subiculum_Connectivity</th>\n",
       "      <th>Standardized_Subiculum_Grey_Matter</th>\n",
       "      <th>Standardized_Subiculum_White_Matter</th>\n",
       "      <th>Standardized_Subiculum_CSF</th>\n",
       "      <th>Standardized_Subiculum_Total</th>\n",
       "      <th>One_Hot_Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>0.447264</td>\n",
       "      <td>0.494596</td>\n",
       "      <td>0.418688</td>\n",
       "      <td>1.646994</td>\n",
       "      <td>0.510111</td>\n",
       "      <td>2.975675</td>\n",
       "      <td>1.280978</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>-1.179578</td>\n",
       "      <td>1.970962</td>\n",
       "      <td>0.663981</td>\n",
       "      <td>1.773701</td>\n",
       "      <td>1.789087</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>0.502192</td>\n",
       "      <td>0.417569</td>\n",
       "      <td>-0.865158</td>\n",
       "      <td>-1.615736</td>\n",
       "      <td>0.740780</td>\n",
       "      <td>-1.560273</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-1.635523</td>\n",
       "      <td>-0.125938</td>\n",
       "      <td>-1.611307</td>\n",
       "      <td>-0.272553</td>\n",
       "      <td>-1.093456</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>0.497749</td>\n",
       "      <td>0.581148</td>\n",
       "      <td>0.500706</td>\n",
       "      <td>0.319460</td>\n",
       "      <td>-0.796399</td>\n",
       "      <td>0.532453</td>\n",
       "      <td>-0.341032</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.888064</td>\n",
       "      <td>-0.524419</td>\n",
       "      <td>0.862866</td>\n",
       "      <td>-0.734373</td>\n",
       "      <td>-0.463295</td>\n",
       "      <td>0.143505</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>0.432617</td>\n",
       "      <td>0.520518</td>\n",
       "      <td>0.455778</td>\n",
       "      <td>-0.440643</td>\n",
       "      <td>-0.159752</td>\n",
       "      <td>0.405263</td>\n",
       "      <td>-0.358042</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-1.921065</td>\n",
       "      <td>-0.857915</td>\n",
       "      <td>0.228406</td>\n",
       "      <td>-0.052972</td>\n",
       "      <td>-0.579749</td>\n",
       "      <td>0.126248</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>0.193389</td>\n",
       "      <td>0.491742</td>\n",
       "      <td>0.455764</td>\n",
       "      <td>-0.328427</td>\n",
       "      <td>-0.229875</td>\n",
       "      <td>-0.988805</td>\n",
       "      <td>-0.339817</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>0.512509</td>\n",
       "      <td>-1.055024</td>\n",
       "      <td>0.322072</td>\n",
       "      <td>-0.128024</td>\n",
       "      <td>-1.856148</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>37</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-2.158273</td>\n",
       "      <td>17.978233</td>\n",
       "      <td>-0.243491</td>\n",
       "      <td>0.103399</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.694444</td>\n",
       "      <td>14.611144</td>\n",
       "      <td>-0.639372</td>\n",
       "      <td>-0.329546</td>\n",
       "      <td>-0.052089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.225352</td>\n",
       "      <td>21.551613</td>\n",
       "      <td>-0.293216</td>\n",
       "      <td>0.094811</td>\n",
       "      <td>0.203586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.836879</td>\n",
       "      <td>20.200341</td>\n",
       "      <td>-0.493639</td>\n",
       "      <td>-0.114161</td>\n",
       "      <td>0.108190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.468351</td>\n",
       "      <td>-0.463787</td>\n",
       "      <td>-0.119392</td>\n",
       "      <td>0.069417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   Age  Percent_Cognitive_Improvement  Subiculum_Connectivity  \\\n",
       "0       101  62.0                     -21.428571               56.864683   \n",
       "1       102  77.0                     -36.363636               52.970984   \n",
       "2       103  76.0                     -78.947368               62.459631   \n",
       "3       104  65.0                    -129.411765               59.611631   \n",
       "4       105  50.0                     -10.526316               57.928350   \n",
       "..      ...   ...                            ...                     ...   \n",
       "77       37  64.0                      -2.158273               17.978233   \n",
       "78       38   NaN                      -0.694444               14.611144   \n",
       "79       39   NaN                      -4.225352               21.551613   \n",
       "80       43   NaN                      -2.836879               20.200341   \n",
       "81       44   NaN                       0.000000               14.468351   \n",
       "\n",
       "    Amnesia_Lesion_T_Map  Memory_Network_T  Memory_Network_R  \\\n",
       "0               0.447264          0.494596          0.418688   \n",
       "1               0.436157          0.502192          0.417569   \n",
       "2               0.497749          0.581148          0.500706   \n",
       "3               0.432617          0.520518          0.455778   \n",
       "4               0.193389          0.491742          0.455764   \n",
       "..                   ...               ...               ...   \n",
       "77             -0.243491          0.103399          0.185311   \n",
       "78             -0.639372         -0.329546         -0.052089   \n",
       "79             -0.293216          0.094811          0.203586   \n",
       "80             -0.493639         -0.114161          0.108190   \n",
       "81             -0.463787         -0.119392          0.069417   \n",
       "\n",
       "    Subiculum_Grey_Matter  Subiculum_White_Matter  Subiculum_CSF  \\\n",
       "0                1.646994                0.510111       2.975675   \n",
       "1               -0.865158               -1.615736       0.740780   \n",
       "2                0.319460               -0.796399       0.532453   \n",
       "3               -0.440643               -0.159752       0.405263   \n",
       "4               -0.328427               -0.229875      -0.988805   \n",
       "..                    ...                     ...            ...   \n",
       "77                    NaN                     NaN            NaN   \n",
       "78                    NaN                     NaN            NaN   \n",
       "79                    NaN                     NaN            NaN   \n",
       "80                    NaN                     NaN            NaN   \n",
       "81                    NaN                     NaN            NaN   \n",
       "\n",
       "    Subiculum_Total    Disease  Standardized_Age  \\\n",
       "0          1.280978  Alzheimer          0.289341   \n",
       "1         -1.560273  Alzheimer         -0.016378   \n",
       "2         -0.341032  Alzheimer         -0.888064   \n",
       "3         -0.358042  Alzheimer         -1.921065   \n",
       "4         -0.339817  Alzheimer          0.512509   \n",
       "..              ...        ...               ...   \n",
       "77              NaN        NaN               NaN   \n",
       "78              NaN        NaN               NaN   \n",
       "79              NaN        NaN               NaN   \n",
       "80              NaN        NaN               NaN   \n",
       "81              NaN        NaN               NaN   \n",
       "\n",
       "    Standardized_Percent_Improvement  Standardized_Subiculum_Connectivity  \\\n",
       "0                           0.289341                            -1.179578   \n",
       "1                          -0.016378                            -1.635523   \n",
       "2                          -0.888064                            -0.524419   \n",
       "3                          -1.921065                            -0.857915   \n",
       "4                           0.512509                            -1.055024   \n",
       "..                               ...                                  ...   \n",
       "77                               NaN                                  NaN   \n",
       "78                               NaN                                  NaN   \n",
       "79                               NaN                                  NaN   \n",
       "80                               NaN                                  NaN   \n",
       "81                               NaN                                  NaN   \n",
       "\n",
       "    Standardized_Subiculum_Grey_Matter  Standardized_Subiculum_White_Matter  \\\n",
       "0                             1.970962                             0.663981   \n",
       "1                            -0.125938                            -1.611307   \n",
       "2                             0.862866                            -0.734373   \n",
       "3                             0.228406                            -0.052972   \n",
       "4                             0.322072                            -0.128024   \n",
       "..                                 ...                                  ...   \n",
       "77                                 NaN                                  NaN   \n",
       "78                                 NaN                                  NaN   \n",
       "79                                 NaN                                  NaN   \n",
       "80                                 NaN                                  NaN   \n",
       "81                                 NaN                                  NaN   \n",
       "\n",
       "    Standardized_Subiculum_CSF  Standardized_Subiculum_Total  One_Hot_Disease  \n",
       "0                     1.773701                      1.789087              1.0  \n",
       "1                    -0.272553                     -1.093456              1.0  \n",
       "2                    -0.463295                      0.143505              1.0  \n",
       "3                    -0.579749                      0.126248              1.0  \n",
       "4                    -1.856148                      0.144737              1.0  \n",
       "..                         ...                           ...              ...  \n",
       "77                         NaN                           NaN              NaN  \n",
       "78                         NaN                           NaN              NaN  \n",
       "79                         NaN                           NaN              NaN  \n",
       "80                         NaN                           NaN              NaN  \n",
       "81                         NaN                           NaN              NaN  \n",
       "\n",
       "[82 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------------------------------------------------user input above----------------------------------------------------------------\n",
    "from calvin_utils.file_utils.dataframe_utilities import preprocess_colnames_for_regression\n",
    "if os.path.basename(conn_path).split('.')[1] == 'csv':\n",
    "    data_df = pd.read_csv(conn_path)\n",
    "else:\n",
    "    data_df = pd.read_excel(conn_path, sheet_name=sheet_name)\n",
    "data_df = preprocess_colnames_for_regression(data_df.reset_index(drop=True))\n",
    "display(data_df)\n",
    "# \n",
    "# display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_index=[11, 47, 48, 49]\n",
    "data_df = data_df.drop(index=outlier_index)\n",
    "# data_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the outcome variable\n",
    "# outcome_variable =  data_df.pop('%_Change_from_baseline_(ADAS_Cog11)')\n",
    "# data_df['outcome'] = outcome_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encode Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-hot encode as needed\n",
    "# data_df['Cognitive_Status'] = np.where(data_df['Cognitive_Status'] == 'MCI', 1, 0)\n",
    "\n",
    "# data_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate DF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop a specific set of rows\n",
    "data_df = data_df[data_df['Disease'] == 'Alzheimer']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tailor the Dataframe\n",
    "# data_df = data_df.loc[:, ['Age','Mesial_Temporal_Grade','Subiculum_Connectivity', 'outcome']]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter Columns you Don't want to standardize into a lsit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Cohort']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to standardize column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/946tskyj68b6htgvndtppmz80000gp/T/ipykernel_12191/1437211930.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[col] = (data_df[col] - np.mean(data_df[col])) / np.std(data_df[col])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient___CDR__ADAS</th>\n",
       "      <th>Age</th>\n",
       "      <th>__Change_from_baseline__ADAS_Cog11_</th>\n",
       "      <th>Subiculum_Connectivity</th>\n",
       "      <th>Amnesia_Lesion_T_Map</th>\n",
       "      <th>Memory_Network_T</th>\n",
       "      <th>Memory_Network_R</th>\n",
       "      <th>Subiculum_Grey_Matter</th>\n",
       "      <th>Subiculum_White_Matter</th>\n",
       "      <th>Subiculum_CSF</th>\n",
       "      <th>Subiculum_Total</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Standardized_Age</th>\n",
       "      <th>Standardized_Percent_Improvement</th>\n",
       "      <th>Standardized_Subiculum_Connectivity</th>\n",
       "      <th>Standardized_Subiculum_Grey_Matter</th>\n",
       "      <th>Standardized_Subiculum_White_Matter</th>\n",
       "      <th>Standardized_Subiculum_CSF</th>\n",
       "      <th>Standardized_Subiculum_Total</th>\n",
       "      <th>One_Hot_Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.668629</td>\n",
       "      <td>-0.618120</td>\n",
       "      <td>0.317537</td>\n",
       "      <td>-1.296803</td>\n",
       "      <td>-0.075475</td>\n",
       "      <td>-1.732372</td>\n",
       "      <td>-1.915962</td>\n",
       "      <td>1.923652</td>\n",
       "      <td>0.690521</td>\n",
       "      <td>1.804283</td>\n",
       "      <td>1.812765</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.317537</td>\n",
       "      <td>0.317537</td>\n",
       "      <td>-1.296803</td>\n",
       "      <td>1.923652</td>\n",
       "      <td>0.690521</td>\n",
       "      <td>1.804283</td>\n",
       "      <td>1.812765</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.600823</td>\n",
       "      <td>1.277448</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>-1.780375</td>\n",
       "      <td>-0.200665</td>\n",
       "      <td>-1.624891</td>\n",
       "      <td>-1.931023</td>\n",
       "      <td>-0.170576</td>\n",
       "      <td>-1.631460</td>\n",
       "      <td>-0.219924</td>\n",
       "      <td>-1.146615</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>-1.780375</td>\n",
       "      <td>-0.170576</td>\n",
       "      <td>-1.631460</td>\n",
       "      <td>-0.219924</td>\n",
       "      <td>-1.146615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.533016</td>\n",
       "      <td>1.151077</td>\n",
       "      <td>-0.850871</td>\n",
       "      <td>-0.601947</td>\n",
       "      <td>0.493589</td>\n",
       "      <td>-0.507664</td>\n",
       "      <td>-0.812619</td>\n",
       "      <td>0.816968</td>\n",
       "      <td>-0.736530</td>\n",
       "      <td>-0.408611</td>\n",
       "      <td>0.123318</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.850871</td>\n",
       "      <td>-0.850871</td>\n",
       "      <td>-0.601947</td>\n",
       "      <td>0.816968</td>\n",
       "      <td>-0.736530</td>\n",
       "      <td>-0.408611</td>\n",
       "      <td>0.123318</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.465210</td>\n",
       "      <td>-0.239006</td>\n",
       "      <td>-1.875980</td>\n",
       "      <td>-0.955650</td>\n",
       "      <td>-0.240569</td>\n",
       "      <td>-1.365571</td>\n",
       "      <td>-1.417006</td>\n",
       "      <td>0.183316</td>\n",
       "      <td>-0.041145</td>\n",
       "      <td>-0.523810</td>\n",
       "      <td>0.105601</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-1.875980</td>\n",
       "      <td>-1.875980</td>\n",
       "      <td>-0.955650</td>\n",
       "      <td>0.183316</td>\n",
       "      <td>-0.041145</td>\n",
       "      <td>-0.523810</td>\n",
       "      <td>0.105601</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.397403</td>\n",
       "      <td>-2.134575</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>-1.164703</td>\n",
       "      <td>-2.937139</td>\n",
       "      <td>-1.772763</td>\n",
       "      <td>-1.417194</td>\n",
       "      <td>0.276863</td>\n",
       "      <td>-0.117738</td>\n",
       "      <td>-1.786457</td>\n",
       "      <td>0.124583</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>-1.164703</td>\n",
       "      <td>0.276863</td>\n",
       "      <td>-0.117738</td>\n",
       "      <td>-1.786457</td>\n",
       "      <td>0.124583</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.329597</td>\n",
       "      <td>-0.112635</td>\n",
       "      <td>-0.028462</td>\n",
       "      <td>-0.494611</td>\n",
       "      <td>0.674839</td>\n",
       "      <td>-0.308079</td>\n",
       "      <td>-0.521108</td>\n",
       "      <td>1.518574</td>\n",
       "      <td>-0.378759</td>\n",
       "      <td>-1.205923</td>\n",
       "      <td>0.850983</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.028462</td>\n",
       "      <td>-0.028462</td>\n",
       "      <td>-0.494611</td>\n",
       "      <td>1.518574</td>\n",
       "      <td>-0.378759</td>\n",
       "      <td>-1.205923</td>\n",
       "      <td>0.850983</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.261790</td>\n",
       "      <td>-0.365378</td>\n",
       "      <td>0.440311</td>\n",
       "      <td>-1.737296</td>\n",
       "      <td>-2.309392</td>\n",
       "      <td>-1.582906</td>\n",
       "      <td>-1.307928</td>\n",
       "      <td>-1.154590</td>\n",
       "      <td>-1.339197</td>\n",
       "      <td>0.034024</td>\n",
       "      <td>-1.668636</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.440311</td>\n",
       "      <td>0.440311</td>\n",
       "      <td>-1.737296</td>\n",
       "      <td>-1.154590</td>\n",
       "      <td>-1.339197</td>\n",
       "      <td>0.034024</td>\n",
       "      <td>-1.668636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.193984</td>\n",
       "      <td>-0.870863</td>\n",
       "      <td>0.160349</td>\n",
       "      <td>-1.158354</td>\n",
       "      <td>-2.332131</td>\n",
       "      <td>-1.114775</td>\n",
       "      <td>-0.863014</td>\n",
       "      <td>0.223029</td>\n",
       "      <td>-1.262604</td>\n",
       "      <td>1.134655</td>\n",
       "      <td>-0.632821</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.160349</td>\n",
       "      <td>0.160349</td>\n",
       "      <td>-1.158354</td>\n",
       "      <td>0.223029</td>\n",
       "      <td>-1.262604</td>\n",
       "      <td>1.134655</td>\n",
       "      <td>-0.632821</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.126177</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.134589</td>\n",
       "      <td>-0.044180</td>\n",
       "      <td>0.684226</td>\n",
       "      <td>0.649885</td>\n",
       "      <td>0.378964</td>\n",
       "      <td>0.304221</td>\n",
       "      <td>0.708662</td>\n",
       "      <td>-0.179633</td>\n",
       "      <td>0.663055</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.134589</td>\n",
       "      <td>0.134589</td>\n",
       "      <td>-0.044180</td>\n",
       "      <td>0.304221</td>\n",
       "      <td>0.708662</td>\n",
       "      <td>-0.179633</td>\n",
       "      <td>0.663055</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.058371</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>-0.966009</td>\n",
       "      <td>0.243516</td>\n",
       "      <td>0.864200</td>\n",
       "      <td>1.266511</td>\n",
       "      <td>1.296961</td>\n",
       "      <td>1.661543</td>\n",
       "      <td>0.402289</td>\n",
       "      <td>0.812893</td>\n",
       "      <td>1.443871</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.966009</td>\n",
       "      <td>-0.966009</td>\n",
       "      <td>0.243516</td>\n",
       "      <td>1.661543</td>\n",
       "      <td>0.402289</td>\n",
       "      <td>0.812893</td>\n",
       "      <td>1.443871</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.990564</td>\n",
       "      <td>-0.618120</td>\n",
       "      <td>1.599222</td>\n",
       "      <td>2.026214</td>\n",
       "      <td>0.678820</td>\n",
       "      <td>1.378117</td>\n",
       "      <td>1.729578</td>\n",
       "      <td>0.658113</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>-0.793364</td>\n",
       "      <td>0.490947</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.599222</td>\n",
       "      <td>1.599222</td>\n",
       "      <td>2.026214</td>\n",
       "      <td>0.658113</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>-0.793364</td>\n",
       "      <td>0.490947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.854951</td>\n",
       "      <td>0.266478</td>\n",
       "      <td>-0.465984</td>\n",
       "      <td>0.304551</td>\n",
       "      <td>0.490881</td>\n",
       "      <td>-0.029604</td>\n",
       "      <td>-0.218026</td>\n",
       "      <td>0.067705</td>\n",
       "      <td>-0.713350</td>\n",
       "      <td>-0.410314</td>\n",
       "      <td>-0.399335</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.465984</td>\n",
       "      <td>-0.465984</td>\n",
       "      <td>0.304551</td>\n",
       "      <td>0.067705</td>\n",
       "      <td>-0.713350</td>\n",
       "      <td>-0.410314</td>\n",
       "      <td>-0.399335</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.787145</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.425189</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>0.079576</td>\n",
       "      <td>-0.177748</td>\n",
       "      <td>-0.319218</td>\n",
       "      <td>-0.553592</td>\n",
       "      <td>-1.283767</td>\n",
       "      <td>-0.424785</td>\n",
       "      <td>-1.202930</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.425189</td>\n",
       "      <td>0.425189</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>-0.553592</td>\n",
       "      <td>-1.283767</td>\n",
       "      <td>-0.424785</td>\n",
       "      <td>-1.202930</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.719338</td>\n",
       "      <td>-0.870863</td>\n",
       "      <td>-0.227826</td>\n",
       "      <td>0.478026</td>\n",
       "      <td>-2.547637</td>\n",
       "      <td>-0.679873</td>\n",
       "      <td>-0.296811</td>\n",
       "      <td>-0.412388</td>\n",
       "      <td>-0.965301</td>\n",
       "      <td>2.246635</td>\n",
       "      <td>-0.901740</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.227826</td>\n",
       "      <td>-0.227826</td>\n",
       "      <td>0.478026</td>\n",
       "      <td>-0.412388</td>\n",
       "      <td>-0.965301</td>\n",
       "      <td>2.246635</td>\n",
       "      <td>-0.901740</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.651532</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>-0.664926</td>\n",
       "      <td>0.354131</td>\n",
       "      <td>-0.327410</td>\n",
       "      <td>-0.432201</td>\n",
       "      <td>0.554858</td>\n",
       "      <td>0.100956</td>\n",
       "      <td>1.604246</td>\n",
       "      <td>0.461207</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>-0.664926</td>\n",
       "      <td>0.554858</td>\n",
       "      <td>0.100956</td>\n",
       "      <td>1.604246</td>\n",
       "      <td>0.461207</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.515919</td>\n",
       "      <td>-1.881833</td>\n",
       "      <td>0.689346</td>\n",
       "      <td>0.650896</td>\n",
       "      <td>-2.327480</td>\n",
       "      <td>-1.033131</td>\n",
       "      <td>0.083065</td>\n",
       "      <td>0.205379</td>\n",
       "      <td>-0.974371</td>\n",
       "      <td>-1.134988</td>\n",
       "      <td>-0.464509</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.689346</td>\n",
       "      <td>0.689346</td>\n",
       "      <td>0.650896</td>\n",
       "      <td>0.205379</td>\n",
       "      <td>-0.974371</td>\n",
       "      <td>-1.134988</td>\n",
       "      <td>-0.464509</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.448113</td>\n",
       "      <td>1.024706</td>\n",
       "      <td>-0.897646</td>\n",
       "      <td>-0.644299</td>\n",
       "      <td>0.555234</td>\n",
       "      <td>0.130399</td>\n",
       "      <td>-0.199947</td>\n",
       "      <td>-0.461809</td>\n",
       "      <td>-0.137894</td>\n",
       "      <td>0.076018</td>\n",
       "      <td>-0.417685</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.897646</td>\n",
       "      <td>-0.897646</td>\n",
       "      <td>-0.644299</td>\n",
       "      <td>-0.461809</td>\n",
       "      <td>-0.137894</td>\n",
       "      <td>0.076018</td>\n",
       "      <td>-0.417685</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.380306</td>\n",
       "      <td>0.140107</td>\n",
       "      <td>0.188562</td>\n",
       "      <td>-0.790605</td>\n",
       "      <td>-1.910090</td>\n",
       "      <td>-2.800318</td>\n",
       "      <td>-2.453932</td>\n",
       "      <td>0.479843</td>\n",
       "      <td>-0.137894</td>\n",
       "      <td>-0.580842</td>\n",
       "      <td>0.257461</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.188562</td>\n",
       "      <td>0.188562</td>\n",
       "      <td>-0.790605</td>\n",
       "      <td>0.479843</td>\n",
       "      <td>-0.137894</td>\n",
       "      <td>-0.580842</td>\n",
       "      <td>0.257461</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.312500</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.664506</td>\n",
       "      <td>0.486480</td>\n",
       "      <td>0.338560</td>\n",
       "      <td>0.948412</td>\n",
       "      <td>1.008805</td>\n",
       "      <td>-1.676162</td>\n",
       "      <td>-0.880646</td>\n",
       "      <td>-0.283198</td>\n",
       "      <td>-1.754690</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.664506</td>\n",
       "      <td>0.664506</td>\n",
       "      <td>0.486480</td>\n",
       "      <td>-1.676162</td>\n",
       "      <td>-0.880646</td>\n",
       "      <td>-0.283198</td>\n",
       "      <td>-1.754690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.244693</td>\n",
       "      <td>-1.123605</td>\n",
       "      <td>-2.091064</td>\n",
       "      <td>0.192491</td>\n",
       "      <td>0.655642</td>\n",
       "      <td>0.783472</td>\n",
       "      <td>0.590722</td>\n",
       "      <td>-1.008974</td>\n",
       "      <td>-0.768779</td>\n",
       "      <td>-0.484086</td>\n",
       "      <td>-1.206094</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-2.091064</td>\n",
       "      <td>-2.091064</td>\n",
       "      <td>0.192491</td>\n",
       "      <td>-1.008974</td>\n",
       "      <td>-0.768779</td>\n",
       "      <td>-0.484086</td>\n",
       "      <td>-1.206094</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.176887</td>\n",
       "      <td>-2.513689</td>\n",
       "      <td>1.860835</td>\n",
       "      <td>-0.345382</td>\n",
       "      <td>0.605188</td>\n",
       "      <td>0.302724</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>3.173306</td>\n",
       "      <td>0.288407</td>\n",
       "      <td>-0.402653</td>\n",
       "      <td>2.456274</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.860835</td>\n",
       "      <td>1.860835</td>\n",
       "      <td>-0.345382</td>\n",
       "      <td>3.173306</td>\n",
       "      <td>0.288407</td>\n",
       "      <td>-0.402653</td>\n",
       "      <td>2.456274</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.109080</td>\n",
       "      <td>-0.744491</td>\n",
       "      <td>0.371948</td>\n",
       "      <td>-0.873803</td>\n",
       "      <td>0.068986</td>\n",
       "      <td>-1.349706</td>\n",
       "      <td>-1.475606</td>\n",
       "      <td>-1.197834</td>\n",
       "      <td>1.598553</td>\n",
       "      <td>-0.516433</td>\n",
       "      <td>0.144831</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.371948</td>\n",
       "      <td>0.371948</td>\n",
       "      <td>-0.873803</td>\n",
       "      <td>-1.197834</td>\n",
       "      <td>1.598553</td>\n",
       "      <td>-0.516433</td>\n",
       "      <td>0.144831</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.041274</td>\n",
       "      <td>0.771963</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>0.542678</td>\n",
       "      <td>0.287759</td>\n",
       "      <td>0.930711</td>\n",
       "      <td>0.872378</td>\n",
       "      <td>-0.986028</td>\n",
       "      <td>-0.079441</td>\n",
       "      <td>-0.199494</td>\n",
       "      <td>-0.756840</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>0.542678</td>\n",
       "      <td>-0.986028</td>\n",
       "      <td>-0.079441</td>\n",
       "      <td>-0.199494</td>\n",
       "      <td>-0.756840</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.266478</td>\n",
       "      <td>-1.278524</td>\n",
       "      <td>1.093533</td>\n",
       "      <td>0.265323</td>\n",
       "      <td>0.937762</td>\n",
       "      <td>0.903851</td>\n",
       "      <td>0.951994</td>\n",
       "      <td>1.993612</td>\n",
       "      <td>1.638862</td>\n",
       "      <td>1.934254</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-1.278524</td>\n",
       "      <td>-1.278524</td>\n",
       "      <td>1.093533</td>\n",
       "      <td>0.951994</td>\n",
       "      <td>1.993612</td>\n",
       "      <td>1.638862</td>\n",
       "      <td>1.934254</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.094339</td>\n",
       "      <td>0.898335</td>\n",
       "      <td>-0.800559</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.534792</td>\n",
       "      <td>0.203951</td>\n",
       "      <td>0.145733</td>\n",
       "      <td>-1.274614</td>\n",
       "      <td>-0.799014</td>\n",
       "      <td>0.094461</td>\n",
       "      <td>-1.415535</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.800559</td>\n",
       "      <td>-0.800559</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>-1.274614</td>\n",
       "      <td>-0.799014</td>\n",
       "      <td>0.094461</td>\n",
       "      <td>-1.415535</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.162146</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.329628</td>\n",
       "      <td>1.021402</td>\n",
       "      <td>0.647270</td>\n",
       "      <td>0.585991</td>\n",
       "      <td>0.551519</td>\n",
       "      <td>0.658996</td>\n",
       "      <td>1.105737</td>\n",
       "      <td>-0.990280</td>\n",
       "      <td>1.166726</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.329628</td>\n",
       "      <td>0.329628</td>\n",
       "      <td>1.021402</td>\n",
       "      <td>0.658996</td>\n",
       "      <td>1.105737</td>\n",
       "      <td>-0.990280</td>\n",
       "      <td>1.166726</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.229952</td>\n",
       "      <td>0.266478</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>1.285578</td>\n",
       "      <td>0.476165</td>\n",
       "      <td>1.240884</td>\n",
       "      <td>1.330397</td>\n",
       "      <td>1.274997</td>\n",
       "      <td>0.238017</td>\n",
       "      <td>-0.854652</td>\n",
       "      <td>1.063587</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>1.285578</td>\n",
       "      <td>1.274997</td>\n",
       "      <td>0.238017</td>\n",
       "      <td>-0.854652</td>\n",
       "      <td>1.063587</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.297759</td>\n",
       "      <td>-0.112635</td>\n",
       "      <td>0.607730</td>\n",
       "      <td>-0.163014</td>\n",
       "      <td>0.642456</td>\n",
       "      <td>0.311831</td>\n",
       "      <td>0.183098</td>\n",
       "      <td>-0.381500</td>\n",
       "      <td>2.196181</td>\n",
       "      <td>-0.049963</td>\n",
       "      <td>1.105349</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.607730</td>\n",
       "      <td>0.607730</td>\n",
       "      <td>-0.163014</td>\n",
       "      <td>-0.381500</td>\n",
       "      <td>2.196181</td>\n",
       "      <td>-0.049963</td>\n",
       "      <td>1.105349</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.365565</td>\n",
       "      <td>0.140107</td>\n",
       "      <td>-0.059714</td>\n",
       "      <td>0.582625</td>\n",
       "      <td>0.049328</td>\n",
       "      <td>0.153526</td>\n",
       "      <td>0.069102</td>\n",
       "      <td>0.259213</td>\n",
       "      <td>0.046534</td>\n",
       "      <td>-0.523243</td>\n",
       "      <td>0.215067</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.059714</td>\n",
       "      <td>-0.059714</td>\n",
       "      <td>0.582625</td>\n",
       "      <td>0.259213</td>\n",
       "      <td>0.046534</td>\n",
       "      <td>-0.523243</td>\n",
       "      <td>0.215067</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.501178</td>\n",
       "      <td>0.898335</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.949039</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>0.770758</td>\n",
       "      <td>0.842493</td>\n",
       "      <td>0.137424</td>\n",
       "      <td>-0.225573</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>-0.043096</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.949039</td>\n",
       "      <td>0.137424</td>\n",
       "      <td>-0.225573</td>\n",
       "      <td>-0.433013</td>\n",
       "      <td>-0.043096</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.568985</td>\n",
       "      <td>-0.112635</td>\n",
       "      <td>1.350282</td>\n",
       "      <td>0.615874</td>\n",
       "      <td>-0.583256</td>\n",
       "      <td>0.913376</td>\n",
       "      <td>1.171979</td>\n",
       "      <td>1.096728</td>\n",
       "      <td>-0.861497</td>\n",
       "      <td>-2.178019</td>\n",
       "      <td>0.245439</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.350282</td>\n",
       "      <td>1.350282</td>\n",
       "      <td>0.615875</td>\n",
       "      <td>1.096728</td>\n",
       "      <td>-0.861497</td>\n",
       "      <td>-2.178019</td>\n",
       "      <td>0.245439</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.636791</td>\n",
       "      <td>-1.249976</td>\n",
       "      <td>-0.175791</td>\n",
       "      <td>-0.489940</td>\n",
       "      <td>0.311270</td>\n",
       "      <td>-0.249508</td>\n",
       "      <td>-0.152272</td>\n",
       "      <td>-0.991324</td>\n",
       "      <td>0.129174</td>\n",
       "      <td>-0.316963</td>\n",
       "      <td>-0.629657</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.175791</td>\n",
       "      <td>-0.175791</td>\n",
       "      <td>-0.489940</td>\n",
       "      <td>-0.991324</td>\n",
       "      <td>0.129174</td>\n",
       "      <td>-0.316963</td>\n",
       "      <td>-0.629657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.772404</td>\n",
       "      <td>-1.249976</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>-0.212429</td>\n",
       "      <td>0.361575</td>\n",
       "      <td>-0.057083</td>\n",
       "      <td>-0.139227</td>\n",
       "      <td>-0.265007</td>\n",
       "      <td>0.785255</td>\n",
       "      <td>-0.403504</td>\n",
       "      <td>0.303019</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>-0.212429</td>\n",
       "      <td>-0.265007</td>\n",
       "      <td>0.785255</td>\n",
       "      <td>-0.403504</td>\n",
       "      <td>0.303019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.840211</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.559364</td>\n",
       "      <td>1.652356</td>\n",
       "      <td>0.198994</td>\n",
       "      <td>1.020969</td>\n",
       "      <td>1.274764</td>\n",
       "      <td>-0.461809</td>\n",
       "      <td>0.569585</td>\n",
       "      <td>-1.058094</td>\n",
       "      <td>0.026507</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.559364</td>\n",
       "      <td>0.559364</td>\n",
       "      <td>1.652356</td>\n",
       "      <td>-0.461809</td>\n",
       "      <td>0.569585</td>\n",
       "      <td>-1.058094</td>\n",
       "      <td>0.026507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.908017</td>\n",
       "      <td>-1.123605</td>\n",
       "      <td>-1.398015</td>\n",
       "      <td>-0.776928</td>\n",
       "      <td>0.754540</td>\n",
       "      <td>0.195315</td>\n",
       "      <td>-0.096069</td>\n",
       "      <td>0.198318</td>\n",
       "      <td>2.939940</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>1.988038</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-1.398015</td>\n",
       "      <td>-1.398015</td>\n",
       "      <td>-0.776928</td>\n",
       "      <td>0.198318</td>\n",
       "      <td>2.939940</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>1.988038</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.975824</td>\n",
       "      <td>0.771963</td>\n",
       "      <td>1.122162</td>\n",
       "      <td>0.668180</td>\n",
       "      <td>-0.437912</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.895347</td>\n",
       "      <td>-2.076828</td>\n",
       "      <td>-0.010911</td>\n",
       "      <td>-0.219073</td>\n",
       "      <td>-1.495894</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.122162</td>\n",
       "      <td>1.122162</td>\n",
       "      <td>0.668180</td>\n",
       "      <td>-2.076828</td>\n",
       "      <td>-0.010911</td>\n",
       "      <td>-0.219073</td>\n",
       "      <td>-1.495894</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.043630</td>\n",
       "      <td>0.645592</td>\n",
       "      <td>0.752826</td>\n",
       "      <td>1.182675</td>\n",
       "      <td>0.558811</td>\n",
       "      <td>1.167165</td>\n",
       "      <td>1.359614</td>\n",
       "      <td>0.169195</td>\n",
       "      <td>-1.755420</td>\n",
       "      <td>-0.857206</td>\n",
       "      <td>-0.980834</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.752826</td>\n",
       "      <td>0.752826</td>\n",
       "      <td>1.182675</td>\n",
       "      <td>0.169195</td>\n",
       "      <td>-1.755420</td>\n",
       "      <td>-0.857206</td>\n",
       "      <td>-0.980834</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.111437</td>\n",
       "      <td>1.277448</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.331913</td>\n",
       "      <td>0.766285</td>\n",
       "      <td>-0.319534</td>\n",
       "      <td>-0.445533</td>\n",
       "      <td>-0.252651</td>\n",
       "      <td>0.144291</td>\n",
       "      <td>0.579658</td>\n",
       "      <td>-0.090552</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.331913</td>\n",
       "      <td>-0.252651</td>\n",
       "      <td>0.144291</td>\n",
       "      <td>0.579658</td>\n",
       "      <td>-0.090552</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.179243</td>\n",
       "      <td>0.519221</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>-0.120041</td>\n",
       "      <td>1.297212</td>\n",
       "      <td>0.853710</td>\n",
       "      <td>0.449418</td>\n",
       "      <td>-0.961318</td>\n",
       "      <td>-0.219526</td>\n",
       "      <td>1.041304</td>\n",
       "      <td>-0.827076</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>0.879785</td>\n",
       "      <td>-0.120041</td>\n",
       "      <td>-0.961318</td>\n",
       "      <td>-0.219526</td>\n",
       "      <td>1.041304</td>\n",
       "      <td>-0.827076</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.247050</td>\n",
       "      <td>1.530191</td>\n",
       "      <td>-0.497236</td>\n",
       "      <td>-1.044063</td>\n",
       "      <td>0.190382</td>\n",
       "      <td>-1.522554</td>\n",
       "      <td>-1.553507</td>\n",
       "      <td>-0.104387</td>\n",
       "      <td>0.515163</td>\n",
       "      <td>-0.147002</td>\n",
       "      <td>0.248603</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.497236</td>\n",
       "      <td>-0.497236</td>\n",
       "      <td>-1.044063</td>\n",
       "      <td>-0.104387</td>\n",
       "      <td>0.515163</td>\n",
       "      <td>-0.147002</td>\n",
       "      <td>0.248603</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.314856</td>\n",
       "      <td>0.898335</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>2.207664</td>\n",
       "      <td>-0.371849</td>\n",
       "      <td>0.808559</td>\n",
       "      <td>1.026628</td>\n",
       "      <td>-0.031138</td>\n",
       "      <td>-0.509774</td>\n",
       "      <td>2.665720</td>\n",
       "      <td>-0.342388</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>0.865679</td>\n",
       "      <td>2.207664</td>\n",
       "      <td>-0.031138</td>\n",
       "      <td>-0.509774</td>\n",
       "      <td>2.665720</td>\n",
       "      <td>-0.342388</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.382663</td>\n",
       "      <td>1.151077</td>\n",
       "      <td>-0.355183</td>\n",
       "      <td>-2.604944</td>\n",
       "      <td>0.175968</td>\n",
       "      <td>-0.345214</td>\n",
       "      <td>-0.555732</td>\n",
       "      <td>-0.978086</td>\n",
       "      <td>1.460484</td>\n",
       "      <td>0.456514</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-0.355183</td>\n",
       "      <td>-0.355183</td>\n",
       "      <td>-2.604944</td>\n",
       "      <td>-0.978086</td>\n",
       "      <td>1.460484</td>\n",
       "      <td>0.456514</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.450469</td>\n",
       "      <td>-0.997234</td>\n",
       "      <td>0.656095</td>\n",
       "      <td>0.265871</td>\n",
       "      <td>0.858991</td>\n",
       "      <td>0.731072</td>\n",
       "      <td>0.459497</td>\n",
       "      <td>0.388061</td>\n",
       "      <td>-1.356329</td>\n",
       "      <td>-0.237799</td>\n",
       "      <td>-0.573342</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>0.656095</td>\n",
       "      <td>0.656095</td>\n",
       "      <td>0.265871</td>\n",
       "      <td>0.388061</td>\n",
       "      <td>-1.356329</td>\n",
       "      <td>-0.237799</td>\n",
       "      <td>-0.573342</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.518276</td>\n",
       "      <td>-2.008204</td>\n",
       "      <td>-3.466132</td>\n",
       "      <td>1.047371</td>\n",
       "      <td>0.405024</td>\n",
       "      <td>0.479549</td>\n",
       "      <td>0.428651</td>\n",
       "      <td>0.059762</td>\n",
       "      <td>0.081807</td>\n",
       "      <td>0.652295</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-3.466132</td>\n",
       "      <td>-3.466132</td>\n",
       "      <td>1.047371</td>\n",
       "      <td>0.059762</td>\n",
       "      <td>0.081807</td>\n",
       "      <td>0.652295</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.586082</td>\n",
       "      <td>1.277448</td>\n",
       "      <td>-1.075389</td>\n",
       "      <td>-0.658092</td>\n",
       "      <td>0.299463</td>\n",
       "      <td>0.258017</td>\n",
       "      <td>0.192421</td>\n",
       "      <td>-0.066439</td>\n",
       "      <td>1.113799</td>\n",
       "      <td>0.518370</td>\n",
       "      <td>0.651666</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>-1.075389</td>\n",
       "      <td>-1.075389</td>\n",
       "      <td>-0.658092</td>\n",
       "      <td>-0.066439</td>\n",
       "      <td>1.113799</td>\n",
       "      <td>0.518370</td>\n",
       "      <td>0.651666</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.653889</td>\n",
       "      <td>0.519221</td>\n",
       "      <td>1.029828</td>\n",
       "      <td>0.767750</td>\n",
       "      <td>0.087472</td>\n",
       "      <td>1.189511</td>\n",
       "      <td>1.269510</td>\n",
       "      <td>-1.775005</td>\n",
       "      <td>0.047542</td>\n",
       "      <td>1.614744</td>\n",
       "      <td>-1.242794</td>\n",
       "      <td>Alzheimer</td>\n",
       "      <td>1.029828</td>\n",
       "      <td>1.029828</td>\n",
       "      <td>0.767750</td>\n",
       "      <td>-1.775005</td>\n",
       "      <td>0.047542</td>\n",
       "      <td>1.614744</td>\n",
       "      <td>-1.242794</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Patient___CDR__ADAS       Age  __Change_from_baseline__ADAS_Cog11_  \\\n",
       "0             -1.668629 -0.618120                             0.317537   \n",
       "1             -1.600823  1.277448                             0.014153   \n",
       "2             -1.533016  1.151077                            -0.850871   \n",
       "3             -1.465210 -0.239006                            -1.875980   \n",
       "4             -1.397403 -2.134575                             0.539000   \n",
       "5             -1.329597 -0.112635                            -0.028462   \n",
       "6             -1.261790 -0.365378                             0.440311   \n",
       "7             -1.193984 -0.870863                             0.160349   \n",
       "8             -1.126177  0.645592                             0.134589   \n",
       "9             -1.058371  0.645592                            -0.966009   \n",
       "10            -0.990564 -0.618120                             1.599222   \n",
       "12            -0.854951  0.266478                            -0.465984   \n",
       "13            -0.787145  0.013736                             0.425189   \n",
       "14            -0.719338 -0.870863                            -0.227826   \n",
       "15            -0.651532  0.013736                             0.004434   \n",
       "16            -0.515919 -1.881833                             0.689346   \n",
       "17            -0.448113  1.024706                            -0.897646   \n",
       "18            -0.380306  0.140107                             0.188562   \n",
       "19            -0.312500  0.645592                             0.664506   \n",
       "20            -0.244693 -1.123605                            -2.091064   \n",
       "21            -0.176887 -2.513689                             1.860835   \n",
       "22            -0.109080 -0.744491                             0.371948   \n",
       "23            -0.041274  0.771963                            -0.008930   \n",
       "24             0.026533  0.266478                            -1.278524   \n",
       "25             0.094339  0.898335                            -0.800559   \n",
       "26             0.162146  0.645592                             0.329628   \n",
       "27             0.229952  0.266478                             0.027344   \n",
       "28             0.297759 -0.112635                             0.607730   \n",
       "29             0.365565  0.140107                            -0.059714   \n",
       "30             0.501178  0.898335                             0.879785   \n",
       "31             0.568985 -0.112635                             1.350282   \n",
       "32             0.636791 -1.249976                            -0.175791   \n",
       "33             0.772404 -1.249976                            -0.816854   \n",
       "34             0.840211  0.645592                             0.559364   \n",
       "35             0.908017 -1.123605                            -1.398015   \n",
       "36             0.975824  0.771963                             1.122162   \n",
       "37             1.043630  0.645592                             0.752826   \n",
       "38             1.111437  1.277448                             0.865679   \n",
       "39             1.179243  0.519221                             0.879785   \n",
       "40             1.247050  1.530191                            -0.497236   \n",
       "41             1.314856  0.898335                             0.865679   \n",
       "42             1.382663  1.151077                            -0.355183   \n",
       "43             1.450469 -0.997234                             0.656095   \n",
       "44             1.518276 -2.008204                            -3.466132   \n",
       "45             1.586082  1.277448                            -1.075389   \n",
       "46             1.653889  0.519221                             1.029828   \n",
       "\n",
       "    Subiculum_Connectivity  Amnesia_Lesion_T_Map  Memory_Network_T  \\\n",
       "0                -1.296803             -0.075475         -1.732372   \n",
       "1                -1.780375             -0.200665         -1.624891   \n",
       "2                -0.601947              0.493589         -0.507664   \n",
       "3                -0.955650             -0.240569         -1.365571   \n",
       "4                -1.164703             -2.937139         -1.772763   \n",
       "5                -0.494611              0.674839         -0.308079   \n",
       "6                -1.737296             -2.309392         -1.582906   \n",
       "7                -1.158354             -2.332131         -1.114775   \n",
       "8                -0.044180              0.684226          0.649885   \n",
       "9                 0.243516              0.864200          1.266511   \n",
       "10                2.026214              0.678820          1.378117   \n",
       "12                0.304551              0.490881         -0.029604   \n",
       "13                0.015603              0.079576         -0.177748   \n",
       "14                0.478026             -2.547637         -0.679873   \n",
       "15               -0.664926              0.354131         -0.327410   \n",
       "16                0.650896             -2.327480         -1.033131   \n",
       "17               -0.644299              0.555234          0.130399   \n",
       "18               -0.790605             -1.910090         -2.800318   \n",
       "19                0.486480              0.338560          0.948412   \n",
       "20                0.192491              0.655642          0.783472   \n",
       "21               -0.345382              0.605188          0.302724   \n",
       "22               -0.873803              0.068986         -1.349706   \n",
       "23                0.542678              0.287759          0.930711   \n",
       "24                1.093533              0.265323          0.937762   \n",
       "25                0.010097              0.534792          0.203951   \n",
       "26                1.021402              0.647270          0.585991   \n",
       "27                1.285578              0.476165          1.240884   \n",
       "28               -0.163014              0.642456          0.311831   \n",
       "29                0.582625              0.049328          0.153526   \n",
       "30                0.949039              0.560647          0.770758   \n",
       "31                0.615874             -0.583256          0.913376   \n",
       "32               -0.489940              0.311270         -0.249508   \n",
       "33               -0.212429              0.361575         -0.057083   \n",
       "34                1.652356              0.198994          1.020969   \n",
       "35               -0.776928              0.754540          0.195315   \n",
       "36                0.668180             -0.437912          0.688525   \n",
       "37                1.182675              0.558811          1.167165   \n",
       "38                0.331913              0.766285         -0.319534   \n",
       "39               -0.120041              1.297212          0.853710   \n",
       "40               -1.044063              0.190382         -1.522554   \n",
       "41                2.207664             -0.371849          0.808559   \n",
       "42               -2.604944              0.175968         -0.345214   \n",
       "43                0.265871              0.858991          0.731072   \n",
       "44                1.047371              0.405024          0.479549   \n",
       "45               -0.658092              0.299463          0.258017   \n",
       "46                0.767750              0.087472          1.189511   \n",
       "\n",
       "    Memory_Network_R  Subiculum_Grey_Matter  Subiculum_White_Matter  \\\n",
       "0          -1.915962               1.923652                0.690521   \n",
       "1          -1.931023              -0.170576               -1.631460   \n",
       "2          -0.812619               0.816968               -0.736530   \n",
       "3          -1.417006               0.183316               -0.041145   \n",
       "4          -1.417194               0.276863               -0.117738   \n",
       "5          -0.521108               1.518574               -0.378759   \n",
       "6          -1.307928              -1.154590               -1.339197   \n",
       "7          -0.863014               0.223029               -1.262604   \n",
       "8           0.378964               0.304221                0.708662   \n",
       "9           1.296961               1.661543                0.402289   \n",
       "10          1.729578               0.658113                0.030409   \n",
       "12         -0.218026               0.067705               -0.713350   \n",
       "13         -0.319218              -0.553592               -1.283767   \n",
       "14         -0.296811              -0.412388               -0.965301   \n",
       "15         -0.432201               0.554858                0.100956   \n",
       "16          0.083065               0.205379               -0.974371   \n",
       "17         -0.199947              -0.461809               -0.137894   \n",
       "18         -2.453932               0.479843               -0.137894   \n",
       "19          1.008805              -1.676162               -0.880646   \n",
       "20          0.590722              -1.008974               -0.768779   \n",
       "21          0.009441               3.173306                0.288407   \n",
       "22         -1.475606              -1.197834                1.598553   \n",
       "23          0.872378              -0.986028               -0.079441   \n",
       "24          0.903851               0.951994                1.993612   \n",
       "25          0.145733              -1.274614               -0.799014   \n",
       "26          0.551519               0.658996                1.105737   \n",
       "27          1.330397               1.274997                0.238017   \n",
       "28          0.183098              -0.381500                2.196181   \n",
       "29          0.069102               0.259213                0.046534   \n",
       "30          0.842493               0.137424               -0.225573   \n",
       "31          1.171979               1.096728               -0.861497   \n",
       "32         -0.152272              -0.991324                0.129174   \n",
       "33         -0.139227              -0.265007                0.785255   \n",
       "34          1.274764              -0.461809                0.569585   \n",
       "35         -0.096069               0.198318                2.939940   \n",
       "36          0.895347              -2.076828               -0.010911   \n",
       "37          1.359614               0.169195               -1.755420   \n",
       "38         -0.445533              -0.252651                0.144291   \n",
       "39          0.449418              -0.961318               -0.219526   \n",
       "40         -1.553507              -0.104387                0.515163   \n",
       "41          1.026628              -0.031138               -0.509774   \n",
       "42         -0.555732              -0.978086                1.460484   \n",
       "43          0.459497               0.388061               -1.356329   \n",
       "44          0.428651               0.059762                0.081807   \n",
       "45          0.192421              -0.066439                1.113799   \n",
       "46          1.269510              -1.775005                0.047542   \n",
       "\n",
       "    Subiculum_CSF  Subiculum_Total    Disease  Standardized_Age  \\\n",
       "0        1.804283         1.812765  Alzheimer          0.317537   \n",
       "1       -0.219924        -1.146615  Alzheimer          0.014153   \n",
       "2       -0.408611         0.123318  Alzheimer         -0.850871   \n",
       "3       -0.523810         0.105601  Alzheimer         -1.875980   \n",
       "4       -1.786457         0.124583  Alzheimer          0.539000   \n",
       "5       -1.205923         0.850983  Alzheimer         -0.028462   \n",
       "6        0.034024        -1.668636  Alzheimer          0.440311   \n",
       "7        1.134655        -0.632821  Alzheimer          0.160349   \n",
       "8       -0.179633         0.663055  Alzheimer          0.134589   \n",
       "9        0.812893         1.443871  Alzheimer         -0.966009   \n",
       "10      -0.793364         0.490947  Alzheimer          1.599222   \n",
       "12      -0.410314        -0.399335  Alzheimer         -0.465984   \n",
       "13      -0.424785        -1.202930  Alzheimer          0.425189   \n",
       "14       2.246635        -0.901740  Alzheimer         -0.227826   \n",
       "15       1.604246         0.461207  Alzheimer          0.004434   \n",
       "16      -1.134988        -0.464509  Alzheimer          0.689346   \n",
       "17       0.076018        -0.417685  Alzheimer         -0.897646   \n",
       "18      -0.580842         0.257461  Alzheimer          0.188562   \n",
       "19      -0.283198        -1.754690  Alzheimer          0.664506   \n",
       "20      -0.484086        -1.206094  Alzheimer         -2.091064   \n",
       "21      -0.402653         2.456274  Alzheimer          1.860835   \n",
       "22      -0.516433         0.144831  Alzheimer          0.371948   \n",
       "23      -0.199494        -0.756840  Alzheimer         -0.008930   \n",
       "24       1.638862         1.934254  Alzheimer         -1.278524   \n",
       "25       0.094461        -1.415535  Alzheimer         -0.800559   \n",
       "26      -0.990280         1.166726  Alzheimer          0.329628   \n",
       "27      -0.854652         1.063587  Alzheimer          0.027344   \n",
       "28      -0.049963         1.105349  Alzheimer          0.607730   \n",
       "29      -0.523243         0.215067  Alzheimer         -0.059714   \n",
       "30      -0.433013        -0.043096  Alzheimer          0.879785   \n",
       "31      -2.178019         0.245439  Alzheimer          1.350282   \n",
       "32      -0.316963        -0.629657  Alzheimer         -0.175791   \n",
       "33      -0.403504         0.303019  Alzheimer         -0.816854   \n",
       "34      -1.058094         0.026507  Alzheimer          0.559364   \n",
       "35       0.848644         1.988038  Alzheimer         -1.398015   \n",
       "36      -0.219073        -1.495894  Alzheimer          1.122162   \n",
       "37      -0.857206        -0.980834  Alzheimer          0.752826   \n",
       "38       0.579658        -0.090552  Alzheimer          0.865679   \n",
       "39       1.041304        -0.827076  Alzheimer          0.879785   \n",
       "40      -0.147002         0.248603  Alzheimer         -0.497236   \n",
       "41       2.665720        -0.342388  Alzheimer          0.865679   \n",
       "42       0.456514         0.215700  Alzheimer         -0.355183   \n",
       "43      -0.237799        -0.573342  Alzheimer          0.656095   \n",
       "44       0.652295         0.094211  Alzheimer         -3.466132   \n",
       "45       0.518370         0.651666  Alzheimer         -1.075389   \n",
       "46       1.614744        -1.242794  Alzheimer          1.029828   \n",
       "\n",
       "    Standardized_Percent_Improvement  Standardized_Subiculum_Connectivity  \\\n",
       "0                           0.317537                            -1.296803   \n",
       "1                           0.014153                            -1.780375   \n",
       "2                          -0.850871                            -0.601947   \n",
       "3                          -1.875980                            -0.955650   \n",
       "4                           0.539000                            -1.164703   \n",
       "5                          -0.028462                            -0.494611   \n",
       "6                           0.440311                            -1.737296   \n",
       "7                           0.160349                            -1.158354   \n",
       "8                           0.134589                            -0.044180   \n",
       "9                          -0.966009                             0.243516   \n",
       "10                          1.599222                             2.026214   \n",
       "12                         -0.465984                             0.304551   \n",
       "13                          0.425189                             0.015603   \n",
       "14                         -0.227826                             0.478026   \n",
       "15                          0.004434                            -0.664926   \n",
       "16                          0.689346                             0.650896   \n",
       "17                         -0.897646                            -0.644299   \n",
       "18                          0.188562                            -0.790605   \n",
       "19                          0.664506                             0.486480   \n",
       "20                         -2.091064                             0.192491   \n",
       "21                          1.860835                            -0.345382   \n",
       "22                          0.371948                            -0.873803   \n",
       "23                         -0.008930                             0.542678   \n",
       "24                         -1.278524                             1.093533   \n",
       "25                         -0.800559                             0.010097   \n",
       "26                          0.329628                             1.021402   \n",
       "27                          0.027344                             1.285578   \n",
       "28                          0.607730                            -0.163014   \n",
       "29                         -0.059714                             0.582625   \n",
       "30                          0.879785                             0.949039   \n",
       "31                          1.350282                             0.615875   \n",
       "32                         -0.175791                            -0.489940   \n",
       "33                         -0.816854                            -0.212429   \n",
       "34                          0.559364                             1.652356   \n",
       "35                         -1.398015                            -0.776928   \n",
       "36                          1.122162                             0.668180   \n",
       "37                          0.752826                             1.182675   \n",
       "38                          0.865679                             0.331913   \n",
       "39                          0.879785                            -0.120041   \n",
       "40                         -0.497236                            -1.044063   \n",
       "41                          0.865679                             2.207664   \n",
       "42                         -0.355183                            -2.604944   \n",
       "43                          0.656095                             0.265871   \n",
       "44                         -3.466132                             1.047371   \n",
       "45                         -1.075389                            -0.658092   \n",
       "46                          1.029828                             0.767750   \n",
       "\n",
       "    Standardized_Subiculum_Grey_Matter  Standardized_Subiculum_White_Matter  \\\n",
       "0                             1.923652                             0.690521   \n",
       "1                            -0.170576                            -1.631460   \n",
       "2                             0.816968                            -0.736530   \n",
       "3                             0.183316                            -0.041145   \n",
       "4                             0.276863                            -0.117738   \n",
       "5                             1.518574                            -0.378759   \n",
       "6                            -1.154590                            -1.339197   \n",
       "7                             0.223029                            -1.262604   \n",
       "8                             0.304221                             0.708662   \n",
       "9                             1.661543                             0.402289   \n",
       "10                            0.658113                             0.030409   \n",
       "12                            0.067705                            -0.713350   \n",
       "13                           -0.553592                            -1.283767   \n",
       "14                           -0.412388                            -0.965301   \n",
       "15                            0.554858                             0.100956   \n",
       "16                            0.205379                            -0.974371   \n",
       "17                           -0.461809                            -0.137894   \n",
       "18                            0.479843                            -0.137894   \n",
       "19                           -1.676162                            -0.880646   \n",
       "20                           -1.008974                            -0.768779   \n",
       "21                            3.173306                             0.288407   \n",
       "22                           -1.197834                             1.598553   \n",
       "23                           -0.986028                            -0.079441   \n",
       "24                            0.951994                             1.993612   \n",
       "25                           -1.274614                            -0.799014   \n",
       "26                            0.658996                             1.105737   \n",
       "27                            1.274997                             0.238017   \n",
       "28                           -0.381500                             2.196181   \n",
       "29                            0.259213                             0.046534   \n",
       "30                            0.137424                            -0.225573   \n",
       "31                            1.096728                            -0.861497   \n",
       "32                           -0.991324                             0.129174   \n",
       "33                           -0.265007                             0.785255   \n",
       "34                           -0.461809                             0.569585   \n",
       "35                            0.198318                             2.939940   \n",
       "36                           -2.076828                            -0.010911   \n",
       "37                            0.169195                            -1.755420   \n",
       "38                           -0.252651                             0.144291   \n",
       "39                           -0.961318                            -0.219526   \n",
       "40                           -0.104387                             0.515163   \n",
       "41                           -0.031138                            -0.509774   \n",
       "42                           -0.978086                             1.460484   \n",
       "43                            0.388061                            -1.356329   \n",
       "44                            0.059762                             0.081807   \n",
       "45                           -0.066439                             1.113799   \n",
       "46                           -1.775005                             0.047542   \n",
       "\n",
       "    Standardized_Subiculum_CSF  Standardized_Subiculum_Total  One_Hot_Disease  \n",
       "0                     1.804283                      1.812765              NaN  \n",
       "1                    -0.219924                     -1.146615              NaN  \n",
       "2                    -0.408611                      0.123318              NaN  \n",
       "3                    -0.523810                      0.105601              NaN  \n",
       "4                    -1.786457                      0.124583              NaN  \n",
       "5                    -1.205923                      0.850983              NaN  \n",
       "6                     0.034024                     -1.668636              NaN  \n",
       "7                     1.134655                     -0.632821              NaN  \n",
       "8                    -0.179633                      0.663055              NaN  \n",
       "9                     0.812893                      1.443871              NaN  \n",
       "10                   -0.793364                      0.490947              NaN  \n",
       "12                   -0.410314                     -0.399335              NaN  \n",
       "13                   -0.424785                     -1.202930              NaN  \n",
       "14                    2.246635                     -0.901740              NaN  \n",
       "15                    1.604246                      0.461207              NaN  \n",
       "16                   -1.134988                     -0.464509              NaN  \n",
       "17                    0.076018                     -0.417685              NaN  \n",
       "18                   -0.580842                      0.257461              NaN  \n",
       "19                   -0.283198                     -1.754690              NaN  \n",
       "20                   -0.484086                     -1.206094              NaN  \n",
       "21                   -0.402653                      2.456274              NaN  \n",
       "22                   -0.516433                      0.144831              NaN  \n",
       "23                   -0.199494                     -0.756840              NaN  \n",
       "24                    1.638862                      1.934254              NaN  \n",
       "25                    0.094461                     -1.415535              NaN  \n",
       "26                   -0.990280                      1.166726              NaN  \n",
       "27                   -0.854652                      1.063587              NaN  \n",
       "28                   -0.049963                      1.105349              NaN  \n",
       "29                   -0.523243                      0.215067              NaN  \n",
       "30                   -0.433013                     -0.043096              NaN  \n",
       "31                   -2.178019                      0.245439              NaN  \n",
       "32                   -0.316963                     -0.629657              NaN  \n",
       "33                   -0.403504                      0.303019              NaN  \n",
       "34                   -1.058094                      0.026507              NaN  \n",
       "35                    0.848644                      1.988038              NaN  \n",
       "36                   -0.219073                     -1.495894              NaN  \n",
       "37                   -0.857206                     -0.980834              NaN  \n",
       "38                    0.579658                     -0.090552              NaN  \n",
       "39                    1.041304                     -0.827076              NaN  \n",
       "40                   -0.147002                      0.248603              NaN  \n",
       "41                    2.665720                     -0.342388              NaN  \n",
       "42                    0.456514                      0.215700              NaN  \n",
       "43                   -0.237799                     -0.573342              NaN  \n",
       "44                    0.652295                      0.094211              NaN  \n",
       "45                    0.518370                      0.651666              NaN  \n",
       "46                    1.614744                     -1.242794              NaN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardize the data\n",
    "import numpy as np\n",
    "\n",
    "preserved_df = data_df.copy()\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# Select the columns to be standardized\n",
    "for col in data_df.columns:\n",
    "    if col not in cols_not_to_standardize:\n",
    "        try:\n",
    "            data_df[col] = (data_df[col] - np.mean(data_df[col])) / np.std(data_df[col])\n",
    "            # scaler.fit_transform(data_df[col])\n",
    "        # cols_to_standardize = [col for col in data_df.columns if col not in cols_not_to_standardize]\n",
    "        except:\n",
    "            print('Unable to standardize column.')\n",
    "    \n",
    "# Standardize\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over = data_df['basline'] >= np.mean(data_df['basline'])\n",
    "# under = data_df['basline'] < np.mean(data_df['basline'])\n",
    "# data_df['basline'][over] = 1\n",
    "# data_df['basline'][under] = 0\n",
    "\n",
    "# display(data_df)\n",
    "# print(np.max(data_df.age))\n",
    "mc_test = data_df.copy()\n",
    "# mc_test.pop('outcome')\n",
    "# mc_test = mc_test.loc[:, ['Ventral_Attention', 'Limbic']]\n",
    "# mc_test['interaction'] = mc_test['Limbic']*mc_test['Ventral_Attention']\n",
    "from calvin_utils.statistical_utils.statistical_measurements import calculate_vif\n",
    "calculate_vif(mc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET INFORMATION ABOUT THE DATA\n",
    "# sns.pairplot(data_df)\n",
    "data_df.describe().transpose()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Statsmodel Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Patient___CDR__ADAS', 'Age', '__Change_from_baseline__ADAS_Cog11_',\n",
       "       'Subiculum_Connectivity', 'Amnesia_Lesion_T_Map', 'Memory_Network_T',\n",
       "       'Memory_Network_R', 'Subiculum_Grey_Matter', 'Subiculum_White_Matter',\n",
       "       'Subiculum_CSF', 'Subiculum_Total', 'Disease', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'One_Hot_Disease'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Results: Ordinary least squares\n",
      "====================================================================================\n",
      "Model:              OLS                                 Adj. R-squared:     0.012   \n",
      "Dependent Variable: __Change_from_baseline__ADAS_Cog11_ AIC:                134.8064\n",
      "Date:               2023-10-23 10:06                    BIC:                142.1210\n",
      "No. Observations:   46                                  Log-Likelihood:     -63.403 \n",
      "Df Model:           3                                   F-statistic:        1.184   \n",
      "Df Residuals:       42                                  Prob (F-statistic): 0.327   \n",
      "R-squared:          0.078                               Scale:              1.0098  \n",
      "---------------------------------------------------------------------------------------\n",
      "                           Coef.     Std.Err.       t       P>|t|      [0.025    0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept                 -0.0313      0.1497    -0.2089    0.8356    -0.3333    0.2708\n",
      "Age                        0.1034      0.1544     0.6696    0.5067    -0.2083    0.4151\n",
      "Memory_Network_R           0.1180      0.1521     0.7758    0.4422    -0.1890    0.4250\n",
      "Age:Memory_Network_R       0.2671      0.1804     1.4802    0.1463    -0.0970    0.6312\n",
      "------------------------------------------------------------------------------------\n",
      "Omnibus:                    7.095               Durbin-Watson:                 2.240\n",
      "Prob(Omnibus):              0.029               Jarque-Bera (JB):              6.097\n",
      "Skew:                       -0.703              Prob(JB):                      0.047\n",
      "Kurtosis:                   4.098               Condition No.:                 1    \n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "formula = '__Change_from_baseline__ADAS_Cog11_~Age * Memory_Network_R'\n",
    "#----------------------------------------------------------------DO NOT TOUCH\n",
    "results = smf.ols(formula, data=data_df).fit()\n",
    "print(results.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.317537\n",
       "1     0.014153\n",
       "2    -0.850871\n",
       "3    -1.875980\n",
       "4     0.539000\n",
       "5    -0.028462\n",
       "6     0.440311\n",
       "7     0.160349\n",
       "8     0.134589\n",
       "9    -0.966009\n",
       "10    1.599222\n",
       "12   -0.465984\n",
       "13    0.425189\n",
       "14   -0.227826\n",
       "15    0.004434\n",
       "16    0.689346\n",
       "17   -0.897646\n",
       "18    0.188562\n",
       "19    0.664506\n",
       "20   -2.091064\n",
       "21    1.860835\n",
       "22    0.371948\n",
       "23   -0.008930\n",
       "24   -1.278524\n",
       "25   -0.800559\n",
       "26    0.329628\n",
       "27    0.027344\n",
       "28    0.607730\n",
       "29   -0.059714\n",
       "30    0.879785\n",
       "31    1.350282\n",
       "32   -0.175791\n",
       "33   -0.816854\n",
       "34    0.559364\n",
       "35   -1.398015\n",
       "36    1.122162\n",
       "37    0.752826\n",
       "38    0.865679\n",
       "39    0.879785\n",
       "40   -0.497236\n",
       "41    0.865679\n",
       "42   -0.355183\n",
       "43    0.656095\n",
       "44   -3.466132\n",
       "45   -1.075389\n",
       "46    1.029828\n",
       "Name: __Change_from_baseline__ADAS_Cog11_, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.predict(data_df)\n",
    "data_df['__Change_from_baseline__ADAS_Cog11_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Diagnostics on the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import model_diagnostics\n",
    "model_diagnostics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation - LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def leave_one_out_cv(data_df: pd.DataFrame, formula: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs Leave-One-Out Cross Validation (LOOCV) on the provided data using the specified formula.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_df: A pandas DataFrame containing the data.\n",
    "    - formula: A string in the format 'response ~ predictors' specifying the regression formula.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing:\n",
    "        * Average Root Mean Squared Error from LOOCV\n",
    "        * Pearson Correlation Coefficient between actual and predicted values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure formula is correctly formatted\n",
    "    formula_parts = formula.split(\"~\")\n",
    "    if len(formula_parts) != 2:\n",
    "        raise ValueError(\"Formula should be in the format 'response ~ predictors'.\")\n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(data_df)\n",
    "\n",
    "    models = []\n",
    "    squared_error_list = []\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "\n",
    "    for train_index, test_index in loo.split(data_df):\n",
    "        train_data, test_data = data_df.iloc[train_index], data_df.iloc[test_index]\n",
    "\n",
    "        model = smf.ols(f'{formula_parts[0]} ~ {formula_parts[1]}', data=train_data).fit()\n",
    "        models.append(model)\n",
    "\n",
    "        test_x = test_data.drop(columns=[formula_parts[0]])\n",
    "        test_y = test_data[formula_parts[0]]\n",
    "        pred_y = model.predict(test_x)\n",
    "\n",
    "        squared_error = np.square(pred_y - test_y)\n",
    "        squared_error_list.extend(squared_error)\n",
    "\n",
    "        predictions.extend(pred_y)\n",
    "        actual_values.extend(test_y)\n",
    "\n",
    "    average_squared_error = np.mean(squared_error_list)\n",
    "    average_rmse = np.sqrt(average_squared_error)\n",
    "    correlation, _ = pearsonr(predictions, actual_values)\n",
    "\n",
    "    return average_rmse, correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse, pear_corr = leave_one_out_cv(data_df, results.model.formula)\n",
    "print('LOOCV Metrics')\n",
    "print('RMSE is : ', rmse)\n",
    "print('Pearson R is : ', pear_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation - K Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "class KFoldsCrossValidation:\n",
    "    \"\"\"\n",
    "    This class performs k-folds cross-validation on a given DataFrame and regression formula.\n",
    "    It calculates both in-sample and out-of-sample R-squared values as well as the RMSE (Root Mean Squared Error).\n",
    "    \n",
    "    Parameters:\n",
    "    - data_df (pd.DataFrame): The data as a Pandas DataFrame.\n",
    "    - formula (str): The regression formula in the format 'response ~ predictors'.\n",
    "    - n_splits (int): The number of folds for k-folds cross-validation. Default is 5.\n",
    "    \n",
    "    Methods:\n",
    "    - run: Performs the k-folds cross-validation and returns metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_df, formula, n_splits=5):\n",
    "        \"\"\"\n",
    "        Initialize the KFoldsCrossValidation class with data, formula, and number of splits.\n",
    "        \"\"\"\n",
    "        self.data_df = data_df\n",
    "        self.formula = formula\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run k-folds cross-validation and calculate metrics.\n",
    "        \n",
    "        Returns:\n",
    "        - A tuple containing:\n",
    "            * List of out-of-sample R-squared values for each fold\n",
    "            * List of in-sample R-squared values for each fold\n",
    "            * List of RMSE values for each fold\n",
    "            * Average out-of-sample R-squared value\n",
    "            * Average in-sample R-squared value\n",
    "            * Average RMSE value\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=1)\n",
    "        \n",
    "        out_of_sample_r_squared_values = []\n",
    "        in_sample_r_squared_values = []\n",
    "        rmse_values = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(self.data_df):\n",
    "            train, test = self.data_df.iloc[train_index], self.data_df.iloc[test_index]\n",
    "            \n",
    "            # Fit the model\n",
    "            results = smf.ols(self.formula, data=train).fit()\n",
    "            \n",
    "            # Calculate in-sample R-squared\n",
    "            in_sample_r_squared = results.rsquared\n",
    "            in_sample_r_squared_values.append(in_sample_r_squared)\n",
    "            \n",
    "            # Calculate predictions for the test set\n",
    "            predictions = results.predict(test)\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(mean_squared_error(test['outcome'], predictions))\n",
    "            rmse_values.append(rmse)\n",
    "            \n",
    "            # Calculate out-of-sample R-squared for the test set\n",
    "            sse = np.sum((test['outcome'] - predictions) ** 2)\n",
    "            tss = np.sum((test['outcome'] - np.mean(test['outcome'])) ** 2)\n",
    "            out_of_sample_r_squared = 1 - (sse / tss)\n",
    "            out_of_sample_r_squared_values.append(out_of_sample_r_squared)\n",
    "            \n",
    "        # Calculate the average out-of-sample and in-sample R-squared and RMSE\n",
    "        avg_out_of_sample_r_squared = np.mean(out_of_sample_r_squared_values)\n",
    "        avg_in_sample_r_squared = np.mean(in_sample_r_squared_values)\n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "        \n",
    "        return out_of_sample_r_squared_values, in_sample_r_squared_values, rmse_values, avg_out_of_sample_r_squared, avg_in_sample_r_squared, avg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KFoldsCrossValidation class and run the k-folds cross-validation\n",
    "kf_cv = KFoldsCrossValidation(data_df, formula, n_splits=35)\n",
    "out_of_sample_r_squared_values, in_sample_r_squared_values, rmse_values, avg_out_of_sample_r_squared, avg_in_sample_r_squared, avg_rmse = kf_cv.run()\n",
    "\n",
    "print('Folded cross-validation out-of-sample r-squared: ', avg_out_of_sample_r_squared)\n",
    "print('Folded cross-validation in-sample r-squared: ', avg_in_sample_r_squared)\n",
    "print('Folded cross-validation RMSE: ', avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Elimination of DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RegressionFormulaGeneratorUpdated:\n",
    "    def __init__(self, base_formula):\n",
    "        self.base_formula = base_formula\n",
    "        self.base_predictors = self.extract_base_predictors()\n",
    "        self.all_formulas = []\n",
    "\n",
    "    def extract_base_predictors(self):\n",
    "        return self.base_formula.split(\"~\")[1].split(\"*\")\n",
    "\n",
    "    def generate_lower_level_interactions(self, predictors):\n",
    "        lower_level_interactions = []\n",
    "        for r in range(2, len(predictors) + 1):\n",
    "            for subset in combinations(predictors, r):\n",
    "                lower_level_interactions.append(\":\".join(subset))\n",
    "        return lower_level_interactions\n",
    "\n",
    "    def is_valid_formula(self, terms):\n",
    "        interactions = [term for term in terms if \":\" in term]\n",
    "        individual_components = [term for term in terms if \":\" not in term]\n",
    "        \n",
    "        # Ensure all individual components of each interaction are present\n",
    "        for interaction in interactions:\n",
    "            components = interaction.split(\":\")\n",
    "            if not all(comp in individual_components for comp in components):\n",
    "                return False\n",
    "\n",
    "        # Ensure all lower-level interactions of each higher-level interaction are present\n",
    "        for interaction in interactions:\n",
    "            components = interaction.split(\":\")\n",
    "            if len(components) > 2:\n",
    "                lower_level_combinations = self.generate_lower_level_interactions(components)\n",
    "                if not all(lower in interactions for lower in lower_level_combinations):\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def generate_all_formulas(self):\n",
    "        predictors = self.base_predictors\n",
    "        lower_level_interactions = self.generate_lower_level_interactions(predictors)\n",
    "\n",
    "        # Generate power set of all possible terms (base predictors + interactions)\n",
    "        for r in range(1, len(predictors) + len(lower_level_interactions) + 1):\n",
    "            for subset in combinations(predictors + lower_level_interactions, r):\n",
    "                if self.is_valid_formula(subset):\n",
    "                    formula = f\"outcome~{'+'.join(subset)}\"\n",
    "                    self.all_formulas.append(formula)\n",
    "\n",
    "    def get_all_formulas(self):\n",
    "        if not self.all_formulas:\n",
    "            self.generate_all_formulas()\n",
    "        return self.all_formulas\n",
    "\n",
    "class RegressionEvaluator:\n",
    "    def __init__(self, data_df: pd.DataFrame, formulas_df: pd.DataFrame):\n",
    "        self.data_df = data_df\n",
    "        self.formulas_df = formulas_df\n",
    "        self.results_df = pd.DataFrame()\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        r_squared_list = []\n",
    "        loocv_rmse_list = []\n",
    "        loocv_r_list = []\n",
    "\n",
    "        for index, row in tqdm(self.formulas_df.iterrows()):\n",
    "            formula = row['formula']\n",
    "\n",
    "            # Fit the model and get R-squared\n",
    "            results = smf.ols(f\"{formula}\", data=self.data_df).fit()\n",
    "            r_squared = results.rsquared\n",
    "\n",
    "            # Perform LOOCV\n",
    "            loocv_rmse, loocv_r = leave_one_out_cv(self.data_df, formula)\n",
    "\n",
    "            r_squared_list.append(r_squared)\n",
    "            loocv_rmse_list.append(loocv_rmse)\n",
    "            loocv_r_list.append(loocv_r)\n",
    "\n",
    "        self.results_df['formula'] = self.formulas_df['formula']\n",
    "        self.results_df['r_squared'] = r_squared_list\n",
    "        self.results_df['loocv_rmse'] = loocv_rmse_list\n",
    "        self.results_df['loocv_r'] = loocv_r_list\n",
    "        \n",
    "    def return_results(self):\n",
    "        sorted_results_df = self.results_df.sort_values(by='loocv_rmse', ascending=True)\n",
    "        return sorted_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify All Possible Backward Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all valid regression formulas using the updated class\n",
    "generator_updated = RegressionFormulaGeneratorUpdated(formula)\n",
    "all_formulas_updated = generator_updated.get_all_formulas()\n",
    "\n",
    "# Create a new Pandas DataFrame to store the updated formulas\n",
    "df_formulas_updated = pd.DataFrame(all_formulas_updated, columns=['formula'])\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "df_formulas_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Backward Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminator = RegressionEvaluator(data_df, df_formulas_updated)\n",
    "eliminator.evaluate_models()\n",
    "results_df = eliminator.return_results()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structural Equation Modelling - Structural Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "def structural_coefficients(model: statsmodels.regression.linear_model.RegressionResultsWrapper, \n",
    "                            data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function calculates the structural coefficients of a linear regression model, along with their \n",
    "    associated model coefficients (beta weights). It also computes a 'suppressor index' which indicates \n",
    "    the likelihood of a variable being a suppressor variable (high beta weight, near-zero structural coefficient).\n",
    "    A suppressor index over 10 is a good heuristic for identifying a suppressor variable\n",
    "    \n",
    "    Parameters:\n",
    "    model (statsmodels.regression.linear_model.RegressionResultsWrapper): The fitted linear regression model.\n",
    "    data (pd.DataFrame): The dataset used in the model.\n",
    "    \n",
    "    Returns:\n",
    "    structural_coefs_df (pd.DataFrame): A dataframe containing the predictors, their structural coefficients,\n",
    "                                        model coefficients (beta weights), and suppressor index.\n",
    "                                        \n",
    "                                        If the sum of the structure coefficients is higher than 1, they are correlated (multicollinear)\n",
    "    \"\"\"\n",
    "    # Calculating the predicted values\n",
    "    y_predicted = model.predict(data)\n",
    "\n",
    "    # Creating a temporary dataframe to store interaction terms\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    # Calculating the structural coefficients\n",
    "    structural_coefs = {}\n",
    "    for pred in model.params.index:\n",
    "        if pred == 'Intercept':\n",
    "            continue\n",
    "\n",
    "        # Check if the predictor is an interaction term\n",
    "        if ':' in pred:\n",
    "            # Split the interaction term into its components\n",
    "            components = pred.split(':')\n",
    "            # Multiply the components and store the result in the temporary dataframe\n",
    "            temp_product = data[components[0]]\n",
    "            for component in components[1:]:\n",
    "                temp_product *= data[component]\n",
    "            temp_df[pred] = temp_product\n",
    "        else:\n",
    "            temp_df[pred] = data[pred]\n",
    "        \n",
    "        coef, _ = pearsonr(temp_df[pred], y_predicted)\n",
    "        structural_coefs[pred] = np.square(coef)\n",
    "\n",
    "    # Calculating the model coefficients\n",
    "    model_coefs = model.params.drop('Intercept')\n",
    "\n",
    "    # Creating a dataframe to store the results\n",
    "    structural_coefs_df = pd.DataFrame(list(zip(structural_coefs.keys(), structural_coefs.values(), model_coefs.values)), \n",
    "                                       columns=['predictor', 'structural_coefficient', 'model_coefficient'])\n",
    "\n",
    "    # Adding suppressor index column\n",
    "    structural_coefs_df['suppressor_index'] = structural_coefs_df['model_coefficient'].abs() / structural_coefs_df['structural_coefficient']\n",
    "\n",
    "    # Sorting the dataframe by the structural coefficients in descending order\n",
    "    structural_coefs_df.sort_values(by='structural_coefficient', ascending=False, inplace=True)\n",
    "    structural_coefs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return structural_coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_structural_coefs_df = structural_coefficients(results, data_df.copy())\n",
    "squared_structural_coefs_df.to_csv(os.path.join(out_dir, 'structural_coefficient_analysis.csv'))\n",
    "print('saved to: ', os.path.join(out_dir, 'structural_coefficient_analysis.csv'))\n",
    "display(squared_structural_coefs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sqrt, mean\n",
    "\n",
    "# Calculate the squared errors\n",
    "squared_errors = (data_df['percent_change_adascog11'] - results.fittedvalues) ** 2\n",
    "\n",
    "# Calculate the mean of the squared errors\n",
    "mse = mean(squared_errors)\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "print(\"Root Mean Squared Error:\", rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Two Lienar Regressions Using F-Test (ANOVA_LM In Statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.api import anova_lm\n",
    "smaller_formula = 'Percent_Cognitive_Improvement ~ Age + Subiculum_Connectivity'\n",
    "\n",
    "larger_formula = 'Percent_Cognitive_Improvement ~ Age * Subiculum_Connectivity'\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------DO NOT TOUCH!----------------------------------------------------------------\n",
    "table1 = anova_lm(smf.ols(smaller_formula, data=data_df).fit(), smf.ols(larger_formula, data=data_df).fit())\n",
    "print(table1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use 2D Interaction Plot to Visualize Interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Interaciton Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Variable Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def save_fig(fig, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    fig.savefig(os.path.join(out_dir, 'create_2D_interaction_plot.png'))\n",
    "    fig.savefig(os.path.join(out_dir, 'create_2D_interaction_plot.svg'))\n",
    "    print('Saved to: ', os.path.join(out_dir, 'create_2D_interaction_plot.svg'))\n",
    "\n",
    "def permutation_test_for_interaction(data_df: pd.DataFrame, \n",
    "                                     x_one_col: str, \n",
    "                                     x_two_col: str, \n",
    "                                     outcome_col: str, \n",
    "                                     results: GLMResults, \n",
    "                                     n_permutations: int = 10000) -> float:\n",
    "    \"\"\"\n",
    "    Performs a permutation test to assess the significance of the interaction effect.\n",
    "    \n",
    "    Returns the p-value.\n",
    "    \"\"\"\n",
    "    # Generate predictions for observed data\n",
    "    x_two_mean = data_df[x_two_col].mean()\n",
    "    x_two_std = data_df[x_two_col].std()\n",
    "    x_two_minus_2sd = x_two_mean - 2 * x_two_std\n",
    "    x_two_plus_2sd = x_two_mean + 2 * x_two_std\n",
    "    \n",
    "    X_pred_minus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * x_two_minus_2sd\n",
    "    })\n",
    "    X_pred_plus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * x_two_plus_2sd\n",
    "    })\n",
    "    \n",
    "    y_pred_minus_2sd = results.predict(X_pred_minus_2sd)\n",
    "    y_pred_plus_2sd = results.predict(X_pred_plus_2sd)\n",
    "    \n",
    "    observed_area_between_lines = np.abs(y_pred_minus_2sd - y_pred_plus_2sd).sum()\n",
    "    \n",
    "    # Initialize array to store areas from permutations\n",
    "    permuted_areas = np.zeros(n_permutations)\n",
    "    \n",
    "    # Perform permutations\n",
    "    for i in range(n_permutations):\n",
    "        # Permute the predictor variables\n",
    "        permuted_x_one = np.random.permutation(data_df[x_one_col])\n",
    "        permuted_x_two = np.random.permutation(data_df[x_two_col])\n",
    "        \n",
    "        # Recalculate mean and standard deviation for permuted x_two\n",
    "        x_two_mean_permuted = permuted_x_two.mean()\n",
    "        x_two_std_permuted = permuted_x_two.std()\n",
    "        x_two_minus_2sd_permuted = x_two_mean_permuted - 2 * x_two_std_permuted\n",
    "        x_two_plus_2sd_permuted = x_two_mean_permuted + 2 * x_two_std_permuted\n",
    "        \n",
    "        # Generate predictions for permuted data\n",
    "        X_pred_minus_2sd_permuted = pd.DataFrame({\n",
    "            x_one_col: permuted_x_one,\n",
    "            x_two_col: np.ones_like(permuted_x_one) * x_two_minus_2sd_permuted\n",
    "        })\n",
    "        X_pred_plus_2sd_permuted = pd.DataFrame({\n",
    "            x_one_col: permuted_x_one,\n",
    "            x_two_col: np.ones_like(permuted_x_one) * x_two_plus_2sd_permuted\n",
    "        })\n",
    "        \n",
    "        y_pred_minus_2sd_permuted = results.predict(X_pred_minus_2sd_permuted)\n",
    "        y_pred_plus_2sd_permuted = results.predict(X_pred_plus_2sd_permuted)\n",
    "        \n",
    "        permuted_area = np.abs(y_pred_minus_2sd_permuted - y_pred_plus_2sd_permuted).sum()\n",
    "        \n",
    "        # Store the permuted area\n",
    "        permuted_areas[i] = permuted_area\n",
    "    \n",
    "    # Calculate p-value\n",
    "    p_value = np.mean(permuted_areas >= observed_area_between_lines)\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "# Modifying the function to include the visual updates\n",
    "from typing import List, Optional\n",
    "\n",
    "def create_2D_interaction_plot(data_df: pd.DataFrame, \n",
    "                               x_one: Dict[str, str], \n",
    "                               x_two: Dict[str, str], \n",
    "                               outcome: Dict[str, str], \n",
    "                               results: GLMResults, \n",
    "                               legend_labels: Optional[List[str]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Creates a 2D interaction plot visualizing the effect of x_one on the outcome at two levels of x_two.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pd.DataFrame\n",
    "        Dataframe containing the predictor variables and outcome.\n",
    "    x_one : Dict[str, str]\n",
    "        Dictionary with label as key and column name of the first predictor variable in the dataframe as value.\n",
    "    x_two : Dict[str, str]\n",
    "        Dictionary with label as key and column name of the second predictor variable in the dataframe as value.\n",
    "    outcome : Dict[str, str]\n",
    "        Dictionary with label as key and column name of the outcome variable in the dataframe as value.\n",
    "    results : GLMResults\n",
    "        Fitted model used for making predictions.\n",
    "    legend_labels : List[str], optional\n",
    "        Labels to be used in the legend. Default is None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract column names from the dictionaries\n",
    "    x_one_label, x_one_col = list(x_one.keys())[0], list(x_one.values())[0]\n",
    "    x_two_label, x_two_col = list(x_two.keys())[0], list(x_two.values())[0]\n",
    "    outcome_label, outcome_col = list(outcome.keys())[0], list(outcome.values())[0]\n",
    "    \n",
    "    # Calculate p-value using permutation test\n",
    "    p_value = permutation_test_for_interaction(data_df, x_one_col, x_two_col, outcome_col, results)\n",
    "     \n",
    "    # Calculate mean and standard deviation for x_two\n",
    "    x_two_mean = data_df[x_two_col].mean()\n",
    "    x_two_std = data_df[x_two_col].std()\n",
    "    \n",
    "    # Create arrays for x_two at -2 and +2 standard deviations from the mean\n",
    "    x_two_minus_2sd = x_two_mean - 2 * x_two_std\n",
    "    x_two_plus_2sd = x_two_mean + 2 * x_two_std\n",
    "    \n",
    "    # Create DataFrames for prediction\n",
    "    X_pred_minus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * x_two_minus_2sd\n",
    "    })\n",
    "    \n",
    "    X_pred_plus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * x_two_plus_2sd\n",
    "    })\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred_minus_2sd = results.predict(X_pred_minus_2sd)\n",
    "    y_pred_plus_2sd = results.predict(X_pred_plus_2sd)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Generate predictions (this part could be refactored)\n",
    "    X_pred_minus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * (data_df[x_two_col].mean() - 2 * data_df[x_two_col].std())\n",
    "    })\n",
    "    X_pred_plus_2sd = pd.DataFrame({\n",
    "        x_one_col: data_df[x_one_col],\n",
    "        x_two_col: np.ones_like(data_df[x_one_col]) * (data_df[x_two_col].mean() + 2 * data_df[x_two_col].std())\n",
    "    })\n",
    "    y_pred_minus_2sd = results.predict(X_pred_minus_2sd)\n",
    "    y_pred_plus_2sd = results.predict(X_pred_plus_2sd)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.plot(data_df[x_one_col], y_pred_minus_2sd, label=legend_labels[0] if legend_labels else f\"{x_two_label} at -2 SD\", color='blue')\n",
    "    plt.plot(data_df[x_one_col], y_pred_plus_2sd, label=legend_labels[1] if legend_labels else f\"{x_two_label} at +2 SD\", color='red')\n",
    "    \n",
    "    plt.xlabel(x_one_label)\n",
    "    plt.ylabel(outcome_label)\n",
    "    legend = plt.legend(frameon=False)\n",
    "    \n",
    "    # Add p-value to the title\n",
    "    plt.title(f\"p-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Despine the plot\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "# The function can be used like this:\n",
    "# p_value = permutation_test_for_interaction(data_df, {'X1 Label': 'x1_col'}, {'X2 Label': 'x2_col'}, {'Outcome Label': 'outcome_col'}, results)\n",
    "# Then, the p-value can be set as the title in the plot function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_2D_interaction_plot(data_df.copy(), \n",
    "                                 x_one={'Subiculum Connectivity': 'Subiculum_Connectivity'}, \n",
    "                                 x_two={'Age': 'Age'}, \n",
    "                                 outcome={'Percent Improvement (MDRS)': 'outcome'}, \n",
    "                                 results=results,\n",
    "                                 legend_labels=['Young', 'Old'])\n",
    "save_fig(fig, out_dir)\n",
    "print('Saved to: ', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-Driven Split Interaction Plot\n",
    "\n",
    "\n",
    "This code is designed to create an interaction plot to visualize the effects of two factors and their interaction on the outcome variable.\n",
    "\n",
    "The interaction_plot function takes as input a dataframe, two factors (x_one and x_two), two corresponding labels for the conditions when the values of these factors are under the mean (x_one_under_mean and x_two_under_mean) and over the mean (x_one_over_mean and x_two_over_mean), and the response variable (outcome). If binarize is set to True, it converts the two factors into binary variables based on whether their values are above or below the mean. The function then creates a mapping for the x_two variable to numerical values for the purpose of plotting.\n",
    "\n",
    "It uses the interaction_plot function from the statsmodels package to create the plot. In the plot, x_two is represented on the x-axis, x_one is used to color the lines, and the outcome variable is plotted on the y-axis. The function also sets the labels for the x and y axes and the tick labels on the x-axis according to the inputs provided.\n",
    "\n",
    "The function also allows for saving the plot to an output directory specified by the user. If save is set to True, it saves the plot in both PNG and SVG formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the function to incorporate the changes\n",
    "from statsmodels.graphics.api import interaction_plot\n",
    "\n",
    "def two_dimensional_interaction_plot_v4(data_df, \n",
    "                    x_one, x_one_under_mean, x_one_over_mean, x_one_split_point,\n",
    "                    x_two, x_two_under_mean, x_two_over_mean, x_two_split_point,\n",
    "                    response, \n",
    "                    binarize=True, plot_error_bars=True,\n",
    "                    x_label='Subiculum Connectivity', y_label='Percent Improvement (MDRS)',\n",
    "                    save=False, out_dir=None):\n",
    "    \"\"\"\n",
    "    Function to create an interaction plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pandas.DataFrame\n",
    "        The dataframe containing the data.\n",
    "    x_one, x_two : str\n",
    "        Column names of the two factors.\n",
    "    x_one_under_mean, x_two_under_mean : str\n",
    "        Labels to be used when the values of x_one and x_two are under the mean, respectively.\n",
    "    x_one_over_mean, x_two_over_mean : str\n",
    "        Labels to be used when the values of x_one and x_two are over the mean, respectively.\n",
    "    x_one_split_point, x_two_split_point: int | None\n",
    "        Value to split the data of x by. If None, then x will be split by mean\n",
    "    response : str\n",
    "        Column name of the outcome variable.\n",
    "    binarize : bool, optional\n",
    "        Whether to convert x_one and x_two into binary variables.\n",
    "    plot_error_bars : bool, optional\n",
    "        Whether to plot error bars representing SEM.\n",
    "    x_label, y_label : str, optional\n",
    "        Labels for the x-axis and y-axis.\n",
    "    save : bool, optional\n",
    "        Whether to save the plot.\n",
    "    out_dir : str, optional\n",
    "        Directory where the plot will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The original dataframe with modified x_one and x_two if binarize is True.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Binarize x_two variable\n",
    "    if binarize:\n",
    "        if x_two_split_point is not None:\n",
    "            data_df[x_two] = np.where(data_df[x_two] <= x_two_split_point, f'{x_two_under_mean}', f'{x_two_over_mean}')\n",
    "        else:\n",
    "            data_df[x_two] = np.where(data_df[x_two] <= data_df[x_two].mean(), f'{x_two_under_mean}', f'{x_two_over_mean}')\n",
    "    \n",
    "    # Binarize x_one variable\n",
    "    if binarize:\n",
    "        if x_one_split_point is not None:\n",
    "            data_df[x_one] = np.where(data_df[x_one] <= x_one_split_point, f'{x_one_under_mean}', f'{x_one_over_mean}')\n",
    "        else:\n",
    "            data_df[x_one] = np.where(data_df[x_one] <= data_df[x_one].mean(), f'{x_one_under_mean}', f'{x_one_over_mean}')\n",
    "    \n",
    "    # Map the x_two categories to numbers for plotting\n",
    "    mapping = {x_two_under_mean: 0, x_two_over_mean: 1}\n",
    "    data_df[x_two + '_mapped'] = data_df[x_two].map(mapping)\n",
    "    \n",
    "    # Extracting means and SEM for the binarized groups\n",
    "    means = data_df.groupby([x_one, x_two + '_mapped'])[response].mean()\n",
    "    sem = data_df.groupby([x_one, x_two + '_mapped'])[response].sem()\n",
    "\n",
    "    # Plotting the interaction plot\n",
    "    fig, ax = plt.subplots()\n",
    "    colors_dict = {f'{x_one_under_mean}': 'Blue', f'{x_one_over_mean}': 'Red'}\n",
    "\n",
    "    for group, color in colors_dict.items():\n",
    "        group_data = [(0 if combo[1] == 0 else 1, means[combo], sem[combo]) \n",
    "                      for combo in means.index if combo[0] == group]\n",
    "        group_data.sort(key=lambda x: x[0])  # Sort by x-value for plotting\n",
    "        \n",
    "        x_vals = [item[0] for item in group_data]\n",
    "        y_vals = [item[1] for item in group_data]\n",
    "        y_errs = [item[2] for item in group_data] if plot_error_bars else None\n",
    "        \n",
    "        ax.errorbar(x_vals, y_vals, yerr=y_errs, color=color, fmt='-o', capsize=5, capthick=2, elinewidth=2, label=group, linestyle='-')\n",
    "\n",
    "    # Setting labels and other plot properties\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([x_two_under_mean, x_two_over_mean])\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "\n",
    "    # Displaying the plot\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function to display the interaction plot\n",
    "save = False\n",
    "#----------------------------------------------------------------\n",
    "interaction_figure = two_dimensional_interaction_plot_v4(data_df.copy(), \n",
    "                 x_one='Age', x_one_under_mean='Young', x_one_over_mean='Old', x_one_split_point=65.7,\n",
    "                 x_two='Subiculum_Connectivity', x_two_under_mean='Low Connectivity', x_two_over_mean='High Connectivity', x_two_split_point=61,\n",
    "                 response='outcome', \n",
    "                 x_label='Memory ROI Connectivity', \n",
    "                 y_label='Percent Improvement (ADAS-Cog11)',\n",
    "                 plot_error_bars=False)\n",
    "\n",
    "interaction_figure\n",
    "if save:\n",
    "    interaction_figure.savefig(os.path.join(out_dir, '2D_interaction_figure_pd.png'))\n",
    "    interaction_figure.savefig(os.path.join(out_dir, '2D_interaction_figure_pd.svg'))\n",
    "    print(f'saved to: {os.path.join(out_dir, \"2D_interaction_figure.png\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def permute_outcome_and_get_proportions_fixed(data_df, \n",
    "                                              x_one, x_one_under_mean, x_one_over_mean, x_one_split_point,\n",
    "                                              x_two, x_two_under_mean, x_two_over_mean, x_two_split_point,\n",
    "                                              response, binarize=True):\n",
    "    \"\"\"\n",
    "    Function to perform permutation testing on the outcome and compute the proportion of permuted means \n",
    "    greater than observed means for both x_one and x_two.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pandas.DataFrame\n",
    "        The dataframe containing the data.\n",
    "    x_one, x_two : str\n",
    "        Column names of the two factors.\n",
    "    x_one_under_mean, x_two_under_mean : str\n",
    "        Labels to be used when the values of x_one and x_two are under the mean, respectively.\n",
    "    x_one_over_mean, x_two_over_mean : str\n",
    "        Labels to be used when the values of x_one and x_two are over the mean, respectively.\n",
    "    x_one_split_point, x_two_split_point: int\n",
    "        Value to split the data of x by.\n",
    "    response : str\n",
    "        Column name of the outcome variable.\n",
    "    binarize : bool\n",
    "        Whether to convert x_one and x_two into binary variables.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Proportion of permuted means greater than observed mean for x_one and x_two.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy the data\n",
    "    df_copy = data_df.copy()\n",
    "    \n",
    "    # Binarize the variables to get observed values\n",
    "    if binarize:\n",
    "        if x_two_split_point is not None:\n",
    "            df_copy[x_two] = np.where(df_copy[x_two] <= x_two_split_point, 0, 1)\n",
    "        else:\n",
    "            df_copy[x_two] = np.where(df_copy[x_two] <= df_copy[x_two].mean(), 0, 1)\n",
    "        \n",
    "        if x_one_split_point is not None:\n",
    "            df_copy[x_one] = np.where(df_copy[x_one] <= x_one_split_point, 0, 1)\n",
    "        else:\n",
    "            df_copy[x_one] = np.where(df_copy[x_one] <= df_copy[x_one].mean(), 0, 1)\n",
    "    \n",
    "    #Observed Values between within a level of x_one across levels of x_two\n",
    "    delta_x_one_low = df_copy[response][(df_copy[x_one]==0) & (df_copy[x_two]==0)].mean() - df_copy[response][(df_copy[x_one]==0) & (df_copy[x_two]==1)].mean()\n",
    "    delta_x_one_high = df_copy[response][(df_copy[x_one]==1) & (df_copy[x_two]==0)].mean() - df_copy[response][(df_copy[x_one]==1) & (df_copy[x_two]==1)].mean()    \n",
    "    #Empiric Values\n",
    "    delta_x_one_low_list = []\n",
    "    delta_x_one_high_list = []\n",
    "    \n",
    "    #Observed Values between within a level of x_two across levels of x_one\n",
    "    delta_x_two_low = df_copy[response][(df_copy[x_two]==0) & (df_copy[x_one]==0)].mean() - df_copy[response][(df_copy[x_two]==0) & (df_copy[x_one]==1)].mean()\n",
    "    delta_x_two_high = df_copy[response][(df_copy[x_two]==1) & (df_copy[x_one]==0)].mean() - df_copy[response][(df_copy[x_two]==1) & (df_copy[x_one]==1)].mean()    \n",
    "    #Empiric Values\n",
    "    delta_x_two_low_list = []\n",
    "    delta_x_two_high_list = []\n",
    "    \n",
    "    \n",
    "    # Permute outcome and calculate means\n",
    "    for _ in tqdm(range(10000)):\n",
    "        permuted_data = data_df.copy()\n",
    "        permuted_data[response] = np.random.permutation(permuted_data[response].values)\n",
    "        \n",
    "            # Binarize the variables to get observed values\n",
    "        if binarize:\n",
    "            if x_two_split_point is not None:\n",
    "                permuted_data[x_two] = np.where(permuted_data[x_two] <= x_two_split_point, 0, 1)\n",
    "            else:\n",
    "                permuted_data[x_two] = np.where(permuted_data[x_two] <= permuted_data[x_two].mean(), 0, 1)\n",
    "            \n",
    "            if x_one_split_point is not None:\n",
    "                permuted_data[x_one] = np.where(permuted_data[x_one] <= x_one_split_point, 0, 1)\n",
    "            else:\n",
    "                permuted_data[x_one] = np.where(permuted_data[x_one] <= permuted_data[x_one].mean(), 0, 1)\n",
    "        \n",
    "        #Observed Values\n",
    "        delta_x_one_low_list.append(permuted_data[response][(permuted_data[x_one]==0) & (permuted_data[x_two]==0)].mean() - permuted_data[response][(permuted_data[x_one]==0) & (permuted_data[x_two]==1)].mean())\n",
    "        delta_x_one_high_list.append(permuted_data[response][(permuted_data[x_one]==1) & (permuted_data[x_two]==0)].mean() - permuted_data[response][(permuted_data[x_one]==1) & (permuted_data[x_two]==1)].mean())\n",
    "        delta_x_two_low_list.append(permuted_data[response][(permuted_data[x_two]==0) & (permuted_data[x_one]==0)].mean() - permuted_data[response][(permuted_data[x_two]==0) & (permuted_data[x_one]==1)].mean())\n",
    "        delta_x_two_high_list.append(permuted_data[response][(permuted_data[x_two]==1) & (permuted_data[x_one]==0)].mean() - permuted_data[response][(permuted_data[x_two]==1) & (permuted_data[x_one]==1)].mean())\n",
    "        \n",
    "    \n",
    "    x_one_proportion_level1 = np.mean(np.array(delta_x_one_low_list) > delta_x_one_low)\n",
    "    x_one_proportion_level2 = np.mean(np.array(delta_x_one_high_list) > delta_x_one_high)\n",
    "    x_two_proportion_level1 = np.mean(np.array(delta_x_two_low_list) > delta_x_two_low)\n",
    "    x_two_proportion_level2 = np.mean(np.array(delta_x_two_high_list) > delta_x_two_high)\n",
    "    \n",
    "    return x_one_proportion_level1, x_one_proportion_level2, x_two_proportion_level1, x_two_proportion_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the fixed function\n",
    "x_one='Age'; x_one_low_label='Young'; x_one_high_label='Old'\n",
    "x_one_split_point=63\n",
    "\n",
    "x_two='Subiculum_Connectivity'; x_two_low_label='Low Connectivity'; x_two_high_label='High Connectivity'\n",
    "x_two_split_point=66\n",
    "\n",
    "response='outcome'\n",
    "#----------------------------------------------------------------\n",
    "x_one_proportion_level1, x_one_proportion_level2, x_two_proportion_level1, x_two_proportion_level2 = permute_outcome_and_get_proportions_fixed(data_df.copy(), \n",
    "                 x_one=x_one, x_one_under_mean=x_one_low_label, x_one_over_mean=x_one_high_label, x_one_split_point=x_one_split_point,\n",
    "                 x_two=x_two, x_two_under_mean=x_two_low_label, x_two_over_mean=x_two_high_label, x_two_split_point=x_two_split_point,\n",
    "                 response=response)\n",
    "\n",
    "print(f'Significance of {x_one} between levels of {x_two} is: {x_one_proportion_level1} at the {x_one_low_label} {x_one} and {x_one_proportion_level2} at the {x_one_high_label} {x_one}')\n",
    "print(f'Significance of {x_two} between levels of {x_one} is: {x_two_proportion_level1} at the {x_two_low_label} {x_two} and {x_two_proportion_level2} at the {x_two_high_label} {x_two}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use response plane to visualize interaction effect\n",
    "- This models the marginal distribution of variables\n",
    "- If the model has 2 predictors and 1 response, then choose option A\n",
    "- If the model has more than 2 predictors, then choose option B. You will need to manually vary across the additional predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 variable method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "def create_interaction_plot(data_df: pd.DataFrame,\n",
    "                            x_one: str,\n",
    "                            x_two: str,\n",
    "                            outcome: str,\n",
    "                            results: GLMResults,\n",
    "                            num_slices: int = 100,\n",
    "                            out_dir: str = './',\n",
    "                            labels: dict = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a 3D plot visualizing the interaction of two predictor variables on the outcome.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pd.DataFrame\n",
    "        Dataframe containing the predictor variables and outcome.\n",
    "    x_one : str\n",
    "        Column name of the first predictor variable in the dataframe.\n",
    "    x_two : str\n",
    "        Column name of the second predictor variable in the dataframe.\n",
    "    outcome : str\n",
    "        Column name of the outcome variable in the dataframe.\n",
    "    results : GLMResults\n",
    "        Fitted model used for making predictions.\n",
    "    num_slices : int, optional\n",
    "        Number of slices to create in the 3D grid, default is 100.\n",
    "    out_dir : str, optional\n",
    "        Directory to save the output PNG and SVG files, default is the current directory.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        The created figure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create grid of predictor variable values\n",
    "    x1 = np.linspace(min(data_df[x_one]), max(data_df[x_one]), num_slices)\n",
    "    x2 = np.linspace(min(data_df[x_two]), max(data_df[x_two]), num_slices)\n",
    "    x1v, x2v = np.meshgrid(x1, x2)\n",
    "\n",
    "    # Set plot style\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette('Greys', 1, desat=1)\n",
    "\n",
    "    # Flattening the matrices to create a DataFrame for prediction\n",
    "    X_grid = pd.DataFrame({\n",
    "        x_one: x1v.ravel(),\n",
    "        x_two: x2v.ravel(),\n",
    "    })\n",
    "\n",
    "    # Generate response values for grid\n",
    "    y_pred = results.predict(X_grid).values.reshape(num_slices, num_slices)\n",
    "\n",
    "    # Create a new figure for plotting\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot the surface\n",
    "    ax.plot_surface(x1v, x2v, y_pred, cmap='Greys', alpha=1.0)\n",
    "\n",
    "    # Set the axes labels\n",
    "    format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "    # Set the axes labels\n",
    "    if labels is not None:\n",
    "        ax.set_xlabel(labels.get('x', format_label(x_one)))\n",
    "        ax.set_ylabel(labels.get('y', format_label(x_two)))\n",
    "        ax.set_zlabel(labels.get('z', format_label(outcome)))\n",
    "    else:\n",
    "        ax.set_xlabel(format_label(x_one))\n",
    "        ax.set_ylabel(format_label(x_two))\n",
    "        ax.set_zlabel(format_label(outcome))\n",
    "\n",
    "\n",
    "    # Save the plot as PNG and SVG files\n",
    "    fig.savefig(os.path.join(out_dir, '2_variable.png'), dpi=300)\n",
    "    fig.savefig(os.path.join(out_dir, '2_variable.svg'), format='svg')\n",
    "    print('Saved to file', out_dir)\n",
    "\n",
    "    return fig, y_pred, np.meshgrid(x1, x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = 'Age'\n",
    "x_two = 'Subiculum_Connectivity'\n",
    "outcome = 'outcome'\n",
    "#----------------------------------------------------------------DO NOT TOUCH THIS!----------------------------------------------------------------\n",
    "fig, y_pred, X_grid = create_interaction_plot(data_df, \n",
    "                        x_one=x_one, \n",
    "                        x_two=x_two, \n",
    "                        outcome=outcome, \n",
    "                        results=results, \n",
    "                        out_dir='/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/misc', \n",
    "                        labels={'x': 'Age', 'y': 'Subiculum Connectivity', 'z': 'Percent Improvement (MDRS)'});"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Variable Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import imageio\n",
    "import os\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "from typing import List\n",
    "from matplotlib import colors\n",
    "\n",
    "def create_interaction_gifs(data_df: pd.DataFrame, \n",
    "                            x_one: str, \n",
    "                            x_two: str, \n",
    "                            x_three: str, \n",
    "                            outcome: str, \n",
    "                            results: GLMResults, \n",
    "                            num_slices: int = 100,\n",
    "                            gif_duration: float = 0.3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates gifs visualizing the interaction of predictor variables on the outcome.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pd.DataFrame\n",
    "        Dataframe containing the predictor variables and outcome.\n",
    "    x_one : str\n",
    "        Column name of the first predictor variable in the dataframe.\n",
    "    x_two : str\n",
    "        Column name of the second predictor variable in the dataframe.\n",
    "    x_three : str\n",
    "        Column name of the third predictor variable in the dataframe.\n",
    "    outcome : str\n",
    "        Column name of the outcome variable in the dataframe.\n",
    "    results : GLMResults\n",
    "        Fitted model used for making predictions.\n",
    "    num_slices : int, optional\n",
    "        Number of slices to create in the gifs, default is 100.\n",
    "    gif_duration : float, optional\n",
    "        Duration of each frame in the gif in seconds, default is 0.3.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of file paths to the created gif files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if x_three is binary\n",
    "    unique_values = data_df[x_three].unique()\n",
    "    is_binary = len(unique_values) == 2\n",
    "\n",
    "    # Create grid of predictor variable values\n",
    "    x1 = np.linspace(min(data_df[x_one]), max(data_df[x_one]), num_slices)\n",
    "    x2 = np.linspace(min(data_df[x_two]), max(data_df[x_two]), num_slices)\n",
    "    \n",
    "    # Set plot style\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette('Greys', 1, desat=1)\n",
    "\n",
    "    gif_paths = []\n",
    "\n",
    "    if is_binary:\n",
    "        for x_three_value in unique_values:\n",
    "            # Flattening the matrices to create a DataFrame for prediction\n",
    "            X_grid = pd.DataFrame({\n",
    "                x_one: np.tile(x1, len(x2)),\n",
    "                x_two: np.repeat(x2, len(x1)),\n",
    "                x_three: np.ones_like(np.tile(x1, len(x2))) * x_three_value\n",
    "            })\n",
    "\n",
    "            # Generate response values for grid\n",
    "            y_pred = results.predict(X_grid).values.reshape(len(x2), len(x1))\n",
    "\n",
    "            # Create a new figure for plotting\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Plot the surface\n",
    "            ax.plot_surface(np.tile(x1, (len(x2), 1)), np.repeat(x2[:, np.newaxis], len(x1), axis=1), y_pred, cmap='bwr', alpha=0.8)\n",
    "\n",
    "            # Set the axes labels\n",
    "            format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "            ax.set_xlabel(format_label(x_one))\n",
    "            ax.set_ylabel(format_label(x_two))\n",
    "            ax.set_zlabel(format_label(outcome))\n",
    "\n",
    "            # Set the title with the actual value of x_three\n",
    "            title = f'{x_three} = {x_three_value}'\n",
    "            ax.set_title(title)\n",
    "\n",
    "            # Save the plot as an image file\n",
    "            image_path = f'plot_{x_three}_{x_three_value}.png'\n",
    "            plt.savefig(image_path, dpi=300)\n",
    "            gif_paths.append(image_path)\n",
    "            plt.close()\n",
    "    else:\n",
    "        # Flattening the matrices to create a DataFrame for prediction\n",
    "        x3 = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)\n",
    "        x1v, x2v, x3v = np.meshgrid(x1, x2, x3)\n",
    "\n",
    "        X_grid = pd.DataFrame({\n",
    "            x_one: x1v.ravel(),\n",
    "            x_two: x2v.ravel(),\n",
    "            x_three: x3v.ravel(),\n",
    "        })\n",
    "\n",
    "        # Generate response values for grid\n",
    "        y_pred = results.predict(X_grid).values.reshape(num_slices, num_slices, num_slices)\n",
    "\n",
    "        # Collect the file paths of images\n",
    "        image_paths = []\n",
    "\n",
    "        # Iterating over x3 (third predictor variable) and saving each plot as an image\n",
    "        for i in range(num_slices):\n",
    "            x3_slice = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)[i]\n",
    "\n",
    "            # Create a new figure for plotting\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Plot the surface\n",
    "            norm = colors.TwoSlopeNorm(vcenter=0)\n",
    "            ax.plot_surface(x1v[:, :, i], x2v[:, :, i], y_pred[:, :, i], cmap='Greys', alpha=0.8, norm=norm)\n",
    "\n",
    "            # Set the axes labels\n",
    "            format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "            ax.set_xlabel(format_label(x_one))\n",
    "            ax.set_ylabel(format_label(x_two))\n",
    "            ax.set_zlabel(format_label(outcome))\n",
    "\n",
    "            # Set the title with the actual value of x_three\n",
    "            title = f'{x_three} = {x3_slice:.2f}'\n",
    "            ax.set_title(title)\n",
    "\n",
    "            # Save the plot as an image file\n",
    "            image_path = f'plot_slice_{i+1}.png'\n",
    "            plt.savefig(image_path, dpi=300)\n",
    "            image_paths.append(image_path)\n",
    "            plt.close()\n",
    "\n",
    "        # Create a gif from the image files\n",
    "        gif_path = f'plots.gif'\n",
    "        images = [imageio.imread(image_path) for image_path in image_paths]\n",
    "        imageio.mimsave(gif_path, images, duration=gif_duration)\n",
    "        gif_paths.append(gif_path)\n",
    "\n",
    "        # Remove the individual image files\n",
    "        for image_path in image_paths:\n",
    "            os.remove(image_path)\n",
    "\n",
    "    return gif_paths\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# gifs = create_interaction_gifs(data_df, 'x_one', 'x_two', 'x_three', 'outcome', results)\n",
    "# print(\"GIFs created:\", gifs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = 'Subiculum_Connectivity'\n",
    "x_two = 'Age' \n",
    "x_three = 'Anticorrelated_Memory_Atrophy'\n",
    "outcome = 'cognitive_improvement'\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "gifs = create_interaction_gifs(data_df, \n",
    "                               x_one, \n",
    "                               x_two, \n",
    "                               x_three, \n",
    "                               outcome, \n",
    "                               results)\n",
    "print(\"GIFs created:\", gifs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Variable Method\n",
    "- 4th variable is expected to be 1-hot encoded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import imageio\n",
    "import os\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def create_interaction_gifs(data_df: pd.DataFrame, \n",
    "                            x_one: str, \n",
    "                            x_two: str, \n",
    "                            x_three: str, \n",
    "                            x_four: str, \n",
    "                            x_four_dict: dict,\n",
    "                            outcome: str, \n",
    "                            results: GLMResults, \n",
    "                            num_slices: int = 100,\n",
    "                            gif_duration: float = 0.3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates gifs visualizing the interaction of predictor variables on the outcome,\n",
    "    and the effect of a one-hot encoded variable (x_four).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pd.DataFrame\n",
    "        Dataframe containing the predictor variables and outcome.\n",
    "    x_one : str\n",
    "        Column name of the first predictor variable in the dataframe.\n",
    "    x_two : str\n",
    "        Column name of the second predictor variable in the dataframe.\n",
    "    x_three : str\n",
    "        Column name of the third predictor variable in the dataframe.\n",
    "    x_four : str\n",
    "        Column name of the one-hot encoded predictor variable (values should be 0 or 1) in the dataframe.\n",
    "    x_four_list : list\n",
    "        List of the x_four variables corresponding to their one hot encoded outcomes.\n",
    "    outcome : str\n",
    "        Column name of the outcome variable in the dataframe.\n",
    "    results : GLMResults\n",
    "        Fitted model used for making predictions.\n",
    "    num_slices : int, optional\n",
    "        Number of slices to create in the gifs, default is 100.\n",
    "    gif_duration : float, optional\n",
    "        Duration of each frame in the gif in seconds, default is 0.3.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of file paths to the created gif files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set plot style\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette('Greys', 1, desat=1)\n",
    "\n",
    "    gif_paths = []\n",
    "\n",
    "    # Iterate over x_four values 0 and 1\n",
    "    for x_four_value in [0, 1]:\n",
    "        distribution_df = data_df.copy()\n",
    "        distribution_df = distribution_df[distribution_df['Disease'] == x_four_value]\n",
    "        # Create grid of predictor variable values\n",
    "        x1 = np.linspace(min(distribution_df[x_one]), max(distribution_df[x_one]), num_slices)\n",
    "        x2 = np.linspace(min(distribution_df[x_two]), max(distribution_df[x_two]), num_slices)\n",
    "        x3 = np.linspace(min(distribution_df[x_three]), max(distribution_df[x_three]), num_slices)\n",
    "        x1v, x2v, x3v = np.meshgrid(x1, x2, x3)\n",
    "        \n",
    "        # Flattening the matrices to create a DataFrame for prediction\n",
    "        X_grid = pd.DataFrame({\n",
    "            x_one: x1v.ravel(),\n",
    "            x_two: x2v.ravel(),\n",
    "            x_three: x3v.ravel(),\n",
    "            x_four: np.ones_like(x1v).ravel() * x_four_value\n",
    "        })\n",
    "\n",
    "        # Generate response values for grid\n",
    "        y_pred = results.predict(X_grid).values.reshape(num_slices, num_slices, num_slices)\n",
    "\n",
    "        # Collect the file paths of images\n",
    "        image_paths = []\n",
    "\n",
    "        # Iterating over x3 (third predictor variable) and saving each plot as an image\n",
    "        for i in range(num_slices):\n",
    "            x3_slice = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)[i]\n",
    "            \n",
    "            # Create a new figure for plotting\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Plot the surface\n",
    "            ax.plot_surface(x1v[:, :, i], x2v[:, :, i], y_pred[:, :, i], cmap='bwr', alpha=0.8)\n",
    "\n",
    "            # Set the axes labels\n",
    "            format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "            ax.set_xlabel(format_label(x_one))\n",
    "            ax.set_ylabel(format_label(x_two))\n",
    "            ax.set_zlabel(format_label(outcome))\n",
    "            \n",
    "            # Set the title with the actual value of x_three and x_four\n",
    "            title = f'{x_four_dict[x_four_value]}, {x_three} = {x3_slice:.2f}'\n",
    "            ax.set_title(title)\n",
    "\n",
    "            # Save the plot as an image file\n",
    "            image_path = f'plot_{x_four}_{x_four_dict[x_four_value]}_slice_{i+1}.png'\n",
    "            plt.savefig(image_path, dpi=300)\n",
    "            image_paths.append(image_path)\n",
    "            plt.close()\n",
    "\n",
    "        # Create a gif from the image files\n",
    "        gif_path = f'{x_four}_{x_four_dict[x_four_value]}_plots.gif'\n",
    "        images = [imageio.imread(image_path) for image_path in image_paths]\n",
    "        imageio.mimsave(gif_path, images, duration=gif_duration)\n",
    "        gif_paths.append(gif_path)\n",
    "\n",
    "        # Remove the individual image files\n",
    "        for image_path in image_paths:\n",
    "            os.remove(image_path)\n",
    "\n",
    "    return gif_paths\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# gifs = create_interaction_gifs(data_df, 'Subiculum_Connectivity', 'Subiculum_Grey_Matter', 'Age', 'Disease', 'outcome', results)\n",
    "# print(\"GIFs created:\", gifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = 'Subiculum_Connectivity'\n",
    "x_two = 'Subiculum_Grey_Matter' \n",
    "x_three = 'Age'\n",
    "x_four = 'Disease'\n",
    "x_four_dict = {1: \"Alzheimer's\", 0: \"Parkinson's\"}\n",
    "outcome = 'cognitive_improvement'\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "gifs = create_interaction_gifs(data_df, \n",
    "                               x_one, \n",
    "                               x_two, \n",
    "                               x_three, \n",
    "                               x_four,\n",
    "                               x_four_dict,\n",
    "                               outcome, \n",
    "                               results)\n",
    "print(\"GIFs created:\", gifs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Variable Method\n",
    "- This is identical to the 4 variable method, but takes the 5th variable \n",
    "and calculates is at -2 and +2 standard deviations, then plots a response plane for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import imageio\n",
    "import os\n",
    "from statsmodels.genmod.generalized_linear_model import GLMResults\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def create_interaction_gifs_with_5_variables(data_df: pd.DataFrame, \n",
    "                                             x_one: str, \n",
    "                                             x_two: str, \n",
    "                                             x_three: str, \n",
    "                                             x_four: str, \n",
    "                                             x_four_dict: dict,\n",
    "                                             x_five: str,\n",
    "                                             x_five_factor_levels: int,\n",
    "                                             outcome: str, \n",
    "                                             results: GLMResults, \n",
    "                                             num_slices: int = 100,\n",
    "                                             gif_duration: float = 0.3) -> List[str]:\n",
    "    \n",
    "    # Create grid of predictor variable values\n",
    "    x1 = np.linspace(min(data_df[x_one]), max(data_df[x_one]), num_slices)\n",
    "    x2 = np.linspace(min(data_df[x_two]), max(data_df[x_two]), num_slices)\n",
    "    x3 = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)\n",
    "    x1v, x2v, x3v = np.meshgrid(x1, x2, x3)\n",
    "\n",
    "    # Set plot style\n",
    "    sns.set_style('white')\n",
    "    sns.set_palette('Greys', 1, desat=1)\n",
    "\n",
    "    gif_paths = []\n",
    "\n",
    "    # Iterate over x_four values 0 and 1\n",
    "    for x_four_value in [0, 1]:\n",
    "        \n",
    "        # Collect the file paths of images\n",
    "        image_paths = []\n",
    "\n",
    "        # Iterating over x3 (third predictor variable) and saving each plot as an image\n",
    "        for i in range(num_slices):\n",
    "            x3_slice = np.linspace(min(data_df[x_three]), max(data_df[x_three]), num_slices)[i]\n",
    "            \n",
    "            # Create a new figure for plotting\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Choose to use x_five mean\n",
    "            x_five_mean = data_df[x_five].mean()\n",
    "            x_five_std = data_df[x_five].std()\n",
    "            if x_five_factor_levels == 1:\n",
    "                x_five_level = [x_five_mean]\n",
    "            # Choose to use x_five +/- 2 standard deviations\n",
    "            else:\n",
    "                x_five_level = [x_five_mean-(2*x_five_std), x_five_mean+(2*x_five_std)]\n",
    "            for level in range(0, len(x_five_level)):\n",
    "                \n",
    "                # Flattening the matrices to create a DataFrame for prediction\n",
    "                X_grid = pd.DataFrame({\n",
    "                    x_one: x1v.ravel(),\n",
    "                    x_two: x2v.ravel(),\n",
    "                    x_three: x3v.ravel(),\n",
    "                    x_four: np.ones_like(x1v).ravel() * x_four_value,\n",
    "                    x_five: np.ones_like(x1v).ravel() * x_five_level[level]\n",
    "                })\n",
    "\n",
    "                # Generate response values for grid\n",
    "                y_pred = results.predict(X_grid).values.reshape(num_slices, num_slices, num_slices)\n",
    "                \n",
    "                # Plot the surface\n",
    "                cmap = 'viridis' if x_five_level[level] == data_df[x_five].mean() else ('cool' if x_five_level[level] < 0 else 'magma')\n",
    "                ax.plot_surface(x1v[:, :, i], x2v[:, :, i], y_pred[:, :, i], cmap=cmap, alpha=0.5)\n",
    "\n",
    "            # Set the axes labels\n",
    "            format_label = lambda x: ' '.join(word.capitalize() for word in x.split('_'))\n",
    "            ax.set_xlabel(format_label(x_one))\n",
    "            ax.set_ylabel(format_label(x_two))\n",
    "            ax.set_zlabel(format_label(outcome))\n",
    "            \n",
    "            # Set the title with the actual value of x_three and x_four\n",
    "            title = f'{x_four_dict[x_four_value]}, {x_three} = {x3_slice:.2f}'\n",
    "            ax.set_title(title)\n",
    "\n",
    "            # Save the plot as an image file\n",
    "            image_path = f'plot_{x_four}_{x_four_dict[x_four_value]}_slice_{i+1}.png'\n",
    "            plt.savefig(image_path, dpi=300)\n",
    "            image_paths.append(image_path)\n",
    "            plt.close()\n",
    "\n",
    "        # Create a gif from the image files\n",
    "        gif_path = f'{x_four}_{x_four_dict[x_four_value]}_plots.gif'\n",
    "        images = [imageio.imread(image_path) for image_path in image_paths]\n",
    "        imageio.mimsave(gif_path, images, duration=gif_duration)\n",
    "        gif_paths.append(gif_path)\n",
    "\n",
    "        # Remove the individual image files\n",
    "        for image_path in image_paths:\n",
    "            os.remove(image_path)\n",
    "\n",
    "    return gif_paths\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one = 'Subiculum_Connectivity'\n",
    "x_two = 'Subiculum_Grey_Matter' \n",
    "x_three = 'Age'\n",
    "x_four = 'Disease'\n",
    "x_four_dict = {1: \"Alzheimer's\", 0: \"Parkinson's\"}\n",
    "x_five = 'Subiculum_CSF'\n",
    "x_five_factor_levels = 1\n",
    "outcome = 'cognitive_improvement'\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "gifs = create_interaction_gifs_with_5_variables(data_df, \n",
    "                               x_one, \n",
    "                               x_two, \n",
    "                               x_three, \n",
    "                               x_four,\n",
    "                               x_four_dict,\n",
    "                               x_five,\n",
    "                               x_five_factor_levels,\n",
    "                               outcome, \n",
    "                               results)\n",
    "print(\"GIFs created:\", gifs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Inflection Point of Joint Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Local Maxima/Minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "class SaddlePointFinderMaxMin:\n",
    "    \"\"\"\n",
    "    A class to identify saddle points in a 2D array using local maxima and minima method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the class with the 2D array to be analyzed.\n",
    "\n",
    "        Parameters:\n",
    "        data (np.ndarray): 2D array for which the saddle points are to be identified.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        \n",
    "    def find_local_minima_maxima(self, array):\n",
    "        \"\"\"\n",
    "        Find local minima and maxima in a 1D array.\n",
    "        \n",
    "        Parameters:\n",
    "        array (np.ndarray): 1D data array\n",
    "        \n",
    "        Returns:\n",
    "        tuple: Indices of local minima and maxima in the array.\n",
    "        \"\"\"\n",
    "        local_minima = argrelextrema(array, np.less)[0]\n",
    "        local_maxima = argrelextrema(array, np.greater)[0]\n",
    "        return local_minima, local_maxima\n",
    "    \n",
    "    def find_critical_points(self):\n",
    "        \"\"\"\n",
    "        Identify the saddle points in the 2D array using local maxima and minima method.\n",
    "\n",
    "        Returns:\n",
    "        list: List of coordinates (as tuples) of the identified saddle points.\n",
    "        \"\"\"\n",
    "        row_minima_coords = []\n",
    "        row_maxima_coords = []\n",
    "        for i, row in enumerate(self.data):\n",
    "            local_minima, local_maxima = self.find_local_minima_maxima(row)\n",
    "            for coord in local_minima:\n",
    "                row_minima_coords.append((i, coord))\n",
    "            for coord in local_maxima:\n",
    "                row_maxima_coords.append((i, coord))\n",
    "                \n",
    "        col_minima_coords = []\n",
    "        col_maxima_coords = []\n",
    "        for i, col in enumerate(self.data.T):\n",
    "            local_minima, local_maxima = self.find_local_minima_maxima(col)\n",
    "            for coord in local_minima:\n",
    "                col_minima_coords.append((coord, i))\n",
    "            for coord in local_maxima:\n",
    "                col_maxima_coords.append((coord, i))\n",
    "        \n",
    "        saddle_points = list(set(row_minima_coords).intersection(col_maxima_coords) | set(row_maxima_coords).intersection(col_minima_coords))\n",
    "        \n",
    "        return saddle_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the class with the given data\n",
    "finder = SaddlePointFinderMaxMin(y_pred)\n",
    "finder.find_critical_points()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema\n",
    "from matplotlib import colors \n",
    "\n",
    "class GradientCriticalPointFinder:\n",
    "    \"\"\"\n",
    "    A class to identify and visualize the approximate critical points in a 2D array using gradient-based methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_array):\n",
    "        \"\"\"\n",
    "        Initialize the GradientCriticalPointFinder with the 2D array to be analyzed.\n",
    "\n",
    "        Parameters:\n",
    "        data_array (np.ndarray): 2D array for which the critical points are to be identified.\n",
    "        \"\"\"\n",
    "        self.data_array = data_array\n",
    "        self.gradient_x = None\n",
    "        self.gradient_y = None\n",
    "        self.combined_gradient_abs = None\n",
    "        self.critical_points = None\n",
    "\n",
    "    def _compute_gradients(self):\n",
    "        \"\"\"\n",
    "        Computes the gradients for each dimension of the data array.\n",
    "        \"\"\"\n",
    "        self.gradient_x = np.gradient(self.data_array, axis=0)\n",
    "        self.gradient_y = np.gradient(self.data_array, axis=1)\n",
    "        combined_gradient = self.gradient_x * self.gradient_y\n",
    "        self.combined_gradient_abs = np.abs(combined_gradient)\n",
    "\n",
    "    def set_indices_to_one(self, zero_array, row_indices, col_indices):\n",
    "        for r, c in zip(row_indices, col_indices):\n",
    "            zero_array[r, c] = 1\n",
    "        return zero_array\n",
    "    \n",
    "    def _identify_critical_points(self, use_absolute_minima=False):\n",
    "        \"\"\"\n",
    "        Identify the approximate critical points by finding intersections between the minima of gradient_x and gradient_y.\n",
    "        \"\"\"\n",
    "        gradient_x_min = np.zeros_like(self.gradient_x)\n",
    "        gradient_y_min = np.zeros_like(self.gradient_y)\n",
    "\n",
    "        if use_absolute_minima:\n",
    "            # Find the index of the global minimum in each column for gradient_x\n",
    "            for i in range(0, self.gradient_x.shape[0]):\n",
    "                gradient_x_min[i, :] = np.where(self.gradient_x[i,:] == np.min(self.gradient_x[i,:]), 1, 0)\n",
    "                \n",
    "            # Find the index of the global minimum in each row for gradient_y\n",
    "            for i in range(0, self.gradient_y.shape[1]):\n",
    "                gradient_y_min[:, i] = np.where(self.gradient_y[:,i] == np.min(self.gradient_y[:,i]), 1, 0)\n",
    "\n",
    "        # Find coincident hits\n",
    "        critical_points_array = gradient_x_min * gradient_y_min\n",
    "        \n",
    "        # At the end of the method:\n",
    "        rows, cols = np.where(critical_points_array == 1)\n",
    "        self.critical_points = (rows, cols)\n",
    "        print('Critical points found at rows:', self.critical_points[0], 'cols:', self.critical_points[1])\n",
    "\n",
    "\n",
    "    def plot(self, use_absolute_minima=False):\n",
    "        \"\"\"\n",
    "        Generate the required plots.\n",
    "        \"\"\"\n",
    "        # Ensure gradients are computed\n",
    "        if self.gradient_x is None or self.gradient_y is None:\n",
    "            self._compute_gradients()\n",
    "\n",
    "        # Identify critical points\n",
    "        self._identify_critical_points(use_absolute_minima)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "        norm = colors.TwoSlopeNorm(vcenter=0)\n",
    "    \n",
    "        # Plot gradients with the norm applied\n",
    "        cax1 = axes[0].imshow(self.gradient_x, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[0].set_title(\"Gradient X\")\n",
    "        plt.colorbar(cax1, ax=axes[0], orientation='vertical')\n",
    "\n",
    "        cax2 = axes[1].imshow(self.gradient_y, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[1].set_title(\"Gradient Y\")\n",
    "        plt.colorbar(cax2, ax=axes[1], orientation='vertical')\n",
    "\n",
    "        # Plot absolute value of the multiplied gradients\n",
    "        cax3 = axes[2].imshow(self.combined_gradient_abs, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[2].set_title(\"Abs(Gradient X * Gradient Y)\")\n",
    "        plt.colorbar(cax3, ax=axes[2], orientation='vertical')\n",
    "        \n",
    "        # Plot data_array with critical points and the norm applied\n",
    "        cax4 = axes[3].imshow(self.data_array, cmap='bwr', origin='lower', norm=norm)\n",
    "        if len(self.critical_points[0]) > 0:\n",
    "            axes[3].scatter(self.critical_points[1], self.critical_points[0], color='black', marker='X', label='Critical Point')\n",
    "        axes[3].set_title(\"Function with Critical Points\")\n",
    "        plt.colorbar(cax4, ax=axes[3], orientation='vertical')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_critical_points(self):\n",
    "        \"\"\"\n",
    "        Return the coordinates of the identified critical points.\n",
    "        \"\"\"\n",
    "        return self.critical_points\n",
    "finder = GradientCriticalPointFinder(y_pred)\n",
    "finder.plot(use_absolute_minima=True)\n",
    "finder.get_critical_points()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema\n",
    "\n",
    "class GradientCriticalPointFinder:\n",
    "    \"\"\"\n",
    "    A class to identify and visualize the approximate critical points in a 2D array using gradient-based methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_array):\n",
    "        \"\"\"\n",
    "        Initialize the GradientCriticalPointFinder with the 2D array to be analyzed.\n",
    "\n",
    "        Parameters:\n",
    "        data_array (np.ndarray): 2D array for which the critical points are to be identified.\n",
    "        \"\"\"\n",
    "        self.data_array = data_array\n",
    "        self.gradient_x = None\n",
    "        self.gradient_y = None\n",
    "        self.combined_gradient_abs = None\n",
    "        self.critical_points = None\n",
    "\n",
    "    def _compute_gradients(self):\n",
    "        \"\"\"\n",
    "        Computes the gradients for each dimension of the data array.\n",
    "        \"\"\"\n",
    "        self.gradient_x = np.gradient(self.data_array, axis=0)\n",
    "        self.gradient_y = np.gradient(self.data_array, axis=1)\n",
    "\n",
    "    def _identify_critical_points(self):\n",
    "        \"\"\"\n",
    "        Identify the approximate critical points using element-wise multiplication and rounding.\n",
    "        \"\"\"\n",
    "        combined_gradient = self.gradient_x * self.gradient_y\n",
    "        self.combined_gradient_abs = np.abs(combined_gradient)\n",
    "        \n",
    "        # Find local minima\n",
    "        local_minima = argrelextrema(self.combined_gradient_abs, np.less)\n",
    "        self.critical_points = local_minima\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Generate the required plots.\n",
    "        \"\"\"\n",
    "        # Ensure gradients are computed\n",
    "        if self.gradient_x is None or self.gradient_y is None:\n",
    "            self._compute_gradients()\n",
    "\n",
    "        # Identify critical points\n",
    "        if self.critical_points is None:\n",
    "            self._identify_critical_points()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "        norm = colors.TwoSlopeNorm(vcenter=0)\n",
    "    \n",
    "        # Plot gradients with the norm applied\n",
    "        cax1 = axes[0].imshow(self.gradient_x, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[0].set_title(\"Gradient X\")\n",
    "        plt.colorbar(cax1, ax=axes[0], orientation='vertical')\n",
    "\n",
    "        cax2 = axes[1].imshow(self.gradient_y, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[1].set_title(\"Gradient Y\")\n",
    "        plt.colorbar(cax2, ax=axes[1], orientation='vertical')\n",
    "\n",
    "        # Plot absolute value of the multiplied gradients\n",
    "        cax3 = axes[2].imshow(self.combined_gradient_abs, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[2].set_title(\"Abs(Gradient X * Gradient Y)\")\n",
    "        plt.colorbar(cax3, ax=axes[2], orientation='vertical')\n",
    "        \n",
    "        # Plot data_array with critical points and the norm applied\n",
    "        cax4 = axes[3].imshow(self.data_array, cmap='bwr', origin='lower', norm=norm)\n",
    "        axes[3].scatter(self.critical_points[1], self.critical_points[0], color='black', marker='X', label='Critical Point')\n",
    "        axes[3].set_title(\"Function with Critical Points\")\n",
    "        plt.colorbar(cax4, ax=axes[3], orientation='vertical')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_critical_points(self):\n",
    "        \"\"\"\n",
    "        Return the coordinates of the identified critical points.\n",
    "        \"\"\"\n",
    "        if self.critical_points is None:\n",
    "            self._identify_critical_points()\n",
    "        \n",
    "        return self.critical_points\n",
    "finder = GradientCriticalPointFinder(y_pred)\n",
    "finder.plot()\n",
    "finder.get_critical_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_Grid[0] corresponds to the x-axis, and this derives the value of it at the gradient min.\n",
    "#In your experiements, this is generally age. \n",
    "print(X_grid[0][42, 58]) #<---row is the 0 value in gradient y, col is the 0 value in gradient x\n",
    "#X_Grid[1] corresponds to the z-axis, and this derives the value of it at the gradient min\n",
    "print(X_grid[1][42, 58])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Hessian discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianSaddlePointFinder:\n",
    "    \"\"\"\n",
    "    A class to identify the saddle points in a 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y_pred):\n",
    "        \"\"\"\n",
    "        Initialize the SaddlePointFinder with the 2D array to be analyzed.\n",
    "\n",
    "        Parameters:\n",
    "        y_pred (np.ndarray): 2D array for which the saddle points are to be identified.\n",
    "        \"\"\"\n",
    "        self.y_pred = y_pred\n",
    "\n",
    "    def _compute_derivatives(self):\n",
    "        \"\"\"\n",
    "        Computes the first order partial derivatives with respect to both dimensions.\n",
    "\n",
    "        Returns:\n",
    "        tuple: First order partial derivatives with respect to x and y dimensions.\n",
    "        \"\"\"\n",
    "        dy_d1 = np.gradient(self.y_pred, axis=0)\n",
    "        dy_d2 = np.gradient(self.y_pred, axis=1)\n",
    "        \n",
    "        return dy_d1, dy_d2\n",
    "\n",
    "    def _compute_second_mixed_derivatives(self):\n",
    "        \"\"\"\n",
    "        Computes the second order partial derivatives and both mixed partial derivatives.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Second order partial derivatives with respect to x and y dimensions, and both mixed partial derivatives.\n",
    "        \"\"\"\n",
    "        dy_d1, dy_d2 = self._compute_derivatives()\n",
    "        \n",
    "        # Second order partial derivative with respect to x dimension (rows)\n",
    "        d2y_d1_2 = np.gradient(dy_d1, axis=0)\n",
    "        \n",
    "        # Second order partial derivative with respect to y dimension (columns)\n",
    "        d2y_d2_2 = np.gradient(dy_d2, axis=1)\n",
    "        \n",
    "        # Mixed partial derivatives\n",
    "        d2y_d1d2 = np.gradient(dy_d1, axis=1)\n",
    "        d2y_d2d1 = np.gradient(dy_d2, axis=0)\n",
    "        \n",
    "        return d2y_d1_2, d2y_d2_2, d2y_d1d2, d2y_d2d1\n",
    "\n",
    "    def find_saddle_points(self):\n",
    "        \"\"\"\n",
    "        Identify the saddle points in the 2D array.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Coordinates of the saddle point or (None, None) if no saddle point exists.\n",
    "        \"\"\"\n",
    "        second_deriv_1, second_deriv_2, mixed_deriv_1, mixed_deriv_2 = self._compute_second_mixed_derivatives()\n",
    "        \n",
    "        # Check equivalence of mixed partial derivatives\n",
    "        if not np.allclose(mixed_deriv_1, mixed_deriv_2):\n",
    "            return \"Error: Mixed partial derivatives are not equivalent.\"\n",
    "        \n",
    "        # Calculate D\n",
    "        D = second_deriv_1 * second_deriv_2 - mixed_deriv_1**2\n",
    "        \n",
    "        # Identify saddle points\n",
    "        saddle_points = np.where(D < 0)\n",
    "        \n",
    "        print('Saddle points found at coordinates:')\n",
    "        print(np.min(D))\n",
    "        if saddle_points[0].size == 0:\n",
    "            return None, None\n",
    "        else:\n",
    "            return saddle_points[0][0], saddle_points[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test proof of saddle point idenfitication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the final class with the sample_array\n",
    "z = np.linspace(0,10,100)\n",
    "x = z \n",
    "\n",
    "# Create the grid of coordinates\n",
    "xv, zv = np.meshgrid(x,z)\n",
    "\n",
    "# Calculate y at every coordinate set\n",
    "sample_array = xv**2 - zv**2\n",
    "\n",
    "# Identify the single saddle point, which should be at 0,0\n",
    "saddle_finder_final = SaddlePointFinderFinal(sample_array)\n",
    "saddle_coordinates_final = saddle_finder_final.find_saddle_points()\n",
    "saddle_coordinates_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Saddle Point of Your Array of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saddle_finder_final = SaddlePointFinderFinal(y_pred)\n",
    "saddle_coordinates_final = saddle_finder_final.find_saddle_points()\n",
    "saddle_coordinates_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[saddle_coordinates_final[0], saddle_coordinates_final[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape\n",
    "from numpy import savetxt\n",
    "import os\n",
    "savetxt(os.path.join(out_dir,'y_pred.csv'), y_pred, delimiter=',')\n",
    "print('saved to: ', os.path.join(out_dir,'y_pred.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid.to_numpy().shape\n",
    "\n",
    "print(X_grid.to_numpy()[saddle_coordinates_final[0], 0])\n",
    "print(X_grid.to_numpy()[saddle_coordinates_final[1], 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Within Joint Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_statistics_and_create_dataframe(resampled_metrics, observed_metric, test_type, test_tails='two_tailed'):\n",
    "    \"\"\"\n",
    "    Compute p-value and confidence interval and organize them in a DataFrame.\n",
    "    \n",
    "    :param resampled_metrics: List of metrics computed from the resampled or permuted data\n",
    "    :param observed_metric: Observed metric computed from the original data\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :param test_type: Type of test performed ('resampling' or 'permutation')\n",
    "    :param test_tails: Type of tail for p-value testing ('two_tailed', 'right_tailed', or 'left_tailed')\n",
    "    :return: DataFrame with p-value and confidence interval\n",
    "    \"\"\"\n",
    "    # Calculate p-value and confidence interval for low and high values\n",
    "    if test_type =='permutation':\n",
    "        if test_tails == 'two_tailed':\n",
    "            print('Two-tailed p-value testing')\n",
    "            p_value_low = np.mean(np.abs([x[0] for x in resampled_metrics]) >= np.abs(observed_metric[0]))\n",
    "            p_value_high = np.mean(np.abs([x[1] for x in resampled_metrics]) >= np.abs(observed_metric[1]))\n",
    "        elif test_tails == 'right_tailed':\n",
    "            print('Right-tailed p-value testing')\n",
    "            p_value_low = np.mean([x[0] for x in resampled_metrics] >= observed_metric[0])\n",
    "            p_value_high = np.mean([x[1] for x in resampled_metrics] >= observed_metric[1])\n",
    "        elif test_tails == 'left_tailed':\n",
    "            print('Left-tailed p-value testing.')\n",
    "            p_value_low = np.mean([x[0] for x in resampled_metrics] <= observed_metric[0])\n",
    "            p_value_high = np.mean([x[1] for x in resampled_metrics] <= observed_metric[1])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for test_tails. Must be one of 'two_tailed', 'right_tailed', or 'left_tailed'.\")\n",
    "    elif test_type == 'resampling':\n",
    "        p_value_low = np.mean(np.sign(np.mean([x[0] for x in resampled_metrics])) != np.sign(observed_metric[0]))\n",
    "        p_value_high = np.mean(np.sign(np.mean([x[1] for x in resampled_metrics])) != np.sign(observed_metric[1]))\n",
    "    else:\n",
    "        raise ValueError(f\"Test type is not supported: {test_type}\")\n",
    "    \n",
    "    conf_int_low = np.percentile([x[0] for x in resampled_metrics], [2.5, 97.5])\n",
    "    conf_int_high = np.percentile([x[1] for x in resampled_metrics], [2.5, 97.5])\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        'Observed Metric': [observed_metric[0], observed_metric[1]],\n",
    "        'P-value': [p_value_low, p_value_high],\n",
    "        '95% CI Lower': [conf_int_low[0], conf_int_high[0]],\n",
    "        '95% CI Upper': [conf_int_low[1], conf_int_high[1]]\n",
    "    }\n",
    "    index = ['Delta Y pred indep_var_2 +/- stdev holding indep_var_1 at -2stdev', \n",
    "                'Delta Y pred indep_var_2 +/- stdev holding indep_var_1 at +2stdev']\n",
    "\n",
    "    return pd.DataFrame(data, index=[index])\n",
    "\n",
    "\n",
    "def compare_responses(df, dependent_var, independent_var1, independent_var2, interaction=True):\n",
    "    \"\"\"\n",
    "    Compare responses of a dataset using point testing method.\n",
    "    \n",
    "    :param df: DataFrame containing the dataset\n",
    "    :param dependent_var: Name of the dependent variable\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param interaction: Boolean indicating if interaction between independent variables should be considered\n",
    "    :param test_tails: Type of tail for p-value testing ('two_tailed', 'right_tailed', or 'left_tailed')\n",
    "    :return: Comparison metric (absolute differences between low and high points)\n",
    "    \"\"\"\n",
    "    if interaction:\n",
    "        formula = f'{dependent_var} ~ {independent_var1} * {independent_var2}'\n",
    "    else:\n",
    "        formula = f'{dependent_var} ~ {independent_var1} + {independent_var2}'\n",
    "\n",
    "    # Fit the model\n",
    "    model = smf.ols(formula=formula, data=df).fit()\n",
    "\n",
    "    # Evaluate at specific points\n",
    "    diff_low, diff_high = evaluate_at_specific_points(df, independent_var1, independent_var2, model)\n",
    "\n",
    "    # Return the absolute differences\n",
    "    return abs(diff_low), abs(diff_high)\n",
    "\n",
    "def evaluate_at_specific_points(df, independent_var1, independent_var2, model):\n",
    "    \"\"\"\n",
    "    Evaluate the model at specific points.\n",
    "    \n",
    "    :param df: DataFrame containing the dataset\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param model: Fitted model\n",
    "    :return: Differences between low and high points\n",
    "    \"\"\"\n",
    "    # Determine the low and high values of the independent variables\n",
    "    iv1_sd = data_df[independent_var1].std()\n",
    "    iv2_sd = data_df[independent_var2].std()\n",
    "    iv1_mean = data_df[independent_var1].mean()\n",
    "    iv2_mean = data_df[independent_var2].mean()\n",
    "\n",
    "    # Create the dataframes for low_1 and high_1 predictions\n",
    "    low_1_df = pd.DataFrame({\n",
    "        independent_var1: iv1_mean - 2 * iv1_sd,\n",
    "        independent_var2: [iv2_mean - 2 * iv2_sd, iv2_mean + 2 * iv2_sd]\n",
    "    })\n",
    "\n",
    "    high_1_df = pd.DataFrame({\n",
    "        independent_var1: iv1_mean + 2 * iv1_sd,\n",
    "        independent_var2: [iv2_mean - 2 * iv2_sd, iv2_mean + 2 * iv2_sd]\n",
    "    })\n",
    "\n",
    "    # Perform the predictions on the dataframes\n",
    "    low_1_predictions = model.predict(low_1_df)\n",
    "    high_1_predictions = model.predict(high_1_df)\n",
    "\n",
    "    # Calculate the differences between the predictions\n",
    "    difference_low = low_1_predictions[1] - low_1_predictions[0]\n",
    "    difference_high = high_1_predictions[1] - high_1_predictions[0]\n",
    "\n",
    "    return difference_low, difference_high\n",
    "\n",
    "def resample_or_permutation_test(df, dependent_var, independent_var1, independent_var2, interaction=True, num_resamples=1000, test_type='permutation', test_tails='two_tails'):\n",
    "    \"\"\"\n",
    "    Perform resampling or permutation test to compare responses of two datasets and return results in a DataFrame.\n",
    "    \n",
    "    :param df: DataFrame containing the dataset\n",
    "    :param dependent_var: Name of the dependent variable\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param interaction: Boolean indicating if interaction between independent variables should be considered\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :param num_slices: Number of values to take for each variable to form the grid (used in correlation)\n",
    "    :param num_resamples: Number of resamples to perform\n",
    "    :param test_type: Type of test to perform ('resampling' or 'permutation')\n",
    "    :return: p-value and confidence interval\n",
    "    \"\"\"\n",
    "    # Calculate observed metric\n",
    "    observed_metric = compare_responses(df, dependent_var, independent_var1, independent_var2, interaction)\n",
    "    \n",
    "    # Initialize results\n",
    "    resampled_metrics = []\n",
    "\n",
    "    for i in tqdm(range(num_resamples)):\n",
    "        # Perform resampling or permutation\n",
    "        if test_type == 'resampling':\n",
    "            sample_df = df.sample(frac=1, replace=True)\n",
    "        elif test_type == 'permutation':\n",
    "            sample_df = df.copy()\n",
    "            sample_df[dependent_var] = np.random.permutation(sample_df[dependent_var].values)\n",
    "        else:\n",
    "            raise ValueError(\"test_type must be either 'resampling' or 'permutation'.\")\n",
    "        \n",
    "        # Calculate resampled metric\n",
    "        resampled_metric = compare_responses(sample_df, dependent_var, independent_var1, independent_var2, interaction)\n",
    "        resampled_metrics.append(resampled_metric)\n",
    "    \n",
    "    # Calculate statistics and create DataFrame\n",
    "    results_dataframe = compute_statistics_and_create_dataframe(resampled_metrics, observed_metric, test_type, test_tails=test_tails)\n",
    "    \n",
    "    return results_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dataframe 1 and dataframe 2 for comaprison by permutation test\n",
    "df_1 = data_df[data_df['Disease'] == 1]\n",
    "df_2 = data_df[data_df['Disease'] == 0]\n",
    "dependent_var = 'outcome'\n",
    "independent_var1 = 'Age'\n",
    "independent_var2 = 'Subiculum_Connectivity'\n",
    "enable_interaction = True\n",
    "num_iterations = 10000\n",
    "test_tails = 'two_tailed'\n",
    "#:param test_tails: Type of tail for p-value testing ('two_tailed', 'right_tailed', or 'left_tailed')\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "permutation_results = resample_or_permutation_test(df = df_1,\n",
    "                                                    dependent_var = dependent_var,\n",
    "                                                    independent_var1 = independent_var1,\n",
    "                                                    independent_var2 = independent_var2,\n",
    "                                                    interaction = enable_interaction,\n",
    "                                                    num_resamples=num_iterations, \n",
    "                                                    test_type='permutation',\n",
    "                                                    test_tails=test_tails)\n",
    "permutation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "bootstrap_results = resample_or_permutation_test(df = df_2,\n",
    "                                                    dependent_var = dependent_var,\n",
    "                                                    independent_var1 = independent_var1,\n",
    "                                                    independent_var2 = independent_var2,\n",
    "                                                    interaction = enable_interaction,\n",
    "                                                    num_resamples=num_iterations, \n",
    "                                                    test_type='resampling')\n",
    "bootstrap_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Between Joint Distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of 2 Joint Distributions**\n",
    "1) Joint Distributions and Multiple Regression:\n",
    "\n",
    "The concept of \"joint distributions\" refers to the combined distribution of multiple variables, taking into account their mutual relationships. In the context of multiple regression analysis, we utilize a statistical model to examine the joint distribution of the independent variables and their relationship with the dependent variable. By fitting a regression model, such as the one used in this code, we can investigate how the independent variables collectively influence the dependent variable.\n",
    "\n",
    "In the provided code, we compare the responses of two datasets based on their joint distributions. This means that we analyze the relationship between the independent variables (independent_var1 and independent_var2) as a pair and their collective influence on the dependent variable. We fit separate regression models for each dataset, considering both independent variables as predictors and the dependent variable as the response.\n",
    "\n",
    "Analyzing the joint distributions allows us to gain insights into how the independent variables interact with each other and jointly impact the dependent variable. By considering the joint relationship between the variables, we can understand how changes in one independent variable may affect the response when the other independent variable is also taken into account.\n",
    "\n",
    "2) Permutation vs Bootstrap:\n",
    "\n",
    "The code employs two different approaches for hypothesis testing: permutation testing and bootstrap resampling. Both methods aim to assess the significance of the observed metrics by generating alternative datasets.\n",
    "\n",
    "Permutation testing involves randomly permuting the values of the dependent variable while keeping the independent variables unchanged. This process breaks the relationship between the dependent and independent variables, creating new datasets under the assumption of no association. By comparing the observed metric with metrics obtained from the permuted datasets, we can estimate p-values and confidence intervals, providing a measure of the statistical significance of our results. A permuted dataset is one wherein the independent variables are unrelated to the dependent variable. They are the null distribution. \n",
    "\n",
    "Bootstrap resampling, on the other hand, involves randomly sampling data points from the original datasets with replacement. This resampling process allows us to create multiple datasets of the same size as the original datasets. By generating new datasets through resampling, we can examine the distribution of the metrics of interest and estimate their variability. This information can be used to estimate confidence intervals and assess the statistical significance of our findings. A bootstrap resampling process creates alternative datasets wherein the independent variables are still related to the dependent variable. These define significance by demonstrating the confidence intervals of the dataset do not cross a threshold, which is typically 0.\n",
    "\n",
    "3) Correlation vs Point-test:\n",
    "\n",
    "The code provides two different methods for comparing responses based on the joint distributions: correlation and point testing.\n",
    "\n",
    "The correlation analysis aims to assess the spatial correlation between the joint distributions of the two datasets. This analysis examines the overall patterns and similarities in the responses across the range of the joint distributions. By calculating the correlation coefficient between the predicted responses, we can determine if there is a consistent relationship between the joint distributions of the independent variables and the dependent variable across the datasets. Then, we compare this to bootstrapped or permuted datasets to derive significance estimates.\n",
    "\n",
    "In contrast, point testing focuses on evaluating the differences in the predicted responses at specific points of the joint distributions. Specifically, we examine the differences at +/- 2 standard deviations of the independent variables. This analysis allows us to determine if the differences between the responses at these specific points significantly vary between the two datasets. By comparing the differences in response values at these critical points, we can identify regions of the joint distributions where the relationship between the independent variables and the dependent variable differs significantly. The purpose of this is to condense a 3-dimensional joint distribution into 2-dimensions at set values which is good for generating figures. Then, we compare this to bootstrapped or permuted datasets to derive significance estimates.\n",
    "\n",
    "In summary, comparing the joint distributions enables us to analyze the combined effects of the independent variables on the dependent variable. The correlation and point testing methods offer different perspectives on the relationship between the variables, while permutation testing and bootstrap resampling provide means to assess the statistical significance of our findings.\n",
    "\n",
    "**Instructions**\n",
    "Testing: 'correlation' or 'point_testing'\n",
    "Significance Testing: 'permutation' or 'resampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_statistics_and_create_dataframe(resampled_metrics, observed_metric, comparison_method, test_type, test_tails='two_tailed'):\n",
    "    \"\"\"\n",
    "    Compute p-value and confidence interval and organize them in a DataFrame.\n",
    "    \n",
    "    :param resampled_metrics: List of metrics computed from the resampled or permuted data\n",
    "    :param observed_metric: Observed metric computed from the original data\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :return: DataFrame with p-value and confidence interval\n",
    "    \"\"\"\n",
    "    if comparison_method == 'correlation':\n",
    "        # Calculate p-value and confidence interval\n",
    "        p_value = np.mean(np.abs(resampled_metrics) >= np.abs(observed_metric))\n",
    "        conf_int = np.percentile(resampled_metrics, [2.5, 97.5])\n",
    "\n",
    "        # Create DataFrame\n",
    "        data = {\n",
    "            'Observed Metric': [observed_metric],\n",
    "            'P-value': [p_value],\n",
    "            '95% CI Lower': [conf_int[0]],\n",
    "            '95% CI Upper': [conf_int[1]]\n",
    "        }\n",
    "        index = 'Spatial correlation of the joint distributions'\n",
    "    \n",
    "    elif comparison_method == 'point_testing':\n",
    "        # Calculate p-value and confidence interval for low and high values\n",
    "        if test_type =='permutation':\n",
    "            if test_tails == 'two_tailed':\n",
    "                print('Two-tailed p-value testing')\n",
    "                p_value_low = np.mean(np.abs([x[0] for x in resampled_metrics]) >= np.abs(observed_metric[0]))\n",
    "                p_value_high = np.mean(np.abs([x[1] for x in resampled_metrics]) >= np.abs(observed_metric[1]))\n",
    "            elif test_tails == 'right_tailed':\n",
    "                print('Right-tailed p-value testing')\n",
    "                p_value_low = np.mean([x[0] for x in resampled_metrics] >= observed_metric[0])\n",
    "                p_value_high = np.mean([x[1] for x in resampled_metrics] >= observed_metric[1])\n",
    "            elif test_tails == 'left_tailed':\n",
    "                print('Left-tailed p-value testing.')\n",
    "                p_value_low = np.mean([x[0] for x in resampled_metrics] <= observed_metric[0])\n",
    "                p_value_high = np.mean([x[1] for x in resampled_metrics] <= observed_metric[1])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid value for test_tails. Must be one of 'two_tailed', 'right_tailed', or 'left_tailed'.\")\n",
    "        elif test_type == 'resampling':\n",
    "            p_value_low = np.mean(np.sign(np.mean([x[0] for x in resampled_metrics])) != (np.sign(np.mean([x[0] for x in resampled_metrics]))))\n",
    "            p_value_high = np.mean(np.sign(np.mean([x[1] for x in resampled_metrics])) != (np.sign(np.mean([x[1] for x in resampled_metrics]))))\n",
    "        else:\n",
    "            raise ValueError (f'Test type is not supported {test_type}')\n",
    "        conf_int_low = np.percentile(resampled_metrics[0], [2.5, 97.5])\n",
    "        conf_int_high = np.percentile(resampled_metrics[1], [2.5, 97.5])\n",
    "\n",
    "        # Create DataFrame\n",
    "        data = {\n",
    "            'Observed Metric': [observed_metric[0], observed_metric[1]],\n",
    "            'P-value': [p_value_low, p_value_high],\n",
    "            '95% CI Lower': [conf_int_low[0], conf_int_high[0]],\n",
    "            '95% CI Upper': [conf_int_low[1], conf_int_high[1]]\n",
    "        }\n",
    "        index = ['Delta Y pred indep_var_2 +/- stdev holding indep_var_1 at -2stdev', \n",
    "                 'Delta Y pred indep_var_2 +/- stdev holding indep_var_1 at +2stdev']\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"comparison_method must be either 'correlation' or 'point_testing'.\")\n",
    "\n",
    "    return pd.DataFrame(data, index=[index])\n",
    "\n",
    "\n",
    "def calculate_predicted_response(data_df, independent_var1, independent_var2, model, num_slices=100):\n",
    "    \"\"\"\n",
    "    Calculate predicted response values for a grid of values of two independent variables.\n",
    "    \n",
    "    :param data_df: DataFrame containing data\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param model: Fitted model\n",
    "    :param num_slices: Number of values to take for each variable to form the grid\n",
    "    :return: Flattened array of predicted responses\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(min(data_df[independent_var1]), max(data_df[independent_var1]), num_slices)\n",
    "    x2 = np.linspace(min(data_df[independent_var2]), max(data_df[independent_var2]), num_slices)\n",
    "    x1v, x2v = np.meshgrid(x1, x2)\n",
    "\n",
    "    X_grid = pd.DataFrame({\n",
    "        independent_var1: x1v.ravel(),\n",
    "        independent_var2: x2v.ravel(),\n",
    "    })\n",
    "\n",
    "    y_pred = model.predict(X_grid).values.reshape(num_slices, num_slices)\n",
    "\n",
    "    return y_pred.ravel()\n",
    "\n",
    "def evaluate_at_specific_points(data_df, independent_var1, independent_var2, model):\n",
    "    \"\"\"\n",
    "    Evaluate the model at specific points: +/- 2 standard deviations of the independent variables.\n",
    "\n",
    "    :param data_df: DataFrame containing data\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param model: Fitted model\n",
    "    :return: Differences between the predictions at specific points\n",
    "    \"\"\"\n",
    "    iv1_sd = data_df[independent_var1].std()\n",
    "    iv2_sd = data_df[independent_var2].std()\n",
    "    iv1_mean = data_df[independent_var1].mean()\n",
    "    iv2_mean = data_df[independent_var2].mean()\n",
    "\n",
    "    # Create the dataframes for low_1 and high_1 predictions\n",
    "    low_1_df = pd.DataFrame({\n",
    "        independent_var1: iv1_mean - 2 * iv1_sd,\n",
    "        independent_var2: [iv2_mean - 2 * iv2_sd, iv2_mean + 2 * iv2_sd]\n",
    "    })\n",
    "\n",
    "    high_1_df = pd.DataFrame({\n",
    "        independent_var1: iv1_mean + 2 * iv1_sd,\n",
    "        independent_var2: [iv2_mean - 2 * iv2_sd, iv2_mean + 2 * iv2_sd]\n",
    "    })\n",
    "\n",
    "    # Perform the predictions on the dataframes\n",
    "    low_1_predictions = model.predict(low_1_df)\n",
    "    high_1_predictions = model.predict(high_1_df)\n",
    "\n",
    "    # Calculate the differences between the predictions\n",
    "    difference_low = low_1_predictions[1] - low_1_predictions[0]\n",
    "    difference_high = high_1_predictions[1] - high_1_predictions[0]\n",
    "\n",
    "    return difference_low, difference_high\n",
    "\n",
    "def compare_responses(df1, df2, dependent_var, independent_var1, independent_var2, interaction=True, comparison_method='correlation', num_slices=100):\n",
    "    \"\"\"\n",
    "    Compare responses of two datasets using correlation or point testing method.\n",
    "    \n",
    "    :param df1: DataFrame containing first dataset\n",
    "    :param df2: DataFrame containing second dataset\n",
    "    :param dependent_var: Name of the dependent variable\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param interaction: Boolean indicating if interaction between independent variables should be considered\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :param num_slices: Number of values to take for each variable to form the grid (used in correlation)\n",
    "    :return: Comparison metric\n",
    "    \"\"\"\n",
    "    if interaction:\n",
    "        formula = f'{dependent_var} ~ {independent_var1} * {independent_var2}'\n",
    "    else:\n",
    "        formula = f'{dependent_var} ~ {independent_var1} + {independent_var2}'\n",
    "\n",
    "    # Fit the models\n",
    "    model1 = smf.ols(formula=formula, data=df1).fit()\n",
    "    model2 = smf.ols(formula=formula, data=df2).fit()\n",
    "\n",
    "    if comparison_method == 'correlation':\n",
    "        # Calculate predicted responses\n",
    "        response_1 = calculate_predicted_response(df1, independent_var1, independent_var2, model1, num_slices)\n",
    "        response_2 = calculate_predicted_response(df2, independent_var1, independent_var2, model2, num_slices)\n",
    "\n",
    "        # Calculate correlation between the predicted responses\n",
    "        return stats.pearsonr(response_1, response_2)[0]\n",
    "\n",
    "    elif comparison_method == 'point_testing':\n",
    "        # Evaluate at specific points\n",
    "        diff_low_1, diff_high_1 = evaluate_at_specific_points(df1, independent_var1, independent_var2, model1)\n",
    "        diff_low_2, diff_high_2 = evaluate_at_specific_points(df2, independent_var1, independent_var2, model2)\n",
    "        \n",
    "        # Return the absolute differences\n",
    "        return abs(diff_low_1 - diff_low_2), abs(diff_high_1 - diff_high_2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"comparison_method must be either 'correlation' or 'point_testing'.\")\n",
    "\n",
    "# Update the resample_or_permutation_test function\n",
    "def resample_or_permutation_test(df1, df2, dependent_var, independent_var1, independent_var2, interaction=True, comparison_method='correlation', num_slices=100, num_resamples=1000, test_type='permutation'):\n",
    "    \"\"\"\n",
    "    Perform resampling or permutation test to compare responses of two datasets and return results in a DataFrame.\n",
    "    \n",
    "    :param df1: DataFrame containing first dataset\n",
    "    :param df2: DataFrame containing second dataset\n",
    "    :param dependent_var: Name of the dependent variable\n",
    "    :param independent_var1: Name of the first independent variable\n",
    "    :param independent_var2: Name of the second independent variable\n",
    "    :param interaction: Boolean indicating if interaction between independent variables should be considered\n",
    "    :param comparison_method: Method for comparing responses ('correlation' or 'point_testing')\n",
    "    :param num_slices: Number of values to take for each variable to form the grid (used in correlation)\n",
    "    :param num_resamples: Number of resamples to perform\n",
    "    :param test_type: Type of test to perform ('resampling' or 'permutation')\n",
    "    :return: p-value and confidence interval\n",
    "    \"\"\"\n",
    "    # Calculate observed metric\n",
    "    observed_metric = compare_responses(df1, df2, dependent_var, independent_var1, independent_var2, interaction, comparison_method, num_slices)\n",
    "    \n",
    "    # Initialize results\n",
    "    resampled_metrics = []\n",
    "\n",
    "    for i in tqdm(range(num_resamples)):\n",
    "        # Perform resampling or permutation\n",
    "        if test_type == 'resampling':\n",
    "            sample_df1 = df1.sample(frac=1, replace=True)\n",
    "            sample_df2 = df2.sample(frac=1, replace=True)\n",
    "        elif test_type == 'permutation':\n",
    "            sample_df1 = df1.copy()\n",
    "            sample_df2 = df2.copy()\n",
    "            sample_df1[dependent_var] = np.random.permutation(sample_df1[dependent_var].values)\n",
    "            sample_df2[dependent_var] = np.random.permutation(sample_df2[dependent_var].values)\n",
    "        else:\n",
    "            raise ValueError(\"test_type must be either 'resampling' or 'permutation'.\")\n",
    "        \n",
    "        # Calculate resampled metric\n",
    "        resampled_metric = compare_responses(sample_df1, sample_df2, dependent_var, independent_var1, independent_var2, interaction, comparison_method, num_slices)\n",
    "        resampled_metrics.append(resampled_metric)\n",
    "    \n",
    "    # Calculate statistics and create DataFrame\n",
    "    results_dataframe = compute_statistics_and_create_dataframe(resampled_metrics, observed_metric, comparison_method, test_type)\n",
    "    \n",
    "    return results_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dataframe 1 and dataframe 2 for comaprison by permutation test\n",
    "df_1 = data_df[data_df['Disease'] == 1]\n",
    "df_2 = data_df[data_df['Disease'] == 0]\n",
    "dependent_var = 'outcome'\n",
    "independent_var1 = 'age'\n",
    "independent_var2 = 'memory_roi'\n",
    "enable_interaction = True\n",
    "num_iterations = 10000\n",
    "comparison_method = 'correlation' \n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "permutation_results = resample_or_permutation_test(df1 = df_1,\n",
    "                                                    df2 = df_2,\n",
    "                                                    dependent_var = dependent_var,\n",
    "                                                    independent_var1 = independent_var1,\n",
    "                                                    independent_var2 = independent_var2,\n",
    "                                                    interaction = enable_interaction,\n",
    "                                                    test_type = 'permutation',\n",
    "                                                    comparison_method = comparison_method,\n",
    "                                                    num_resamples = num_iterations)\n",
    "permutation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "bootstrap_results = resample_or_permutation_test(df1 = df_1,\n",
    "                                                    df2 = df_2,\n",
    "                                                    dependent_var = dependent_var,\n",
    "                                                    independent_var1 = independent_var1,\n",
    "                                                    independent_var2 = independent_var2,\n",
    "                                                    interaction = enable_interaction,\n",
    "                                                    test_type = 'resampling',\n",
    "                                                    comparison_method = comparison_method,\n",
    "                                                    num_resamples = num_iterations)\n",
    "bootstrap_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See If Two Regressions Are Significantly Different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (if you haven't already)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RegressionComparison:\n",
    "    def __init__(self, df, dependent_var, independent_vars, split_var, split_value, constant_value_for_second_predictor=np.nan):\n",
    "        \"\"\"\n",
    "        Initialize the RegressionComparison object with data and variable names.\n",
    "        \"\"\"\n",
    "        self.df1 = df[df[split_var] <= split_value]\n",
    "        self.df2 = df[df[split_var] > split_value]\n",
    "        self.dependent_var = dependent_var\n",
    "        self.independent_vars = independent_vars\n",
    "        self.formula = f\"{dependent_var} ~ {' + '.join(independent_vars)}\"\n",
    "        self.constant_value_for_second_predictor=constant_value_for_second_predictor\n",
    "        if self.df1.empty or self.df2.empty:\n",
    "            raise ValueError(\"One of the datasets is empty after the split. Check your split variable and value.\")\n",
    "    \n",
    "    def fit_models(self):\n",
    "        \"\"\"\n",
    "        Fit OLS regression models for each dataset.\n",
    "        \"\"\"\n",
    "        self.model1 = smf.ols(formula=self.formula, data=self.df1).fit()\n",
    "        self.model2 = smf.ols(formula=self.formula, data=self.df2).fit()\n",
    "        \n",
    "    def create_evaluation_dataframe(self, n_points=100):\n",
    "        \"\"\"\n",
    "        Create a DataFrame for evaluation with different ranges for the first predictor\n",
    "        and a constant value for the second predictor.\n",
    "        \n",
    "        :param constant_value_for_second_predictor: The constant value to set for the second predictor.\n",
    "        :param n_points: Number of points where the model will be evaluated.\n",
    "        :return: Two DataFrames for evaluation, one for each dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # For the first dataset\n",
    "        x_values_df1 = np.linspace(min(self.df1[self.independent_vars[0]]), \n",
    "                                max(self.df1[self.independent_vars[0]]), n_points)\n",
    "        eval_df1 = pd.DataFrame({self.independent_vars[0]: x_values_df1})\n",
    "        eval_df1[self.independent_vars[1]] = self.constant_value_for_second_predictor\n",
    "        \n",
    "        # For the second dataset\n",
    "        x_values_df2 = np.linspace(min(self.df2[self.independent_vars[0]]), \n",
    "                                max(self.df2[self.independent_vars[0]]), n_points)\n",
    "        eval_df2 = pd.DataFrame({self.independent_vars[0]: x_values_df2})\n",
    "        eval_df2[self.independent_vars[1]] = self.constant_value_for_second_predictor\n",
    "        \n",
    "        return eval_df1, eval_df2\n",
    "\n",
    "\n",
    "    def calculate_observed_delta(self, n_points=100):\n",
    "        \"\"\"\n",
    "        Calculate the observed delta between the two regression models for 100 datapoints.\n",
    "        \"\"\"\n",
    "        self.x_df1, self.x_df2 = self.create_evaluation_dataframe()\n",
    "\n",
    "        pred1 = self.model1.predict(self.x_df1)\n",
    "        pred2 = self.model2.predict(self.x_df2)\n",
    "\n",
    "        self.observed_delta = np.sum(pred1 - pred2)\n",
    "        \n",
    "\n",
    "    def permute_outcomes(self):\n",
    "        \"\"\"Permute the dependent variable in df1 and df2.\"\"\"\n",
    "        permuted_y1 = np.random.permutation(self.df1[self.dependent_var].values)\n",
    "        permuted_y2 = np.random.permutation(self.df2[self.dependent_var].values)\n",
    "\n",
    "        perm_df1 = self.df1.copy()\n",
    "        perm_df2 = self.df2.copy()\n",
    "\n",
    "        perm_df1[self.dependent_var] = permuted_y1\n",
    "        perm_df2[self.dependent_var] = permuted_y2\n",
    "\n",
    "        return perm_df1, perm_df2\n",
    "\n",
    "\n",
    "    def calculate_empirical_delta(self, n_permutations=1000, n_points=100):\n",
    "        \"\"\"Calculate the empirical delta using permuted data.\"\"\"\n",
    "        self.empirical_deltas = []\n",
    "\n",
    "        for _ in tqdm(range(n_permutations)):\n",
    "            # Step 1: Permute the outcomes\n",
    "            perm_df1, perm_df2 = self.permute_outcomes()\n",
    "\n",
    "            # Step 2: Fit models on permuted data\n",
    "            model1 = smf.ols(formula=self.formula, data=perm_df1).fit()\n",
    "            model2 = smf.ols(formula=self.formula, data=perm_df2).fit()\n",
    "\n",
    "            # Step 3: Create evaluation DataFrames for permuted data\n",
    "            x_df1, x_df2 = self.create_evaluation_dataframe(n_points)\n",
    "\n",
    "            # Step 4: Make predictions on evaluation DataFrames\n",
    "            pred1 = model1.predict(x_df1)\n",
    "            pred2 = model2.predict(x_df2)\n",
    "\n",
    "            # Step 5: Calculate and store empirical delta\n",
    "            empirical_delta = np.sum(pred1 - pred2)\n",
    "            self.empirical_deltas.append(empirical_delta)\n",
    "\n",
    "    def calculate_significance(self):\n",
    "        \"\"\"\n",
    "        Calculate the p-value based on empirical deltas.\n",
    "        \"\"\"\n",
    "        mask = self.empirical_deltas >= self.observed_delta\n",
    "        self.p_value = np.mean(mask)\n",
    "        \n",
    "        return self.p_value\n",
    "\n",
    "    def run(self, n_permutations=1000, n_points=100):\n",
    "        \"\"\"\n",
    "        Orchestrates the complete pipeline.\n",
    "        \n",
    "        :param n_permutations: Number of permutations for empirical delta calculation.\n",
    "        :param n_points: Number of points to use in observed delta calculation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Fit the models\n",
    "        self.fit_models()\n",
    "        \n",
    "        # Step 2: Create evaluation DataFrames\n",
    "        self.x_df1, self.x_df2 = self.create_evaluation_dataframe(n_points)\n",
    "        \n",
    "        # Step 3: Calculate observed delta\n",
    "        self.calculate_observed_delta(n_points)\n",
    "        \n",
    "        # Step 4: Calculate empirical delta\n",
    "        self.calculate_empirical_delta(n_permutations)\n",
    "        \n",
    "        # Step 5: Calculate significance\n",
    "        p_value = self.calculate_significance()\n",
    "        \n",
    "        return p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punch In Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables based on your specific case\n",
    "dependent_var = 'Z_Scored_Cognitive_Improvement_By_Group'\n",
    "independent_vars = ['Subiculum_Connectivity', 'Age']\n",
    "split_var = 'Cohort'\n",
    "split_value = 0.5\n",
    "value_to_set_to = 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class with the variables\n",
    "reg_comp = RegressionComparison(df=data_df, \n",
    "                                dependent_var=dependent_var, \n",
    "                                independent_vars=independent_vars, \n",
    "                                split_var=split_var, \n",
    "                                split_value=split_value, \n",
    "                                constant_value_for_second_predictor=value_to_set_to)\n",
    "\n",
    "# Fit the models\n",
    "p = reg_comp.run()\n",
    "print(\"Significance Count:\", p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Fancy Response Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.import_matrices import import_matrices_from_folder\n",
    "from calvin_utils.generate_nifti import nifti_from_matrix\n",
    "from nimlab import datasets as nimds\n",
    "import numpy as np\n",
    "from nilearn import image, plotting, maskers\n",
    "\n",
    "#Name variables to plot\n",
    "var_one = 'Limbic'\n",
    "var_two = 'Ventral_Attention'\n",
    "val_var_two = 65.4\n",
    "\n",
    "#get conectivity values of interest\n",
    "matrix_path = '/Users/cu135/Dropbox (Partners HealthCare)/memory/functional_networks/seed_generated_networks/yeo_networks_from_thick_yeo_seeds/all_05_limbic_T.nii'\n",
    "matrix = import_matrices_from_folder(matrix_path, file_pattern='')\n",
    "\n",
    "#get conectivity values of interest\n",
    "matrix_path_2 = '/Users/cu135/Dropbox (Partners HealthCare)/memory/functional_networks/seed_generated_networks/yeo_networks_from_thick_yeo_seeds/all_04_ventral_attention_T.nii'\n",
    "matrix_2 = import_matrices_from_folder(matrix_path, file_pattern='')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------DO NOT MODIFY!------------------------------------------------------------------------------------------------\n",
    "#Set reression input values into a dataframe\n",
    "input_df = pd.DataFrame()\n",
    "input_df[var_one] = matrix.iloc[:,0]\n",
    "input_df[var_two] = matrix_2.iloc[:,0]\n",
    "\n",
    "from nimlab import datasets as nimds\n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_data = mni_mask.get_fdata().flatten()\n",
    "brain_indices = np.where(mask_data > 0)[0]\n",
    "input_df = input_df.iloc[brain_indices, :]\n",
    "\n",
    "#Standardize matrix via z score\n",
    "from calvin_utils.z_score_matrix import z_score_matrix\n",
    "for col in input_df.columns:\n",
    "    input_df.loc[:, col] = z_score_matrix(input_df.loc[:, col])\n",
    "\n",
    "#Display results\n",
    "display(input_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "#Generate a new matrix from the input dataframe\n",
    "responses = []\n",
    "\n",
    "#Work on a voxel-wise basis by iterating over index\n",
    "for i in tqdm.tqdm(range(0, len(input_df))):\n",
    "    #Do not calculate on zero values as intercept will be applied in the regressoin\n",
    "    if np.sum(input_df.iloc[i,0]) != 0:\n",
    "        #Assign a temporary dataframe with values that the statsmodels model is expecting\n",
    "        temp_df = pd.DataFrame({var_one: input_df.iloc[i,0], var_two: input_df.iloc[i,1]}, index=['temp_vals'])\n",
    "        #Calculate the voxelwise predicted outcome at a given voxel\n",
    "        responses.append(results.predict(temp_df)[0])\n",
    "    else:\n",
    "        #If voxel is zero-connectivity, assign zero so as to avoid application of intercept\n",
    "        responses.append(0)\n",
    "        \n",
    "#Store responses in a dataframe\n",
    "response_df = pd.DataFrame()\n",
    "response_df['response_topology'] = responses\n",
    "display(response_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Place back in brain mask\n",
    "mask_data[brain_indices] = response_df.loc[:, 'response_topology']\n",
    "response_toplogy = mask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.generate_nifti import view_and_save_nifti\n",
    "matrix_img = view_and_save_nifti(response_toplogy, out_dir)\n",
    "matrix_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Driven Search for Most Relevant Regressors\n",
    "Combines:\n",
    "- All possible subsets regression combined with leave-one-out cross validation\n",
    "- Identifies formula with lowest RMSE and highest Pearson Correlation of predictor to observed outcome values\n",
    "- Presents the optimal formulas and allows selection of which formula to interpret with structural analysis and coefficient analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_interaction_terms(df, predictors, max_interaction_level=3):\n",
    "    interaction_df = df[predictors].copy()\n",
    "\n",
    "    def add_interaction_terms(df, columns, interaction_level):\n",
    "        if interaction_level == 1:\n",
    "            return df\n",
    "\n",
    "        for col_combination in itertools.combinations(columns, interaction_level):\n",
    "            interaction_term = ':'.join(col_combination)\n",
    "            interaction_values = np.prod(df[list(col_combination)], axis=1)\n",
    "            df[interaction_term] = interaction_values\n",
    "\n",
    "        return add_interaction_terms(df, columns, interaction_level - 1)\n",
    "\n",
    "    return add_interaction_terms(interaction_df, predictors, max_interaction_level)\n",
    "\n",
    "def loocv_regression(df, interaction_df, formula, outcome_var):\n",
    "    full_df = interaction_df.copy()\n",
    "    full_df[outcome_var] = df[outcome_var]\n",
    "    model = sm.formula.ols(formula, data=full_df).fit()\n",
    "    y_actual, y_predicted = [], []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_test = interaction_df.iloc[[i]]\n",
    "        x_train = interaction_df.drop(i)\n",
    "        y_train = df[outcome_var].drop(i)\n",
    "        \n",
    "        model_train_df = x_train.join(y_train)\n",
    "        model_train = sm.formula.ols(formula, data=model_train_df).fit()\n",
    "        y_actual.append(df[outcome_var].iloc[i])\n",
    "        y_predicted.append(model_train.predict(x_test).values[0])\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    corr, _ = pearsonr(y_actual, y_predicted)\n",
    "    return rmse, corr\n",
    "\n",
    "def all_subsets_regression(df, outcome_var):\n",
    "    predictor_columns = [col for col in df.columns if col != outcome_var]\n",
    "    interaction_df = generate_interaction_terms(df, predictor_columns, max_interaction_level=3)\n",
    "\n",
    "    results = []\n",
    "    for n_predictors in tqdm(range(1, len(predictor_columns) + 1)):\n",
    "        for predictor_combination in itertools.combinations(predictor_columns, n_predictors):\n",
    "            formula = f\"{outcome_var} ~ {' + '.join(predictor_combination)}\"\n",
    "            loocv_rmse, loocv_corr = loocv_regression(df, interaction_df, formula, outcome_var)\n",
    "\n",
    "            results.append({\n",
    "                'formula': formula,\n",
    "                'rmse': loocv_rmse,\n",
    "                'correlation': loocv_corr\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def all_subsets_regression_no_loocv(df, outcome_var):\n",
    "    predictor_columns = [col for col in df.columns if col != outcome_var]\n",
    "\n",
    "    results = []\n",
    "    for n_predictors in tqdm(range(1, len(predictor_columns) + 1)):\n",
    "        for predictor_combination in itertools.combinations(predictor_columns, n_predictors):\n",
    "            formula = f\"{outcome_var} ~ {' + '.join(predictor_combination)}\"\n",
    "            model = sm.formula.ols(formula, data=df).fit()\n",
    "\n",
    "            y_actual = df[outcome_var].values\n",
    "            y_predicted = model.predict(df[predictor_columns])\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "            corr, _ = pearsonr(y_actual, y_predicted)\n",
    "\n",
    "            results.append({\n",
    "                'formula': formula,\n",
    "                'rmse': rmse,\n",
    "                'correlation': corr\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def all_subsets_regression(df, outcome_var, limit_to_two_way_interactions=False, max_predictors=None):\n",
    "    predictor_columns = [col for col in df.columns if col != outcome_var]\n",
    "\n",
    "    results = []\n",
    "    for n_predictors in tqdm(range(1, min(max_predictors + 1, len(predictor_columns) + 1))):\n",
    "        for predictor_combination in itertools.combinations(predictor_columns, n_predictors):\n",
    "            # Check whether to limit interactions to two-way\n",
    "            if limit_to_two_way_interactions and n_predictors > 1:\n",
    "                # Generate combinations for two-way interactions\n",
    "                interaction_combinations = itertools.combinations(predictor_combination, 2)\n",
    "                interaction_terms = [' : '.join(interaction) for interaction in interaction_combinations]\n",
    "                # Include both individual predictors and interaction terms\n",
    "                formula = f\"{outcome_var} ~ {' + '.join(predictor_combination)} + {' + '.join(interaction_terms)}\"\n",
    "            else:\n",
    "                # Original formula generation with all possible interactions\n",
    "                formula = f\"{outcome_var} ~ {' * '.join(predictor_combination)}\"\n",
    "            \n",
    "            print(formula)\n",
    "            loocv_rmse, loocv_corr = loocv_regression(df, formula, outcome_var)\n",
    "\n",
    "            results.append({\n",
    "                'formula': formula,\n",
    "                'rmse': loocv_rmse,\n",
    "                'correlation': loocv_corr\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def loocv_regression(df, formula, outcome_var):\n",
    "    model = sm.formula.ols(formula, data=df).fit()\n",
    "    y_actual, y_predicted = [], []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        index_label = df.index[i]\n",
    "        x_test = df.iloc[[i]].drop(outcome_var, axis=1)\n",
    "        x_train = df.drop(index_label).drop(outcome_var, axis=1)\n",
    "        y_train = df[outcome_var].drop(index_label)\n",
    "\n",
    "        model_train_df = x_train.join(y_train)\n",
    "        model_train = sm.formula.ols(formula, data=model_train_df).fit()\n",
    "        y_actual.append(df[outcome_var].iloc[i])\n",
    "        y_predicted.append(model_train.predict(x_test).values[0])\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    corr, _ = pearsonr(y_actual, y_predicted)\n",
    "    return rmse, corr\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# outcome_var = 'your_outcome_variable'\n",
    "# results_df = all_subsets_regression(df, outcome_var)\n",
    "# print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_var = 'outcome'\n",
    "limit_to_two_way_interactions = False\n",
    "regressor_limit = 3\n",
    "#----------------------------------------------------------------DO NOT TOUCH----------------------------------------------------------------\n",
    "df = data_df.copy()\n",
    "results_df = all_subsets_regression(df, outcome_var, limit_to_two_way_interactions=limit_to_two_way_interactions, max_predictors=regressor_limit)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 20 formulas by Correlation of y-hat to y')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 20 formulas by RMSE of y-hat to y')\n",
    "# Sort DataFrame by 'rmse' in ascending order\n",
    "rmse_df = results_df.copy().sort_values(by='rmse', ascending=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "rmse_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing top 5 formulas\n",
    "\n",
    "top_five_formulas = results_df.head(5)['formula']\n",
    "print(\"Top 5 Formulas:\")\n",
    "print(top_five_formulas)\n",
    "\n",
    "\n",
    "print('---Lowest RMSE Information---')\n",
    "index_of_min_rmse = results_df['rmse'].idxmin()\n",
    "print(results_df.loc[index_of_min_rmse, 'formula'])\n",
    "print('RMSE: ',results_df.loc[index_of_min_rmse, 'rmse'])\n",
    "print('Pearson R: ', results_df.loc[index_of_min_rmse, 'correlation'])\n",
    "print('Formula index: ', index_of_min_rmse)\n",
    "\n",
    "print('\\n---Highest Correlation Information---')\n",
    "index_of_max_corr = results_df['correlation'].idxmax()\n",
    "print(results_df.loc[index_of_max_corr, 'formula'])\n",
    "print('RMSE: ',results_df.loc[index_of_max_corr, 'rmse'])\n",
    "print('Pearson R: ', results_df.loc[index_of_max_corr, 'correlation'])\n",
    "print('Formula index: ', index_of_max_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "formula_to_test = 6\n",
    "#----------------------------------------------------------------\n",
    "results = smf.ols(results_df.loc[formula_to_test, 'formula'], data=df).fit()\n",
    "# results = smf.ols('percent_change_adascog11 ~ Subiculum*Age*total_hippocampal_csf*frontal', data=data_df).fit()\n",
    "\n",
    "\n",
    "print(results.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import model_diagnostics\n",
    "model_diagnostics(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Optimal Interaction Subset of Optimal Model by Backward Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import itertools\n",
    "\n",
    "\n",
    "def backward_subsets_loocv_regression(df, formula):\n",
    "    # Extract outcome_var from the formula\n",
    "    outcome_var = formula.split(' ~ ')[0].strip()\n",
    "    \n",
    "    y_actual, y_predicted = [], []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        index_label = df.index[i]\n",
    "        x_test = df.iloc[[i]].drop(outcome_var, axis=1)\n",
    "        x_train = df.drop(index_label).drop(outcome_var, axis=1)\n",
    "        y_train = df[outcome_var].drop(index_label)\n",
    "\n",
    "        model_train_df = x_train.join(y_train)\n",
    "        model_train = smf.ols(formula, data=model_train_df).fit()\n",
    "        y_actual.append(df[outcome_var].iloc[i])\n",
    "        y_predicted.append(model_train.predict(x_test).values[0])\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    corr, _ = pearsonr(y_actual, y_predicted)\n",
    "    return rmse, corr\n",
    "\n",
    "\n",
    "def serial_interaction_removal_loocv(df, formula):\n",
    "    # Split the formula into the left and right side\n",
    "    left, right = formula.split(\"~\")\n",
    "    \n",
    "    # Get all the terms from the right side of the formula\n",
    "    terms = right.strip().split(\"*\")\n",
    "    terms = [term.strip() for term in terms]\n",
    "    \n",
    "    # Generate all combinations of + and *\n",
    "    operators_combinations = itertools.product([\"+\", \"*\"], repeat=len(terms)-1)\n",
    "    \n",
    "    # Generate the formulas\n",
    "    formulas = []\n",
    "    for operators in operators_combinations:\n",
    "        combined_formula = left + \" ~ \" + terms[0]\n",
    "        for op, term in zip(operators, terms[1:]):\n",
    "            combined_formula += \" \" + op + \" \" + term\n",
    "        formulas.append(combined_formula.strip())\n",
    "\n",
    "    # Run LOOCV on each formula\n",
    "    loocv_results = []\n",
    "    for sub_formula in formulas:\n",
    "        loocv_rmse, loocv_corr = backward_subsets_loocv_regression(df, sub_formula)\n",
    "        loocv_results.append({\n",
    "            'formula': sub_formula,\n",
    "            'rmse': loocv_rmse,\n",
    "            'correlation': loocv_corr\n",
    "        })\n",
    "    \n",
    "    # Convert results to DataFrame, sort by correlation and reset index\n",
    "    loocv_results_df = pd.DataFrame(loocv_results)\n",
    "    loocv_results_df.sort_values(by='correlation', ascending=False, inplace=True)\n",
    "    loocv_results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return loocv_results_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `df` is your DataFrame, and `formula` is a string like \"y ~ a * b * c\"\n",
    "# best_formula = \"y ~ a * b * c\"\n",
    "# serial_loocv_df = serial_interaction_removal_loocv(df, best_formula)\n",
    "# print(serial_loocv_df)\n",
    "\n",
    "best_formula = results_df.loc[formula_to_test, 'formula']\n",
    "optimal_interaction_loocv_df = serial_interaction_removal_loocv(df, best_formula)\n",
    "optimal_interaction_loocv_df.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Coefficient Analysis of Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "def structural_coefficients(model, data):\n",
    "    # Extracting the predictor names\n",
    "    predictor_names = model.model.exog_names\n",
    "    if 'Intercept' in predictor_names:\n",
    "        predictor_names.remove('Intercept')\n",
    "\n",
    "    # Calculating the predicted values\n",
    "    y_predicted = model.predict(data)\n",
    "\n",
    "    # Creating a temporary dataframe to store interaction terms\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    for pred in predictor_names:\n",
    "        # Check if the predictor is an interaction term\n",
    "        if ':' in pred:\n",
    "            # Split the interaction term into its components\n",
    "            components = pred.split(':')\n",
    "            # Multiply the components and store the result in the temporary dataframe\n",
    "            temp_product = data[components[0]]\n",
    "            for component in components[1:]:\n",
    "                temp_product *= data[component]\n",
    "            temp_df[pred] = temp_product\n",
    "        else:\n",
    "            temp_df[pred] = data[pred]\n",
    "\n",
    "    # Calculating the structural coefficients\n",
    "    structural_coefs = {}\n",
    "    for pred in predictor_names:\n",
    "        coef, _ = pearsonr(temp_df[pred], y_predicted)\n",
    "        structural_coefs[pred] = np.square(coef)\n",
    "\n",
    "    # Creating a dataframe to store the results\n",
    "    structural_coefs_df = pd.DataFrame(structural_coefs.items(), columns=['predictor', 'structural_coefficient'])\n",
    "\n",
    "    # Sorting the dataframe by the structural coefficients in descending order\n",
    "    structural_coefs_df.sort_values(by='structural_coefficient', ascending=False, inplace=True)\n",
    "    structural_coefs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return structural_coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "def structural_coefficients(model: statsmodels.regression.linear_model.RegressionResultsWrapper, \n",
    "                            data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function calculates the structural coefficients of a linear regression model, along with their \n",
    "    associated model coefficients (beta weights). It also computes a 'suppressor index' which indicates \n",
    "    the likelihood of a variable being a suppressor variable (high beta weight, near-zero structural coefficient).\n",
    "    A suppressor index over 10 is a good heuristic for identifying a suppressor variable\n",
    "    \n",
    "    Parameters:\n",
    "    model (statsmodels.regression.linear_model.RegressionResultsWrapper): The fitted linear regression model.\n",
    "    data (pd.DataFrame): The dataset used in the model.\n",
    "    \n",
    "    Returns:\n",
    "    structural_coefs_df (pd.DataFrame): A dataframe containing the predictors, their structural coefficients,\n",
    "                                        model coefficients (beta weights), and suppressor index.\n",
    "                                        \n",
    "                                        If the sum of the structure coefficients is higher than 1, they are correlated (multicollinear)\n",
    "    \"\"\"\n",
    "    # Calculating the predicted values\n",
    "    y_predicted = model.predict(data)\n",
    "\n",
    "    # Creating a temporary dataframe to store interaction terms\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    # Calculating the structural coefficients\n",
    "    structural_coefs = {}\n",
    "    for pred in model.params.index:\n",
    "        if pred == 'Intercept':\n",
    "            continue\n",
    "\n",
    "        # Check if the predictor is an interaction term\n",
    "        if ':' in pred:\n",
    "            # Split the interaction term into its components\n",
    "            components = pred.split(':')\n",
    "            # Multiply the components and store the result in the temporary dataframe\n",
    "            temp_product = data[components[0]]\n",
    "            for component in components[1:]:\n",
    "                temp_product *= data[component]\n",
    "            temp_df[pred] = temp_product\n",
    "        else:\n",
    "            temp_df[pred] = data[pred]\n",
    "        \n",
    "        coef, _ = pearsonr(temp_df[pred], y_predicted)\n",
    "        structural_coefs[pred] = np.square(coef)\n",
    "\n",
    "    # Calculating the model coefficients\n",
    "    model_coefs = model.params.drop('Intercept')\n",
    "\n",
    "    # Creating a dataframe to store the results\n",
    "    structural_coefs_df = pd.DataFrame(list(zip(structural_coefs.keys(), structural_coefs.values(), model_coefs.values)), \n",
    "                                       columns=['predictor', 'structural_coefficient', 'model_coefficient'])\n",
    "\n",
    "    # Adding suppressor index column\n",
    "    structural_coefs_df['suppressor_index'] = structural_coefs_df['model_coefficient'].abs() / structural_coefs_df['structural_coefficient']\n",
    "\n",
    "    # Sorting the dataframe by the structural coefficients in descending order\n",
    "    structural_coefs_df.sort_values(by='structural_coefficient', ascending=False, inplace=True)\n",
    "    structural_coefs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return structural_coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_structural_coefs_df = structural_coefficients(results, df.copy())\n",
    "squared_structural_coefs_df.to_csv(os.path.join(out_dir, 'structural_coefficient_analysis.csv'))\n",
    "print('saved to: ', os.path.join(out_dir, 'structural_coefficient_analysis.csv'))\n",
    "display(squared_structural_coefs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the sum of the structure coefficients is higher than 1, they are correlated\n",
    "print(squared_structural_coefs_df.loc[:, 'structural_coefficient'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary2())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check optimal model without LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "no_loocv_results_df = all_subsets_regression_no_loocv(df, outcome_var)\n",
    "no_loocv_results_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing top 5 formulas\n",
    "no_loocv_top_five_formulas = no_loocv_results_df.head(10)['formula']\n",
    "print(\"Top 5 Formulas:\")\n",
    "print(no_loocv_top_five_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "formula_to_test = 0\n",
    "#----------------------------------------------------------------\n",
    "results = smf.ols(no_loocv_results_df.loc[formula_to_test, 'formula'], data=df).fit()\n",
    "print(results.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Out Consistently High-Performing Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class ModelPerformanceClusterRefined:\n",
    "    def __init__(self, data, rmse_threshold, pearsonr_threshold, normative_thresholding=False):\n",
    "        self.data = data\n",
    "        self.metric_df = None\n",
    "        self.rmse_threshold = rmse_threshold\n",
    "        self.pearsonr_threshold = pearsonr_threshold\n",
    "        self.normative_thresholding=normative_thresholding\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Remove rows with RMSE values more than 2 standard deviations from the mean.\n",
    "        \"\"\"\n",
    "        if self.normative_thresholding:\n",
    "            rmse_mean = self.data['rmse'].mean()\n",
    "            rmse_std = self.data['rmse'].std()\n",
    "            corr_mean = self.data['correlation'].mean()\n",
    "            corr_std = self.data['correlation'].std()\n",
    "            self.data = self.data[~(self.data['rmse'] > rmse_mean + 2*rmse_std)]\n",
    "            self.data = self.data[~(self.data['correlation'] < corr_mean - 2*corr_std)]\n",
    "        else:\n",
    "            self.data = self.data[~(self.data['rmse'] > self.rmse_threshold)]\n",
    "            self.data = self.data[~(self.data['correlation'] < self.pearsonr_threshold)]\n",
    "\n",
    "    def extract_predictors_metrics(self):\n",
    "        \"\"\"\n",
    "        Extract predictors from regression formula strings and relate them to RMSE and Pearson R.\n",
    "        \"\"\"\n",
    "        # Placeholder for storing extracted data\n",
    "        extracted_data = []\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            # Splitting to get the predictors\n",
    "            terms = row['formula'].split(\"~\")[1].strip().split(\" + \")\n",
    "            \n",
    "            predictors = []\n",
    "            for term in terms:\n",
    "                # Check for interaction terms\n",
    "                if '*' in term:\n",
    "                    predictors.extend(term.split('*'))\n",
    "                else:\n",
    "                    predictors.append(term)\n",
    "\n",
    "            for predictor in predictors:\n",
    "                extracted_data.append({\n",
    "                    'Predictor': predictor.strip(),\n",
    "                    'RMSE': row['rmse'],\n",
    "                    'PearsonR': row['correlation']\n",
    "                })\n",
    "\n",
    "        self.metric_df = pd.DataFrame(extracted_data)\n",
    "\n",
    "    def apply_hierarchical_clustering(self, n_clusters=3):\n",
    "        \"\"\"\n",
    "        Perform Agglomerative Hierarchical Clustering on RMSE and Pearson R.\n",
    "        \"\"\"\n",
    "        # Extract the metric values for clustering\n",
    "        X = self.metric_df[[\"RMSE\", \"PearsonR\"]].values\n",
    "\n",
    "        # Apply Hierarchical Clustering\n",
    "        cluster = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n",
    "        self.metric_df['Cluster_Label'] = cluster.fit_predict(X)\n",
    "\n",
    "        return self.metric_df['Cluster_Label']\n",
    "\n",
    "    def visualize_scatter(self):\n",
    "        \"\"\"\n",
    "        Visualize a scatter plot of Pearson R vs. RMSE, colored by the cluster label.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.scatterplot(data=self.metric_df, x='PearsonR', y='RMSE', hue='Cluster_Label', palette='tab10', s=100, alpha=0.7)\n",
    "        plt.title(\"Scatter Plot of Pearson R vs. RMSE\")\n",
    "        plt.legend(title='Cluster Label', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def predictors_in_cluster(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Identify and return the unique predictors associated with a specific cluster.\n",
    "        \"\"\"\n",
    "        # Filter the metric_df by the given cluster label\n",
    "        cluster_data = self.metric_df[self.metric_df['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        # Return unique predictors\n",
    "        return cluster_data['Predictor'].unique()\n",
    "\n",
    "\n",
    "# The class has been updated with the preprocess_data method. You can use this refined class in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = 6\n",
    "RMSE_threshold = 1.2\n",
    "R_threshold = 0.2\n",
    "normative_thresholding=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the use of the Hierarchical clustering with the sample dataframe\n",
    "cluster_obj_hierarchical = ModelPerformanceClusterRefined(results_df, RMSE_threshold, R_threshold,normative_thresholding)\n",
    "cluster_obj_hierarchical.preprocess_data()\n",
    "cluster_obj_hierarchical.extract_predictors_metrics()\n",
    "cluster_obj_hierarchical.apply_hierarchical_clustering(n_clusters=number_clusters)\n",
    "cluster_obj_hierarchical.visualize_scatter()\n",
    "cluster_obj_hierarchical.predictors_in_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_obj_hierarchical.predictors_in_cluster(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class ModelPerformanceClusterFinal(ModelPerformanceClusterRefined):\n",
    "    \n",
    "    def compute_r_squared(self, data_df):\n",
    "        \"\"\"\n",
    "        Compute R-squared for each regression formula on data_df.\n",
    "        \"\"\"\n",
    "        adj_r_squared_values = []\n",
    "        r_squared_values = []\n",
    "        bic_values = []\n",
    "        aic_values = []\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            formula = row['formula']\n",
    "            model = smf.ols(formula=formula, data=data_df).fit()\n",
    "            r_squared_values.append(model.rsquared)\n",
    "            adj_r_squared_values.append(model.rsquared_adj)\n",
    "            bic_values.append(model.aic)\n",
    "            aic_values.append(model.bic)\n",
    "\n",
    "        self.data['r_squared'] = r_squared_values\n",
    "        self.data['adj_r_squared'] = adj_r_squared_values\n",
    "        self.data['bic'] = bic_values\n",
    "        self.data['aic'] = aic_values\n",
    "\n",
    "    def apply_agglomerative_clustering(self, n_clusters=5):\n",
    "        \"\"\"\n",
    "        Perform Agglomerative Clustering considering R-squared, LOOCV Pearson R, and LOOCV RMSE.\n",
    "        \"\"\"\n",
    "        # Extract the metric values for clustering\n",
    "        X = self.data[[\"adj_r_squared\", \"correlation\", \"rmse\"]].values\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        # Apply Agglomerative Clustering\n",
    "        cluster = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward').fit(X)\n",
    "        self.data['Cluster_Label'] = cluster.labels_\n",
    "        \n",
    "    def visualize_3D_scatter(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize a 3D scatter plot of R-squared vs. Pearson R vs. RMSE, colored by Cluster label and shape by number of predictors.\n",
    "        \"\"\"\n",
    "        \n",
    "        def count_predictors(formula):\n",
    "            right_side = formula.split(\"~\")[1].strip()\n",
    "\n",
    "            # Check for both '*' and '+' in the formula\n",
    "            if '*' in right_side and '+' in right_side:\n",
    "                terms = []\n",
    "                for term in right_side.split('+'):\n",
    "                    terms.extend(term.split('*'))\n",
    "                return len(terms)\n",
    "\n",
    "            # Check for '*' in the formula\n",
    "            elif '*' in right_side:\n",
    "                return len(right_side.split('*'))\n",
    "\n",
    "            # Check for '+' in the formula\n",
    "            elif '+' in right_side:\n",
    "                return len(right_side.split('+'))\n",
    "\n",
    "            # If neither exists, it means there's only one predictor\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "        # Calculate the number of predictors for each formula\n",
    "        self.data['num_predictors'] = self.data['formula'].apply(count_predictors)\n",
    "        \n",
    "        # Map the number of predictors to a specific symbol\n",
    "        self.data['symbol'] = self.data['num_predictors'].map({\n",
    "            1: 'circle',\n",
    "            2: 'square',\n",
    "            3: 'diamond',\n",
    "            4: 'cross'\n",
    "        })\n",
    "        # Assign a default symbol for formulas with more than 4 predictors\n",
    "        self.data['symbol'].fillna('cross', inplace=True)\n",
    "        \n",
    "        fig = px.scatter_3d(self.data, x='r_squared', y='correlation', z='rmse',\n",
    "                            color='Cluster_Label', opacity=0.7, hover_name='formula',\n",
    "                            hover_data=['adj_r_squared', 'bic', 'aic'], symbol='symbol',\n",
    "                            labels={'adj_r_squared': 'Adj. R-Squared', 'r_squared': 'R-Squared', 'correlation': 'LOOCV Pearson R', 'rmse': 'LOOCV RMSE'},\n",
    "                            color_continuous_scale=px.colors.sequential.Viridis)\n",
    "        fig.update_layout(title=\"3D Scatter Plot of R-Squared, Pearson R, and RMSE\", \n",
    "                        autosize=True, width=1200, height=800)\n",
    "        # fig.show()\n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            save_path = os.path.join(save_path, '3d_scatterplot.html')\n",
    "            fig.write_html(save_path)\n",
    "            fig.show()\n",
    "        else:\n",
    "            fig.show()\n",
    "        \n",
    "    def get_essential_data(self):\n",
    "        \"\"\"\n",
    "        Return a dataframe with formulae, R^2, RMSE, PearsonR, and cluster label.\n",
    "        \"\"\"\n",
    "        return self.data[['r_squared', 'rmse', 'correlation', 'formula', 'Cluster_Label']]\n",
    "\n",
    "# This final class integrates R-squared computation, preprocessing based on your refined approach, Agglomerative Clustering, and 3D scatter plot visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine Number of 3D Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the object\n",
    "num_clusters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------DO NOT TOUCH--\n",
    "cluster_obj_final = ModelPerformanceClusterFinal(results_df, rmse_threshold=RMSE_threshold, pearsonr_threshold=R_threshold, normative_thresholding=True)\n",
    "# Preprocess the data\n",
    "cluster_obj_final.preprocess_data()\n",
    "\n",
    "# Compute R-squared for all regression formulas in results_df using data_df\n",
    "cluster_obj_final.compute_r_squared(data_df)\n",
    "\n",
    "# Apply Agglomerative Clustering\n",
    "cluster_obj_final.apply_agglomerative_clustering(n_clusters=num_clusters)\n",
    "\n",
    "# Visualize the 3D scatter plot\n",
    "cluster_obj_final.visualize_3D_scatter(save_path=out_dir)\n",
    "cluster_df = cluster_obj_final.get_essential_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the Optimal Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import re \n",
    "\n",
    "class ModelPerformanceAnalysis:\n",
    "\n",
    "    def __init__(self, cluster_df):\n",
    "        self.data = cluster_df\n",
    "\n",
    "    def filter_by_cluster(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Filter the data by the specified cluster label.\n",
    "        \"\"\"\n",
    "        return self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "\n",
    "    def apply_pca(self):\n",
    "        \"\"\"\n",
    "        Apply PCA on performance metrics across all clusters after standardizing the data.\n",
    "        \"\"\"\n",
    "        pca_data = self.data[['r_squared', 'correlation']].copy()\n",
    "        pca_data['Inv_RMSE'] = 1 / self.data['rmse']\n",
    "        # pca_data['Inv_aic'] = 1 / self.data['aic']\n",
    "        # pca_data['Inv_bic'] = 1 / self.data['bic']\n",
    "\n",
    "        # Standardizing the data\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(pca_data)\n",
    "        \n",
    "        pca = PCA(n_components=1)\n",
    "        pca_result = pca.fit_transform(standardized_data)\n",
    "        \n",
    "        self.data['PCA_1'] = pca_result\n",
    "\n",
    "    def get_pca_dataframe(self):\n",
    "        \"\"\"\n",
    "        Return the dataframe with PCA values.\n",
    "        \"\"\"\n",
    "        return self.data\n",
    "\n",
    "    def visualize_pca_performance(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Visualize the first principal component against each predictor in the specified cluster.\n",
    "        \"\"\"\n",
    "        pca_cluster_data = self.filter_by_cluster(cluster_label)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='PCA_1', y='formula', data=pca_cluster_data.sort_values(by='PCA_1', ascending=False))\n",
    "        \n",
    "        plt.title(f\"Principal Component Analysis for Cluster {cluster_label}\")\n",
    "        plt.xlabel(\"First Principal Component\")\n",
    "        plt.ylabel(\"Formula\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def extract_predictors(self, data):\n",
    "        \"\"\"\n",
    "        Extract predictors from the 'formula' column of the given data.\n",
    "        \"\"\"\n",
    "        predictors_list = []\n",
    "        for formula in data['formula']:\n",
    "            terms = re.split(r' ~ | \\+ |\\*', formula)\n",
    "            predictors_list.extend(terms[1:])\n",
    "        # Strip white spaces and ensure uniqueness\n",
    "        return [predictor.strip() for predictor in predictors_list]\n",
    "\n",
    "\n",
    "    def visualize_predictor_incidence(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Visualize the incidence of each predictor within the specified cluster.\n",
    "        \"\"\"\n",
    "        cluster_data = self.filter_by_cluster(cluster_label)\n",
    "        \n",
    "        # Extract predictors from regression formula strings\n",
    "        predictors_list = self.extract_predictors(cluster_data)\n",
    "        predictors_series = pd.Series(predictors_list)\n",
    "        \n",
    "        # Count the incidence of each predictor\n",
    "        predictor_counts = predictors_series.value_counts()\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=predictor_counts.values, y=predictor_counts.index, color='skyblue')\n",
    "        \n",
    "        plt.title(f\"Predictor Incidence in Cluster {cluster_label}\")\n",
    "        plt.xlabel(\"Count\")\n",
    "        plt.ylabel(\"Predictor\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_predictor_average_pca(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Compute the average PCA score and standard error for each predictor within the specified cluster.\n",
    "        \"\"\"\n",
    "        cluster_data = self.filter_by_cluster(cluster_label)\n",
    "        \n",
    "        # A set to store unique predictors\n",
    "        unique_predictors = set(self.extract_predictors(cluster_data))\n",
    "        \n",
    "        # Dictionaries to store average PCA scores and standard errors for each predictor\n",
    "        predictor_pca_avg = defaultdict(float)\n",
    "        predictor_pca_std_err = defaultdict(float)\n",
    "        \n",
    "        # Go through each predictor and get rows where it appears, then compute average PCA and standard error\n",
    "        for predictor in unique_predictors:\n",
    "            predictor_rows = cluster_data[cluster_data['formula'].str.contains(f\"\\\\b{predictor}\\\\b\", regex=True)]\n",
    "            unique_pca_scores = predictor_rows['PCA_1'].nunique()\n",
    "            print(f\"Number of unique PCA scores for predictor '{predictor}': {unique_pca_scores}\")\n",
    "            avg_pca = predictor_rows['PCA_1'].mean()\n",
    "            std_err = predictor_rows['PCA_1'].std() / (len(predictor_rows) ** 0.5)\n",
    "            predictor_pca_avg[predictor] = avg_pca\n",
    "            predictor_pca_std_err[predictor] = std_err\n",
    "        for predictor, std_err in predictor_pca_std_err.items():\n",
    "            print(f\"Standard error for predictor '{predictor}': {std_err}\")\n",
    "\n",
    "            \n",
    "        return predictor_pca_avg, predictor_pca_std_err\n",
    "\n",
    "    def visualize_predictor_average_pca_performance(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Visualize the average PCA score with standard errors for each predictor within the specified cluster using Seaborn.\n",
    "        \"\"\"\n",
    "        predictor_pca_avg, predictor_pca_std_err = self.get_predictor_average_pca(cluster_label)\n",
    "        predictor_data = pd.DataFrame({\n",
    "            'Predictor': list(predictor_pca_avg.keys()),\n",
    "            'Avg_PCA': list(predictor_pca_avg.values()),\n",
    "            'Std_Err': list(predictor_pca_std_err.values())\n",
    "        }).sort_values(by='Avg_PCA', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        sns.barplot(x='Avg_PCA', y='Predictor', data=predictor_data, palette=\"viridis\")\n",
    "\n",
    "        for index, value in enumerate(predictor_data['Avg_PCA']):\n",
    "            plt.errorbar(value, index, xerr=predictor_data['Std_Err'].iloc[index], color='black', capsize=5, fmt='none')\n",
    "        \n",
    "        plt.title(f\"Average PCA Scores for Predictors in Cluster {cluster_label}\")\n",
    "        plt.xlabel(\"Average PCA Score\")\n",
    "        plt.ylabel(\"Predictor\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What cluster would you like to evaluate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_of_interest=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_obj = ModelPerformanceAnalysis(cluster_obj_final.get_essential_data())\n",
    "analysis_obj.apply_pca()\n",
    "analysis_obj.visualize_pca_performance(cluster_of_interest)\n",
    "analysis_obj.get_pca_dataframe().to_csv(os.path.join(out_dir, 'raw_data/cluster_df_pc1.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_obj.visualize_predictor_average_pca_performance(cluster_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_obj.visualize_predictor_incidence(cluster_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Relationships within Clusters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relate Each Predictor (Dataframe column) to a Class of Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you Want to Map Specific Predictors to a 'Class of Information' set class_equal_predictor to false. \n",
    "\n",
    "ie) age, gender, education are all demographic factors and could be labeled 'demographic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_predictors(data):\n",
    "        \"\"\"\n",
    "        Extract predictors from the 'formula' column of the given data.\n",
    "        \"\"\"\n",
    "        predictors_list = []\n",
    "        for formula in data['formula']:\n",
    "            terms = re.split(r' ~ | \\+ |\\*', formula)\n",
    "            predictors_list.extend(terms[1:])\n",
    "        # Strip white spaces and ensure uniqueness\n",
    "        return pd.DataFrame({'Predictors': [predictor.strip() for predictor in predictors_list]})\n",
    "\n",
    "def prep_class_dict(data_df, class_equal_predictor=False):\n",
    "    if class_equal_predictor:\n",
    "        print('Copy-paste this example dictionary to map each predictor to a given class of information')\n",
    "        print('{')\n",
    "        for predictor in data_df['Predictors'].unique():\n",
    "            print(f'\"{predictor}\": \"{predictor}\",')\n",
    "        print('}')\n",
    "    else:\n",
    "        print('Copy-paste this example dictionary to map each predictor to a given class of information')\n",
    "        print('{')\n",
    "        for predictor in data_df['Predictors'].unique():\n",
    "            print(f'\"{predictor}\": \" \",')\n",
    "        print('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_equal_predictor = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_class_dict(extract_predictors(data_df), class_equal_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_to_class_dict = {\n",
    "    # Demographic\n",
    "    'Age': 'Demographic',\n",
    "    'subject_id': 'Demographic',\n",
    "    'Baseline': 'Demographic',# This seems like an identifier, but I'm mapping it to Demographic for now.\n",
    "\n",
    "    # Subiculum Stim\n",
    "    'Subiculum_Connectivity': 'Subiculum Stim',\n",
    "\n",
    "    # Network Stim\n",
    "    'Visual_Connectivity': 'Network Stim',\n",
    "    'Somatomotor_Connectivity': 'Network Stim',\n",
    "    'Dorsal_Attention_Connectivity': 'Network Stim',\n",
    "    'Ventral_Attention_Connectivity': 'Network Stim',\n",
    "    'Limbic_Connectivity': 'Network Stim',\n",
    "    'Frontoparietal_Connectivity': 'Network Stim',\n",
    "    'Default_Connectivity': 'Network Stim',\n",
    "\n",
    "    # Lobe Atrophy\n",
    "    'Frontal_Atrophy': 'Lobe Atrophy',\n",
    "    'Insula_Atrophy': 'Lobe Atrophy',\n",
    "    'Temporal_Atrophy': 'Lobe Atrophy',\n",
    "    'Occipital_Atrophy': 'Lobe Atrophy',\n",
    "    'Parietal_Atrophy': 'Lobe Atrophy',\n",
    "\n",
    "    # Network Atrophy\n",
    "    'Default_Atrophy': 'Network Atrophy',\n",
    "    'Visual_Atrophy': 'Network Atrophy',\n",
    "    'Limbic_Atrophy': 'Network Atrophy',\n",
    "    'Somatomotor_Atrophy': 'Network Atrophy',\n",
    "    'Dorsal_Attention_Atrophy': 'Network Atrophy',\n",
    "    'Ventral_Attention_Atrophy': 'Network Atrophy',\n",
    "    'Frontoparietal_Atrophy': 'Network Atrophy',\n",
    "\n",
    "    # Subiculum Atrophy\n",
    "    'Subiculum_Atrophy': 'Subiculum Atrophy',\n",
    "\n",
    "    # Unclassified (will need your inputs on these):\n",
    "    'Abs_Stim_Composite_Atophy_SpCorrel': 'Stim Atrophy Match',\n",
    "    'Raw_Stim_Composite_Atrophy_SpCorrel': 'Stim Atrophy Match',\n",
    "    'Cerebellar_Atrophy': 'Lobe Atrophy',\n",
    "    'Total_Atrophy_Voxels': 'Total Atrophy',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "# size = count * 15\n",
    "\n",
    "class InteractionVisualizer:\n",
    "    \n",
    "    def __init__(self, essential_data_df):\n",
    "        \"\"\"\n",
    "        Initializes the InteractionVisualizer class.\n",
    "        \n",
    "        :param essential_data_df: Dataframe containing formula, r_squared, rmse, correlation, and Cluster_Label columns.\n",
    "        \"\"\"\n",
    "        self.data = essential_data_df\n",
    "        \n",
    "    def compute_overall_interaction_counts(self, predictor_to_class_dict):\n",
    "        \"\"\"\n",
    "        Computes the overall interaction counts between every pair of classes across the entire dataset.\n",
    "        \"\"\"\n",
    "        interactions = defaultdict(int)\n",
    "        for formula in self.data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            for i in range(len(classes)):\n",
    "                for j in range(i+1, len(classes)):\n",
    "                    interactions[tuple(sorted([classes[i], classes[j]]))] += 1\n",
    "        return interactions\n",
    "\n",
    "    def extract_interactions_for_cluster(self, cluster_label, predictor_to_class_dict):\n",
    "        \"\"\"\n",
    "        Extract interactions from the formula column of the dataframe for a specific cluster.\n",
    "        Returns a dictionary with classes as keys and their normalized occurrence counts as values.\n",
    "        \"\"\"\n",
    "        overall_counts = self.compute_overall_class_counts(predictor_to_class_dict)\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        cluster_counts = defaultdict(int)\n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            for pred in predictors:\n",
    "                pred_class = predictor_to_class_dict.get(pred, \"Unknown\")\n",
    "                cluster_counts[pred_class] += 1\n",
    "\n",
    "        # Normalize the counts by dividing by the overall counts\n",
    "        normalized_counts = {k: v / overall_counts[k] for k, v in cluster_counts.items()}\n",
    "        return normalized_counts\n",
    "\n",
    "    def extract_normalized_interactions_for_cluster(self, cluster_label, predictor_to_class_dict):\n",
    "        \"\"\"\n",
    "        Extract and normalize interaction counts for a specific cluster.\n",
    "        \"\"\"\n",
    "        overall_interactions = self.compute_overall_interaction_counts(predictor_to_class_dict)\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        cluster_interactions = defaultdict(int)\n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            for i in range(len(classes)):\n",
    "                for j in range(i+1, len(classes)):\n",
    "                    cluster_interactions[tuple(sorted([classes[i], classes[j]]))] += 1\n",
    "\n",
    "        # Normalize the interaction counts\n",
    "        normalized_interactions = {k: v / overall_interactions[k] for k, v in cluster_interactions.items()}\n",
    "        return normalized_interactions\n",
    "\n",
    "    def map_interactions_to_classes(self, interaction_counts, predictor_to_class_dict):\n",
    "        \"\"\"\n",
    "        Maps the interactions to their respective classes.\n",
    "        \n",
    "        :param interaction_counts: A dictionary containing predictors and their occurrence counts.\n",
    "        :param predictor_to_class_dict: A dictionary mapping predictors to their respective classes.\n",
    "        \n",
    "        Returns a dictionary with classes as keys and the summed occurrence counts as values.\n",
    "        \"\"\"\n",
    "        class_counts = defaultdict(int)\n",
    "        for predictor, count in interaction_counts.items():\n",
    "            predictor_class = predictor_to_class_dict.get(predictor, \"Unknown\")\n",
    "            class_counts[predictor_class] += count\n",
    "        return class_counts\n",
    "    \n",
    "    def visualize_class_interactions_for_cluster(self, cluster_label, predictor_to_class_dict, class_scaling_factor=15, interaction_scaling_factor=1):\n",
    "        \"\"\"\n",
    "        Visualizes the interaction between classes for a specific cluster using a graph representation.\n",
    "        \n",
    "        :param cluster_label: The specific cluster label for which interactions are to be visualized.\n",
    "        :param predictor_to_class_dict: A dictionary mapping predictors to their respective classes.\n",
    "        :param class_scaling_factor: A scaling factor to adjust the node size.\n",
    "        \"\"\"\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Dictionary to store class-pair interactions\n",
    "        class_pair_counts = defaultdict(int)\n",
    "        \n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            \n",
    "            # Create pairs of predictors for each interaction in the formula\n",
    "            for i in range(len(predictors)):\n",
    "                for j in range(i+1, len(predictors)):\n",
    "                    class1 = predictor_to_class_dict.get(predictors[i], \"Unknown\")\n",
    "                    class2 = predictor_to_class_dict.get(predictors[j], \"Unknown\")\n",
    "                    \n",
    "                    # Sort class pair to ensure (class1, class2) and (class2, class1) are treated the same\n",
    "                    class_pair = tuple(sorted([class1, class2]))\n",
    "                    class_pair_counts[class_pair] += 1\n",
    "                    \n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Create nodes for each class\n",
    "        all_classes = set(predictor_to_class_dict.values())\n",
    "        for class_name in all_classes:\n",
    "            size = sum([count for pair, count in class_pair_counts.items() if class_name in pair])\n",
    "            size *= class_scaling_factor\n",
    "            G.add_node(class_name, size=size)\n",
    "            \n",
    "        # Create edges between class nodes with weights proportional to their interaction count\n",
    "        for (class1, class2), weight in class_pair_counts.items():\n",
    "            G.add_edge(class1, class2, weight=weight)\n",
    "            \n",
    "        # Plot the graph\n",
    "        # pos = nx.spring_layout(G)\n",
    "        pos = nx.spring_layout(G, k=.95, iterations=100)\n",
    "        sizes = [G.nodes[node]['size'] for node in G]\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=\"skyblue\")\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "        \n",
    "        # Draw edges with weights\n",
    "        for (u, v, d) in G.edges(data=True):\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=d['weight'] * 0.05)  # Adjust the factor (0.05) as needed\n",
    "        \n",
    "        plt.title(f\"Class Interaction Graph for Cluster {cluster_label}\")\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_all_clusters(self, predictor_to_class_dict, class_scaling_factor=15, interaction_scaling_factor=1):\n",
    "        \"\"\"\n",
    "        Visualizes the class interactions for all unique clusters in the data using subplots.\n",
    "        \n",
    "        :param predictor_to_class_dict: A dictionary mapping predictors to their respective classes.\n",
    "        \"\"\"\n",
    "        unique_clusters = self.data['Cluster_Label'].unique()\n",
    "\n",
    "        for cluster_label in unique_clusters:\n",
    "            plt.figure()\n",
    "            self.visualize_class_interactions_for_cluster(cluster_label, predictor_to_class_dict, class_scaling_factor, interaction_scaling_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionVisualizerFinal:\n",
    "\n",
    "    def __init__(self, essential_data_df):\n",
    "        self.data = essential_data_df\n",
    "\n",
    "    def compute_overall_interaction_counts(self, predictor_to_class_dict):\n",
    "        interactions = defaultdict(int)\n",
    "        for formula in self.data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            for i in range(len(classes)):\n",
    "                for j in range(i+1, len(classes)):\n",
    "                    interactions[tuple(sorted([classes[i], classes[j]]))] += 1\n",
    "        return interactions\n",
    "\n",
    "    def compute_overall_class_counts(self, predictor_to_class_dict):\n",
    "        class_counts = defaultdict(int)\n",
    "        for formula in self.data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            for pred in predictors:\n",
    "                pred_class = predictor_to_class_dict.get(pred, \"Unknown\")\n",
    "                class_counts[pred_class] += 1\n",
    "                if pred not in predictor_to_class_dict:\n",
    "                    print(f\"Unknown predictor: {pred}\")\n",
    "        return class_counts\n",
    "    \n",
    "    def extract_normalized_classes_for_cluster(self, cluster_label, predictor_to_class_dict, normalize=True):\n",
    "        overall_class_counts = self.compute_overall_class_counts(predictor_to_class_dict)\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        cluster_class_counts = defaultdict(int)\n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            for pred in predictors:\n",
    "                pred_class = predictor_to_class_dict.get(pred, \"Unknown\")\n",
    "                cluster_class_counts[pred_class] += 1\n",
    "\n",
    "        # Normalize the class counts if required\n",
    "        if normalize:\n",
    "            normalized_class_counts = {k: v / overall_class_counts[k] for k, v in cluster_class_counts.items()}\n",
    "        else:\n",
    "            normalized_class_counts = cluster_class_counts\n",
    "        return normalized_class_counts\n",
    "\n",
    "    def extract_normalized_interactions_for_cluster(self, cluster_label, predictor_to_class_dict, normalize=True):\n",
    "        overall_interactions = self.compute_overall_interaction_counts(predictor_to_class_dict)\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        \n",
    "        cluster_interactions = defaultdict(int)\n",
    "        for formula in cluster_data['formula']:\n",
    "            predictors = formula.split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            for i in range(len(classes)):\n",
    "                for j in range(i+1, len(classes)):\n",
    "                    cluster_interactions[tuple(sorted([classes[i], classes[j]]))] += 1\n",
    "\n",
    "        # Normalize the interaction counts if required\n",
    "        if normalize:\n",
    "            normalized_interactions = {k: v / overall_interactions[k] for k, v in cluster_interactions.items()}\n",
    "        else:\n",
    "            normalized_interactions = cluster_interactions\n",
    "        return normalized_interactions\n",
    "\n",
    "\n",
    "    def compute_average_pca1_for_classes_within_cluster(self, cluster_label, predictor_to_class_dict):\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        class_pca1 = defaultdict(list)\n",
    "\n",
    "        for index, row in cluster_data.iterrows():\n",
    "            predictors = row['formula'].split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            for pred in predictors:\n",
    "                pred_class = predictor_to_class_dict.get(pred, \"Unknown\")\n",
    "                class_pca1[pred_class].append(row['PCA_1'])\n",
    "\n",
    "        avg_pca1 = {k: sum(v) / len(v) for k, v in class_pca1.items()}\n",
    "        return avg_pca1\n",
    "\n",
    "    def compute_average_pca1_for_interactions_within_cluster(self, cluster_label, predictor_to_class_dict):\n",
    "        cluster_data = self.data[self.data['Cluster_Label'] == cluster_label]\n",
    "        interaction_pca1 = defaultdict(list)\n",
    "\n",
    "        for index, row in cluster_data.iterrows():\n",
    "            predictors = row['formula'].split(\"~\")[1].strip().split(\"*\")\n",
    "            predictors = [p.strip() for p in predictors]\n",
    "            classes = [predictor_to_class_dict.get(pred, \"Unknown\") for pred in predictors]\n",
    "            interaction = tuple(sorted(classes))\n",
    "            interaction_pca1[interaction].append(row['PCA_1'])\n",
    "\n",
    "        avg_pca1 = {k: sum(v) / len(v) for k, v in interaction_pca1.items()}\n",
    "        return avg_pca1\n",
    "\n",
    "    def visualize_class_interactions_for_cluster(self, cluster_label, predictor_to_class_dict, class_scaling_factor, edge_scaling_factor, clustering_constant, normalize, color_by_pca1, out_dir):\n",
    "        normalized_class_counts = self.extract_normalized_classes_for_cluster(cluster_label, predictor_to_class_dict, normalize)\n",
    "        normalized_interactions = self.extract_normalized_interactions_for_cluster(cluster_label, predictor_to_class_dict, normalize)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        class_avg_pca1 = None\n",
    "        interaction_avg_pca1 = None\n",
    "        if color_by_pca1:\n",
    "            class_avg_pca1 = self.compute_average_pca1_for_classes_within_cluster(cluster_label, predictor_to_class_dict)\n",
    "            interaction_avg_pca1 = self.compute_average_pca1_for_interactions_within_cluster(cluster_label, predictor_to_class_dict)\n",
    "            overall_min_pca1 = self.data['PCA_1'].min()\n",
    "            overall_max_pca1 = self.data['PCA_1'].max()\n",
    "            color_map = plt.get_cmap(\"viridis\")\n",
    "        \n",
    "        for class_name, normalized_count in normalized_class_counts.items():\n",
    "            size = np.power(normalized_count * class_scaling_factor, 1.8)\n",
    "            if color_by_pca1:\n",
    "                normalized_pca1_value = (class_avg_pca1[class_name] - overall_min_pca1) / (overall_max_pca1 - overall_min_pca1)\n",
    "                color = color_map(normalized_pca1_value)\n",
    "            else:\n",
    "                color = \"skyblue\"\n",
    "            G.add_node(class_name, size=size, color=color)\n",
    "\n",
    "        for (class1, class2), normalized_weight in normalized_interactions.items():\n",
    "            interaction = tuple(sorted([class1, class2]))\n",
    "            if color_by_pca1:\n",
    "                normalized_pca1_value = (interaction_avg_pca1.get(interaction, overall_min_pca1) - overall_min_pca1) / (overall_max_pca1 - overall_min_pca1)\n",
    "                color = color_map(normalized_pca1_value)\n",
    "            else:\n",
    "                color = \"skyblue\"\n",
    "            G.add_edge(class1, class2, weight=np.power(normalized_weight * edge_scaling_factor, 1), color=color)\n",
    "\n",
    "        pos = nx.spring_layout(G, k=clustering_constant, iterations=100)\n",
    "        sizes = [G.nodes[node]['size'] for node in G]\n",
    "        \n",
    "        node_colors = [G.nodes[node]['color'] for node in G]\n",
    "        edge_colors = [d['color'] for u, v, d in G.edges(data=True)]\n",
    "        edge_widths = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=node_colors)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "        nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths)\n",
    "        \n",
    "        if color_by_pca1:\n",
    "            sm = plt.cm.ScalarMappable(cmap=color_map, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "            sm.set_array([])\n",
    "            plt.colorbar(sm, label=\"Normalized Component 1\")\n",
    "        \n",
    "        plt.title(f\"Class Interaction Graph for Cluster {cluster_label}\")\n",
    "    \n",
    "        if out_dir is not None:\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "            plt.savefig(os.path.join(out_dir, f\"Cluster_{cluster_label}.png\"), bbox_inches='tight')\n",
    "            plt.savefig(os.path.join(out_dir, f\"Cluster_{cluster_label}.svg\"), bbox_inches='tight')\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def visualize_all_clusters(self, predictor_to_class_dict, class_scaling_factor=250, edge_scaling_factor=15, clustering_constant=0.05, normalize=True, color_by_pca1=True, out_dir=None):\n",
    "        \"\"\"\n",
    "        Visualizes the class interactions for all unique clusters in the data.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            class_scaling_factor = class_scaling_factor*1000\n",
    "            edge_scaling_factor = edge_scaling_factor*100\n",
    "        unique_clusters = self.data['Cluster_Label'].unique()\n",
    "        for cluster_label in unique_clusters:\n",
    "            self.visualize_class_interactions_for_cluster(cluster_label, predictor_to_class_dict, class_scaling_factor, edge_scaling_factor, clustering_constant, normalize, color_by_pca1, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer_final = InteractionVisualizerFinal(analysis_obj.get_pca_dataframe())\n",
    "# out_dir=None\n",
    "visualizer_final.visualize_all_clusters(predictor_to_class_dict,\n",
    "                                        class_scaling_factor=.5, \n",
    "                                        edge_scaling_factor=.2, \n",
    "                                        clustering_constant=295, \n",
    "                                        normalize=True, \n",
    "                                        color_by_pca1=True, \n",
    "                                        out_dir=os.path.join(out_dir, 'raw_graphs'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation of the Data-Driven Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the PermutationPValueCalculator from your module\n",
    "from calvin_utils.permutation_analysis_utils.statistical_utils.p_value_statistics import PermutationPValueCalculator\n",
    "\n",
    "class PermutationTest(PermutationPValueCalculator):\n",
    "    \"\"\"\n",
    "    This class is designed to perform a permutation test for all-subsets regression outcomes.\n",
    "    It assesses the stability of the PCA outcomes against permutations of the data.\n",
    "    Inherits functionalities from the PermutationPValueCalculator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_df, observed_outcomes_df):\n",
    "        super().__init__(observed_outcomes_df, []) \n",
    "        self.data_df = data_df\n",
    "        self.observed_outcomes_df = observed_outcomes_df\n",
    "\n",
    "    def loocv_regression(self, permuted_data, formula):\n",
    "        \"\"\"\n",
    "        Performs Leave-One-Out Cross-Validation (LOOCV) regression on the permuted data using the given formula.\n",
    "\n",
    "        Parameters:\n",
    "        - permuted_data: DataFrame with permuted outcomes.\n",
    "        - formula: String specifying the regression formula.\n",
    "\n",
    "        Returns:\n",
    "        - rmse: Root Mean Squared Error from the regression.\n",
    "        - r_value: Pearson correlation coefficient.\n",
    "        - r_squared: R-squared value of the model.\n",
    "        \"\"\"\n",
    "        outcome_var = formula.split(\"~\")[0].strip()\n",
    "        model = smf.ols(formula=formula, data=permuted_data).fit()\n",
    "        \n",
    "        # Get y_actual and y_predicted for RMSE and correlation\n",
    "        y_actual = permuted_data[outcome_var]\n",
    "        y_predicted = model.predict(permuted_data)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "        r_value, _ = pearsonr(y_actual, y_predicted)\n",
    "        r_squared = model.rsquared\n",
    "        aic = model.aic\n",
    "        bic = model.bic\n",
    "        adj_r_squared = model.rsquared_adj\n",
    "        \n",
    "        return 1/rmse, r_value, r_squared, 1/aic, 1/bic, adj_r_squared\n",
    "\n",
    "    def perform_permutation(self):\n",
    "        \"\"\"\n",
    "        Performs a single permutation test by permuting every single column to destroy all structure.\n",
    "\n",
    "        Returns:\n",
    "        - pca_results: List of PCA values for each regression formula.\n",
    "        - regression_metrics: List of regression metrics (rmse, r_value, r_squared) for each regression formula.\n",
    "        \"\"\"\n",
    "        # Permute only outcomes\n",
    "        permuted_data = self.data_df.copy()\n",
    "        permuted_data['outcome'] = np.random.permutation(permuted_data['outcome'])\n",
    "        # Permute every column\n",
    "        # permuted_data = self.data_df.apply(np.random.permutation)\n",
    "        \n",
    "        # Store metrics for all formulas\n",
    "        metrics = []\n",
    "\n",
    "        # Go through each formula in observed outcomes\n",
    "        for _, row in self.observed_outcomes_df.iterrows():\n",
    "            formula = row['formula']\n",
    "            inv_rmse, r_value, r_squared, inv_aic, inv_bic, adj_r_squared = self.loocv_regression(permuted_data, formula)\n",
    "            metrics.append([inv_rmse, r_value, r_squared])\n",
    "\n",
    "        # Standardize using observed metrics mean and standard deviation\n",
    "        observed_metrics = self.observed_outcomes_df[['r_squared', 'rmse', 'correlation']].values\n",
    "\n",
    "        # Standardize using observed metrics mean and standard deviation\n",
    "        scaler = StandardScaler().fit(observed_metrics)  # Fit the scaler to observed metrics\n",
    "        standardized_metrics = scaler.transform(metrics)\n",
    "\n",
    "        # Apply PCA on standardized metrics\n",
    "        pca = PCA(n_components=1)\n",
    "        pca_results = pca.fit_transform(standardized_metrics)\n",
    "        \n",
    "        return pca_results\n",
    "\n",
    "    def run_permutations(self, n_permutations=10):\n",
    "        \"\"\"\n",
    "        Runs the specified number of permutation tests.\n",
    "\n",
    "        Parameters:\n",
    "        - n_permutations: Number of permutations to perform.\n",
    "\n",
    "        Returns:\n",
    "        - results_df: DataFrame containing PCA values for each formula and permutation.\n",
    "        - regression_metrics_list: List of regression metrics (rmse, r_value, r_squared) for each permutation.\n",
    "        \"\"\"\n",
    "        all_pca_results = []\n",
    "\n",
    "        for _ in tqdm(range(n_permutations)):\n",
    "            pca_results = self.perform_permutation()\n",
    "            all_pca_results.append(pca_results)\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        all_pca_results_array = np.array(all_pca_results)\n",
    "        print('Shape ', all_pca_results_array.shape)\n",
    "        # Convert the 2D array into a DataFrame\n",
    "        reshaped_array = all_pca_results_array.squeeze().T\n",
    "        results_df = pd.DataFrame(reshaped_array, columns=[f\"Permutation_{i}\" for i in range(n_permutations)])\n",
    "        results_df['formula'] = self.observed_outcomes_df['formula']\n",
    "        return results_df\n",
    "\n",
    "    def calculate_p_values_for_formulas(self, results_df):\n",
    "        \"\"\"\n",
    "        Calculate the uncorrected and FWER-corrected p-values for each formula.\n",
    "\n",
    "        Parameters:\n",
    "        - results_df: DataFrame containing PCA values for each formula and permutation.\n",
    "\n",
    "        Returns:\n",
    "        - p_values_df: DataFrame containing formulae, uncorrected p-values, and FWER-corrected p-values.\n",
    "        \"\"\"\n",
    "        # For uncorrected p-values\n",
    "        uncorrected_p_values = []\n",
    "        for _, row in self.observed_outcomes_df.iterrows():\n",
    "            observed_value = row['PCA_1']\n",
    "            permuted_values = results_df[results_df['formula'] == row['formula']].drop(columns='formula').values.flatten()\n",
    "            uncorrected_p = (permuted_values > observed_value).mean()\n",
    "            uncorrected_p_values.append(uncorrected_p)\n",
    "\n",
    "        # For FWER-corrected p-values\n",
    "        maxima_array = results_df.drop(columns='formula').max(axis=0).values\n",
    "\n",
    "        # Calculate percentiles for observed PCA values in maxima array\n",
    "        fwer_corrected_p_values = [1 - (np.sum(observed_value > maxima_array) / len(maxima_array)) for observed_value in self.observed_outcomes_df['PCA_1']]\n",
    "\n",
    "        # Construct the p-values DataFrame\n",
    "        p_values_df = pd.DataFrame({\n",
    "            'formula': self.observed_outcomes_df['formula'],\n",
    "            'uncorrected_p_value': uncorrected_p_values,\n",
    "            'fwer_corrected_p_value': fwer_corrected_p_values\n",
    "        })\n",
    "\n",
    "        return p_values_df\n",
    "    \n",
    "    \n",
    "    def bonferroni_adjustment(self, df_series):\n",
    "        \"\"\"\n",
    "        Adjusts the uncorrected p-values using Bonferroni correction.\n",
    "        \n",
    "        Parameters:\n",
    "        - p_values_df: DataFrame containing uncorrected p-values.\n",
    "        \n",
    "        Returns:\n",
    "        - p_values_df: DataFrame with an additional column for Bonferroni-adjusted p-values.\n",
    "        \"\"\"\n",
    "        return np.minimum(df_series * df_series.shape[0], 1)\n",
    "    \n",
    "    def variance_smoothed_fwe(self, results_df):\n",
    "        \"\"\"\n",
    "        Applies variance smoothing to the results DataFrame and calculates the FWER-corrected p-values.\n",
    "\n",
    "        Parameters:\n",
    "        - results_df: DataFrame containing PCA values for each formula and permutation.\n",
    "\n",
    "        Returns:\n",
    "        - results_df: Smoothed DataFrame.\n",
    "        - fwer_p_values: List of FWER-corrected p-values.\n",
    "        \"\"\"\n",
    "        # Step 1: Variance Smoothing\n",
    "        smoothed_results = self.apply_smoothing(results_df.drop(columns='formula'))\n",
    "        results_df[smoothed_results.columns] = smoothed_results\n",
    "\n",
    "        # Step 2: Calculate FWER-corrected p-values\n",
    "        fwer_p_values = self.calculate_fwe(results_df)\n",
    "        \n",
    "        return results_df, fwer_p_values\n",
    "\n",
    "    def apply_smoothing(self, df):\n",
    "        \"\"\"\n",
    "        Applies the desired smoothing technique to a DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame to be smoothed.\n",
    "\n",
    "        Returns:\n",
    "        - df: Smoothed DataFrame.\n",
    "        \"\"\"\n",
    "        # Assuming moving average smoothing for simplicity, with a window of 3\n",
    "        # Replace this with the desired smoothing method.\n",
    "        return df.rolling(window=3).mean()\n",
    "\n",
    "    def calculate_fwe(self, results_df):\n",
    "        \"\"\"\n",
    "        Calculates FWER-corrected p-values using the smoothed results DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - results_df: Smoothed DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        - fwer_corrected_p_values: List of FWER-corrected p-values.\n",
    "        \"\"\"\n",
    "        maxima_array = results_df.drop(columns='formula').max(axis=0).values\n",
    "        fwer_corrected_p_values = [1 - (np.sum(observed_value > maxima_array) / len(maxima_array)) for observed_value in self.observed_outcomes_df['PCA_1']]\n",
    "        return fwer_corrected_p_values\n",
    "\n",
    "\n",
    "    def run(self, n_permutations=10):\n",
    "        \"\"\"\n",
    "        Orchestrates the entire permutation test process.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_permutations: Number of permutations to perform.\n",
    "        \n",
    "        Returns:\n",
    "        - p_values_df: DataFrame containing formulae, uncorrected p-values, Bonferroni-adjusted p-values, and FWER-corrected p-values.\n",
    "        \"\"\"\n",
    "        results_df = self.run_permutations(n_permutations=n_permutations)\n",
    "        p_values_df = self.calculate_p_values_for_formulas(results_df)\n",
    "        p_values_df['bonferonni_adj'] = self.bonferroni_adjustment(p_values_df['uncorrected_p_value'])\n",
    "        _, fwer_p_values = self.variance_smoothed_fwe(results_df)\n",
    "        p_values_df['variance_smoothed_fwe'] = fwer_p_values\n",
    "        return p_values_df, results_df\n",
    "\n",
    "    def save_to_csv(self, p_values_df, out_dir):\n",
    "        \"\"\"\n",
    "        Saves the p_values_df to a specified directory.\n",
    "\n",
    "        Parameters:\n",
    "        - p_values_df: DataFrame containing p-values.\n",
    "        - out_dir: String specifying the directory to save the file.\n",
    "\n",
    "        Returns:\n",
    "        None.\n",
    "        \"\"\"\n",
    "        filename = \"p_values_results.csv\"\n",
    "        full_path = os.path.join(out_dir, filename)\n",
    "        \n",
    "        p_values_df.to_csv(full_path, index=False)\n",
    "        print(f\"Results saved to: {full_path}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# perm_test = PermutationTest(data_df, observed_outcomes_df)\n",
    "# results = perm_test.run_permutations(n_permutations=10)\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_test = PermutationTest(data_df, analysis_obj.get_pca_dataframe())\n",
    "p_values_df, permutation_results = perm_test.run(n_permutations=1000);\n",
    "perm_test.save_to_csv(p_values_df, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Loading In P-Values, Use This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_df = pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/clinical_analyses/ses-01/sub-all/all_data/all_metadata_spreadsheet/ols_linear_regression/p_values_results.csv')\n",
    "p_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Revised Clusters With P-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPerformanceClusterWithOpacity(ModelPerformanceClusterFinal):\n",
    "    \n",
    "    def visualize_3D_scatter(self, opacity_column='correlation', save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize a 3D scatter plot of R-squared vs. Pearson R vs. RMSE, colored by Cluster label and shape by number of predictors.\n",
    "        Opacity of the dots is coded by 1 minus the value in the specified column.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the original visualize_3D_scatter method to set 'num_predictors' and 'symbol' columns\n",
    "        super().visualize_3D_scatter()\n",
    "        \n",
    "        # Ensure the opacity_column exists in the dataframe\n",
    "        if opacity_column not in self.data.columns:\n",
    "            raise ValueError(f\"The specified opacity_column '{opacity_column}' does not exist in the dataframe.\")\n",
    "        \n",
    "        # Compute opacity values\n",
    "        self.data['custom_opacity'] = 1 - self.data[opacity_column]\n",
    "        \n",
    "        fig = px.scatter_3d(self.data, x='r_squared', y='correlation', z='rmse',\n",
    "                    color='Cluster_Label', hover_name='formula', opacity=0.8,\n",
    "                    hover_data=['adj_r_squared', 'bic', 'aic'], symbol='symbol',\n",
    "                    labels={'adj_r_squared': 'Adj. R-Squared', 'r_squared': 'R-Squared', 'correlation': 'LOOCV Pearson R', 'rmse': 'LOOCV RMSE'},\n",
    "                    color_continuous_scale=px.colors.sequential.Viridis)\n",
    "        \n",
    "        fig.update_layout(title=\"3D Scatter Plot of R-Squared, Pearson R, and RMSE with Custom Opacity\", \n",
    "                        autosize=True, width=1200, height=800)\n",
    "        # fig.show()\n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            save_path = os.path.join(save_path, '3d_scatterplot.html')\n",
    "            fig.write_html(save_path)\n",
    "        else:\n",
    "            fig.show()\n",
    "\n",
    "    def plot_3D_with_custom_opacities(self, x_column, y_column, z_column, opacity_column):\n",
    "        \"\"\"\n",
    "        Plot a 3D scatter plot using given columns for X, Y, and Z axes with custom opacities.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure the columns exist in the dataframe\n",
    "        for col in [x_column, y_column, z_column, opacity_column]:\n",
    "            if col not in self.data.columns:\n",
    "                raise ValueError(f\"The specified column '{col}' does not exist in the dataframe.\")\n",
    "        \n",
    "        # Extract data\n",
    "        X = self.data[x_column].values\n",
    "        Y = self.data[y_column].values\n",
    "        Z = self.data[z_column].values\n",
    "        \n",
    "        # Compute custom opacity\n",
    "        custom_opacity = 1 - self.data[opacity_column].values\n",
    "        max_opacity = np.max(custom_opacity)\n",
    "        custom_opacity = custom_opacity / max_opacity\n",
    "        \n",
    "        # 3D scatter plot\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        # Plot each point with its respective opacity\n",
    "        for x, y, z, opacity in zip(X, Y, Z, custom_opacity):\n",
    "            ax.scatter(x, y, z, color='b', alpha=opacity)\n",
    "\n",
    "        ax.set_xlabel(x_column)\n",
    "        ax.set_ylabel(y_column)\n",
    "        ax.set_zlabel(z_column)\n",
    "        ax.set_title(f\"3D Scatter plot of {x_column}, {y_column}, and {z_column} with varying opacity\")\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df_to_merge_1, df_to_merge_2):\n",
    "    results_df = df_to_merge_1.merge(df_to_merge_2, on='formula', how='left')\n",
    "    results_df.fillna(1, inplace=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = preprocess_inputs(pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/clinical_analyses/ses-01/sub-all/all_data/all_metadata_spreadsheet/ols_linear_regression/p_values_results.csv'), \n",
    "                               pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/clinical_analyses/ses-01/sub-all/all_data/all_metadata_spreadsheet/ols_linear_regression/raw_data/cluster_df_pc1.csv'))\n",
    "# results_df.to_csv(os.path.join(out_dir, 'loocv_results_and_p_vals.csv'))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a P-Value To Threshold By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "p_value_method = 'bonferonni_adj'\n",
    "\n",
    "RMSE_threshold  = 1.2\n",
    "R_threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the new class with the merged dataframe\n",
    "cluster_instance = ModelPerformanceClusterWithOpacity(results_df[results_df[p_value_method]<0.05], rmse_threshold=RMSE_threshold, pearsonr_threshold=R_threshold, normative_thresholding=True)\n",
    "cluster_instance.preprocess_data()\n",
    "cluster_instance.compute_r_squared(data_df)\n",
    "# cluster_instance.apply_agglomerative_clustering(n_clusters=num_clusters)\n",
    "cluster_instance.visualize_3D_scatter(save_path=os.path.join(out_dir, f'p_value_{p_value_method}_adjusted'))\n",
    "\n",
    "# cluster_instance.plot_3D_with_custom_opacities('r_squared', 'correlation', 'rmse', p_value_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Information Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Information Categories First**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = preprocess_inputs(cluster_instance.get_essential_data(), results_df[['PCA_1', 'formula']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_class_dict(extract_predictors(graph_data), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_predictor_to_class_dict = {\n",
    "    # Demographic\n",
    "    'Age': 'Demographic',\n",
    "    'Baseline': 'Demographic',\n",
    "    \n",
    "    # Network Stim\n",
    "    'Subiculum_Connectivity': 'Subiculum Stim',\n",
    "    'Visual_Connectivity': 'Network Stim',\n",
    "    'Dorsal_Attention_Connectivity': 'Network Stim',\n",
    "    'Ventral_Attention_Connectivity': 'Network Stim',\n",
    "    'Limbic_Connectivity': 'Network Stim',\n",
    "    'Frontoparietal_Connectivity': 'Network Stim',\n",
    "    'Default_Connectivity': 'Network Stim',\n",
    "    'Somatomotor_Connectivity': 'Network Stim',\n",
    "    \n",
    "    # Lobe Atrophy - Note: You haven't specified which ones are \"Lobe Atrophy\" in the provided dict, so I'm not categorizing any predictors under this for now.\n",
    "    \n",
    "    # Network Atrophy\n",
    "    'Cerebellar_Atrophy': 'Lobe Atrophy',\n",
    "    'Frontal_Atrophy': 'Lobe Atrophy',\n",
    "    'Dorsal_Attention_Atrophy': 'Network Atrophy',\n",
    "    'Visual_Atrophy': 'Network Atrophy',\n",
    "    'Frontoparietal_Atrophy': 'Network Atrophy',\n",
    "    'Abs_Stim_Composite_Atophy_SpCorrel': 'Network Stim-Atrophy Match',\n",
    "    'Parietal_Atrophy': 'Lobe Atrophy',\n",
    "    'Ventral_Attention_Atrophy': 'Network Atrophy',\n",
    "    'Limbic_Atrophy': 'Network Atrophy',\n",
    "    'Occipital_Atrophy': 'Lobe Atrophy',\n",
    "    'Default_Atrophy': 'Network Atrophy',\n",
    "    'Somatomotor_Atrophy': 'Network Atrophy',\n",
    "    'Raw_Stim_Composite_Atrophy_SpCorrel': 'Network Stim-Atrophy Match',\n",
    "    'Subiculum_Atrophy': 'Subiculum Atrophy',\n",
    "    'Temporal_Atrophy': 'Lobe Atrophy',\n",
    "    'Total_Atrophy_Voxels': 'Total Atrophy',\n",
    "    \n",
    "    # Unclassified (will need your inputs on these):\n",
    "    # None for now as all items from the provided dict have been classified.\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer_final = InteractionVisualizerFinal(graph_data)\n",
    "# out_dir=None\n",
    "visualizer_final.visualize_all_clusters(significant_predictor_to_class_dict,\n",
    "                                        class_scaling_factor=.95,\n",
    "                                        edge_scaling_factor=.2,\n",
    "                                        clustering_constant=100,\n",
    "                                        normalize=False,\n",
    "                                        color_by_pca1=True,\n",
    "                                        out_dir=os.path.join(out_dir, f'p_value_{p_value_method}_adjusted/class_graphs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Specific Information Within Each Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_class_dict(extract_predictors(graph_data), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_predictor_dict = {\n",
    "\"Age\": \"Age\",\n",
    "\"Cerebellar_Atrophy\": \"Cerebellar_Atrophy\",\n",
    "\"Dorsal_Attention_Atrophy\": \"Dorsal_Attention_Atrophy\",\n",
    "\"Subiculum_Connectivity\": \"Subiculum_Connectivity\",\n",
    "\"Visual_Atrophy\": \"Visual_Atrophy\",\n",
    "\"Frontoparietal_Atrophy\": \"Frontoparietal_Atrophy\",\n",
    "\"Abs_Stim_Composite_Atophy_SpCorrel\": \"Abs_Stim_Composite_Atophy_SpCorrel\",\n",
    "\"Parietal_Atrophy\": \"Parietal_Atrophy\",\n",
    "\"Ventral_Attention_Connectivity\": \"Ventral_Attention_Connectivity\",\n",
    "\"Baseline\": \"Baseline\",\n",
    "\"Ventral_Attention_Atrophy\": \"Ventral_Attention_Atrophy\",\n",
    "\"Limbic_Atrophy\": \"Limbic_Atrophy\",\n",
    "\"Occipital_Atrophy\": \"Occipital_Atrophy\",\n",
    "\"Visual_Connectivity\": \"Visual_Connectivity\",\n",
    "\"Default_Connectivity\": \"Default_Connectivity\",\n",
    "\"Limbic_Connectivity\": \"Limbic_Connectivity\",\n",
    "\"Default_Atrophy\": \"Default_Atrophy\",\n",
    "\"Somatomotor_Atrophy\": \"Somatomotor_Atrophy\",\n",
    "\"Dorsal_Attention_Connectivity\": \"Dorsal_Attention_Connectivity\",\n",
    "\"Frontal_Atrophy\": \"Frontal_Atrophy\",\n",
    "\"Frontoparietal_Connectivity\": \"Frontoparietal_Connectivity\",\n",
    "\"Somatomotor_Connectivity\": \"Somatomotor_Connectivity\",\n",
    "\"Raw_Stim_Composite_Atrophy_SpCorrel\": \"Raw_Stim_Composite_Atrophy_SpCorrel\",\n",
    "\"Subiculum_Atrophy\": \"Subiculum_Atrophy\",\n",
    "\"Temporal_Atrophy\": \"Temporal_Atrophy\",\n",
    "\"Total_Atrophy_Voxels\": \"Total_Atrophy_Voxels\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer_final = InteractionVisualizerFinal(graph_data)\n",
    "# out_dir=None\n",
    "visualizer_final.visualize_all_clusters(predictor_predictor_dict, \n",
    "                                        class_scaling_factor=.5, \n",
    "                                        edge_scaling_factor=.2, \n",
    "                                        clustering_constant=100, \n",
    "                                        normalize=False, \n",
    "                                        color_by_pca1=True, \n",
    "                                        out_dir=os.path.join(out_dir, f'p_value_{p_value_method}_adjusted/predictor_graphs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
